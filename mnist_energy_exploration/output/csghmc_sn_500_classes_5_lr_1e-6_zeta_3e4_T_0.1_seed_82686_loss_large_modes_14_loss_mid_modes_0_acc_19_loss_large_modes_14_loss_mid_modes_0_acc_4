Namespace(N=50000, T=0.1, batch=5000, c='csghmc', classes=5, div=10, filters=16, gpu=0, hidden=10, ifprint=1.0, ifsave=1.0, lr=1e-06, part=1000000, seed=82686, sn=500, stepsize=0.01, warm=0.5, wdecay=25, zeta=30000.0)
adjust the learning rate 2.000e-06 weight decay 1.200e+01
(16, 1, 5, 5)
(16,)
(32, 16, 5, 5)
(32,)
(10, 1568)
(10,)
(5, 10)
(5,)
Current Theta
tensor([1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06, 1.0000e-06,
        1.0000e-06], device='cuda:0')
Epoch 0 Iter 0 subLoss 48568.5 multi 1.00 import weight 1.00
Epoch 0 Iter 1 subLoss 47934.2 multi 1.00 import weight 1.00
Epoch 0 Iter 2 subLoss 47404.8 multi 1.00 import weight 1.00
Epoch 0 Iter 3 subLoss 46712.9 multi 1.00 import weight 1.00
Epoch 0 Iter 4 subLoss 46307.9 multi 1.00 import weight 1.00
Epoch 0 Iter 5 subLoss 45754.6 multi 1.00 import weight 1.00
Epoch 0 Iter 6 subLoss 45315.7 multi 1.00 import weight 1.00
Epoch 0 Iter 7 subLoss 44435.7 multi 1.00 import weight 1.00
Epoch 0 Iter 8 subLoss 44021.3 multi 1.00 import weight 1.00
Epoch 0 Iter 9 subLoss 43632.5 multi 1.00 import weight 1.00
Epoch 0 Iter 10 subLoss 43854.8 multi 1.00 import weight 1.00
Epoch 0 Iter 11 subLoss 45436.1 multi 1.00 import weight 1.00
Epoch 0 Acc: 52.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 4543 train Loss: 46110.4 test Loss: 7246.4
Epoch 1 Iter 0 subLoss 45925.7 multi 1.00 import weight 1.00
Epoch 1 Iter 1 subLoss 42614.1 multi 1.00 import weight 1.00
Epoch 1 Iter 2 subLoss 41981.9 multi 1.00 import weight 1.00
Epoch 1 Iter 3 subLoss 40972.9 multi 1.00 import weight 1.00
Epoch 1 Iter 4 subLoss 40984.1 multi -1.99 import weight 1.00
Epoch 1 Iter 5 subLoss 62139.0 multi 1.00 import weight 1.00
Epoch 1 Iter 6 subLoss 45690.4 multi 1.00 import weight 1.00
Epoch 1 Iter 7 subLoss 44045.1 multi 1.00 import weight 1.00
Epoch 1 Iter 8 subLoss 42913.8 multi 1.00 import weight 1.00
Epoch 1 Iter 9 subLoss 42364.0 multi 1.00 import weight 1.00
Epoch 1 Iter 10 subLoss 41479.8 multi 1.00 import weight 1.00
Epoch 1 Iter 11 subLoss 40616.7 multi 1.00 import weight 1.00
Epoch 1 Acc: 58.79 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 4061 train Loss: 40865.6 test Loss: 6192.1
Epoch 2 Iter 0 subLoss 39973.4 multi 1.00 import weight 1.00
Epoch 2 Iter 1 subLoss 39679.4 multi 1.00 import weight 1.00
Epoch 2 Iter 2 subLoss 40441.2 multi 1.00 import weight 1.00
Epoch 2 Iter 3 subLoss 45589.9 multi 1.00 import weight 1.00
Epoch 2 Iter 4 subLoss 42294.1 multi 1.00 import weight 1.00
Epoch 2 Iter 5 subLoss 38023.0 multi 1.00 import weight 1.00
Epoch 2 Iter 6 subLoss 36852.1 multi 1.00 import weight 1.00
Epoch 2 Iter 7 subLoss 36017.7 multi 1.00 import weight 1.00
Epoch 2 Iter 8 subLoss 35281.2 multi 1.00 import weight 1.00
Epoch 2 Iter 9 subLoss 35093.3 multi 1.00 import weight 1.00
Epoch 2 Iter 10 subLoss 36289.9 multi 1.00 import weight 1.00
Epoch 2 Iter 11 subLoss 43816.8 multi 1.00 import weight 1.00
Epoch 2 Acc: 51.41 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 4381 train Loss: 43328.4 test Loss: 6228.4
Epoch 3 Iter 0 subLoss 42227.4 multi 1.00 import weight 1.00
Epoch 3 Iter 1 subLoss 37369.9 multi 1.00 import weight 1.00
Epoch 3 Iter 2 subLoss 35304.3 multi 1.00 import weight 1.00
Epoch 3 Iter 3 subLoss 33751.7 multi 1.00 import weight 1.00
Epoch 3 Iter 4 subLoss 33274.8 multi 1.00 import weight 1.00
Epoch 3 Iter 5 subLoss 34551.0 multi 1.00 import weight 1.00
Epoch 3 Iter 6 subLoss 38785.5 multi 1.00 import weight 1.00
Epoch 3 Iter 7 subLoss 43516.3 multi 1.00 import weight 1.00
Epoch 3 Iter 8 subLoss 37152.6 multi 1.00 import weight 1.00
Epoch 3 Iter 9 subLoss 30432.6 multi 1.00 import weight 1.00
Epoch 3 Iter 10 subLoss 30248.0 multi 1.00 import weight 1.00
Epoch 3 Iter 11 subLoss 30147.6 multi 1.00 import weight 1.00
Epoch 3 Acc: 74.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 3014 train Loss: 31055.6 test Loss: 3530.0
Epoch 4 Iter 0 subLoss 29875.6 multi 1.00 import weight 1.00
Epoch 4 Iter 1 subLoss 31564.2 multi 1.00 import weight 1.00
Epoch 4 Iter 2 subLoss 35456.2 multi 1.00 import weight 1.00
Epoch 4 Iter 3 subLoss 31578.8 multi -1.99 import weight 1.00
Epoch 4 Iter 4 subLoss 68191.8 multi 1.00 import weight 1.00
Epoch 4 Iter 5 subLoss 43762.1 multi 1.00 import weight 1.00
Epoch 4 Iter 6 subLoss 36658.4 multi 1.00 import weight 1.00
Epoch 4 Iter 7 subLoss 32879.8 multi 1.00 import weight 1.00
Epoch 4 Iter 8 subLoss 31048.2 multi 1.00 import weight 1.00
Epoch 4 Iter 9 subLoss 29761.7 multi 1.00 import weight 1.00
Epoch 4 Iter 10 subLoss 28873.1 multi 1.00 import weight 1.00
Epoch 4 Iter 11 subLoss 27976.0 multi 1.00 import weight 1.00
Epoch 4 Acc: 86.42 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 2797 train Loss: 27941.8 test Loss: 3084.8
Epoch 5 Iter 0 subLoss 27732.0 multi 1.00 import weight 1.00
Epoch 5 Iter 1 subLoss 27315.7 multi 1.00 import weight 1.00
Epoch 5 Iter 2 subLoss 26552.0 multi 1.00 import weight 1.00
Epoch 5 Iter 3 subLoss 26782.2 multi 1.00 import weight 1.00
Epoch 5 Iter 4 subLoss 29073.1 multi 1.00 import weight 1.00
Epoch 5 Iter 5 subLoss 31307.5 multi 1.00 import weight 1.00
Epoch 5 Iter 6 subLoss 37636.6 multi 1.00 import weight 1.00
Epoch 5 Iter 7 subLoss 31126.7 multi 1.00 import weight 1.00
Epoch 5 Iter 8 subLoss 27206.7 multi 1.00 import weight 1.00
Epoch 5 Iter 9 subLoss 24905.1 multi 1.00 import weight 1.00
Epoch 5 Iter 10 subLoss 24225.9 multi 1.00 import weight 1.00
Epoch 5 Iter 11 subLoss 24484.9 multi 1.00 import weight 1.00
Epoch 5 Acc: 75.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 2448 train Loss: 26249.1 test Loss: 2987.2
Epoch 6 Iter 0 subLoss 25286.4 multi 1.00 import weight 1.00
Epoch 6 Iter 1 subLoss 29269.6 multi 1.00 import weight 1.00
Epoch 6 Iter 2 subLoss 28169.6 multi 1.00 import weight 1.00
Epoch 6 Iter 3 subLoss 24126.8 multi 1.00 import weight 1.00
Epoch 6 Iter 4 subLoss 23147.7 multi 1.00 import weight 1.00
Epoch 6 Iter 5 subLoss 21937.5 multi 1.00 import weight 1.00
Epoch 6 Iter 6 subLoss 21776.6 multi 1.00 import weight 1.00
Epoch 6 Iter 7 subLoss 20837.5 multi 1.00 import weight 1.00
Epoch 6 Iter 8 subLoss 21361.7 multi 1.00 import weight 1.00
Epoch 6 Iter 9 subLoss 23222.6 multi 1.00 import weight 1.00
Epoch 6 Iter 10 subLoss 23599.8 multi 1.00 import weight 1.00
Epoch 6 Iter 11 subLoss 27760.1 multi 1.00 import weight 1.00
Epoch 6 Acc: 84.65 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 2776 train Loss: 21844.1 test Loss: 2442.4
Epoch 7 Iter 0 subLoss 20929.5 multi 1.00 import weight 1.00
Epoch 7 Iter 1 subLoss 20182.8 multi 1.00 import weight 1.00
Epoch 7 Iter 2 subLoss 20251.0 multi 1.00 import weight 1.00
Epoch 7 Iter 3 subLoss 19411.5 multi 1.00 import weight 1.00
Epoch 7 Iter 4 subLoss 20161.2 multi 1.00 import weight 1.00
Epoch 7 Iter 5 subLoss 21847.2 multi 1.00 import weight 1.00
Epoch 7 Iter 6 subLoss 29213.4 multi 1.00 import weight 1.00
Epoch 7 Iter 7 subLoss 21124.1 multi 1.00 import weight 1.00
Epoch 7 Iter 8 subLoss 24095.2 multi 1.00 import weight 1.00
Epoch 7 Iter 9 subLoss 20800.9 multi 1.00 import weight 1.00
Epoch 7 Iter 10 subLoss 24457.3 multi 1.00 import weight 1.00
Epoch 7 Iter 11 subLoss 21753.5 multi 1.00 import weight 1.00
Epoch 7 Acc: 76.44 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 2175 train Loss: 24791.5 test Loss: 3025.4
Epoch 8 Iter 0 subLoss 24340.7 multi 1.00 import weight 1.00
Epoch 8 Iter 1 subLoss 19021.8 multi 1.00 import weight 1.00
Epoch 8 Iter 2 subLoss 19879.7 multi 1.00 import weight 1.00
Epoch 8 Iter 3 subLoss 18253.5 multi 1.00 import weight 1.00
Epoch 8 Iter 4 subLoss 19339.9 multi 1.00 import weight 1.00
Epoch 8 Iter 5 subLoss 18645.7 multi 1.00 import weight 1.00
Epoch 8 Iter 6 subLoss 19862.4 multi 1.00 import weight 1.00
Epoch 8 Iter 7 subLoss 17438.1 multi 1.00 import weight 1.00
Epoch 8 Iter 8 subLoss 20220.3 multi 1.00 import weight 1.00
Epoch 8 Iter 9 subLoss 18023.6 multi 1.00 import weight 1.00
Epoch 8 Iter 10 subLoss 18631.3 multi 1.00 import weight 1.00
Epoch 8 Iter 11 subLoss 15340.4 multi 1.00 import weight 1.00
Epoch 8 Acc: 90.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 1534 train Loss: 16309.2 test Loss: 1656.1
Epoch 9 Iter 0 subLoss 16460.1 multi 1.00 import weight 1.00
Epoch 9 Iter 1 subLoss 15867.8 multi 1.00 import weight 1.00
Epoch 9 Iter 2 subLoss 17490.5 multi 1.00 import weight 1.00
Epoch 9 Iter 3 subLoss 16953.7 multi 1.00 import weight 1.00
Epoch 9 Iter 4 subLoss 21269.5 multi 1.00 import weight 1.00
Epoch 9 Iter 5 subLoss 21452.8 multi 1.00 import weight 1.00
Epoch 9 Iter 6 subLoss 32929.4 multi 1.00 import weight 1.00
Epoch 9 Iter 7 subLoss 23346.7 multi 1.00 import weight 1.00
Epoch 9 Iter 8 subLoss 16589.9 multi 1.00 import weight 1.00
Epoch 9 Iter 9 subLoss 14727.1 multi 1.00 import weight 1.00
Epoch 9 Iter 10 subLoss 14050.8 multi 1.00 import weight 1.00
Epoch 9 Iter 11 subLoss 13992.7 multi 1.00 import weight 1.00
Epoch 9 Acc: 91.91 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 1399 train Loss: 14956.3 test Loss: 1497.3
Epoch 10 Iter 0 subLoss 14140.3 multi 1.00 import weight 1.00
Epoch 10 Iter 1 subLoss 14969.8 multi 1.00 import weight 1.00
Epoch 10 Iter 2 subLoss 18151.8 multi 1.00 import weight 1.00
Epoch 10 Iter 3 subLoss 21011.2 multi 1.00 import weight 1.00
Epoch 10 Iter 4 subLoss 27399.9 multi 1.00 import weight 1.00
Epoch 10 Iter 5 subLoss 15946.2 multi 1.00 import weight 1.00
Epoch 10 Iter 6 subLoss 15888.6 multi 1.00 import weight 1.00
Epoch 10 Iter 7 subLoss 13696.8 multi 1.00 import weight 1.00
Epoch 10 Iter 8 subLoss 13323.7 multi 1.00 import weight 1.00
Epoch 10 Iter 9 subLoss 14189.6 multi 1.00 import weight 1.00
Epoch 10 Iter 10 subLoss 14656.4 multi 1.00 import weight 1.00
Epoch 10 Iter 11 subLoss 13651.8 multi 1.00 import weight 1.00
Epoch 10 Acc: 84.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 1365 train Loss: 16092.4 test Loss: 1922.6
Epoch 11 Iter 0 subLoss 15563.7 multi 1.00 import weight 1.00
Epoch 11 Iter 1 subLoss 16821.4 multi 1.00 import weight 1.00
Epoch 11 Iter 2 subLoss 16341.1 multi 1.00 import weight 1.00
Epoch 11 Iter 3 subLoss 17041.8 multi 1.00 import weight 1.00
Epoch 11 Iter 4 subLoss 13495.9 multi 1.00 import weight 1.00
Epoch 11 Iter 5 subLoss 13671.8 multi 1.00 import weight 1.00
Epoch 11 Iter 6 subLoss 12502.9 multi 1.00 import weight 1.00
Epoch 11 Iter 7 subLoss 12769.0 multi 1.00 import weight 1.00
Epoch 11 Iter 8 subLoss 12696.9 multi 1.00 import weight 1.00
Epoch 11 Iter 9 subLoss 14858.8 multi 1.00 import weight 1.00
Epoch 11 Iter 10 subLoss 12368.4 multi 1.00 import weight 1.00
Epoch 11 Iter 11 subLoss 15006.3 multi 1.00 import weight 1.00
Epoch 11 Acc: 91.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 1500 train Loss: 15850.1 test Loss: 1360.0
Epoch 12 Iter 0 subLoss 15593.7 multi 1.00 import weight 1.00
Epoch 12 Iter 1 subLoss 23906.2 multi 1.00 import weight 1.00
Epoch 12 Iter 2 subLoss 23178.7 multi 1.00 import weight 1.00
Epoch 12 Iter 3 subLoss 32859.0 multi 1.00 import weight 1.00
Epoch 12 Iter 4 subLoss 16509.2 multi 1.00 import weight 1.00
Epoch 12 Iter 5 subLoss 11868.8 multi 1.00 import weight 1.00
Epoch 12 Iter 6 subLoss 11348.2 multi 1.00 import weight 1.00
Epoch 12 Iter 7 subLoss 10401.3 multi 1.00 import weight 1.00
Epoch 12 Iter 8 subLoss 10407.6 multi 3.99 import weight 1.00
Epoch 12 Iter 9 subLoss 16275.6 multi 1.00 import weight 0.00
Epoch 12 Iter 10 subLoss 26710.0 multi 1.00 import weight 0.00
Epoch 12 Iter 11 subLoss 18307.8 multi 1.00 import weight 0.00
Epoch 12 Acc: 94.42 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1830 train Loss: 11769.2 test Loss: 1115.1
Epoch 13 Iter 0 subLoss 12045.9 multi 1.00 import weight 0.00
Epoch 13 Iter 1 subLoss 11179.3 multi 1.00 import weight 0.00
Epoch 13 Iter 2 subLoss 9907.3 multi 1.00 import weight 0.00
Epoch 13 Iter 3 subLoss 10297.6 multi 1.00 import weight 0.00
Epoch 13 Iter 4 subLoss 9284.4 multi 1.00 import weight 0.00
Epoch 13 Iter 5 subLoss 9020.1 multi 1.00 import weight 0.00
Epoch 13 Iter 6 subLoss 10947.6 multi 1.00 import weight 0.00
Epoch 13 Iter 7 subLoss 11049.4 multi 1.00 import weight 0.00
Epoch 13 Iter 8 subLoss 11488.7 multi 1.00 import weight 0.00
Epoch 13 Iter 9 subLoss 14333.0 multi 1.00 import weight 0.00
Epoch 13 Iter 10 subLoss 13197.7 multi 1.00 import weight 0.00
Epoch 13 Iter 11 subLoss 13799.5 multi 1.00 import weight 0.00
Epoch 13 Acc: 90.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1379 train Loss: 11077.8 test Loss: 1324.5
Epoch 14 Iter 0 subLoss 11054.7 multi -1.99 import weight 0.00
Epoch 14 Iter 1 subLoss 61437.2 multi 1.00 import weight 0.00
Epoch 14 Iter 2 subLoss 55368.2 multi 1.00 import weight 0.00
Epoch 14 Iter 3 subLoss 43715.0 multi 1.00 import weight 0.00
Epoch 14 Iter 4 subLoss 29897.8 multi 1.00 import weight 0.00
Epoch 14 Iter 5 subLoss 24833.8 multi 1.00 import weight 0.00
Epoch 14 Iter 6 subLoss 20628.7 multi 1.00 import weight 0.00
Epoch 14 Iter 7 subLoss 17988.0 multi 1.00 import weight 0.00
Epoch 14 Iter 8 subLoss 15073.8 multi 1.00 import weight 0.00
Epoch 14 Iter 9 subLoss 12987.8 multi 1.00 import weight 0.00
Epoch 14 Iter 10 subLoss 11850.2 multi 1.00 import weight 0.00
Epoch 14 Iter 11 subLoss 11090.0 multi 1.00 import weight 0.00
Epoch 14 Acc: 94.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1109 train Loss: 10893.6 test Loss: 1037.0
Epoch 15 Iter 0 subLoss 10956.6 multi -1.99 import weight 0.00
Epoch 15 Iter 1 subLoss 11191.9 multi 1.00 import weight 0.00
Epoch 15 Iter 2 subLoss 11116.2 multi 1.00 import weight 0.00
Epoch 15 Iter 3 subLoss 11016.1 multi 1.00 import weight 0.00
Epoch 15 Iter 4 subLoss 9917.0 multi -1.99 import weight 0.00
Epoch 15 Iter 5 subLoss 13491.2 multi 3.99 import weight 1.00
Epoch 15 Iter 6 subLoss 46493.2 multi 1.00 import weight 0.00
Epoch 15 Iter 7 subLoss 31531.3 multi 1.00 import weight 0.00
Epoch 15 Iter 8 subLoss 22861.3 multi 1.00 import weight 0.00
Epoch 15 Iter 9 subLoss 18387.3 multi 1.00 import weight 0.00
Epoch 15 Iter 10 subLoss 14521.3 multi 1.00 import weight 0.00
Epoch 15 Iter 11 subLoss 11768.8 multi 1.00 import weight 0.00
Epoch 15 Acc: 93.81 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1176 train Loss: 11203.3 test Loss: 1174.4
Epoch 16 Iter 0 subLoss 11200.9 multi -1.99 import weight 0.00
Epoch 16 Iter 1 subLoss 12864.1 multi 1.00 import weight 0.00
Epoch 16 Iter 2 subLoss 11541.8 multi 1.00 import weight 0.00
Epoch 16 Iter 3 subLoss 10564.2 multi 1.00 import weight 0.00
Epoch 16 Iter 4 subLoss 9482.0 multi 1.00 import weight 0.00
Epoch 16 Iter 5 subLoss 9634.6 multi 1.00 import weight 0.00
Epoch 16 Iter 6 subLoss 9098.7 multi 1.00 import weight 0.00
Epoch 16 Iter 7 subLoss 9055.7 multi 1.00 import weight 0.00
Epoch 16 Iter 8 subLoss 8190.1 multi 1.00 import weight 0.00
Epoch 16 Iter 9 subLoss 8538.5 multi 1.00 import weight 0.00
Epoch 16 Iter 10 subLoss 8248.3 multi 1.00 import weight 0.00
Epoch 16 Iter 11 subLoss 8115.1 multi 1.00 import weight 0.00
Epoch 16 Acc: 95.43 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 811 train Loss: 8214.4 test Loss: 732.2
Epoch 17 Iter 0 subLoss 7709.1 multi 1.00 import weight 0.00
Epoch 17 Iter 1 subLoss 8193.8 multi 3.99 import weight 1.00
Epoch 17 Iter 2 subLoss 7833.1 multi 1.00 import weight 0.00
Epoch 17 Iter 3 subLoss 6982.7 multi 1.00 import weight 0.00
Epoch 17 Iter 4 subLoss 7691.3 multi 1.00 import weight 0.00
Epoch 17 Iter 5 subLoss 7595.3 multi 1.00 import weight 0.00
Epoch 17 Iter 6 subLoss 9139.6 multi 1.00 import weight 0.00
Epoch 17 Iter 7 subLoss 11093.3 multi 3.99 import weight 1.00
Epoch 17 Iter 8 subLoss 87077.9 multi 1.00 import weight 0.00
Epoch 17 Iter 9 subLoss 51085.0 multi 1.00 import weight 0.00
Epoch 17 Iter 10 subLoss 50575.8 multi 1.00 import weight 0.00
Epoch 17 Iter 11 subLoss 35120.4 multi 1.00 import weight 0.00
Epoch 17 Acc: 81.40 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3512 train Loss: 26677.2 test Loss: 3778.8
Epoch 18 Iter 0 subLoss 25637.8 multi 1.00 import weight 0.00
Epoch 18 Iter 1 subLoss 19635.0 multi 1.00 import weight 0.00
Epoch 18 Iter 2 subLoss 14953.8 multi 1.00 import weight 0.00
Epoch 18 Iter 3 subLoss 12208.9 multi 1.00 import weight 0.00
Epoch 18 Iter 4 subLoss 11198.2 multi 3.99 import weight 1.00
Epoch 18 Iter 5 subLoss 9568.9 multi 1.00 import weight 0.00
Epoch 18 Iter 6 subLoss 9148.8 multi -1.99 import weight 0.00
Epoch 18 Iter 7 subLoss 10239.7 multi 1.00 import weight 0.00
Epoch 18 Iter 8 subLoss 8991.7 multi 1.00 import weight 0.00
Epoch 18 Iter 9 subLoss 8924.1 multi 1.00 import weight 0.00
Epoch 18 Iter 10 subLoss 8847.9 multi 1.00 import weight 0.00
Epoch 18 Iter 11 subLoss 8065.7 multi 1.00 import weight 0.00
Epoch 18 Acc: 96.44 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 806 train Loss: 8391.7 test Loss: 708.6
Epoch 19 Iter 0 subLoss 7912.4 multi 1.00 import weight 0.00
Epoch 19 Iter 1 subLoss 8338.8 multi 1.00 import weight 0.00
Epoch 19 Iter 2 subLoss 7879.1 multi 1.00 import weight 0.00
Epoch 19 Iter 3 subLoss 7288.7 multi 1.00 import weight 0.00
Epoch 19 Iter 4 subLoss 7434.8 multi 1.00 import weight 0.00
Epoch 19 Iter 5 subLoss 7106.6 multi 1.00 import weight 0.00
Epoch 19 Iter 6 subLoss 7850.7 multi 1.00 import weight 0.00
Epoch 19 Iter 7 subLoss 7172.0 multi 1.00 import weight 0.00
Epoch 19 Iter 8 subLoss 7354.4 multi 1.00 import weight 0.00
Epoch 19 Iter 9 subLoss 6779.1 multi 1.00 import weight 0.00
Epoch 19 Iter 10 subLoss 6931.1 multi 1.00 import weight 0.00
Epoch 19 Iter 11 subLoss 6826.7 multi 1.00 import weight 0.00
Epoch 19 Acc: 96.89 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 682 train Loss: 7061.9 test Loss: 569.4
Epoch 20 Iter 0 subLoss 7021.8 multi 1.00 import weight 0.00
Epoch 20 Iter 1 subLoss 6912.1 multi 1.00 import weight 0.00
Epoch 20 Iter 2 subLoss 6883.3 multi 1.00 import weight 0.00
Epoch 20 Iter 3 subLoss 6692.7 multi 1.00 import weight 0.00
Epoch 20 Iter 4 subLoss 7062.6 multi 1.00 import weight 0.00
Epoch 20 Iter 5 subLoss 7099.3 multi 1.00 import weight 0.00
Epoch 20 Iter 6 subLoss 6846.5 multi 1.00 import weight 0.00
Epoch 20 Iter 7 subLoss 6074.1 multi 1.00 import weight 0.00
Epoch 20 Iter 8 subLoss 6373.2 multi 1.00 import weight 0.00
Epoch 20 Iter 9 subLoss 5947.1 multi 1.00 import weight 0.00
Epoch 20 Iter 10 subLoss 6595.2 multi 1.00 import weight 0.00
Epoch 20 Iter 11 subLoss 6671.4 multi 1.00 import weight 0.00
Epoch 20 Acc: 96.01 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 667 train Loss: 6708.7 test Loss: 608.4
Epoch 21 Iter 0 subLoss 6252.7 multi 1.00 import weight 0.00
Epoch 21 Iter 1 subLoss 6350.9 multi 1.00 import weight 0.00
Epoch 21 Iter 2 subLoss 7301.5 multi 1.00 import weight 0.00
Epoch 21 Iter 3 subLoss 7431.9 multi 3.99 import weight 1.00
Epoch 21 Iter 4 subLoss 66182.2 multi 1.00 import weight 0.00
Epoch 21 Iter 5 subLoss 92621.7 multi 1.00 import weight 0.00
Epoch 21 Iter 6 subLoss 52252.9 multi 1.00 import weight 0.00
Epoch 21 Iter 7 subLoss 30421.0 multi 1.00 import weight 0.00
Epoch 21 Iter 8 subLoss 27448.1 multi 1.00 import weight 0.00
Epoch 21 Iter 9 subLoss 24030.8 multi 1.00 import weight 0.00
Epoch 21 Iter 10 subLoss 21137.0 multi -1.99 import weight 0.00
Epoch 21 Iter 11 subLoss 26662.0 multi 1.00 import weight 0.00
Epoch 21 Acc: 77.88 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2666 train Loss: 24452.3 test Loss: 3471.4
Epoch 22 Iter 0 subLoss 24082.5 multi 1.00 import weight 0.00
Epoch 22 Iter 1 subLoss 21459.6 multi 3.99 import weight 1.00
Epoch 22 Iter 2 subLoss 12152.2 multi 1.00 import weight 0.00
Epoch 22 Iter 3 subLoss 11115.6 multi 3.99 import weight 1.00
Epoch 22 Iter 4 subLoss 10347.8 multi 1.00 import weight 0.00
Epoch 22 Iter 5 subLoss 8930.5 multi -1.99 import weight 0.00
Epoch 22 Iter 6 subLoss 20304.9 multi 1.00 import weight 0.00
Epoch 22 Iter 7 subLoss 10604.2 multi 1.00 import weight 0.00
Epoch 22 Iter 8 subLoss 9226.7 multi 1.00 import weight 0.00
Epoch 22 Iter 9 subLoss 7860.5 multi -1.99 import weight 0.00
Epoch 22 Iter 10 subLoss 8902.3 multi 1.00 import weight 0.00
Epoch 22 Iter 11 subLoss 8520.1 multi 1.00 import weight 0.00
Epoch 22 Acc: 95.89 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 852 train Loss: 8249.4 test Loss: 756.1
Epoch 23 Iter 0 subLoss 8058.5 multi 1.00 import weight 0.00
Epoch 23 Iter 1 subLoss 7619.4 multi 1.00 import weight 0.00
Epoch 23 Iter 2 subLoss 8025.6 multi 1.00 import weight 0.00
Epoch 23 Iter 3 subLoss 7633.0 multi 1.00 import weight 0.00
Epoch 23 Iter 4 subLoss 7621.6 multi -1.99 import weight 0.00
Epoch 23 Iter 5 subLoss 7851.8 multi 3.99 import weight 1.00
Epoch 23 Iter 6 subLoss 11418.4 multi 1.00 import weight 0.00
Epoch 23 Iter 7 subLoss 8468.8 multi 1.00 import weight 0.00
Epoch 23 Iter 8 subLoss 7954.0 multi 1.00 import weight 0.00
Epoch 23 Iter 9 subLoss 6972.1 multi 1.00 import weight 0.00
Epoch 23 Iter 10 subLoss 7556.8 multi 1.00 import weight 0.00
Epoch 23 Iter 11 subLoss 7001.8 multi 1.00 import weight 0.00
Epoch 23 Acc: 95.74 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 700 train Loss: 7073.2 test Loss: 652.6
Epoch 24 Iter 0 subLoss 6562.0 multi 1.00 import weight 0.00
Epoch 24 Iter 1 subLoss 6614.4 multi 1.00 import weight 0.00
Epoch 24 Iter 2 subLoss 7302.1 multi 3.99 import weight 1.00
Epoch 24 Iter 3 subLoss 7942.6 multi 1.00 import weight 0.00
Epoch 24 Iter 4 subLoss 6790.6 multi 1.00 import weight 0.00
Epoch 24 Iter 5 subLoss 6957.8 multi 1.00 import weight 0.00
Epoch 24 Iter 6 subLoss 6267.7 multi -1.99 import weight 0.00
Epoch 24 Iter 7 subLoss 7396.9 multi 1.00 import weight 0.00
Epoch 24 Iter 8 subLoss 7266.8 multi 1.00 import weight 0.00
Epoch 24 Iter 9 subLoss 6680.8 multi -1.99 import weight 0.00
Epoch 24 Iter 10 subLoss 12313.5 multi 1.00 import weight 0.00
Epoch 24 Iter 11 subLoss 9346.2 multi 1.00 import weight 0.00
Epoch 24 Acc: 96.21 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 934 train Loss: 6901.1 test Loss: 607.4
Epoch 25 Iter 0 subLoss 6914.1 multi 3.99 import weight 1.00
Epoch 25 Iter 1 subLoss 8519.8 multi 1.00 import weight 0.00
Epoch 25 Iter 2 subLoss 6980.7 multi 1.00 import weight 1.00
Epoch 25 Iter 3 subLoss 6519.4 multi 1.00 import weight 0.00
Epoch 25 Iter 4 subLoss 6196.1 multi 1.00 import weight 0.00
Epoch 25 Iter 5 subLoss 6218.4 multi 1.00 import weight 0.00
Epoch 25 Iter 6 subLoss 6023.9 multi 1.00 import weight 0.00
Epoch 25 Iter 7 subLoss 5926.4 multi 1.00 import weight 0.00
Epoch 25 Iter 8 subLoss 5857.4 multi 1.00 import weight 0.00
Epoch 25 Iter 9 subLoss 5808.0 multi 1.00 import weight 0.00
Epoch 25 Iter 10 subLoss 6036.3 multi -1.99 import weight 0.00
Epoch 25 Iter 11 subLoss 6773.6 multi 3.99 import weight 1.00
Epoch 25 Acc: 72.39 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 3.99 Pidx 677 train Loss: 27353.0 test Loss: 3532.4
Epoch 26 Iter 0 subLoss 27368.4 multi 1.00 import weight 0.00
Epoch 26 Iter 1 subLoss 31462.3 multi 1.00 import weight 0.00
Epoch 26 Iter 2 subLoss 20605.9 multi 1.00 import weight 0.00
Epoch 26 Iter 3 subLoss 16660.4 multi 1.00 import weight 0.00
Epoch 26 Iter 4 subLoss 12001.9 multi 1.00 import weight 0.00
Epoch 26 Iter 5 subLoss 9583.7 multi 1.00 import weight 0.00
Epoch 26 Iter 6 subLoss 8154.0 multi 1.00 import weight 0.00
Epoch 26 Iter 7 subLoss 7883.2 multi -1.99 import weight 0.00
Epoch 26 Iter 8 subLoss 8461.0 multi 3.99 import weight 1.00
Epoch 26 Iter 9 subLoss 17269.7 multi 1.00 import weight 0.00
Epoch 26 Iter 10 subLoss 21351.5 multi 1.00 import weight 0.00
Epoch 26 Iter 11 subLoss 7593.2 multi 3.99 import weight 1.00
Epoch 26 Acc: 86.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 3.99 Pidx 759 train Loss: 16577.8 test Loss: 1789.0
Epoch 27 Iter 0 subLoss 15475.8 multi 1.00 import weight 0.00
Epoch 27 Iter 1 subLoss 6596.1 multi 3.99 import weight 1.00
Epoch 27 Iter 2 subLoss 10287.8 multi 1.00 import weight 0.00
Epoch 27 Iter 3 subLoss 6556.9 multi 1.00 import weight 0.00
Epoch 27 Iter 4 subLoss 5886.9 multi 1.00 import weight 0.00
Epoch 27 Iter 5 subLoss 6171.9 multi 1.00 import weight 0.00
Epoch 27 Iter 6 subLoss 5767.6 multi 1.00 import weight 0.00
Epoch 27 Iter 7 subLoss 6054.4 multi 1.00 import weight 0.00
Epoch 27 Iter 8 subLoss 5544.5 multi 1.00 import weight 0.00
Epoch 27 Iter 9 subLoss 5479.3 multi 1.00 import weight 0.00
Epoch 27 Iter 10 subLoss 5502.9 multi 1.00 import weight 0.00
Epoch 27 Iter 11 subLoss 5858.1 multi 3.99 import weight 1.00
Epoch 27 Acc: 97.20 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 3.99 Pidx 585 train Loss: 5810.8 test Loss: 481.9
Epoch 28 Iter 0 subLoss 6167.2 multi 1.00 import weight 0.00
Epoch 28 Iter 1 subLoss 5014.7 multi 1.00 import weight 0.00
Epoch 28 Iter 2 subLoss 5682.3 multi 1.00 import weight 0.00
Epoch 28 Iter 3 subLoss 4853.7 multi 1.00 import weight 0.00
Epoch 28 Iter 4 subLoss 5255.9 multi 1.00 import weight 0.00
Epoch 28 Iter 5 subLoss 5086.1 multi 1.00 import weight 0.00
Epoch 28 Iter 6 subLoss 6082.0 multi -1.99 import weight 0.00
Epoch 28 Iter 7 subLoss 5384.9 multi 1.00 import weight 0.00
Epoch 28 Iter 8 subLoss 4964.1 multi 1.00 import weight 0.00
Epoch 28 Iter 9 subLoss 5037.6 multi 1.00 import weight 0.00
Epoch 28 Iter 10 subLoss 5378.5 multi 1.00 import weight 0.00
Epoch 28 Iter 11 subLoss 4975.9 multi -1.99 import weight 0.00
Epoch 28 Acc: 97.28 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 497 train Loss: 6153.0 test Loss: 495.8
Epoch 29 Iter 0 subLoss 5470.7 multi 3.99 import weight 1.00
Epoch 29 Iter 1 subLoss 20230.4 multi -1.99 import weight 0.00
Epoch 29 Iter 2 subLoss 411164.6 multi 1.00 import weight 0.00
Epoch 29 Iter 3 subLoss 69677.6 multi 1.00 import weight 0.00
Epoch 29 Iter 4 subLoss 42264.1 multi 1.00 import weight 0.00
Epoch 29 Iter 5 subLoss 36321.2 multi 1.00 import weight 0.00
Epoch 29 Iter 6 subLoss 34350.3 multi 1.00 import weight 0.00
Epoch 29 Iter 7 subLoss 33532.6 multi 1.00 import weight 0.00
Epoch 29 Iter 8 subLoss 33002.5 multi 1.00 import weight 0.00
Epoch 29 Iter 9 subLoss 32011.0 multi 1.00 import weight 0.00
Epoch 29 Iter 10 subLoss 31310.0 multi 3.99 import weight 1.00
Epoch 29 Iter 11 subLoss 28019.3 multi 1.00 import weight 0.00
Epoch 29 Acc: 69.14 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2801 train Loss: 27977.5 test Loss: 4124.6
Epoch 30 Iter 0 subLoss 27523.4 multi 1.00 import weight 0.00
Epoch 30 Iter 1 subLoss 26334.1 multi 1.00 import weight 0.00
Epoch 30 Iter 2 subLoss 24834.9 multi 3.99 import weight 1.00
Epoch 30 Iter 3 subLoss 22617.3 multi 1.00 import weight 0.00
Epoch 30 Iter 4 subLoss 21058.4 multi 1.00 import weight 0.00
Epoch 30 Iter 5 subLoss 20846.7 multi -1.99 import weight 0.00
Epoch 30 Iter 6 subLoss 25174.1 multi 1.00 import weight 0.00
Epoch 30 Iter 7 subLoss 23957.6 multi 1.00 import weight 0.00
Epoch 30 Iter 8 subLoss 22130.5 multi 1.00 import weight 0.00
Epoch 30 Iter 9 subLoss 20840.8 multi 1.00 import weight 1.00
Epoch 30 Iter 10 subLoss 19627.8 multi 1.00 import weight 0.00
Epoch 30 Iter 11 subLoss 18999.4 multi 1.00 import weight 0.00
Epoch 30 Acc: 78.30 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1899 train Loss: 18473.1 test Loss: 2322.4
Epoch 31 Iter 0 subLoss 17744.4 multi 1.00 import weight 0.00
Epoch 31 Iter 1 subLoss 17026.4 multi 1.00 import weight 0.00
Epoch 31 Iter 2 subLoss 17019.2 multi 1.00 import weight 0.00
Epoch 31 Iter 3 subLoss 16896.0 multi 1.00 import weight 0.00
Epoch 31 Iter 4 subLoss 16784.8 multi 1.00 import weight 0.00
Epoch 31 Iter 5 subLoss 16458.3 multi 1.00 import weight 0.00
Epoch 31 Iter 6 subLoss 16160.4 multi 1.00 import weight 0.00
Epoch 31 Iter 7 subLoss 15969.3 multi 1.00 import weight 0.00
Epoch 31 Iter 8 subLoss 15655.6 multi 1.00 import weight 0.00
Epoch 31 Iter 9 subLoss 15729.2 multi 1.00 import weight 0.00
Epoch 31 Iter 10 subLoss 16103.9 multi 1.00 import weight 0.00
Epoch 31 Iter 11 subLoss 16386.2 multi 1.00 import weight 0.00
Epoch 31 Acc: 83.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1638 train Loss: 16764.6 test Loss: 2022.2
Epoch 32 Iter 0 subLoss 16591.0 multi -1.99 import weight 0.00
Epoch 32 Iter 1 subLoss 103676.8 multi 1.00 import weight 0.00
Epoch 32 Iter 2 subLoss 41521.7 multi 1.00 import weight 0.00
Epoch 32 Iter 3 subLoss 30786.4 multi 1.00 import weight 0.00
Epoch 32 Iter 4 subLoss 27452.8 multi -1.99 import weight 0.00
Epoch 32 Iter 5 subLoss 30554.6 multi 1.00 import weight 0.00
Epoch 32 Iter 6 subLoss 28019.4 multi 3.99 import weight 1.00
Epoch 32 Iter 7 subLoss 26110.7 multi 1.00 import weight 0.00
Epoch 32 Iter 8 subLoss 25771.2 multi 1.00 import weight 0.00
Epoch 32 Iter 9 subLoss 24046.8 multi -1.99 import weight 0.00
Epoch 32 Iter 10 subLoss 24939.6 multi 1.00 import weight 0.00
Epoch 32 Iter 11 subLoss 25281.2 multi 3.99 import weight 1.00
Epoch 32 Acc: 77.60 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 3.99 Pidx 2528 train Loss: 22813.4 test Loss: 3501.0
Epoch 33 Iter 0 subLoss 23042.0 multi 1.00 import weight 0.00
Epoch 33 Iter 1 subLoss 21106.5 multi 1.00 import weight 0.00
Epoch 33 Iter 2 subLoss 19265.4 multi 1.00 import weight 0.00
Epoch 33 Iter 3 subLoss 18616.8 multi 1.00 import weight 0.00
Epoch 33 Iter 4 subLoss 17370.3 multi 1.00 import weight 0.00
Epoch 33 Iter 5 subLoss 15511.7 multi 1.00 import weight 0.00
Epoch 33 Iter 6 subLoss 14767.0 multi 1.00 import weight 0.00
Epoch 33 Iter 7 subLoss 13894.2 multi 1.00 import weight 0.00
Epoch 33 Iter 8 subLoss 12500.3 multi 3.99 import weight 1.00
Epoch 33 Iter 9 subLoss 11615.3 multi 1.00 import weight 0.00
Epoch 33 Iter 10 subLoss 10958.2 multi 1.00 import weight 1.00
Epoch 33 Iter 11 subLoss 11073.1 multi 1.00 import weight 0.00
Epoch 33 Acc: 92.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1107 train Loss: 10870.9 test Loss: 1428.7
Epoch 34 Iter 0 subLoss 10605.8 multi 3.99 import weight 1.00
Epoch 34 Iter 1 subLoss 10660.1 multi 1.00 import weight 0.00
Epoch 34 Iter 2 subLoss 9796.8 multi 1.00 import weight 0.00
Epoch 34 Iter 3 subLoss 9156.6 multi -1.99 import weight 0.00
Epoch 34 Iter 4 subLoss 11252.2 multi 1.00 import weight 0.00
Epoch 34 Iter 5 subLoss 10767.9 multi 1.00 import weight 0.00
Epoch 34 Iter 6 subLoss 10295.9 multi 1.00 import weight 1.00
Epoch 34 Iter 7 subLoss 9764.7 multi 1.00 import weight 0.00
Epoch 34 Iter 8 subLoss 9567.7 multi 3.99 import weight 1.00
Epoch 34 Iter 9 subLoss 19765.6 multi 1.00 import weight 0.00
Epoch 34 Iter 10 subLoss 34319.9 multi 1.00 import weight 0.00
Epoch 34 Iter 11 subLoss 15859.6 multi 1.00 import weight 0.00
Epoch 34 Acc: 92.26 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1585 train Loss: 10442.3 test Loss: 1269.0
Epoch 35 Iter 0 subLoss 10603.4 multi 6.97 import weight 1.00
Epoch 35 Iter 1 subLoss 59201.5 multi 1.00 import weight 0.00
Epoch 35 Iter 2 subLoss 32770.9 multi 1.00 import weight 0.00
Epoch 35 Iter 3 subLoss 18924.1 multi 1.00 import weight 0.00
Epoch 35 Iter 4 subLoss 14762.2 multi 3.99 import weight 0.00
Epoch 35 Iter 5 subLoss 9299.2 multi -1.99 import weight 0.00
Epoch 35 Iter 6 subLoss 12133.2 multi 1.00 import weight 0.00
Epoch 35 Iter 7 subLoss 10049.9 multi 1.00 import weight 0.00
Epoch 35 Iter 8 subLoss 9059.5 multi 3.99 import weight 0.00
Epoch 35 Iter 9 subLoss 7931.9 multi 1.00 import weight 0.00
Epoch 35 Iter 10 subLoss 8054.7 multi 3.99 import weight 0.00
Epoch 35 Iter 11 subLoss 7259.8 multi 1.00 import weight 0.00
Epoch 35 Acc: 95.91 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 725 train Loss: 7352.9 test Loss: 766.1
Epoch 36 Iter 0 subLoss 7167.1 multi 1.00 import weight 0.00
Epoch 36 Iter 1 subLoss 6910.1 multi 6.97 import weight 1.00
Epoch 36 Iter 2 subLoss 10794.3 multi 1.00 import weight 0.00
Epoch 36 Iter 3 subLoss 9537.0 multi 1.00 import weight 0.00
Epoch 36 Iter 4 subLoss 8829.5 multi 1.00 import weight 0.00
Epoch 36 Iter 5 subLoss 7497.7 multi 1.00 import weight 0.00
Epoch 36 Iter 6 subLoss 6399.2 multi 1.00 import weight 0.00
Epoch 36 Iter 7 subLoss 6177.4 multi 1.00 import weight 0.00
Epoch 36 Iter 8 subLoss 6346.1 multi 1.00 import weight 0.00
Epoch 36 Iter 9 subLoss 5983.0 multi 1.00 import weight 0.00
Epoch 36 Iter 10 subLoss 6200.6 multi -1.99 import weight 0.00
Epoch 36 Iter 11 subLoss 6289.0 multi 1.00 import weight 0.00
Epoch 36 Acc: 95.80 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 628 train Loss: 6111.1 test Loss: 660.8
Epoch 37 Iter 0 subLoss 5928.1 multi 3.99 import weight 0.00
Epoch 37 Iter 1 subLoss 9565.3 multi 6.97 import weight 1.00
Epoch 37 Iter 2 subLoss 343314.9 multi 1.00 import weight 0.00
Epoch 37 Iter 3 subLoss 79641.4 multi 1.00 import weight 0.00
Epoch 37 Iter 4 subLoss 46413.6 multi 1.00 import weight 0.00
Epoch 37 Iter 5 subLoss 44826.4 multi 1.00 import weight 0.00
Epoch 37 Iter 6 subLoss 42920.3 multi -1.99 import weight 0.00
Epoch 37 Iter 7 subLoss 47245.0 multi 1.00 import weight 0.00
Epoch 37 Iter 8 subLoss 45601.2 multi 1.00 import weight 0.00
Epoch 37 Iter 9 subLoss 44021.6 multi 3.99 import weight 0.00
Epoch 37 Iter 10 subLoss 37545.3 multi 1.00 import weight 0.00
Epoch 37 Iter 11 subLoss 34387.4 multi 1.00 import weight 0.00
Epoch 37 Acc: 55.81 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3438 train Loss: 32758.5 test Loss: 4910.8
Epoch 38 Iter 0 subLoss 31478.3 multi -1.99 import weight 0.00
Epoch 38 Iter 1 subLoss 35490.9 multi 1.00 import weight 0.00
Epoch 38 Iter 2 subLoss 32849.5 multi 1.00 import weight 0.00
Epoch 38 Iter 3 subLoss 31877.0 multi 1.00 import weight 0.00
Epoch 38 Iter 4 subLoss 30912.5 multi 1.00 import weight 0.00
Epoch 38 Iter 5 subLoss 30514.1 multi 1.00 import weight 0.00
Epoch 38 Iter 6 subLoss 29655.8 multi 1.00 import weight 0.00
Epoch 38 Iter 7 subLoss 28291.1 multi 1.00 import weight 0.00
Epoch 38 Iter 8 subLoss 27671.3 multi 1.00 import weight 0.00
Epoch 38 Iter 9 subLoss 27141.2 multi 1.00 import weight 0.00
Epoch 38 Iter 10 subLoss 26475.4 multi 1.00 import weight 0.00
Epoch 38 Iter 11 subLoss 25856.2 multi 1.00 import weight 0.00
Epoch 38 Acc: 65.99 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2585 train Loss: 25754.3 test Loss: 3746.4
Epoch 39 Iter 0 subLoss 24802.2 multi 1.00 import weight 0.00
Epoch 39 Iter 1 subLoss 25228.3 multi 1.00 import weight 0.00
Epoch 39 Iter 2 subLoss 23261.8 multi 1.00 import weight 0.00
Epoch 39 Iter 3 subLoss 23565.8 multi 1.00 import weight 0.00
Epoch 39 Iter 4 subLoss 23681.9 multi 1.00 import weight 0.00
Epoch 39 Iter 5 subLoss 22504.4 multi 1.00 import weight 0.00
Epoch 39 Iter 6 subLoss 20873.5 multi 1.00 import weight 0.00
Epoch 39 Iter 7 subLoss 19388.9 multi 1.00 import weight 0.00
Epoch 39 Iter 8 subLoss 18919.7 multi 1.00 import weight 0.00
Epoch 39 Iter 9 subLoss 18484.0 multi 1.00 import weight 0.00
Epoch 39 Iter 10 subLoss 17691.1 multi 1.00 import weight 0.00
Epoch 39 Iter 11 subLoss 16278.9 multi 3.99 import weight 0.00
Epoch 39 Acc: 80.42 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1627 train Loss: 14937.2 test Loss: 1829.7
Epoch 40 Iter 0 subLoss 15211.2 multi 1.00 import weight 0.00
Epoch 40 Iter 1 subLoss 14168.8 multi 1.00 import weight 0.00
Epoch 40 Iter 2 subLoss 13169.5 multi 1.00 import weight 0.00
Epoch 40 Iter 3 subLoss 13768.0 multi 1.00 import weight 0.00
Epoch 40 Iter 4 subLoss 13184.6 multi 1.00 import weight 0.00
Epoch 40 Iter 5 subLoss 12499.7 multi 1.00 import weight 0.00
Epoch 40 Iter 6 subLoss 13224.0 multi 1.00 import weight 0.00
Epoch 40 Iter 7 subLoss 13065.6 multi 1.00 import weight 0.00
Epoch 40 Iter 8 subLoss 19940.1 multi 1.00 import weight 0.00
Epoch 40 Iter 9 subLoss 47349.5 multi 1.00 import weight 0.00
Epoch 40 Iter 10 subLoss 57869.2 multi 1.00 import weight 0.00
Epoch 40 Iter 11 subLoss 22721.7 multi 1.00 import weight 0.00
Epoch 40 Acc: 76.77 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2272 train Loss: 21479.4 test Loss: 3080.3
Epoch 41 Iter 0 subLoss 21197.0 multi 1.00 import weight 0.00
Epoch 41 Iter 1 subLoss 19531.3 multi 1.00 import weight 0.00
Epoch 41 Iter 2 subLoss 18846.3 multi 1.00 import weight 0.00
Epoch 41 Iter 3 subLoss 17766.2 multi 1.00 import weight 0.00
Epoch 41 Iter 4 subLoss 15916.3 multi 1.00 import weight 0.00
Epoch 41 Iter 5 subLoss 15193.9 multi 1.00 import weight 0.00
Epoch 41 Iter 6 subLoss 14317.4 multi 1.00 import weight 0.00
Epoch 41 Iter 7 subLoss 12962.1 multi 1.00 import weight 0.00
Epoch 41 Iter 8 subLoss 12714.5 multi 1.00 import weight 0.00
Epoch 41 Iter 9 subLoss 12599.2 multi 1.00 import weight 0.00
Epoch 41 Iter 10 subLoss 12179.8 multi 1.00 import weight 0.00
Epoch 41 Iter 11 subLoss 11350.1 multi -1.99 import weight 0.00
Epoch 41 Acc: 90.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1135 train Loss: 13096.6 test Loss: 1499.0
Epoch 42 Iter 0 subLoss 12954.5 multi 1.00 import weight 0.00
Epoch 42 Iter 1 subLoss 12085.8 multi 1.00 import weight 0.00
Epoch 42 Iter 2 subLoss 11830.2 multi 1.00 import weight 0.00
Epoch 42 Iter 3 subLoss 11400.8 multi 1.00 import weight 0.00
Epoch 42 Iter 4 subLoss 11169.6 multi 1.00 import weight 0.00
Epoch 42 Iter 5 subLoss 10779.0 multi -1.99 import weight 0.00
Epoch 42 Iter 6 subLoss 11588.7 multi 1.00 import weight 0.00
Epoch 42 Iter 7 subLoss 11636.7 multi 1.00 import weight 0.00
Epoch 42 Iter 8 subLoss 10997.8 multi 1.00 import weight 0.00
Epoch 42 Iter 9 subLoss 11288.0 multi 1.00 import weight 0.00
Epoch 42 Iter 10 subLoss 10378.6 multi 1.00 import weight 0.00
Epoch 42 Iter 11 subLoss 10300.1 multi -4.97 import weight 0.00
Epoch 42 Acc: 94.57 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 1030 train Loss: 11828.4 test Loss: 1334.1
Epoch 43 Iter 0 subLoss 11448.9 multi 1.00 import weight 0.00
Epoch 43 Iter 1 subLoss 11173.8 multi 1.00 import weight 0.00
Epoch 43 Iter 2 subLoss 10755.4 multi 1.00 import weight 0.00
Epoch 43 Iter 3 subLoss 10822.4 multi 1.00 import weight 0.00
Epoch 43 Iter 4 subLoss 10834.1 multi -1.99 import weight 0.00
Epoch 43 Iter 5 subLoss 11238.0 multi 1.00 import weight 0.00
Epoch 43 Iter 6 subLoss 10785.4 multi -1.99 import weight 0.00
Epoch 43 Iter 7 subLoss 11462.2 multi 1.00 import weight 0.00
Epoch 43 Iter 8 subLoss 11186.8 multi -4.97 import weight 0.00
Epoch 43 Iter 9 subLoss 13851.8 multi 1.00 import weight 0.00
Epoch 43 Iter 10 subLoss 11982.5 multi 1.00 import weight 0.00
Epoch 43 Iter 11 subLoss 11497.7 multi -1.99 import weight 0.00
Epoch 43 Acc: 94.45 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1149 train Loss: 12457.3 test Loss: 1406.2
Epoch 44 Iter 0 subLoss 11739.5 multi 1.00 import weight 0.00
Epoch 44 Iter 1 subLoss 12056.7 multi -1.99 import weight 0.00
Epoch 44 Iter 2 subLoss 12191.0 multi 1.00 import weight 0.00
Epoch 44 Iter 3 subLoss 12342.7 multi 1.00 import weight 0.00
Epoch 44 Iter 4 subLoss 11604.3 multi 1.00 import weight 0.00
Epoch 44 Iter 5 subLoss 11827.1 multi 1.00 import weight 0.00
Epoch 44 Iter 6 subLoss 10852.7 multi 1.00 import weight 0.00
Epoch 44 Iter 7 subLoss 10685.0 multi 1.00 import weight 0.00
Epoch 44 Iter 8 subLoss 10788.7 multi 1.00 import weight 0.00
Epoch 44 Iter 9 subLoss 11353.0 multi 1.00 import weight 0.00
Epoch 44 Iter 10 subLoss 10736.9 multi 1.00 import weight 0.00
Epoch 44 Iter 11 subLoss 11210.5 multi -1.99 import weight 0.00
Epoch 44 Acc: 94.20 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1121 train Loss: 11629.8 test Loss: 1297.3
Epoch 45 Iter 0 subLoss 10779.2 multi 1.00 import weight 0.00
Epoch 45 Iter 1 subLoss 10549.5 multi 1.00 import weight 0.00
Epoch 45 Iter 2 subLoss 10751.7 multi 3.99 import weight 0.00
Epoch 45 Iter 3 subLoss 10438.1 multi 1.00 import weight 0.00
Epoch 45 Iter 4 subLoss 9570.9 multi -7.96 import weight 0.00
Epoch 45 Iter 5 subLoss 17512.0 multi 1.00 import weight 0.00
Epoch 45 Iter 6 subLoss 11993.3 multi -1.99 import weight 0.00
Epoch 45 Iter 7 subLoss 20722.0 multi 1.00 import weight 0.00
Epoch 45 Iter 8 subLoss 12751.3 multi 1.00 import weight 0.00
Epoch 45 Iter 9 subLoss 11251.3 multi 3.99 import weight 0.00
Epoch 45 Iter 10 subLoss 11105.4 multi -4.97 import weight 0.00
Epoch 45 Iter 11 subLoss 58661.7 multi 1.00 import weight 0.00
Epoch 45 Acc: 60.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 5866 train Loss: 35756.8 test Loss: 5496.7
Epoch 46 Iter 0 subLoss 34907.8 multi 1.00 import weight 0.00
Epoch 46 Iter 1 subLoss 21469.5 multi -4.97 import weight 0.00
Epoch 46 Iter 2 subLoss 40791.6 multi 1.00 import weight 0.00
Epoch 46 Iter 3 subLoss 22782.2 multi 1.00 import weight 0.00
Epoch 46 Iter 4 subLoss 22569.2 multi 1.00 import weight 0.00
Epoch 46 Iter 5 subLoss 21597.6 multi 1.00 import weight 0.00
Epoch 46 Iter 6 subLoss 21214.6 multi 1.00 import weight 0.00
Epoch 46 Iter 7 subLoss 20540.0 multi 1.00 import weight 0.00
Epoch 46 Iter 8 subLoss 19099.1 multi 1.00 import weight 0.00
Epoch 46 Iter 9 subLoss 19724.7 multi 1.00 import weight 0.00
Epoch 46 Iter 10 subLoss 18471.8 multi 1.00 import weight 0.00
Epoch 46 Iter 11 subLoss 18671.2 multi 1.00 import weight 0.00
Epoch 46 Acc: 82.58 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1867 train Loss: 17832.0 test Loss: 2505.3
Epoch 47 Iter 0 subLoss 17608.7 multi 1.00 import weight 0.00
Epoch 47 Iter 1 subLoss 16328.8 multi 1.00 import weight 0.00
Epoch 47 Iter 2 subLoss 16749.9 multi 1.00 import weight 0.00
Epoch 47 Iter 3 subLoss 14528.4 multi 3.99 import weight 0.00
Epoch 47 Iter 4 subLoss 12174.0 multi 3.99 import weight 0.00
Epoch 47 Iter 5 subLoss 11343.2 multi 3.99 import weight 0.00
Epoch 47 Iter 6 subLoss 12441.7 multi 1.00 import weight 0.00
Epoch 47 Iter 7 subLoss 9578.3 multi -4.97 import weight 0.00
Epoch 47 Iter 8 subLoss 11444.9 multi 3.99 import weight 0.00
Epoch 47 Iter 9 subLoss 10913.3 multi 1.00 import weight 0.00
Epoch 47 Iter 10 subLoss 10665.2 multi 3.99 import weight 0.00
Epoch 47 Iter 11 subLoss 9517.2 multi 1.00 import weight 0.00
Epoch 47 Acc: 95.95 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 951 train Loss: 9746.1 test Loss: 943.3
Epoch 48 Iter 0 subLoss 9438.3 multi 1.00 import weight 0.00
Epoch 48 Iter 1 subLoss 9116.9 multi 1.00 import weight 0.00
Epoch 48 Iter 2 subLoss 8804.8 multi 1.00 import weight 0.00
Epoch 48 Iter 3 subLoss 8653.2 multi 1.00 import weight 0.00
Epoch 48 Iter 4 subLoss 9275.2 multi 1.00 import weight 0.00
Epoch 48 Iter 5 subLoss 9037.3 multi -1.99 import weight 0.00
Epoch 48 Iter 6 subLoss 8492.5 multi 1.00 import weight 0.00
Epoch 48 Iter 7 subLoss 9270.2 multi 3.99 import weight 0.00
Epoch 48 Iter 8 subLoss 8472.5 multi -4.97 import weight 0.00
Epoch 48 Iter 9 subLoss 30292.0 multi 1.00 import weight 0.00
Epoch 48 Iter 10 subLoss 16168.3 multi 3.99 import weight 0.00
Epoch 48 Iter 11 subLoss 53898.4 multi 1.00 import weight 0.00
Epoch 48 Acc: 88.93 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 5389 train Loss: 18890.8 test Loss: 2062.5
Epoch 49 Iter 0 subLoss 18508.4 multi 1.00 import weight 0.00
Epoch 49 Iter 1 subLoss 13767.1 multi 3.99 import weight 0.00
Epoch 49 Iter 2 subLoss 9640.4 multi -1.99 import weight 0.00
Epoch 49 Iter 3 subLoss 10472.9 multi 1.00 import weight 0.00
Epoch 49 Iter 4 subLoss 9501.8 multi 1.00 import weight 0.00
Epoch 49 Iter 5 subLoss 9234.2 multi -1.99 import weight 0.00
Epoch 49 Iter 6 subLoss 9839.2 multi 1.00 import weight 0.00
Epoch 49 Iter 7 subLoss 9243.4 multi -1.99 import weight 0.00
Epoch 49 Iter 8 subLoss 10969.7 multi -4.97 import weight 0.00
Epoch 49 Iter 9 subLoss 14602.7 multi 1.00 import weight 0.00
Epoch 49 Iter 10 subLoss 12797.5 multi 1.00 import weight 0.00
Epoch 49 Iter 11 subLoss 12376.0 multi -1.99 import weight 0.00
Epoch 49 Acc: 88.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1237 train Loss: 13658.3 test Loss: 1530.8
Epoch 50 Iter 0 subLoss 13503.5 multi -4.97 import weight 0.00
Epoch 50 Iter 1 subLoss 20732.9 multi -1.99 import weight 0.00
Epoch 50 Iter 2 subLoss 42550.3 multi 1.00 import weight 0.00
Epoch 50 Iter 3 subLoss 18060.7 multi 1.00 import weight 0.00
Epoch 50 Iter 4 subLoss 16890.6 multi 3.99 import weight 0.00
Epoch 50 Iter 5 subLoss 14034.5 multi 1.00 import weight 0.00
Epoch 50 Iter 6 subLoss 13466.1 multi 1.00 import weight 0.00
Epoch 50 Iter 7 subLoss 13103.0 multi 1.00 import weight 0.00
Epoch 50 Iter 8 subLoss 13018.1 multi 1.00 import weight 0.00
Epoch 50 Iter 9 subLoss 13037.7 multi 1.00 import weight 0.00
Epoch 50 Iter 10 subLoss 11679.0 multi 1.00 import weight 0.00
Epoch 50 Iter 11 subLoss 11897.1 multi 1.00 import weight 0.00
Epoch 50 Acc: 93.79 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1189 train Loss: 11522.0 test Loss: 1272.5
Epoch 51 Iter 0 subLoss 11769.0 multi 3.99 import weight 0.00
Epoch 51 Iter 1 subLoss 9867.2 multi 1.00 import weight 0.00
Epoch 51 Iter 2 subLoss 9805.4 multi -1.99 import weight 0.00
Epoch 51 Iter 3 subLoss 10456.0 multi 1.00 import weight 0.00
Epoch 51 Iter 4 subLoss 9895.0 multi 1.00 import weight 0.00
Epoch 51 Iter 5 subLoss 9895.7 multi 3.99 import weight 0.00
Epoch 51 Iter 6 subLoss 8698.7 multi 1.00 import weight 0.00
Epoch 51 Iter 7 subLoss 8241.4 multi 3.99 import weight 0.00
Epoch 51 Iter 8 subLoss 7459.9 multi 1.00 import weight 0.00
Epoch 51 Iter 9 subLoss 7729.8 multi 1.00 import weight 0.00
Epoch 51 Iter 10 subLoss 7724.9 multi 3.99 import weight 0.00
Epoch 51 Iter 11 subLoss 7596.2 multi 6.97 import weight 1.00
Epoch 51 Acc: 88.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 6.97 Pidx 759 train Loss: 10347.7 test Loss: 1341.4
Epoch 52 Iter 0 subLoss 9244.2 multi 1.00 import weight 0.00
Epoch 52 Iter 1 subLoss 7219.2 multi 1.00 import weight 0.00
Epoch 52 Iter 2 subLoss 6536.6 multi 1.00 import weight 0.00
Epoch 52 Iter 3 subLoss 6688.8 multi 1.00 import weight 0.00
Epoch 52 Iter 4 subLoss 6453.1 multi 1.00 import weight 0.00
Epoch 52 Iter 5 subLoss 6965.0 multi -1.99 import weight 0.00
Epoch 52 Iter 6 subLoss 6856.2 multi -1.99 import weight 0.00
Epoch 52 Iter 7 subLoss 7734.8 multi -4.97 import weight 0.00
Epoch 52 Iter 8 subLoss 11609.3 multi 3.99 import weight 0.00
Epoch 52 Iter 9 subLoss 29065.6 multi 1.00 import weight 0.00
Epoch 52 Iter 10 subLoss 9523.8 multi -1.99 import weight 0.00
Epoch 52 Iter 11 subLoss 15426.5 multi 1.00 import weight 0.00
Epoch 52 Acc: 96.30 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1542 train Loss: 9381.7 test Loss: 887.7
Epoch 53 Iter 0 subLoss 8934.7 multi 1.00 import weight 0.00
Epoch 53 Iter 1 subLoss 8514.0 multi 3.99 import weight 0.00
Epoch 53 Iter 2 subLoss 7903.8 multi 1.00 import weight 0.00
Epoch 53 Iter 3 subLoss 6916.9 multi 9.96 import weight 1.00
Epoch 53 Iter 4 subLoss 6324.5 multi 1.00 import weight 0.00
Epoch 53 Iter 5 subLoss 6077.7 multi 3.99 import weight 0.00
Epoch 53 Iter 6 subLoss 5856.6 multi 6.97 import weight 0.00
Epoch 53 Iter 7 subLoss 5965.4 multi 1.00 import weight 0.00
Epoch 53 Iter 8 subLoss 6102.0 multi 1.00 import weight 0.00
Epoch 53 Iter 9 subLoss 6148.1 multi 1.00 import weight 0.00
Epoch 53 Iter 10 subLoss 5955.0 multi -1.99 import weight 0.00
Epoch 53 Iter 11 subLoss 5996.4 multi -1.99 import weight 0.00
Epoch 53 Acc: 95.95 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 599 train Loss: 6379.5 test Loss: 638.5
Epoch 54 Iter 0 subLoss 6262.8 multi 1.00 import weight 0.00
Epoch 54 Iter 1 subLoss 5583.6 multi 1.00 import weight 0.00
Epoch 54 Iter 2 subLoss 6205.3 multi 1.00 import weight 0.00
Epoch 54 Iter 3 subLoss 5888.6 multi 3.99 import weight 0.00
Epoch 54 Iter 4 subLoss 5498.3 multi 1.00 import weight 0.00
Epoch 54 Iter 5 subLoss 5160.7 multi 1.00 import weight 0.00
Epoch 54 Iter 6 subLoss 5306.5 multi 1.00 import weight 0.00
Epoch 54 Iter 7 subLoss 5683.7 multi 3.99 import weight 0.00
Epoch 54 Iter 8 subLoss 5236.0 multi 1.00 import weight 0.00
Epoch 54 Iter 9 subLoss 5042.1 multi -1.99 import weight 0.00
Epoch 54 Iter 10 subLoss 5518.8 multi -1.99 import weight 0.00
Epoch 54 Iter 11 subLoss 6963.6 multi 1.00 import weight 0.00
Epoch 54 Acc: 96.50 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 696 train Loss: 5618.6 test Loss: 538.5
Epoch 55 Iter 0 subLoss 5885.8 multi 6.97 import weight 0.00
Epoch 55 Iter 1 subLoss 5442.4 multi 1.00 import weight 0.00
Epoch 55 Iter 2 subLoss 4650.2 multi 1.00 import weight 0.00
Epoch 55 Iter 3 subLoss 5185.9 multi 1.00 import weight 0.00
Epoch 55 Iter 4 subLoss 5362.4 multi 1.00 import weight 0.00
Epoch 55 Iter 5 subLoss 6107.6 multi 3.99 import weight 0.00
Epoch 55 Iter 6 subLoss 5076.5 multi 1.00 import weight 0.00
Epoch 55 Iter 7 subLoss 5000.4 multi 1.00 import weight 0.00
Epoch 55 Iter 8 subLoss 5068.6 multi 1.00 import weight 0.00
Epoch 55 Iter 9 subLoss 5094.2 multi -1.99 import weight 0.00
Epoch 55 Iter 10 subLoss 5102.4 multi -1.99 import weight 0.00
Epoch 55 Iter 11 subLoss 5290.9 multi 1.00 import weight 0.00
Epoch 55 Acc: 97.26 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 529 train Loss: 5251.2 test Loss: 449.9
Epoch 56 Iter 0 subLoss 5280.0 multi 1.00 import weight 0.00
Epoch 56 Iter 1 subLoss 5113.8 multi -1.99 import weight 0.00
Epoch 56 Iter 2 subLoss 5611.2 multi 1.00 import weight 0.00
Epoch 56 Iter 3 subLoss 4872.0 multi 1.00 import weight 0.00
Epoch 56 Iter 4 subLoss 4764.1 multi 1.00 import weight 0.00
Epoch 56 Iter 5 subLoss 4965.6 multi 3.99 import weight 0.00
Epoch 56 Iter 6 subLoss 4947.0 multi 1.00 import weight 0.00
Epoch 56 Iter 7 subLoss 4756.2 multi 1.00 import weight 0.00
Epoch 56 Iter 8 subLoss 5051.2 multi -1.99 import weight 0.00
Epoch 56 Iter 9 subLoss 5203.4 multi 1.00 import weight 0.00
Epoch 56 Iter 10 subLoss 5112.9 multi 1.00 import weight 0.00
Epoch 56 Iter 11 subLoss 5033.5 multi 3.99 import weight 0.00
Epoch 56 Acc: 97.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 503 train Loss: 5076.0 test Loss: 419.4
Epoch 57 Iter 0 subLoss 4791.3 multi 1.00 import weight 0.00
Epoch 57 Iter 1 subLoss 4688.4 multi 1.00 import weight 0.00
Epoch 57 Iter 2 subLoss 5030.9 multi 6.97 import weight 0.00
Epoch 57 Iter 3 subLoss 5813.3 multi -1.99 import weight 0.00
Epoch 57 Iter 4 subLoss 17464.1 multi 1.00 import weight 0.00
Epoch 57 Iter 5 subLoss 8046.1 multi 1.00 import weight 0.00
Epoch 57 Iter 6 subLoss 4543.9 multi 1.00 import weight 0.00
Epoch 57 Iter 7 subLoss 5167.7 multi 3.99 import weight 0.00
Epoch 57 Iter 8 subLoss 4718.8 multi 1.00 import weight 0.00
Epoch 57 Iter 9 subLoss 4267.5 multi 1.00 import weight 0.00
Epoch 57 Iter 10 subLoss 4774.2 multi -1.99 import weight 0.00
Epoch 57 Iter 11 subLoss 4921.7 multi 1.00 import weight 0.00
Epoch 57 Acc: 97.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 492 train Loss: 4693.7 test Loss: 429.3
Epoch 58 Iter 0 subLoss 4109.0 multi 1.00 import weight 0.00
Epoch 58 Iter 1 subLoss 4926.0 multi 3.99 import weight 0.00
Epoch 58 Iter 2 subLoss 4810.0 multi 1.00 import weight 0.00
Epoch 58 Iter 3 subLoss 4600.8 multi 1.00 import weight 0.00
Epoch 58 Iter 4 subLoss 4456.7 multi 1.00 import weight 0.00
Epoch 58 Iter 5 subLoss 4230.5 multi 1.00 import weight 0.00
Epoch 58 Iter 6 subLoss 4627.6 multi 1.00 import weight 0.00
Epoch 58 Iter 7 subLoss 4038.6 multi 1.00 import weight 0.00
Epoch 58 Iter 8 subLoss 4289.0 multi 1.00 import weight 0.00
Epoch 58 Iter 9 subLoss 4467.1 multi -1.99 import weight 0.00
Epoch 58 Iter 10 subLoss 4567.0 multi 1.00 import weight 0.00
Epoch 58 Iter 11 subLoss 4601.6 multi 3.99 import weight 0.00
Epoch 58 Acc: 97.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 460 train Loss: 4643.9 test Loss: 385.9
Epoch 59 Iter 0 subLoss 5149.8 multi 1.00 import weight 0.00
Epoch 59 Iter 1 subLoss 4336.2 multi 1.00 import weight 0.00
Epoch 59 Iter 2 subLoss 4007.1 multi 1.00 import weight 0.00
Epoch 59 Iter 3 subLoss 3967.7 multi 1.00 import weight 0.00
Epoch 59 Iter 4 subLoss 4032.5 multi 3.99 import weight 0.00
Epoch 59 Iter 5 subLoss 4200.0 multi 1.00 import weight 0.00
Epoch 59 Iter 6 subLoss 4770.1 multi 1.00 import weight 0.00
Epoch 59 Iter 7 subLoss 4414.3 multi 1.00 import weight 0.00
Epoch 59 Iter 8 subLoss 3970.9 multi -1.99 import weight 0.00
Epoch 59 Iter 9 subLoss 4302.6 multi 1.00 import weight 0.00
Epoch 59 Iter 10 subLoss 3713.7 multi 1.00 import weight 0.00
Epoch 59 Iter 11 subLoss 4233.9 multi 3.99 import weight 0.00
Epoch 59 Acc: 96.87 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 423 train Loss: 4613.6 test Loss: 471.7
Epoch 60 Iter 0 subLoss 5059.8 multi 1.00 import weight 0.00
Epoch 60 Iter 1 subLoss 3399.9 multi 1.00 import weight 0.00
Epoch 60 Iter 2 subLoss 4094.4 multi 1.00 import weight 0.00
Epoch 60 Iter 3 subLoss 4252.1 multi 1.00 import weight 0.00
Epoch 60 Iter 4 subLoss 4046.2 multi -4.97 import weight 0.00
Epoch 60 Iter 5 subLoss 3947.0 multi 1.00 import weight 0.00
Epoch 60 Iter 6 subLoss 4150.9 multi 1.00 import weight 0.00
Epoch 60 Iter 7 subLoss 3669.3 multi 1.00 import weight 0.00
Epoch 60 Iter 8 subLoss 4127.1 multi 1.00 import weight 0.00
Epoch 60 Iter 9 subLoss 4095.3 multi 3.99 import weight 0.00
Epoch 60 Iter 10 subLoss 4330.9 multi 3.99 import weight 0.00
Epoch 60 Iter 11 subLoss 3888.3 multi 1.00 import weight 0.00
Epoch 60 Acc: 97.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 388 train Loss: 4019.7 test Loss: 345.3
Epoch 61 Iter 0 subLoss 3527.1 multi 1.00 import weight 0.00
Epoch 61 Iter 1 subLoss 4053.5 multi -1.99 import weight 0.00
Epoch 61 Iter 2 subLoss 4517.2 multi 1.00 import weight 0.00
Epoch 61 Iter 3 subLoss 3752.8 multi 1.00 import weight 0.00
Epoch 61 Iter 4 subLoss 3849.6 multi 1.00 import weight 0.00
Epoch 61 Iter 5 subLoss 4561.0 multi 3.99 import weight 0.00
Epoch 61 Iter 6 subLoss 4349.1 multi -4.97 import weight 0.00
Epoch 61 Iter 7 subLoss 6215.6 multi -1.98 import weight 0.00
Epoch 61 Iter 8 subLoss 49845.7 multi 1.00 import weight 0.00
Epoch 61 Iter 9 subLoss 40160.5 multi 1.00 import weight 0.00
Epoch 61 Iter 10 subLoss 18779.6 multi 1.00 import weight 0.00
Epoch 61 Iter 11 subLoss 11657.6 multi 1.00 import weight 0.00
Epoch 61 Acc: 96.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1165 train Loss: 8956.9 test Loss: 783.7
Epoch 62 Iter 0 subLoss 8258.3 multi -4.97 import weight 0.00
Epoch 62 Iter 1 subLoss 19318.8 multi 1.00 import weight 0.00
Epoch 62 Iter 2 subLoss 16767.6 multi 1.00 import weight 0.00
Epoch 62 Iter 3 subLoss 14660.0 multi -1.99 import weight 0.00
Epoch 62 Iter 4 subLoss 17865.5 multi 1.00 import weight 0.00
Epoch 62 Iter 5 subLoss 16676.5 multi -1.99 import weight 0.00
Epoch 62 Iter 6 subLoss 18803.4 multi 1.00 import weight 0.00
Epoch 62 Iter 7 subLoss 17565.6 multi 1.00 import weight 0.00
Epoch 62 Iter 8 subLoss 16079.9 multi 1.00 import weight 0.00
Epoch 62 Iter 9 subLoss 16175.8 multi -4.97 import weight 0.00
Epoch 62 Iter 10 subLoss 21052.3 multi 3.99 import weight 0.00
Epoch 62 Iter 11 subLoss 23873.9 multi 1.00 import weight 0.00
Epoch 62 Acc: 78.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2387 train Loss: 17210.8 test Loss: 2346.6
Epoch 63 Iter 0 subLoss 16745.1 multi 3.99 import weight 0.00
Epoch 63 Iter 1 subLoss 14051.8 multi 3.99 import weight 0.00
Epoch 63 Iter 2 subLoss 13884.3 multi 1.00 import weight 0.00
Epoch 63 Iter 3 subLoss 10695.9 multi -1.99 import weight 0.00
Epoch 63 Iter 4 subLoss 13382.3 multi 1.00 import weight 0.00
Epoch 63 Iter 5 subLoss 12379.7 multi 1.00 import weight 0.00
Epoch 63 Iter 6 subLoss 10462.6 multi -1.99 import weight 0.00
Epoch 63 Iter 7 subLoss 12782.5 multi 1.00 import weight 0.00
Epoch 63 Iter 8 subLoss 11311.3 multi 1.00 import weight 0.00
Epoch 63 Iter 9 subLoss 11756.7 multi 1.00 import weight 0.00
Epoch 63 Iter 10 subLoss 10725.3 multi 1.00 import weight 0.00
Epoch 63 Iter 11 subLoss 9052.8 multi 6.97 import weight 0.00
Epoch 63 Acc: 96.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 905 train Loss: 6828.5 test Loss: 656.2
Epoch 64 Iter 0 subLoss 6878.5 multi 1.00 import weight 0.00
Epoch 64 Iter 1 subLoss 6670.6 multi 3.99 import weight 0.00
Epoch 64 Iter 2 subLoss 9250.6 multi -4.97 import weight 0.00
Epoch 64 Iter 3 subLoss 489530.7 multi 1.00 import weight 0.00
Epoch 64 Iter 4 subLoss 89676.6 multi 1.00 import weight 0.00
Epoch 64 Iter 5 subLoss 47637.0 multi 1.00 import weight 0.00
Epoch 64 Iter 6 subLoss 45920.6 multi 3.99 import weight 0.00
Epoch 64 Iter 7 subLoss 36327.6 multi 3.99 import weight 0.00
Epoch 64 Iter 8 subLoss 28247.0 multi 1.00 import weight 0.00
Epoch 64 Iter 9 subLoss 25140.8 multi 1.00 import weight 0.00
Epoch 64 Iter 10 subLoss 23984.3 multi 1.00 import weight 0.00
Epoch 64 Iter 11 subLoss 22692.5 multi 1.00 import weight 0.00
Epoch 64 Acc: 76.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2269 train Loss: 23081.0 test Loss: 3290.2
Epoch 65 Iter 0 subLoss 22495.2 multi 1.00 import weight 0.00
Epoch 65 Iter 1 subLoss 21984.4 multi 1.00 import weight 0.00
Epoch 65 Iter 2 subLoss 21645.2 multi 1.00 import weight 0.00
Epoch 65 Iter 3 subLoss 20581.9 multi 1.00 import weight 0.00
Epoch 65 Iter 4 subLoss 19458.7 multi 1.00 import weight 0.00
Epoch 65 Iter 5 subLoss 20450.1 multi 1.00 import weight 0.00
Epoch 65 Iter 6 subLoss 19002.8 multi -1.99 import weight 0.00
Epoch 65 Iter 7 subLoss 20486.5 multi 1.00 import weight 0.00
Epoch 65 Iter 8 subLoss 19189.6 multi 1.00 import weight 0.00
Epoch 65 Iter 9 subLoss 19362.9 multi 1.00 import weight 0.00
Epoch 65 Iter 10 subLoss 17832.0 multi 1.00 import weight 0.00
Epoch 65 Iter 11 subLoss 18706.5 multi 1.00 import weight 0.00
Epoch 65 Acc: 87.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1870 train Loss: 17821.8 test Loss: 2456.0
Epoch 66 Iter 0 subLoss 16561.3 multi 1.00 import weight 0.00
Epoch 66 Iter 1 subLoss 16857.9 multi 1.00 import weight 0.00
Epoch 66 Iter 2 subLoss 16317.2 multi 1.00 import weight 0.00
Epoch 66 Iter 3 subLoss 17038.8 multi -1.99 import weight 0.00
Epoch 66 Iter 4 subLoss 17702.1 multi -1.99 import weight 0.00
Epoch 66 Iter 5 subLoss 17907.5 multi 1.00 import weight 0.00
Epoch 66 Iter 6 subLoss 17328.3 multi 1.00 import weight 0.00
Epoch 66 Iter 7 subLoss 17466.4 multi 3.99 import weight 0.00
Epoch 66 Iter 8 subLoss 15399.7 multi 1.00 import weight 0.00
Epoch 66 Iter 9 subLoss 14688.4 multi 1.00 import weight 0.00
Epoch 66 Iter 10 subLoss 14273.4 multi 1.00 import weight 0.00
Epoch 66 Iter 11 subLoss 14403.6 multi 1.00 import weight 0.00
Epoch 66 Acc: 91.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1440 train Loss: 14098.0 test Loss: 1839.7
Epoch 67 Iter 0 subLoss 13717.8 multi 1.00 import weight 0.00
Epoch 67 Iter 1 subLoss 13396.0 multi -1.99 import weight 0.00
Epoch 67 Iter 2 subLoss 14082.5 multi 1.00 import weight 0.00
Epoch 67 Iter 3 subLoss 14042.1 multi -1.99 import weight 0.00
Epoch 67 Iter 4 subLoss 15478.0 multi 3.99 import weight 0.00
Epoch 67 Iter 5 subLoss 13738.0 multi 1.00 import weight 0.00
Epoch 67 Iter 6 subLoss 13275.6 multi 1.00 import weight 0.00
Epoch 67 Iter 7 subLoss 12572.9 multi 1.00 import weight 0.00
Epoch 67 Iter 8 subLoss 12352.6 multi -1.99 import weight 0.00
Epoch 67 Iter 9 subLoss 13160.3 multi 3.99 import weight 0.00
Epoch 67 Iter 10 subLoss 12039.1 multi 1.00 import weight 0.00
Epoch 67 Iter 11 subLoss 11808.1 multi 1.00 import weight 0.00
Epoch 67 Acc: 93.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1180 train Loss: 11598.0 test Loss: 1382.8
Epoch 68 Iter 0 subLoss 11832.7 multi 1.00 import weight 0.00
Epoch 68 Iter 1 subLoss 10895.8 multi 1.00 import weight 0.00
Epoch 68 Iter 2 subLoss 10610.8 multi -7.96 import weight 0.00
Epoch 68 Iter 3 subLoss 13798.0 multi 3.99 import weight 0.00
Epoch 68 Iter 4 subLoss 22425.3 multi 1.00 import weight 0.00
Epoch 68 Iter 5 subLoss 12466.7 multi 1.00 import weight 0.00
Epoch 68 Iter 6 subLoss 12189.8 multi -4.97 import weight 0.00
Epoch 68 Iter 7 subLoss 14525.9 multi 6.97 import weight 0.00
Epoch 68 Iter 8 subLoss 34868.0 multi 1.00 import weight 0.00
Epoch 68 Iter 9 subLoss 13367.3 multi 1.00 import weight 0.00
Epoch 68 Iter 10 subLoss 11801.4 multi 3.99 import weight 0.00
Epoch 68 Iter 11 subLoss 11167.9 multi 3.99 import weight 0.00
Epoch 68 Acc: 93.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1116 train Loss: 12185.2 test Loss: 1652.0
Epoch 69 Iter 0 subLoss 12315.7 multi 3.99 import weight 0.00
Epoch 69 Iter 1 subLoss 18840.6 multi 3.99 import weight 0.00
Epoch 69 Iter 2 subLoss 64261.9 multi 1.00 import weight 0.00
Epoch 69 Iter 3 subLoss 20116.4 multi 1.00 import weight 0.00
Epoch 69 Iter 4 subLoss 20671.9 multi 1.00 import weight 0.00
Epoch 69 Iter 5 subLoss 19309.2 multi 1.00 import weight 0.00
Epoch 69 Iter 6 subLoss 18837.9 multi 1.00 import weight 0.00
Epoch 69 Iter 7 subLoss 18116.4 multi 1.00 import weight 0.00
Epoch 69 Iter 8 subLoss 18586.9 multi 1.00 import weight 0.00
Epoch 69 Iter 9 subLoss 16532.5 multi 1.00 import weight 0.00
Epoch 69 Iter 10 subLoss 17434.1 multi 3.99 import weight 0.00
Epoch 69 Iter 11 subLoss 14483.7 multi 1.00 import weight 0.00
Epoch 69 Acc: 91.61 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1448 train Loss: 14791.5 test Loss: 1980.4
Epoch 70 Iter 0 subLoss 14047.0 multi 1.00 import weight 0.00
Epoch 70 Iter 1 subLoss 13814.4 multi 1.00 import weight 0.00
Epoch 70 Iter 2 subLoss 13631.0 multi 1.00 import weight 0.00
Epoch 70 Iter 3 subLoss 12402.8 multi 1.00 import weight 0.00
Epoch 70 Iter 4 subLoss 12264.9 multi 1.00 import weight 0.00
Epoch 70 Iter 5 subLoss 11714.0 multi 1.00 import weight 0.00
Epoch 70 Iter 6 subLoss 11814.4 multi -4.97 import weight 0.00
Epoch 70 Iter 7 subLoss 13520.2 multi 1.00 import weight 0.00
Epoch 70 Iter 8 subLoss 13521.8 multi 3.99 import weight 0.00
Epoch 70 Iter 9 subLoss 11991.0 multi 1.00 import weight 0.00
Epoch 70 Iter 10 subLoss 11435.7 multi 1.00 import weight 0.00
Epoch 70 Iter 11 subLoss 11482.6 multi 3.99 import weight 0.00
Epoch 70 Acc: 96.44 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1148 train Loss: 9832.0 test Loss: 1034.0
Epoch 71 Iter 0 subLoss 9857.5 multi 1.00 import weight 0.00
Epoch 71 Iter 1 subLoss 9118.3 multi 3.99 import weight 0.00
Epoch 71 Iter 2 subLoss 8446.5 multi 1.00 import weight 0.00
Epoch 71 Iter 3 subLoss 8226.8 multi 1.00 import weight 0.00
Epoch 71 Iter 4 subLoss 8300.6 multi 1.00 import weight 0.00
Epoch 71 Iter 5 subLoss 8353.9 multi 1.00 import weight 0.00
Epoch 71 Iter 6 subLoss 7668.8 multi 1.00 import weight 0.00
Epoch 71 Iter 7 subLoss 7849.2 multi -1.99 import weight 0.00
Epoch 71 Iter 8 subLoss 8283.9 multi 1.00 import weight 0.00
Epoch 71 Iter 9 subLoss 7751.3 multi 1.00 import weight 0.00
Epoch 71 Iter 10 subLoss 7571.3 multi 1.00 import weight 0.00
Epoch 71 Iter 11 subLoss 7464.4 multi -1.99 import weight 0.00
Epoch 71 Acc: 96.93 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 746 train Loss: 7944.2 test Loss: 794.1
Epoch 72 Iter 0 subLoss 7609.7 multi -7.96 import weight 0.00
Epoch 72 Iter 1 subLoss 9850.2 multi 3.99 import weight 0.00
Epoch 72 Iter 2 subLoss 9199.9 multi 1.00 import weight 0.00
Epoch 72 Iter 3 subLoss 8763.4 multi 1.00 import weight 0.00
Epoch 72 Iter 4 subLoss 8312.1 multi -1.99 import weight 0.00
Epoch 72 Iter 5 subLoss 8530.9 multi 1.00 import weight 0.00
Epoch 72 Iter 6 subLoss 7667.1 multi 3.99 import weight 0.00
Epoch 72 Iter 7 subLoss 8003.0 multi 1.00 import weight 0.00
Epoch 72 Iter 8 subLoss 7847.8 multi 1.00 import weight 0.00
Epoch 72 Iter 9 subLoss 7606.2 multi -4.97 import weight 0.00
Epoch 72 Iter 10 subLoss 7875.3 multi 1.00 import weight 0.00
Epoch 72 Iter 11 subLoss 7985.9 multi 1.00 import weight 0.00
Epoch 72 Acc: 97.20 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 798 train Loss: 7981.7 test Loss: 763.2
Epoch 73 Iter 0 subLoss 7313.1 multi -4.97 import weight 0.00
Epoch 73 Iter 1 subLoss 8750.7 multi 1.00 import weight 0.00
Epoch 73 Iter 2 subLoss 8086.1 multi 1.00 import weight 0.00
Epoch 73 Iter 3 subLoss 8014.6 multi -1.99 import weight 0.00
Epoch 73 Iter 4 subLoss 8655.0 multi 3.99 import weight 0.00
Epoch 73 Iter 5 subLoss 7841.0 multi 3.98 import weight 0.00
Epoch 73 Iter 6 subLoss 6919.9 multi 12.94 import weight 1.00
Epoch 73 Iter 7 subLoss 10397.7 multi 1.00 import weight 0.00
Epoch 73 Iter 8 subLoss 6704.9 multi -1.99 import weight 0.00
Epoch 73 Iter 9 subLoss 7446.4 multi -4.97 import weight 0.00
Epoch 73 Iter 10 subLoss 66243.3 multi 1.00 import weight 0.00
Epoch 73 Iter 11 subLoss 15262.1 multi 1.00 import weight 0.00
Epoch 73 Acc: 83.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1526 train Loss: 13103.8 test Loss: 1728.5
Epoch 74 Iter 0 subLoss 12345.0 multi 3.99 import weight 0.00
Epoch 74 Iter 1 subLoss 8742.2 multi 1.00 import weight 0.00
Epoch 74 Iter 2 subLoss 7905.7 multi 3.99 import weight 0.00
Epoch 74 Iter 3 subLoss 7443.4 multi -1.98 import weight 0.00
Epoch 74 Iter 4 subLoss 7649.4 multi -1.99 import weight 0.00
Epoch 74 Iter 5 subLoss 8707.0 multi -1.99 import weight 0.00
Epoch 74 Iter 6 subLoss 10429.5 multi 1.00 import weight 0.00
Epoch 74 Iter 7 subLoss 8335.6 multi 3.99 import weight 0.00
Epoch 74 Iter 8 subLoss 7531.7 multi 1.00 import weight 0.00
Epoch 74 Iter 9 subLoss 7403.7 multi -1.99 import weight 0.00
Epoch 74 Iter 10 subLoss 7490.2 multi 3.99 import weight 0.00
Epoch 74 Iter 11 subLoss 7415.8 multi -1.99 import weight 0.00
Epoch 74 Acc: 96.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 741 train Loss: 7720.4 test Loss: 809.2
Epoch 75 Iter 0 subLoss 7170.6 multi 1.00 import weight 0.00
Epoch 75 Iter 1 subLoss 7039.8 multi -1.99 import weight 0.00
Epoch 75 Iter 2 subLoss 7268.8 multi 1.00 import weight 0.00
Epoch 75 Iter 3 subLoss 7660.6 multi 6.97 import weight 0.00
Epoch 75 Iter 4 subLoss 6930.0 multi 3.99 import weight 0.00
Epoch 75 Iter 5 subLoss 7415.9 multi 1.00 import weight 0.00
Epoch 75 Iter 6 subLoss 6570.8 multi -1.99 import weight 0.00
Epoch 75 Iter 7 subLoss 7593.4 multi 9.96 import weight 0.00
Epoch 75 Iter 8 subLoss 33356.8 multi 1.00 import weight 0.00
Epoch 75 Iter 9 subLoss 7238.8 multi 1.00 import weight 0.00
Epoch 75 Iter 10 subLoss 6609.7 multi -4.97 import weight 0.00
Epoch 75 Iter 11 subLoss 11711.8 multi 3.99 import weight 0.00
Epoch 75 Acc: 86.92 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1171 train Loss: 14640.5 test Loss: 2043.7
Epoch 76 Iter 0 subLoss 14291.6 multi 1.00 import weight 0.00
Epoch 76 Iter 1 subLoss 7628.5 multi 1.00 import weight 0.00
Epoch 76 Iter 2 subLoss 6835.2 multi -1.99 import weight 0.00
Epoch 76 Iter 3 subLoss 7002.7 multi 3.99 import weight 0.00
Epoch 76 Iter 4 subLoss 5972.0 multi -1.99 import weight 0.00
Epoch 76 Iter 5 subLoss 6524.4 multi -1.99 import weight 0.00
Epoch 76 Iter 6 subLoss 7216.0 multi 3.99 import weight 0.00
Epoch 76 Iter 7 subLoss 6202.2 multi 3.98 import weight 0.00
Epoch 76 Iter 8 subLoss 5319.1 multi -1.99 import weight 0.00
Epoch 76 Iter 9 subLoss 6219.3 multi -1.99 import weight 0.00
Epoch 76 Iter 10 subLoss 9090.8 multi 3.99 import weight 0.00
Epoch 76 Iter 11 subLoss 11563.5 multi 1.00 import weight 0.00
Epoch 76 Acc: 97.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1156 train Loss: 6084.9 test Loss: 612.3
Epoch 77 Iter 0 subLoss 6216.7 multi 1.00 import weight 0.00
Epoch 77 Iter 1 subLoss 5487.1 multi -4.97 import weight 0.00
Epoch 77 Iter 2 subLoss 7034.9 multi 1.00 import weight 0.00
Epoch 77 Iter 3 subLoss 6145.1 multi 3.99 import weight 0.00
Epoch 77 Iter 4 subLoss 5821.5 multi -1.99 import weight 0.00
Epoch 77 Iter 5 subLoss 6829.7 multi 3.99 import weight 0.00
Epoch 77 Iter 6 subLoss 6795.3 multi 3.99 import weight 0.00
Epoch 77 Iter 7 subLoss 9273.4 multi 6.97 import weight 0.00
Epoch 77 Iter 8 subLoss 36141.6 multi 1.00 import weight 0.00
Epoch 77 Iter 9 subLoss 11448.8 multi 3.98 import weight 0.00
Epoch 77 Iter 10 subLoss 7893.5 multi -1.99 import weight 0.00
Epoch 77 Iter 11 subLoss 8112.0 multi 3.99 import weight 0.00
Epoch 77 Acc: 97.10 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 811 train Loss: 6487.5 test Loss: 654.5
Epoch 78 Iter 0 subLoss 6334.1 multi -1.99 import weight 0.00
Epoch 78 Iter 1 subLoss 6827.0 multi 6.97 import weight 0.00
Epoch 78 Iter 2 subLoss 6664.2 multi 1.00 import weight 0.00
Epoch 78 Iter 3 subLoss 5455.4 multi -1.99 import weight 0.00
Epoch 78 Iter 4 subLoss 5765.7 multi 3.99 import weight 0.00
Epoch 78 Iter 5 subLoss 6441.3 multi 1.00 import weight 0.00
Epoch 78 Iter 6 subLoss 5592.9 multi -1.99 import weight 0.00
Epoch 78 Iter 7 subLoss 5384.0 multi 1.00 import weight 0.00
Epoch 78 Iter 8 subLoss 5465.3 multi -1.99 import weight 0.00
Epoch 78 Iter 9 subLoss 5703.1 multi 1.00 import weight 0.00
Epoch 78 Iter 10 subLoss 5448.9 multi 3.99 import weight 0.00
Epoch 78 Iter 11 subLoss 5151.8 multi -1.99 import weight 0.00
Epoch 78 Acc: 97.39 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 515 train Loss: 5534.7 test Loss: 513.8
Epoch 79 Iter 0 subLoss 5489.9 multi -1.98 import weight 0.00
Epoch 79 Iter 1 subLoss 5456.3 multi -1.98 import weight 0.00
Epoch 79 Iter 2 subLoss 6579.2 multi 1.00 import weight 0.00
Epoch 79 Iter 3 subLoss 6238.4 multi 1.00 import weight 0.00
Epoch 79 Iter 4 subLoss 5622.6 multi -1.99 import weight 0.00
Epoch 79 Iter 5 subLoss 5878.2 multi 1.00 import weight 0.00
Epoch 79 Iter 6 subLoss 5710.8 multi -1.99 import weight 0.00
Epoch 79 Iter 7 subLoss 5974.4 multi 1.00 import weight 0.00
Epoch 79 Iter 8 subLoss 5884.9 multi 6.97 import weight 0.00
Epoch 79 Iter 9 subLoss 5840.1 multi 1.00 import weight 0.00
Epoch 79 Iter 10 subLoss 5345.9 multi 1.00 import weight 0.00
Epoch 79 Iter 11 subLoss 4862.0 multi -1.99 import weight 0.00
Epoch 79 Acc: 97.61 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 486 train Loss: 5327.3 test Loss: 475.3
Epoch 80 Iter 0 subLoss 5073.8 multi 1.00 import weight 0.00
Epoch 80 Iter 1 subLoss 5048.9 multi -4.97 import weight 0.00
Epoch 80 Iter 2 subLoss 6281.8 multi 3.99 import weight 0.00
Epoch 80 Iter 3 subLoss 5672.2 multi 1.00 import weight 0.00
Epoch 80 Iter 4 subLoss 4936.8 multi -4.97 import weight 0.00
Epoch 80 Iter 5 subLoss 6458.2 multi 1.00 import weight 0.00
Epoch 80 Iter 6 subLoss 5676.9 multi 3.99 import weight 0.00
Epoch 80 Iter 7 subLoss 5034.0 multi 9.96 import weight 0.00
Epoch 80 Iter 8 subLoss 9104.3 multi -4.97 import weight 0.00
Epoch 80 Iter 9 subLoss 119266.7 multi 1.00 import weight 0.00
Epoch 80 Iter 10 subLoss 25981.6 multi 1.00 import weight 0.00
Epoch 80 Iter 11 subLoss 14703.8 multi 1.00 import weight 0.00
Epoch 80 Acc: 83.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1470 train Loss: 14037.1 test Loss: 1973.7
Epoch 81 Iter 0 subLoss 14161.6 multi 3.99 import weight 0.00
Epoch 81 Iter 1 subLoss 10126.1 multi 1.00 import weight 0.00
Epoch 81 Iter 2 subLoss 9177.6 multi 1.00 import weight 0.00
Epoch 81 Iter 3 subLoss 8227.6 multi 3.99 import weight 0.00
Epoch 81 Iter 4 subLoss 6833.8 multi -4.97 import weight 0.00
Epoch 81 Iter 5 subLoss 8424.1 multi 1.00 import weight 0.00
Epoch 81 Iter 6 subLoss 7737.6 multi -1.98 import weight 0.00
Epoch 81 Iter 7 subLoss 8532.3 multi 3.98 import weight 0.00
Epoch 81 Iter 8 subLoss 6707.2 multi 1.00 import weight 0.00
Epoch 81 Iter 9 subLoss 6528.1 multi 1.00 import weight 0.00
Epoch 81 Iter 10 subLoss 6465.4 multi -4.97 import weight 0.00
Epoch 81 Iter 11 subLoss 7239.7 multi 3.99 import weight 0.00
Epoch 81 Acc: 97.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 723 train Loss: 6693.8 test Loss: 627.3
Epoch 82 Iter 0 subLoss 6330.9 multi 1.00 import weight 0.00
Epoch 82 Iter 1 subLoss 6297.5 multi -4.97 import weight 0.00
Epoch 82 Iter 2 subLoss 7439.8 multi 6.97 import weight 0.00
Epoch 82 Iter 3 subLoss 6867.9 multi -1.99 import weight 0.00
Epoch 82 Iter 4 subLoss 9394.6 multi 1.00 import weight 0.00
Epoch 82 Iter 5 subLoss 6868.1 multi 1.00 import weight 0.00
Epoch 82 Iter 6 subLoss 6189.4 multi -4.97 import weight 0.00
Epoch 82 Iter 7 subLoss 8928.7 multi 3.99 import weight 0.00
Epoch 82 Iter 8 subLoss 7533.7 multi 3.99 import weight 0.00
Epoch 82 Iter 9 subLoss 5523.2 multi -1.99 import weight 0.00
Epoch 82 Iter 10 subLoss 6704.6 multi 3.98 import weight 0.00
Epoch 82 Iter 11 subLoss 5602.8 multi -1.99 import weight 0.00
Epoch 82 Acc: 96.81 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 560 train Loss: 6705.2 test Loss: 625.9
Epoch 83 Iter 0 subLoss 6497.9 multi 1.00 import weight 0.00
Epoch 83 Iter 1 subLoss 5991.0 multi 1.00 import weight 0.00
Epoch 83 Iter 2 subLoss 5781.6 multi 1.00 import weight 0.00
Epoch 83 Iter 3 subLoss 5058.5 multi 1.00 import weight 0.00
Epoch 83 Iter 4 subLoss 5183.0 multi 3.99 import weight 0.00
Epoch 83 Iter 5 subLoss 5223.1 multi 1.00 import weight 0.00
Epoch 83 Iter 6 subLoss 5371.8 multi 1.00 import weight 0.00
Epoch 83 Iter 7 subLoss 5075.0 multi 3.98 import weight 0.00
Epoch 83 Iter 8 subLoss 4996.3 multi 1.00 import weight 0.00
Epoch 83 Iter 9 subLoss 4610.6 multi -4.97 import weight 0.00
Epoch 83 Iter 10 subLoss 5693.7 multi -4.97 import weight 0.00
Epoch 83 Iter 11 subLoss 8329.2 multi -1.99 import weight 0.00
Epoch 83 Acc: 66.80 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 832 train Loss: 64513.9 test Loss: 13871.6
Epoch 84 Iter 0 subLoss 69035.4 multi 1.00 import weight 0.00
Epoch 84 Iter 1 subLoss 15337.3 multi 1.00 import weight 0.00
Epoch 84 Iter 2 subLoss 7315.7 multi -1.98 import weight 0.00
Epoch 84 Iter 3 subLoss 11545.9 multi 3.99 import weight 0.00
Epoch 84 Iter 4 subLoss 7150.5 multi 1.00 import weight 0.00
Epoch 84 Iter 5 subLoss 7109.2 multi 1.00 import weight 0.00
Epoch 84 Iter 6 subLoss 6012.9 multi 1.00 import weight 0.00
Epoch 84 Iter 7 subLoss 5681.3 multi 1.00 import weight 0.00
Epoch 84 Iter 8 subLoss 5554.1 multi -1.99 import weight 0.00
Epoch 84 Iter 9 subLoss 5895.1 multi -10.94 import weight 0.00
Epoch 84 Iter 10 subLoss 11553.1 multi -4.97 import weight 0.00
Epoch 84 Iter 11 subLoss 128013.5 multi 1.00 import weight 0.00
Epoch 84 Acc: 71.26 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 12801 train Loss: 26143.7 test Loss: 5112.5
Epoch 85 Iter 0 subLoss 25949.8 multi 1.00 import weight 0.00
Epoch 85 Iter 1 subLoss 19793.1 multi 1.00 import weight 0.00
Epoch 85 Iter 2 subLoss 16331.2 multi -1.99 import weight 0.00
Epoch 85 Iter 3 subLoss 22600.0 multi 1.00 import weight 0.00
Epoch 85 Iter 4 subLoss 19378.8 multi -1.99 import weight 0.00
Epoch 85 Iter 5 subLoss 25450.7 multi 1.00 import weight 0.00
Epoch 85 Iter 6 subLoss 22529.2 multi 1.00 import weight 0.00
Epoch 85 Iter 7 subLoss 19986.7 multi 1.00 import weight 0.00
Epoch 85 Iter 8 subLoss 17674.9 multi 1.00 import weight 0.00
Epoch 85 Iter 9 subLoss 15369.0 multi 1.00 import weight 0.00
Epoch 85 Iter 10 subLoss 13428.8 multi 1.00 import weight 0.00
Epoch 85 Iter 11 subLoss 12220.1 multi 1.00 import weight 0.00
Epoch 85 Acc: 92.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1222 train Loss: 10642.2 test Loss: 1652.9
Epoch 86 Iter 0 subLoss 10861.2 multi -1.99 import weight 0.00
Epoch 86 Iter 1 subLoss 12900.3 multi 1.00 import weight 0.00
Epoch 86 Iter 2 subLoss 11577.1 multi -1.99 import weight 0.00
Epoch 86 Iter 3 subLoss 14418.3 multi -1.99 import weight 0.00
Epoch 86 Iter 4 subLoss 18002.6 multi 1.00 import weight 0.00
Epoch 86 Iter 5 subLoss 15806.6 multi 1.00 import weight 0.00
Epoch 86 Iter 6 subLoss 13565.5 multi 1.00 import weight 0.00
Epoch 86 Iter 7 subLoss 12938.8 multi 1.00 import weight 0.00
Epoch 86 Iter 8 subLoss 11951.1 multi 1.00 import weight 0.00
Epoch 86 Iter 9 subLoss 10467.2 multi 1.00 import weight 0.00
Epoch 86 Iter 10 subLoss 9587.5 multi -1.98 import weight 0.00
Epoch 86 Iter 11 subLoss 11382.8 multi 1.00 import weight 0.00
Epoch 86 Acc: 92.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1138 train Loss: 10674.8 test Loss: 1600.5
Epoch 87 Iter 0 subLoss 10749.1 multi -1.99 import weight 0.00
Epoch 87 Iter 1 subLoss 12387.9 multi -4.97 import weight 0.00
Epoch 87 Iter 2 subLoss 18704.3 multi 3.99 import weight 0.00
Epoch 87 Iter 3 subLoss 13960.5 multi 1.00 import weight 0.00
Epoch 87 Iter 4 subLoss 12267.7 multi 3.99 import weight 0.00
Epoch 87 Iter 5 subLoss 9400.9 multi -1.99 import weight 0.00
Epoch 87 Iter 6 subLoss 11107.7 multi -1.98 import weight 0.00
Epoch 87 Iter 7 subLoss 11663.2 multi -1.99 import weight 0.00
Epoch 87 Iter 8 subLoss 13949.0 multi 1.00 import weight 0.00
Epoch 87 Iter 9 subLoss 13036.5 multi 3.99 import weight 0.00
Epoch 87 Iter 10 subLoss 9991.2 multi 1.00 import weight 0.00
Epoch 87 Iter 11 subLoss 9655.3 multi -1.99 import weight 0.00
Epoch 87 Acc: 89.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 965 train Loss: 10920.4 test Loss: 1642.3
Epoch 88 Iter 0 subLoss 10323.8 multi 1.00 import weight 0.00
Epoch 88 Iter 1 subLoss 10285.4 multi 3.99 import weight 0.00
Epoch 88 Iter 2 subLoss 7619.3 multi -1.98 import weight 0.00
Epoch 88 Iter 3 subLoss 8779.3 multi -1.99 import weight 0.00
Epoch 88 Iter 4 subLoss 9628.7 multi 1.00 import weight 0.00
Epoch 88 Iter 5 subLoss 9425.7 multi 1.00 import weight 0.00
Epoch 88 Iter 6 subLoss 8844.0 multi 3.99 import weight 0.00
Epoch 88 Iter 7 subLoss 7062.6 multi 3.99 import weight 0.00
Epoch 88 Iter 8 subLoss 6767.5 multi 1.00 import weight 0.00
Epoch 88 Iter 9 subLoss 5779.4 multi -4.97 import weight 0.00
Epoch 88 Iter 10 subLoss 7000.7 multi 6.97 import weight 0.00
Epoch 88 Iter 11 subLoss 6763.6 multi 3.99 import weight 0.00
Epoch 88 Acc: 93.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 676 train Loss: 7394.4 test Loss: 831.3
Epoch 89 Iter 0 subLoss 7335.4 multi 1.00 import weight 0.00
Epoch 89 Iter 1 subLoss 5723.7 multi -1.99 import weight 0.00
Epoch 89 Iter 2 subLoss 6034.7 multi 1.00 import weight 0.00
Epoch 89 Iter 3 subLoss 5904.0 multi -1.99 import weight 0.00
Epoch 89 Iter 4 subLoss 7109.9 multi 3.98 import weight 0.00
Epoch 89 Iter 5 subLoss 7043.0 multi -4.97 import weight 0.00
Epoch 89 Iter 6 subLoss 32828.7 multi 1.00 import weight 0.00
Epoch 89 Iter 7 subLoss 9070.7 multi 1.00 import weight 0.00
Epoch 89 Iter 8 subLoss 6987.7 multi 3.98 import weight 0.00
Epoch 89 Iter 9 subLoss 5934.6 multi -4.97 import weight 0.00
Epoch 89 Iter 10 subLoss 6972.3 multi -1.98 import weight 0.00
Epoch 89 Iter 11 subLoss 9706.2 multi 1.00 import weight 0.00
Epoch 89 Acc: 96.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 970 train Loss: 7159.9 test Loss: 779.5
Epoch 90 Iter 0 subLoss 6889.0 multi 1.00 import weight 0.00
Epoch 90 Iter 1 subLoss 6424.5 multi 1.00 import weight 0.00
Epoch 90 Iter 2 subLoss 6020.3 multi 1.00 import weight 0.00
Epoch 90 Iter 3 subLoss 6147.5 multi 6.97 import weight 0.00
Epoch 90 Iter 4 subLoss 5503.6 multi 1.00 import weight 0.00
Epoch 90 Iter 5 subLoss 5457.3 multi 1.00 import weight 0.00
Epoch 90 Iter 6 subLoss 5278.9 multi 3.99 import weight 0.00
Epoch 90 Iter 7 subLoss 5314.8 multi 1.00 import weight 0.00
Epoch 90 Iter 8 subLoss 4791.4 multi 3.99 import weight 0.00
Epoch 90 Iter 9 subLoss 4956.9 multi -1.99 import weight 0.00
Epoch 90 Iter 10 subLoss 5313.4 multi 3.98 import weight 0.00
Epoch 90 Iter 11 subLoss 4740.3 multi 1.00 import weight 0.00
Epoch 90 Acc: 97.18 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 474 train Loss: 4843.7 test Loss: 477.0
Epoch 91 Iter 0 subLoss 4727.2 multi -1.99 import weight 0.00
Epoch 91 Iter 1 subLoss 4360.9 multi 1.00 import weight 0.00
Epoch 91 Iter 2 subLoss 4900.7 multi 1.00 import weight 0.00
Epoch 91 Iter 3 subLoss 5382.1 multi 1.00 import weight 0.00
Epoch 91 Iter 4 subLoss 4750.9 multi 1.00 import weight 0.00
Epoch 91 Iter 5 subLoss 4525.4 multi -1.99 import weight 0.00
Epoch 91 Iter 6 subLoss 4336.9 multi 6.97 import weight 0.00
Epoch 91 Iter 7 subLoss 4735.1 multi -1.99 import weight 0.00
Epoch 91 Iter 8 subLoss 4957.8 multi 1.00 import weight 0.00
Epoch 91 Iter 9 subLoss 5092.2 multi 1.00 import weight 0.00
Epoch 91 Iter 10 subLoss 4271.9 multi -1.99 import weight 0.00
Epoch 91 Iter 11 subLoss 4411.6 multi 3.99 import weight 0.00
Epoch 91 Acc: 97.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 441 train Loss: 4894.5 test Loss: 475.8
Epoch 92 Iter 0 subLoss 5195.9 multi -4.97 import weight 0.00
Epoch 92 Iter 1 subLoss 6949.1 multi -4.97 import weight 0.00
Epoch 92 Iter 2 subLoss 51623.3 multi 1.00 import weight 0.00
Epoch 92 Iter 3 subLoss 12418.4 multi -1.99 import weight 0.00
Epoch 92 Iter 4 subLoss 46243.8 multi 1.00 import weight 0.00
Epoch 92 Iter 5 subLoss 7385.1 multi 1.00 import weight 0.00
Epoch 92 Iter 6 subLoss 6837.1 multi -1.99 import weight 0.00
Epoch 92 Iter 7 subLoss 7982.6 multi 3.99 import weight 0.00
Epoch 92 Iter 8 subLoss 5852.1 multi 6.97 import weight 0.00
Epoch 92 Iter 9 subLoss 5236.0 multi 1.00 import weight 0.00
Epoch 92 Iter 10 subLoss 4665.4 multi -1.99 import weight 0.00
Epoch 92 Iter 11 subLoss 5394.1 multi -7.96 import weight 0.00
Epoch 92 Acc: 95.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 539 train Loss: 7757.9 test Loss: 849.0
Epoch 93 Iter 0 subLoss 7676.4 multi -7.96 import weight 0.00
Epoch 93 Iter 1 subLoss 78444.8 multi 1.00 import weight 0.00
Epoch 93 Iter 2 subLoss 13312.7 multi 1.00 import weight 0.00
Epoch 93 Iter 3 subLoss 11387.6 multi 3.99 import weight 0.00
Epoch 93 Iter 4 subLoss 7405.0 multi 1.00 import weight 0.00
Epoch 93 Iter 5 subLoss 6983.5 multi 3.99 import weight 0.00
Epoch 93 Iter 6 subLoss 6257.9 multi 3.99 import weight 0.00
Epoch 93 Iter 7 subLoss 5865.0 multi -10.94 import weight 0.00
Epoch 93 Iter 8 subLoss 8083.9 multi 3.99 import weight 0.00
Epoch 93 Iter 9 subLoss 6928.9 multi -13.93 import weight 0.00
Epoch 93 Iter 10 subLoss 14239.8 multi 1.00 import weight 0.00
Epoch 93 Iter 11 subLoss 10792.1 multi -1.98 import weight 0.00
Epoch 93 Acc: 87.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 1079 train Loss: 15217.7 test Loss: 1984.0
Epoch 94 Iter 0 subLoss 14835.6 multi 1.00 import weight 0.00
Epoch 94 Iter 1 subLoss 11752.7 multi 3.99 import weight 0.00
Epoch 94 Iter 2 subLoss 8319.6 multi 1.00 import weight 0.00
Epoch 94 Iter 3 subLoss 7624.8 multi 1.00 import weight 0.00
Epoch 94 Iter 4 subLoss 7552.6 multi 3.99 import weight 0.00
Epoch 94 Iter 5 subLoss 6938.8 multi 3.98 import weight 0.00
Epoch 94 Iter 6 subLoss 6529.7 multi 3.98 import weight 0.00
Epoch 94 Iter 7 subLoss 6019.1 multi 3.99 import weight 0.00
Epoch 94 Iter 8 subLoss 5382.6 multi 3.99 import weight 0.00
Epoch 94 Iter 9 subLoss 5506.0 multi 3.98 import weight 0.00
Epoch 94 Iter 10 subLoss 5016.6 multi 1.00 import weight 0.00
Epoch 94 Iter 11 subLoss 5122.7 multi -4.97 import weight 0.00
Epoch 94 Acc: 96.85 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 512 train Loss: 5498.8 test Loss: 535.3
Epoch 95 Iter 0 subLoss 5267.3 multi -1.99 import weight 0.00
Epoch 95 Iter 1 subLoss 5563.6 multi -1.99 import weight 0.00
Epoch 95 Iter 2 subLoss 6001.0 multi -4.97 import weight 0.00
Epoch 95 Iter 3 subLoss 8469.4 multi 6.97 import weight 0.00
Epoch 95 Iter 4 subLoss 13752.6 multi 1.00 import weight 0.00
Epoch 95 Iter 5 subLoss 8164.7 multi -1.99 import weight 0.00
Epoch 95 Iter 6 subLoss 11517.0 multi 1.00 import weight 0.00
Epoch 95 Iter 7 subLoss 7998.3 multi -4.97 import weight 0.00
Epoch 95 Iter 8 subLoss 23795.2 multi 1.00 import weight 0.00
Epoch 95 Iter 9 subLoss 9621.9 multi 3.99 import weight 0.00
Epoch 95 Iter 10 subLoss 6125.5 multi 1.00 import weight 0.00
Epoch 95 Iter 11 subLoss 6767.0 multi 6.97 import weight 0.00
Epoch 95 Acc: 97.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 676 train Loss: 5824.6 test Loss: 541.7
Epoch 96 Iter 0 subLoss 6318.6 multi 1.00 import weight 0.00
Epoch 96 Iter 1 subLoss 5323.9 multi -7.96 import weight 0.00
Epoch 96 Iter 2 subLoss 6456.5 multi 3.98 import weight 0.00
Epoch 96 Iter 3 subLoss 5445.4 multi 6.97 import weight 0.00
Epoch 96 Iter 4 subLoss 5014.2 multi 3.98 import weight 0.00
Epoch 96 Iter 5 subLoss 5361.4 multi 3.99 import weight 0.00
Epoch 96 Iter 6 subLoss 4817.8 multi 3.99 import weight 0.00
Epoch 96 Iter 7 subLoss 4591.0 multi 1.00 import weight 0.00
Epoch 96 Iter 8 subLoss 4759.0 multi 3.98 import weight 0.00
Epoch 96 Iter 9 subLoss 4825.6 multi -4.97 import weight 0.00
Epoch 96 Iter 10 subLoss 5038.8 multi 12.94 import weight 1.00
Epoch 96 Iter 11 subLoss 7049.5 multi -1.98 import weight 0.00
Epoch 96 Acc: 90.91 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 704 train Loss: 15024.7 test Loss: 1825.3
Epoch 97 Iter 0 subLoss 15281.8 multi 1.00 import weight 0.00
Epoch 97 Iter 1 subLoss 6124.4 multi 3.99 import weight 0.00
Epoch 97 Iter 2 subLoss 4941.0 multi 1.00 import weight 0.00
Epoch 97 Iter 3 subLoss 4557.3 multi -1.99 import weight 0.00
Epoch 97 Iter 4 subLoss 5036.0 multi 15.93 import weight 1.00
Epoch 97 Iter 5 subLoss 23446.1 multi 1.00 import weight 0.00
Epoch 97 Iter 6 subLoss 6493.2 multi 3.99 import weight 0.00
Epoch 97 Iter 7 subLoss 4247.6 multi -4.97 import weight 0.00
Epoch 97 Iter 8 subLoss 6020.0 multi 1.00 import weight 0.00
Epoch 97 Iter 9 subLoss 5746.9 multi 1.00 import weight 0.00
Epoch 97 Iter 10 subLoss 4673.7 multi -1.99 import weight 0.00
Epoch 97 Iter 11 subLoss 5385.6 multi 6.97 import weight 0.00
Epoch 97 Acc: 94.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 538 train Loss: 6580.6 test Loss: 770.7
Epoch 98 Iter 0 subLoss 6370.5 multi 3.99 import weight 0.00
Epoch 98 Iter 1 subLoss 4946.4 multi 3.98 import weight 0.00
Epoch 98 Iter 2 subLoss 4955.2 multi -1.99 import weight 0.00
Epoch 98 Iter 3 subLoss 5034.9 multi 18.91 import weight 1.00
Epoch 98 Iter 4 subLoss 49363.6 multi 1.00 import weight 0.00
Epoch 98 Iter 5 subLoss 9604.1 multi 1.00 import weight 0.00
Epoch 98 Iter 6 subLoss 8488.7 multi -1.99 import weight 0.00
Epoch 98 Iter 7 subLoss 10883.1 multi 1.00 import weight 0.00
Epoch 98 Iter 8 subLoss 9054.6 multi 9.96 import weight 0.00
Epoch 98 Iter 9 subLoss 7788.9 multi 1.00 import weight 0.00
Epoch 98 Iter 10 subLoss 5365.5 multi 6.97 import weight 0.00
Epoch 98 Iter 11 subLoss 5656.0 multi 1.00 import weight 0.00
Epoch 98 Acc: 96.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 565 train Loss: 4928.4 test Loss: 548.4
Epoch 99 Iter 0 subLoss 4903.1 multi 3.99 import weight 0.00
Epoch 99 Iter 1 subLoss 3822.0 multi 1.00 import weight 0.00
Epoch 99 Iter 2 subLoss 4350.4 multi -1.99 import weight 0.00
Epoch 99 Iter 3 subLoss 4023.5 multi 1.00 import weight 0.00
Epoch 99 Iter 4 subLoss 4337.1 multi 9.96 import weight 0.00
Epoch 99 Iter 5 subLoss 4016.0 multi -1.99 import weight 0.00
Epoch 99 Iter 6 subLoss 5114.9 multi 3.98 import weight 0.00
Epoch 99 Iter 7 subLoss 4365.1 multi 1.00 import weight 0.00
Epoch 99 Iter 8 subLoss 3948.4 multi 3.99 import weight 0.00
Epoch 99 Iter 9 subLoss 4163.2 multi -1.99 import weight 0.00
Epoch 99 Iter 10 subLoss 3476.4 multi 1.00 import weight 0.00
Epoch 99 Iter 11 subLoss 4128.5 multi 3.99 import weight 0.00
Epoch 99 Acc: 97.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 412 train Loss: 3853.4 test Loss: 371.0
Epoch 100 Iter 0 subLoss 3990.1 multi 1.00 import weight 0.00
Epoch 100 Iter 1 subLoss 3504.4 multi 1.00 import weight 0.00
Epoch 100 Iter 2 subLoss 3934.5 multi 1.00 import weight 0.00
Epoch 100 Iter 3 subLoss 3816.7 multi 1.00 import weight 0.00
Epoch 100 Iter 4 subLoss 3716.3 multi 3.99 import weight 0.00
Epoch 100 Iter 5 subLoss 3180.4 multi 1.00 import weight 0.00
Epoch 100 Iter 6 subLoss 3376.3 multi 1.00 import weight 0.00
Epoch 100 Iter 7 subLoss 3850.4 multi -1.99 import weight 0.00
Epoch 100 Iter 8 subLoss 3723.8 multi -4.97 import weight 0.00
Epoch 100 Iter 9 subLoss 3918.3 multi 1.00 import weight 0.00
Epoch 100 Iter 10 subLoss 3486.2 multi -1.99 import weight 0.00
Epoch 100 Iter 11 subLoss 4486.2 multi 1.00 import weight 0.00
Epoch 100 Acc: 97.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 448 train Loss: 3982.1 test Loss: 398.5
Epoch 101 Iter 0 subLoss 4090.4 multi 6.97 import weight 0.00
Epoch 101 Iter 1 subLoss 3740.1 multi 1.00 import weight 0.00
Epoch 101 Iter 2 subLoss 3594.1 multi 1.00 import weight 0.00
Epoch 101 Iter 3 subLoss 3922.2 multi -1.99 import weight 0.00
Epoch 101 Iter 4 subLoss 3605.6 multi -1.99 import weight 0.00
Epoch 101 Iter 5 subLoss 3190.0 multi -1.99 import weight 0.00
Epoch 101 Iter 6 subLoss 4024.7 multi 1.00 import weight 0.00
Epoch 101 Iter 7 subLoss 3873.9 multi 1.00 import weight 0.00
Epoch 101 Iter 8 subLoss 3308.0 multi 1.00 import weight 0.00
Epoch 101 Iter 9 subLoss 3767.7 multi -1.99 import weight 0.00
Epoch 101 Iter 10 subLoss 4277.8 multi 1.00 import weight 0.00
Epoch 101 Iter 11 subLoss 4181.9 multi 1.00 import weight 0.00
Epoch 101 Acc: 97.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 418 train Loss: 3728.3 test Loss: 370.4
Epoch 102 Iter 0 subLoss 4069.4 multi -1.99 import weight 0.00
Epoch 102 Iter 1 subLoss 3620.3 multi 1.00 import weight 0.00
Epoch 102 Iter 2 subLoss 3430.1 multi 1.00 import weight 0.00
Epoch 102 Iter 3 subLoss 3873.3 multi 3.99 import weight 0.00
Epoch 102 Iter 4 subLoss 3643.2 multi 1.00 import weight 0.00
Epoch 102 Iter 5 subLoss 3591.9 multi 3.99 import weight 0.00
Epoch 102 Iter 6 subLoss 3842.8 multi 3.99 import weight 0.00
Epoch 102 Iter 7 subLoss 3456.0 multi 1.00 import weight 0.00
Epoch 102 Iter 8 subLoss 3310.8 multi -1.99 import weight 0.00
Epoch 102 Iter 9 subLoss 3722.7 multi -1.98 import weight 0.00
Epoch 102 Iter 10 subLoss 3887.3 multi -1.98 import weight 0.00
Epoch 102 Iter 11 subLoss 3800.7 multi 1.00 import weight 0.00
Epoch 102 Acc: 97.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 380 train Loss: 3579.5 test Loss: 355.4
Epoch 103 Iter 0 subLoss 3572.6 multi 1.00 import weight 0.00
Epoch 103 Iter 1 subLoss 3141.7 multi 1.00 import weight 0.00
Epoch 103 Iter 2 subLoss 3682.4 multi 1.00 import weight 0.00
Epoch 103 Iter 3 subLoss 3136.3 multi 1.00 import weight 0.00
Epoch 103 Iter 4 subLoss 3699.4 multi -1.99 import weight 0.00
Epoch 103 Iter 5 subLoss 3770.3 multi -1.99 import weight 0.00
Epoch 103 Iter 6 subLoss 3269.5 multi 1.00 import weight 0.00
Epoch 103 Iter 7 subLoss 3017.3 multi 1.00 import weight 0.00
Epoch 103 Iter 8 subLoss 3679.8 multi -1.99 import weight 0.00
Epoch 103 Iter 9 subLoss 4173.8 multi -1.99 import weight 0.00
Epoch 103 Iter 10 subLoss 4257.3 multi 1.00 import weight 0.00
Epoch 103 Iter 11 subLoss 3407.6 multi -1.99 import weight 0.00
Epoch 103 Acc: 97.57 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 340 train Loss: 3845.9 test Loss: 388.0
Epoch 104 Iter 0 subLoss 4510.2 multi 3.99 import weight 0.00
Epoch 104 Iter 1 subLoss 3750.8 multi 1.00 import weight 0.00
Epoch 104 Iter 2 subLoss 3755.1 multi 3.98 import weight 0.00
Epoch 104 Iter 3 subLoss 2997.6 multi 1.00 import weight 0.00
Epoch 104 Iter 4 subLoss 3853.2 multi -1.98 import weight 0.00
Epoch 104 Iter 5 subLoss 3243.3 multi 1.00 import weight 0.00
Epoch 104 Iter 6 subLoss 3516.5 multi -1.99 import weight 0.00
Epoch 104 Iter 7 subLoss 3976.9 multi 1.00 import weight 0.00
Epoch 104 Iter 8 subLoss 4037.9 multi 1.00 import weight 0.00
Epoch 104 Iter 9 subLoss 3417.3 multi -1.99 import weight 0.00
Epoch 104 Iter 10 subLoss 3411.6 multi 1.00 import weight 0.00
Epoch 104 Iter 11 subLoss 3744.9 multi 3.99 import weight 0.00
Epoch 104 Acc: 97.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 374 train Loss: 3552.1 test Loss: 344.5
Epoch 105 Iter 0 subLoss 3549.9 multi 1.00 import weight 0.00
Epoch 105 Iter 1 subLoss 3176.2 multi 1.00 import weight 0.00
Epoch 105 Iter 2 subLoss 3355.8 multi 1.00 import weight 0.00
Epoch 105 Iter 3 subLoss 3534.4 multi -1.99 import weight 0.00
Epoch 105 Iter 4 subLoss 3376.9 multi 3.99 import weight 0.00
Epoch 105 Iter 5 subLoss 3093.7 multi 1.00 import weight 0.00
Epoch 105 Iter 6 subLoss 3903.8 multi 1.00 import weight 0.00
Epoch 105 Iter 7 subLoss 2913.0 multi 1.00 import weight 0.00
Epoch 105 Iter 8 subLoss 3524.1 multi 1.00 import weight 0.00
Epoch 105 Iter 9 subLoss 3649.0 multi 3.99 import weight 0.00
Epoch 105 Iter 10 subLoss 3641.2 multi 6.97 import weight 0.00
Epoch 105 Iter 11 subLoss 3106.6 multi -1.99 import weight 0.00
Epoch 105 Acc: 97.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 310 train Loss: 3482.5 test Loss: 333.5
Epoch 106 Iter 0 subLoss 3174.4 multi 3.99 import weight 0.00
Epoch 106 Iter 1 subLoss 3494.8 multi -1.99 import weight 0.00
Epoch 106 Iter 2 subLoss 3172.1 multi 6.97 import weight 0.00
Epoch 106 Iter 3 subLoss 3837.3 multi -1.99 import weight 0.00
Epoch 106 Iter 4 subLoss 4602.0 multi 3.98 import weight 0.00
Epoch 106 Iter 5 subLoss 3148.0 multi 1.00 import weight 0.00
Epoch 106 Iter 6 subLoss 3830.7 multi 1.00 import weight 0.00
Epoch 106 Iter 7 subLoss 2912.6 multi 3.99 import weight 0.00
Epoch 106 Iter 8 subLoss 2871.0 multi 1.00 import weight 0.00
Epoch 106 Iter 9 subLoss 3309.2 multi 3.99 import weight 0.00
Epoch 106 Iter 10 subLoss 2870.8 multi 3.99 import weight 0.00
Epoch 106 Iter 11 subLoss 3238.7 multi 1.00 import weight 0.00
Epoch 106 Acc: 98.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 323 train Loss: 3171.6 test Loss: 302.5
Epoch 107 Iter 0 subLoss 3166.3 multi 1.00 import weight 0.00
Epoch 107 Iter 1 subLoss 2833.3 multi 1.00 import weight 0.00
Epoch 107 Iter 2 subLoss 2992.0 multi 3.99 import weight 0.00
Epoch 107 Iter 3 subLoss 3088.1 multi 1.00 import weight 0.00
Epoch 107 Iter 4 subLoss 3450.2 multi 3.99 import weight 0.00
Epoch 107 Iter 5 subLoss 3238.7 multi 3.99 import weight 0.00
Epoch 107 Iter 6 subLoss 3172.2 multi 6.97 import weight 0.00
Epoch 107 Iter 7 subLoss 3534.9 multi -1.98 import weight 0.00
Epoch 107 Iter 8 subLoss 3602.9 multi -1.98 import weight 0.00
Epoch 107 Iter 9 subLoss 4753.0 multi 6.97 import weight 0.00
Epoch 107 Iter 10 subLoss 13429.8 multi 3.99 import weight 0.00
Epoch 107 Iter 11 subLoss 26937.3 multi 1.00 import weight 0.00
Epoch 107 Acc: 95.49 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2693 train Loss: 6897.8 test Loss: 748.8
Epoch 108 Iter 0 subLoss 6964.0 multi 3.98 import weight 0.00
Epoch 108 Iter 1 subLoss 3565.1 multi 1.00 import weight 0.00
Epoch 108 Iter 2 subLoss 3932.2 multi 1.00 import weight 0.00
Epoch 108 Iter 3 subLoss 3698.6 multi 1.00 import weight 0.00
Epoch 108 Iter 4 subLoss 3602.9 multi 1.00 import weight 0.00
Epoch 108 Iter 5 subLoss 3715.3 multi 6.97 import weight 0.00
Epoch 108 Iter 6 subLoss 3329.2 multi -1.99 import weight 0.00
Epoch 108 Iter 7 subLoss 3620.4 multi 3.99 import weight 0.00
Epoch 108 Iter 8 subLoss 3117.0 multi -1.99 import weight 0.00
Epoch 108 Iter 9 subLoss 3916.0 multi 1.00 import weight 0.00
Epoch 108 Iter 10 subLoss 3233.1 multi 6.97 import weight 0.00
Epoch 108 Iter 11 subLoss 3003.6 multi -4.97 import weight 0.00
Epoch 108 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 300 train Loss: 3535.3 test Loss: 322.0
Epoch 109 Iter 0 subLoss 3485.5 multi 1.00 import weight 0.00
Epoch 109 Iter 1 subLoss 3224.7 multi 1.00 import weight 0.00
Epoch 109 Iter 2 subLoss 2908.6 multi 1.00 import weight 0.00
Epoch 109 Iter 3 subLoss 3056.1 multi 1.00 import weight 0.00
Epoch 109 Iter 4 subLoss 3123.6 multi -1.99 import weight 0.00
Epoch 109 Iter 5 subLoss 3866.6 multi -4.97 import weight 0.00
Epoch 109 Iter 6 subLoss 3658.7 multi -7.96 import weight 0.00
Epoch 109 Iter 7 subLoss 7131.8 multi 1.00 import weight 0.00
Epoch 109 Iter 8 subLoss 4132.9 multi -4.97 import weight 0.00
Epoch 109 Iter 9 subLoss 9757.6 multi 1.00 import weight 0.00
Epoch 109 Iter 10 subLoss 4508.6 multi 1.00 import weight 0.00
Epoch 109 Iter 11 subLoss 3982.7 multi -4.97 import weight 0.00
Epoch 109 Acc: 95.43 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 398 train Loss: 7577.3 test Loss: 736.3
Epoch 110 Iter 0 subLoss 6903.0 multi 1.00 import weight 0.00
Epoch 110 Iter 1 subLoss 4946.3 multi 6.97 import weight 0.00
Epoch 110 Iter 2 subLoss 4541.9 multi 3.99 import weight 0.00
Epoch 110 Iter 3 subLoss 4243.6 multi -1.98 import weight 0.00
Epoch 110 Iter 4 subLoss 3847.8 multi 1.00 import weight 0.00
Epoch 110 Iter 5 subLoss 3822.6 multi 1.00 import weight 0.00
Epoch 110 Iter 6 subLoss 3836.8 multi 1.00 import weight 0.00
Epoch 110 Iter 7 subLoss 3145.3 multi 3.98 import weight 0.00
Epoch 110 Iter 8 subLoss 3518.0 multi 1.00 import weight 0.00
Epoch 110 Iter 9 subLoss 3346.2 multi 1.00 import weight 0.00
Epoch 110 Iter 10 subLoss 2823.8 multi 1.00 import weight 0.00
Epoch 110 Iter 11 subLoss 3147.6 multi 6.97 import weight 0.00
Epoch 110 Acc: 97.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 314 train Loss: 3287.7 test Loss: 330.2
Epoch 111 Iter 0 subLoss 3300.8 multi 6.97 import weight 0.00
Epoch 111 Iter 1 subLoss 3503.8 multi 1.00 import weight 0.00
Epoch 111 Iter 2 subLoss 3402.0 multi 1.00 import weight 0.00
Epoch 111 Iter 3 subLoss 3189.8 multi -7.96 import weight 0.00
Epoch 111 Iter 4 subLoss 3623.5 multi 6.97 import weight 0.00
Epoch 111 Iter 5 subLoss 3582.0 multi -1.99 import weight 0.00
Epoch 111 Iter 6 subLoss 3216.0 multi 1.00 import weight 0.00
Epoch 111 Iter 7 subLoss 3179.5 multi 9.96 import weight 0.00
Epoch 111 Iter 8 subLoss 3465.9 multi -4.97 import weight 0.00
Epoch 111 Iter 9 subLoss 4410.5 multi 6.97 import weight 0.00
Epoch 111 Iter 10 subLoss 7138.9 multi 3.99 import weight 0.00
Epoch 111 Iter 11 subLoss 9891.8 multi 6.97 import weight 0.00
Epoch 111 Acc: 63.01 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 989 train Loss: 32546.3 test Loss: 5610.8
Epoch 112 Iter 0 subLoss 33484.8 multi 1.00 import weight 0.00
Epoch 112 Iter 1 subLoss 7321.4 multi -4.97 import weight 0.00
Epoch 112 Iter 2 subLoss 19679.5 multi 1.00 import weight 0.00
Epoch 112 Iter 3 subLoss 13391.6 multi 1.00 import weight 0.00
Epoch 112 Iter 4 subLoss 10504.8 multi 1.00 import weight 0.00
Epoch 112 Iter 5 subLoss 7580.5 multi -1.99 import weight 0.00
Epoch 112 Iter 6 subLoss 10620.2 multi -1.99 import weight 0.00
Epoch 112 Iter 7 subLoss 16602.5 multi -1.99 import weight 0.00
Epoch 112 Iter 8 subLoss 27190.6 multi 1.00 import weight 0.00
Epoch 112 Iter 9 subLoss 20573.5 multi 1.00 import weight 0.00
Epoch 112 Iter 10 subLoss 16595.9 multi 1.00 import weight 0.00
Epoch 112 Iter 11 subLoss 12580.3 multi -1.99 import weight 0.00
Epoch 112 Acc: 77.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1258 train Loss: 17918.6 test Loss: 2532.1
Epoch 113 Iter 0 subLoss 16772.9 multi -1.99 import weight 0.00
Epoch 113 Iter 1 subLoss 25019.0 multi 1.00 import weight 0.00
Epoch 113 Iter 2 subLoss 20359.4 multi 1.00 import weight 0.00
Epoch 113 Iter 3 subLoss 16949.8 multi 1.00 import weight 0.00
Epoch 113 Iter 4 subLoss 14217.7 multi 1.00 import weight 0.00
Epoch 113 Iter 5 subLoss 11954.3 multi 3.99 import weight 0.00
Epoch 113 Iter 6 subLoss 6339.5 multi 3.98 import weight 0.00
Epoch 113 Iter 7 subLoss 4107.5 multi -4.97 import weight 0.00
Epoch 113 Iter 8 subLoss 5452.5 multi 1.00 import weight 0.00
Epoch 113 Iter 9 subLoss 4989.3 multi -1.99 import weight 0.00
Epoch 113 Iter 10 subLoss 5714.5 multi 1.00 import weight 0.00
Epoch 113 Iter 11 subLoss 5605.1 multi 1.00 import weight 0.00
Epoch 113 Acc: 97.04 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 560 train Loss: 4946.4 test Loss: 505.7
Epoch 114 Iter 0 subLoss 4781.8 multi -4.97 import weight 0.00
Epoch 114 Iter 1 subLoss 6477.6 multi -1.99 import weight 0.00
Epoch 114 Iter 2 subLoss 8628.3 multi 1.00 import weight 0.00
Epoch 114 Iter 3 subLoss 7096.0 multi 3.99 import weight 0.00
Epoch 114 Iter 4 subLoss 4917.3 multi -4.97 import weight 0.00
Epoch 114 Iter 5 subLoss 7010.2 multi -7.96 import weight 0.00
Epoch 114 Iter 6 subLoss 13431.6 multi -4.97 import weight 0.00
Epoch 114 Iter 7 subLoss 26567.9 multi -1.99 import weight 0.00
Epoch 114 Iter 8 subLoss 94697.6 multi 1.00 import weight 0.00
Epoch 114 Iter 9 subLoss 30570.8 multi 1.00 import weight 0.00
Epoch 114 Iter 10 subLoss 18818.3 multi -1.99 import weight 0.00
Epoch 114 Iter 11 subLoss 23388.8 multi 1.00 import weight 0.00
Epoch 114 Acc: 76.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2338 train Loss: 21004.5 test Loss: 3466.7
Epoch 115 Iter 0 subLoss 21088.8 multi 1.00 import weight 0.00
Epoch 115 Iter 1 subLoss 18678.3 multi 3.99 import weight 0.00
Epoch 115 Iter 2 subLoss 14033.3 multi 3.99 import weight 0.00
Epoch 115 Iter 3 subLoss 10067.2 multi 1.00 import weight 0.00
Epoch 115 Iter 4 subLoss 9579.9 multi -1.99 import weight 0.00
Epoch 115 Iter 5 subLoss 11081.6 multi -1.99 import weight 0.00
Epoch 115 Iter 6 subLoss 12518.5 multi -4.97 import weight 0.00
Epoch 115 Iter 7 subLoss 17887.0 multi 1.00 import weight 0.00
Epoch 115 Iter 8 subLoss 17267.7 multi 3.99 import weight 0.00
Epoch 115 Iter 9 subLoss 13223.9 multi 3.99 import weight 0.00
Epoch 115 Iter 10 subLoss 10467.2 multi 3.98 import weight 0.00
Epoch 115 Iter 11 subLoss 7292.4 multi -1.99 import weight 0.00
Epoch 115 Acc: 93.54 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 729 train Loss: 8529.3 test Loss: 1144.6
Epoch 116 Iter 0 subLoss 8632.3 multi -1.99 import weight 0.00
Epoch 116 Iter 1 subLoss 9872.3 multi -1.99 import weight 0.00
Epoch 116 Iter 2 subLoss 11599.1 multi -1.99 import weight 0.00
Epoch 116 Iter 3 subLoss 13829.8 multi -1.99 import weight 0.00
Epoch 116 Iter 4 subLoss 16169.2 multi 6.97 import weight 0.00
Epoch 116 Iter 5 subLoss 11778.8 multi -4.97 import weight 0.00
Epoch 116 Iter 6 subLoss 55263.5 multi 1.00 import weight 0.00
Epoch 116 Iter 7 subLoss 14937.4 multi 1.00 import weight 0.00
Epoch 116 Iter 8 subLoss 11948.9 multi 1.00 import weight 0.00
Epoch 116 Iter 9 subLoss 11492.6 multi -1.98 import weight 0.00
Epoch 116 Iter 10 subLoss 13068.0 multi 3.99 import weight 0.00
Epoch 116 Iter 11 subLoss 10160.8 multi 1.00 import weight 0.00
Epoch 116 Acc: 93.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1016 train Loss: 9570.5 test Loss: 1273.5
Epoch 117 Iter 0 subLoss 9408.6 multi 1.00 import weight 0.00
Epoch 117 Iter 1 subLoss 8801.9 multi 3.99 import weight 0.00
Epoch 117 Iter 2 subLoss 6798.1 multi 6.97 import weight 0.00
Epoch 117 Iter 3 subLoss 5118.9 multi 6.97 import weight 0.00
Epoch 117 Iter 4 subLoss 3987.4 multi -1.98 import weight 0.00
Epoch 117 Iter 5 subLoss 4592.3 multi 3.99 import weight 0.00
Epoch 117 Iter 6 subLoss 3973.3 multi 3.98 import weight 0.00
Epoch 117 Iter 7 subLoss 4652.0 multi 3.99 import weight 0.00
Epoch 117 Iter 8 subLoss 4060.3 multi 1.00 import weight 0.00
Epoch 117 Iter 9 subLoss 3973.2 multi 6.97 import weight 0.00
Epoch 117 Iter 10 subLoss 3526.4 multi 1.00 import weight 0.00
Epoch 117 Iter 11 subLoss 3244.1 multi -4.97 import weight 0.00
Epoch 117 Acc: 97.43 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 324 train Loss: 3897.9 test Loss: 429.5
Epoch 118 Iter 0 subLoss 4001.5 multi 1.00 import weight 0.00
Epoch 118 Iter 1 subLoss 3773.8 multi 1.00 import weight 0.00
Epoch 118 Iter 2 subLoss 3451.7 multi 6.97 import weight 0.00
Epoch 118 Iter 3 subLoss 3390.0 multi 3.99 import weight 0.00
Epoch 118 Iter 4 subLoss 3396.2 multi 6.97 import weight 0.00
Epoch 118 Iter 5 subLoss 3436.4 multi 3.99 import weight 0.00
Epoch 118 Iter 6 subLoss 3240.5 multi -1.99 import weight 0.00
Epoch 118 Iter 7 subLoss 3055.5 multi 3.99 import weight 0.00
Epoch 118 Iter 8 subLoss 3224.0 multi 1.00 import weight 0.00
Epoch 118 Iter 9 subLoss 3056.7 multi 6.97 import weight 0.00
Epoch 118 Iter 10 subLoss 3645.0 multi 9.96 import weight 0.00
Epoch 118 Iter 11 subLoss 3645.1 multi 12.94 import weight 0.00
Epoch 118 Acc: 97.76 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 364 train Loss: 3873.7 test Loss: 386.7
Epoch 119 Iter 0 subLoss 3987.9 multi -4.97 import weight 0.00
Epoch 119 Iter 1 subLoss 7530.4 multi 6.97 import weight 0.00
Epoch 119 Iter 2 subLoss 18283.6 multi 1.00 import weight 0.00
Epoch 119 Iter 3 subLoss 3921.4 multi -1.98 import weight 0.00
Epoch 119 Iter 4 subLoss 5256.3 multi 3.99 import weight 0.00
Epoch 119 Iter 5 subLoss 3549.8 multi -1.98 import weight 0.00
Epoch 119 Iter 6 subLoss 4217.6 multi 1.00 import weight 0.00
Epoch 119 Iter 7 subLoss 3683.9 multi 1.00 import weight 0.00
Epoch 119 Iter 8 subLoss 3609.8 multi 3.99 import weight 0.00
Epoch 119 Iter 9 subLoss 3084.6 multi 3.99 import weight 0.00
Epoch 119 Iter 10 subLoss 3279.4 multi -1.99 import weight 0.00
Epoch 119 Iter 11 subLoss 3328.0 multi 1.00 import weight 0.00
Epoch 119 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 332 train Loss: 3358.1 test Loss: 318.7
Epoch 120 Iter 0 subLoss 3145.8 multi 9.96 import weight 0.00
Epoch 120 Iter 1 subLoss 2980.0 multi 1.00 import weight 0.00
Epoch 120 Iter 2 subLoss 3521.1 multi 3.99 import weight 0.00
Epoch 120 Iter 3 subLoss 2962.5 multi 1.00 import weight 0.00
Epoch 120 Iter 4 subLoss 2889.2 multi -4.97 import weight 0.00
Epoch 120 Iter 5 subLoss 3240.9 multi 1.00 import weight 0.00
Epoch 120 Iter 6 subLoss 3410.3 multi 1.00 import weight 0.00
Epoch 120 Iter 7 subLoss 3004.3 multi -1.98 import weight 0.00
Epoch 120 Iter 8 subLoss 3071.7 multi 1.00 import weight 0.00
Epoch 120 Iter 9 subLoss 2866.1 multi 1.00 import weight 0.00
Epoch 120 Iter 10 subLoss 3233.7 multi 3.99 import weight 0.00
Epoch 120 Iter 11 subLoss 2894.4 multi -1.99 import weight 0.00
Epoch 120 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 289 train Loss: 3180.7 test Loss: 298.9
Epoch 121 Iter 0 subLoss 2933.9 multi 1.00 import weight 0.00
Epoch 121 Iter 1 subLoss 2693.6 multi 1.00 import weight 0.00
Epoch 121 Iter 2 subLoss 3140.2 multi 12.94 import weight 0.00
Epoch 121 Iter 3 subLoss 3211.6 multi 3.99 import weight 0.00
Epoch 121 Iter 4 subLoss 2802.5 multi 1.00 import weight 0.00
Epoch 121 Iter 5 subLoss 2965.5 multi 3.99 import weight 0.00
Epoch 121 Iter 6 subLoss 2395.6 multi 1.00 import weight 0.00
Epoch 121 Iter 7 subLoss 3370.0 multi -1.99 import weight 0.00
Epoch 121 Iter 8 subLoss 3367.7 multi 1.00 import weight 0.00
Epoch 121 Iter 9 subLoss 2775.4 multi 1.00 import weight 0.00
Epoch 121 Iter 10 subLoss 3043.2 multi 1.00 import weight 0.00
Epoch 121 Iter 11 subLoss 2702.0 multi -1.99 import weight 0.00
Epoch 121 Acc: 98.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 270 train Loss: 2933.5 test Loss: 302.7
Epoch 122 Iter 0 subLoss 2605.1 multi 1.00 import weight 0.00
Epoch 122 Iter 1 subLoss 3038.1 multi 1.00 import weight 0.00
Epoch 122 Iter 2 subLoss 3193.5 multi -1.98 import weight 0.00
Epoch 122 Iter 3 subLoss 2655.2 multi 1.00 import weight 0.00
Epoch 122 Iter 4 subLoss 2950.4 multi 1.00 import weight 0.00
Epoch 122 Iter 5 subLoss 2841.7 multi -1.99 import weight 0.00
Epoch 122 Iter 6 subLoss 3012.6 multi -1.98 import weight 0.00
Epoch 122 Iter 7 subLoss 3701.7 multi -4.97 import weight 0.00
Epoch 122 Iter 8 subLoss 2859.6 multi -1.99 import weight 0.00
Epoch 122 Iter 9 subLoss 3151.8 multi -16.91 import weight 0.00
Epoch 122 Iter 10 subLoss 20664.9 multi 1.00 import weight 0.00
Epoch 122 Iter 11 subLoss 4198.7 multi 1.00 import weight 0.00
Epoch 122 Acc: 97.04 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 419 train Loss: 3657.8 test Loss: 487.1
Epoch 123 Iter 0 subLoss 3284.1 multi -1.99 import weight 0.00
Epoch 123 Iter 1 subLoss 3967.6 multi 3.99 import weight 0.00
Epoch 123 Iter 2 subLoss 3147.3 multi 15.93 import weight 1.00
Epoch 123 Iter 3 subLoss 3320.7 multi 3.98 import weight 0.00
Epoch 123 Iter 4 subLoss 3022.2 multi -4.97 import weight 0.00
Epoch 123 Iter 5 subLoss 3277.6 multi 1.00 import weight 0.00
Epoch 123 Iter 6 subLoss 2873.8 multi 3.98 import weight 0.00
Epoch 123 Iter 7 subLoss 3230.1 multi 6.97 import weight 0.00
Epoch 123 Iter 8 subLoss 3043.1 multi 1.00 import weight 0.00
Epoch 123 Iter 9 subLoss 3294.5 multi -1.99 import weight 0.00
Epoch 123 Iter 10 subLoss 2786.7 multi -1.99 import weight 0.00
Epoch 123 Iter 11 subLoss 3047.3 multi 3.98 import weight 0.00
Epoch 123 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 304 train Loss: 2964.3 test Loss: 292.7
Epoch 124 Iter 0 subLoss 3254.5 multi -10.94 import weight 0.00
Epoch 124 Iter 1 subLoss 4542.8 multi 6.97 import weight 0.00
Epoch 124 Iter 2 subLoss 5183.7 multi 6.97 import weight 0.00
Epoch 124 Iter 3 subLoss 6790.6 multi 9.96 import weight 0.00
Epoch 124 Iter 4 subLoss 26500.2 multi 1.00 import weight 0.00
Epoch 124 Iter 5 subLoss 5601.5 multi 3.98 import weight 0.00
Epoch 124 Iter 6 subLoss 3952.3 multi -4.97 import weight 0.00
Epoch 124 Iter 7 subLoss 4939.2 multi -1.98 import weight 0.00
Epoch 124 Iter 8 subLoss 6422.5 multi 3.99 import weight 0.00
Epoch 124 Iter 9 subLoss 3880.3 multi 1.00 import weight 0.00
Epoch 124 Iter 10 subLoss 3684.8 multi 3.98 import weight 0.00
Epoch 124 Iter 11 subLoss 3510.1 multi 1.00 import weight 0.00
Epoch 124 Acc: 98.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 351 train Loss: 3290.5 test Loss: 334.7
Epoch 125 Iter 0 subLoss 3149.6 multi 18.91 import weight 1.00
Epoch 125 Iter 1 subLoss 3164.6 multi 1.00 import weight 0.00
Epoch 125 Iter 2 subLoss 3402.9 multi -1.99 import weight 0.00
Epoch 125 Iter 3 subLoss 4019.0 multi -1.98 import weight 0.00
Epoch 125 Iter 4 subLoss 4346.7 multi -7.96 import weight 0.00
Epoch 125 Iter 5 subLoss 168005.7 multi 1.00 import weight 0.00
Epoch 125 Iter 6 subLoss 9846.9 multi -1.99 import weight 0.00
Epoch 125 Iter 7 subLoss 27603.0 multi 1.00 import weight 0.00
Epoch 125 Iter 8 subLoss 9782.1 multi 1.00 import weight 0.00
Epoch 125 Iter 9 subLoss 6675.8 multi 3.98 import weight 0.00
Epoch 125 Iter 10 subLoss 3864.2 multi -1.98 import weight 0.00
Epoch 125 Iter 11 subLoss 4946.1 multi 6.97 import weight 0.00
Epoch 125 Acc: 97.90 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 494 train Loss: 3686.2 test Loss: 353.7
Epoch 126 Iter 0 subLoss 3559.9 multi -4.97 import weight 0.00
Epoch 126 Iter 1 subLoss 4904.4 multi 6.97 import weight 0.00
Epoch 126 Iter 2 subLoss 4534.6 multi -1.99 import weight 0.00
Epoch 126 Iter 3 subLoss 7505.0 multi -4.97 import weight 0.00
Epoch 126 Iter 4 subLoss 40384.3 multi 1.00 import weight 0.00
Epoch 126 Iter 5 subLoss 8424.4 multi 3.99 import weight 0.00
Epoch 126 Iter 6 subLoss 4504.1 multi 3.99 import weight 0.00
Epoch 126 Iter 7 subLoss 3522.6 multi 3.99 import weight 0.00
Epoch 126 Iter 8 subLoss 3722.2 multi -1.99 import weight 0.00
Epoch 126 Iter 9 subLoss 3503.2 multi 3.98 import weight 0.00
Epoch 126 Iter 10 subLoss 3861.2 multi 1.00 import weight 0.00
Epoch 126 Iter 11 subLoss 3309.7 multi 6.97 import weight 0.00
Epoch 126 Acc: 98.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 330 train Loss: 3235.2 test Loss: 295.1
Epoch 127 Iter 0 subLoss 3291.8 multi 1.00 import weight 0.00
Epoch 127 Iter 1 subLoss 3241.9 multi -1.98 import weight 0.00
Epoch 127 Iter 2 subLoss 3119.9 multi 1.00 import weight 0.00
Epoch 127 Iter 3 subLoss 3314.0 multi -7.96 import weight 0.00
Epoch 127 Iter 4 subLoss 4045.2 multi -4.97 import weight 0.00
Epoch 127 Iter 5 subLoss 5200.3 multi 1.00 import weight 0.00
Epoch 127 Iter 6 subLoss 3880.1 multi 3.99 import weight 0.00
Epoch 127 Iter 7 subLoss 3171.5 multi 9.96 import weight 0.00
Epoch 127 Iter 8 subLoss 2950.1 multi 3.99 import weight 0.00
Epoch 127 Iter 9 subLoss 3101.9 multi 1.00 import weight 0.00
Epoch 127 Iter 10 subLoss 2949.5 multi -1.99 import weight 0.00
Epoch 127 Iter 11 subLoss 3076.0 multi 3.99 import weight 0.00
Epoch 127 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 307 train Loss: 3107.5 test Loss: 313.1
Epoch 128 Iter 0 subLoss 3059.9 multi 1.00 import weight 0.00
Epoch 128 Iter 1 subLoss 3209.6 multi -4.97 import weight 0.00
Epoch 128 Iter 2 subLoss 3008.8 multi 1.00 import weight 0.00
Epoch 128 Iter 3 subLoss 3257.9 multi -10.94 import weight 0.00
Epoch 128 Iter 4 subLoss 3532.6 multi -7.96 import weight 0.00
Epoch 128 Iter 5 subLoss 5014.8 multi 6.97 import weight 0.00
Epoch 128 Iter 6 subLoss 5877.7 multi 1.00 import weight 0.00
Epoch 128 Iter 7 subLoss 4108.2 multi -1.99 import weight 0.00
Epoch 128 Iter 8 subLoss 5812.8 multi 1.00 import weight 0.00
Epoch 128 Iter 9 subLoss 4707.7 multi 1.00 import weight 0.00
Epoch 128 Iter 10 subLoss 4119.5 multi -7.96 import weight 0.00
Epoch 128 Iter 11 subLoss 10173.7 multi -1.99 import weight 0.00
Epoch 128 Acc: 75.91 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1017 train Loss: 40566.2 test Loss: 5362.7
Epoch 129 Iter 0 subLoss 42440.9 multi 1.00 import weight 0.00
Epoch 129 Iter 1 subLoss 5942.1 multi 1.00 import weight 0.00
Epoch 129 Iter 2 subLoss 5846.5 multi 3.99 import weight 0.00
Epoch 129 Iter 3 subLoss 3835.0 multi 3.99 import weight 0.00
Epoch 129 Iter 4 subLoss 3414.6 multi 1.00 import weight 0.00
Epoch 129 Iter 5 subLoss 3527.9 multi 6.97 import weight 0.00
Epoch 129 Iter 6 subLoss 3007.2 multi 3.99 import weight 0.00
Epoch 129 Iter 7 subLoss 2711.0 multi -1.99 import weight 0.00
Epoch 129 Iter 8 subLoss 3573.0 multi 1.00 import weight 0.00
Epoch 129 Iter 9 subLoss 3530.7 multi -7.96 import weight 0.00
Epoch 129 Iter 10 subLoss 3283.5 multi -1.98 import weight 0.00
Epoch 129 Iter 11 subLoss 3689.4 multi 6.97 import weight 0.00
Epoch 129 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 368 train Loss: 3322.8 test Loss: 296.4
Epoch 130 Iter 0 subLoss 3137.8 multi 1.00 import weight 0.00
Epoch 130 Iter 1 subLoss 3087.0 multi 1.00 import weight 0.00
Epoch 130 Iter 2 subLoss 3092.3 multi -4.97 import weight 0.00
Epoch 130 Iter 3 subLoss 3505.5 multi 6.97 import weight 0.00
Epoch 130 Iter 4 subLoss 3016.3 multi -4.97 import weight 0.00
Epoch 130 Iter 5 subLoss 3319.5 multi -4.97 import weight 0.00
Epoch 130 Iter 6 subLoss 5471.0 multi 3.98 import weight 0.00
Epoch 130 Iter 7 subLoss 3464.3 multi -4.97 import weight 0.00
Epoch 130 Iter 8 subLoss 4233.9 multi 6.97 import weight 0.00
Epoch 130 Iter 9 subLoss 3528.1 multi 9.96 import weight 0.00
Epoch 130 Iter 10 subLoss 3670.1 multi 1.00 import weight 0.00
Epoch 130 Iter 11 subLoss 2902.1 multi 1.00 import weight 0.00
Epoch 130 Acc: 98.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 290 train Loss: 3194.0 test Loss: 311.2
Epoch 131 Iter 0 subLoss 3342.5 multi 3.99 import weight 0.00
Epoch 131 Iter 1 subLoss 3220.0 multi 3.98 import weight 0.00
Epoch 131 Iter 2 subLoss 2789.8 multi 1.00 import weight 0.00
Epoch 131 Iter 3 subLoss 3088.5 multi 3.99 import weight 0.00
Epoch 131 Iter 4 subLoss 2878.1 multi 6.97 import weight 0.00
Epoch 131 Iter 5 subLoss 2848.5 multi 1.00 import weight 0.00
Epoch 131 Iter 6 subLoss 2244.7 multi 1.00 import weight 0.00
Epoch 131 Iter 7 subLoss 2741.6 multi 1.00 import weight 0.00
Epoch 131 Iter 8 subLoss 2461.0 multi 1.00 import weight 0.00
Epoch 131 Iter 9 subLoss 2771.5 multi 3.99 import weight 0.00
Epoch 131 Iter 10 subLoss 2950.8 multi 3.98 import weight 0.00
Epoch 131 Iter 11 subLoss 2652.2 multi 3.99 import weight 0.00
Epoch 131 Acc: 98.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 265 train Loss: 2915.1 test Loss: 285.9
Epoch 132 Iter 0 subLoss 2849.8 multi 3.98 import weight 0.00
Epoch 132 Iter 1 subLoss 2565.9 multi 1.00 import weight 0.00
Epoch 132 Iter 2 subLoss 2751.0 multi -1.99 import weight 0.00
Epoch 132 Iter 3 subLoss 2979.9 multi -1.98 import weight 0.00
Epoch 132 Iter 4 subLoss 3262.8 multi -1.98 import weight 0.00
Epoch 132 Iter 5 subLoss 2765.1 multi -1.99 import weight 0.00
Epoch 132 Iter 6 subLoss 2994.1 multi 6.97 import weight 0.00
Epoch 132 Iter 7 subLoss 3117.6 multi 1.00 import weight 0.00
Epoch 132 Iter 8 subLoss 3052.5 multi 3.99 import weight 0.00
Epoch 132 Iter 9 subLoss 2701.7 multi 1.00 import weight 0.00
Epoch 132 Iter 10 subLoss 2781.6 multi 1.00 import weight 0.00
Epoch 132 Iter 11 subLoss 2570.9 multi -1.99 import weight 0.00
Epoch 132 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 257 train Loss: 2783.7 test Loss: 272.5
Epoch 133 Iter 0 subLoss 2612.3 multi -1.99 import weight 0.00
Epoch 133 Iter 1 subLoss 2790.2 multi -7.96 import weight 0.00
Epoch 133 Iter 2 subLoss 3309.7 multi 6.97 import weight 0.00
Epoch 133 Iter 3 subLoss 2813.2 multi -1.99 import weight 0.00
Epoch 133 Iter 4 subLoss 3063.0 multi -13.93 import weight 0.00
Epoch 133 Iter 5 subLoss 14544.0 multi 1.00 import weight 0.00
Epoch 133 Iter 6 subLoss 4037.7 multi 3.99 import weight 0.00
Epoch 133 Iter 7 subLoss 3426.6 multi -10.94 import weight 0.00
Epoch 133 Iter 8 subLoss 4274.9 multi 3.98 import weight 0.00
Epoch 133 Iter 9 subLoss 2808.5 multi 1.00 import weight 0.00
Epoch 133 Iter 10 subLoss 3321.3 multi 1.00 import weight 0.00
Epoch 133 Iter 11 subLoss 3321.9 multi 3.99 import weight 0.00
Epoch 133 Acc: 98.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 332 train Loss: 3102.3 test Loss: 291.5
Epoch 134 Iter 0 subLoss 3311.9 multi -4.97 import weight 0.00
Epoch 134 Iter 1 subLoss 3081.9 multi 6.97 import weight 0.00
Epoch 134 Iter 2 subLoss 3189.3 multi -10.94 import weight 0.00
Epoch 134 Iter 3 subLoss 3921.7 multi 1.00 import weight 0.00
Epoch 134 Iter 4 subLoss 3943.9 multi 1.00 import weight 0.00
Epoch 134 Iter 5 subLoss 3733.3 multi -7.96 import weight 0.00
Epoch 134 Iter 6 subLoss 7277.1 multi -4.97 import weight 0.00
Epoch 134 Iter 7 subLoss 39213.8 multi 1.00 import weight 0.00
Epoch 134 Iter 8 subLoss 6199.2 multi 1.00 import weight 0.00
Epoch 134 Iter 9 subLoss 5093.0 multi 3.98 import weight 0.00
Epoch 134 Iter 10 subLoss 3830.9 multi 6.97 import weight 0.00
Epoch 134 Iter 11 subLoss 3361.7 multi 3.98 import weight 0.00
Epoch 134 Acc: 98.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 336 train Loss: 3170.8 test Loss: 322.6
Epoch 135 Iter 0 subLoss 3220.0 multi 6.97 import weight 0.00
Epoch 135 Iter 1 subLoss 3145.1 multi 18.91 import weight 1.00
Epoch 135 Iter 2 subLoss 2890.3 multi 1.00 import weight 0.00
Epoch 135 Iter 3 subLoss 2873.5 multi 9.96 import weight 0.00
Epoch 135 Iter 4 subLoss 3238.3 multi 9.96 import weight 0.00
Epoch 135 Iter 5 subLoss 4810.0 multi -4.97 import weight 0.00
Epoch 135 Iter 6 subLoss 11582.8 multi 1.00 import weight 0.00
Epoch 135 Iter 7 subLoss 6100.1 multi 6.97 import weight 0.00
Epoch 135 Iter 8 subLoss 6203.3 multi 3.99 import weight 0.00
Epoch 135 Iter 9 subLoss 2787.0 multi 3.99 import weight 0.00
Epoch 135 Iter 10 subLoss 2873.4 multi 12.94 import weight 0.00
Epoch 135 Iter 11 subLoss 2957.1 multi 6.97 import weight 0.00
Epoch 135 Acc: 98.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 295 train Loss: 2852.6 test Loss: 249.9
Epoch 136 Iter 0 subLoss 2860.7 multi 1.00 import weight 0.00
Epoch 136 Iter 1 subLoss 2595.2 multi 1.00 import weight 0.00
Epoch 136 Iter 2 subLoss 3035.5 multi 1.00 import weight 0.00
Epoch 136 Iter 3 subLoss 2585.7 multi -1.99 import weight 0.00
Epoch 136 Iter 4 subLoss 2709.4 multi 3.98 import weight 0.00
Epoch 136 Iter 5 subLoss 2461.7 multi 3.99 import weight 0.00
Epoch 136 Iter 6 subLoss 2754.2 multi 1.00 import weight 0.00
Epoch 136 Iter 7 subLoss 3042.0 multi 3.99 import weight 0.00
Epoch 136 Iter 8 subLoss 2497.5 multi 1.00 import weight 0.00
Epoch 136 Iter 9 subLoss 2378.9 multi 1.00 import weight 0.00
Epoch 136 Iter 10 subLoss 3090.9 multi -7.96 import weight 0.00
Epoch 136 Iter 11 subLoss 2360.8 multi 1.00 import weight 0.00
Epoch 136 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 236 train Loss: 2816.4 test Loss: 264.5
Epoch 137 Iter 0 subLoss 2846.2 multi 6.97 import weight 0.00
Epoch 137 Iter 1 subLoss 2906.9 multi 1.00 import weight 0.00
Epoch 137 Iter 2 subLoss 2225.2 multi 1.00 import weight 0.00
Epoch 137 Iter 3 subLoss 2431.8 multi 1.00 import weight 0.00
Epoch 137 Iter 4 subLoss 2505.6 multi -1.99 import weight 0.00
Epoch 137 Iter 5 subLoss 2517.2 multi -1.99 import weight 0.00
Epoch 137 Iter 6 subLoss 2725.3 multi -1.99 import weight 0.00
Epoch 137 Iter 7 subLoss 3362.8 multi 6.97 import weight 0.00
Epoch 137 Iter 8 subLoss 2547.9 multi 1.00 import weight 0.00
Epoch 137 Iter 9 subLoss 3109.3 multi -1.99 import weight 0.00
Epoch 137 Iter 10 subLoss 2670.2 multi 1.00 import weight 0.00
Epoch 137 Iter 11 subLoss 3067.4 multi -10.94 import weight 0.00
Epoch 137 Acc: 97.78 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 306 train Loss: 3794.3 test Loss: 384.3
Epoch 138 Iter 0 subLoss 3760.5 multi -4.97 import weight 0.00
Epoch 138 Iter 1 subLoss 10014.0 multi 1.00 import weight 0.00
Epoch 138 Iter 2 subLoss 4354.9 multi -1.98 import weight 0.00
Epoch 138 Iter 3 subLoss 6249.4 multi -1.99 import weight 0.00
Epoch 138 Iter 4 subLoss 23528.1 multi 1.00 import weight 0.00
Epoch 138 Iter 5 subLoss 4077.4 multi -4.97 import weight 0.00
Epoch 138 Iter 6 subLoss 8329.2 multi -1.98 import weight 0.00
Epoch 138 Iter 7 subLoss 20591.2 multi -1.99 import weight 0.00
Epoch 138 Iter 8 subLoss 199904.9 multi 1.00 import weight 0.00
Epoch 138 Iter 9 subLoss 9463.0 multi 1.00 import weight 0.00
Epoch 138 Iter 10 subLoss 7359.5 multi 3.99 import weight 0.00
Epoch 138 Iter 11 subLoss 4694.7 multi -1.99 import weight 0.00
Epoch 138 Acc: 96.93 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 469 train Loss: 5346.0 test Loss: 601.3
Epoch 139 Iter 0 subLoss 5421.2 multi 1.00 import weight 0.00
Epoch 139 Iter 1 subLoss 5177.9 multi -4.97 import weight 0.00
Epoch 139 Iter 2 subLoss 6250.0 multi 1.00 import weight 0.00
Epoch 139 Iter 3 subLoss 5974.9 multi 3.98 import weight 0.00
Epoch 139 Iter 4 subLoss 4812.5 multi 3.98 import weight 0.00
Epoch 139 Iter 5 subLoss 3909.1 multi 3.99 import weight 0.00
Epoch 139 Iter 6 subLoss 4010.8 multi 1.00 import weight 0.00
Epoch 139 Iter 7 subLoss 3549.1 multi -4.97 import weight 0.00
Epoch 139 Iter 8 subLoss 4050.5 multi -1.98 import weight 0.00
Epoch 139 Iter 9 subLoss 4484.5 multi 3.99 import weight 0.00
Epoch 139 Iter 10 subLoss 3660.8 multi 1.00 import weight 0.00
Epoch 139 Iter 11 subLoss 3406.9 multi 1.00 import weight 0.00
Epoch 139 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 340 train Loss: 3855.9 test Loss: 348.3
Epoch 140 Iter 0 subLoss 4003.5 multi 3.98 import weight 0.00
Epoch 140 Iter 1 subLoss 3620.6 multi 9.96 import weight 0.00
Epoch 140 Iter 2 subLoss 3189.2 multi -7.96 import weight 0.00
Epoch 140 Iter 3 subLoss 3983.7 multi -1.99 import weight 0.00
Epoch 140 Iter 4 subLoss 5306.5 multi 1.00 import weight 0.00
Epoch 140 Iter 5 subLoss 4653.1 multi 6.97 import weight 0.00
Epoch 140 Iter 6 subLoss 3461.9 multi -1.99 import weight 0.00
Epoch 140 Iter 7 subLoss 3964.7 multi 3.98 import weight 0.00
Epoch 140 Iter 8 subLoss 2895.9 multi 3.98 import weight 0.00
Epoch 140 Iter 9 subLoss 3123.4 multi -4.97 import weight 0.00
Epoch 140 Iter 10 subLoss 3151.7 multi -22.88 import weight 0.00
Epoch 140 Iter 11 subLoss 6118.7 multi -7.96 import weight 0.00
Epoch 140 Acc: 76.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 611 train Loss: 31573.3 test Loss: 4354.1
Epoch 141 Iter 0 subLoss 32879.2 multi 3.99 import weight 0.00
Epoch 141 Iter 1 subLoss 29377.6 multi 1.00 import weight 0.00
Epoch 141 Iter 2 subLoss 16948.8 multi 3.99 import weight 0.00
Epoch 141 Iter 3 subLoss 9739.1 multi 1.00 import weight 0.00
Epoch 141 Iter 4 subLoss 8014.0 multi 1.00 import weight 0.00
Epoch 141 Iter 5 subLoss 8222.0 multi 6.97 import weight 0.00
Epoch 141 Iter 6 subLoss 6703.6 multi 6.97 import weight 0.00
Epoch 141 Iter 7 subLoss 5857.5 multi 6.97 import weight 0.00
Epoch 141 Iter 8 subLoss 9701.8 multi 3.99 import weight 0.00
Epoch 141 Iter 9 subLoss 11557.0 multi -1.98 import weight 0.00
Epoch 141 Iter 10 subLoss 54349.2 multi 1.00 import weight 0.00
Epoch 141 Iter 11 subLoss 6326.9 multi 1.00 import weight 0.00
Epoch 141 Acc: 96.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 632 train Loss: 5850.8 test Loss: 632.2
Epoch 142 Iter 0 subLoss 6176.3 multi 3.98 import weight 0.00
Epoch 142 Iter 1 subLoss 4140.8 multi -1.99 import weight 0.00
Epoch 142 Iter 2 subLoss 4659.5 multi 9.96 import weight 0.00
Epoch 142 Iter 3 subLoss 3380.5 multi -4.97 import weight 0.00
Epoch 142 Iter 4 subLoss 4010.0 multi 6.97 import weight 0.00
Epoch 142 Iter 5 subLoss 3463.1 multi 1.00 import weight 0.00
Epoch 142 Iter 6 subLoss 4089.3 multi -1.99 import weight 0.00
Epoch 142 Iter 7 subLoss 3873.9 multi -1.99 import weight 0.00
Epoch 142 Iter 8 subLoss 4139.2 multi -1.98 import weight 0.00
Epoch 142 Iter 9 subLoss 5896.7 multi -7.96 import weight 0.00
Epoch 142 Iter 10 subLoss 63262.3 multi 1.00 import weight 0.00
Epoch 142 Iter 11 subLoss 6992.1 multi -10.94 import weight 0.00
Epoch 142 Acc: 62.99 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 699 train Loss: 31588.5 test Loss: 5438.8
Epoch 143 Iter 0 subLoss 31787.2 multi 1.00 import weight 0.00
Epoch 143 Iter 1 subLoss 22361.4 multi 1.00 import weight 0.00
Epoch 143 Iter 2 subLoss 18456.2 multi 1.00 import weight 0.00
Epoch 143 Iter 3 subLoss 16919.7 multi 1.00 import weight 0.00
Epoch 143 Iter 4 subLoss 14338.4 multi 3.99 import weight 0.00
Epoch 143 Iter 5 subLoss 7034.8 multi 3.98 import weight 0.00
Epoch 143 Iter 6 subLoss 5259.0 multi 6.97 import weight 0.00
Epoch 143 Iter 7 subLoss 3568.1 multi 1.00 import weight 0.00
Epoch 143 Iter 8 subLoss 3910.6 multi 1.00 import weight 0.00
Epoch 143 Iter 9 subLoss 4360.4 multi 1.00 import weight 0.00
Epoch 143 Iter 10 subLoss 4090.5 multi 6.97 import weight 0.00
Epoch 143 Iter 11 subLoss 3476.7 multi -7.96 import weight 0.00
Epoch 143 Acc: 97.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 347 train Loss: 4324.6 test Loss: 444.0
Epoch 144 Iter 0 subLoss 3814.9 multi 1.00 import weight 0.00
Epoch 144 Iter 1 subLoss 3868.9 multi 3.99 import weight 0.00
Epoch 144 Iter 2 subLoss 4057.7 multi 1.00 import weight 0.00
Epoch 144 Iter 3 subLoss 3527.3 multi 12.94 import weight 0.00
Epoch 144 Iter 4 subLoss 3368.2 multi 9.96 import weight 0.00
Epoch 144 Iter 5 subLoss 3723.1 multi 1.00 import weight 0.00
Epoch 144 Iter 6 subLoss 3301.2 multi 9.96 import weight 0.00
Epoch 144 Iter 7 subLoss 2992.3 multi 9.96 import weight 0.00
Epoch 144 Iter 8 subLoss 3137.4 multi 1.00 import weight 0.00
Epoch 144 Iter 9 subLoss 3377.4 multi -7.96 import weight 0.00
Epoch 144 Iter 10 subLoss 5032.8 multi 21.90 import weight 0.00
Epoch 144 Iter 11 subLoss 121245.4 multi 1.00 import weight 0.00
Epoch 144 Acc: 83.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 12124 train Loss: 18050.9 test Loss: 2275.7
Epoch 145 Iter 0 subLoss 17849.7 multi -1.99 import weight 0.00
Epoch 145 Iter 1 subLoss 28342.4 multi 1.00 import weight 0.00
Epoch 145 Iter 2 subLoss 19867.9 multi 3.99 import weight 0.00
Epoch 145 Iter 3 subLoss 11045.1 multi 3.99 import weight 0.00
Epoch 145 Iter 4 subLoss 7196.4 multi 1.00 import weight 0.00
Epoch 145 Iter 5 subLoss 6768.2 multi 9.96 import weight 0.00
Epoch 145 Iter 6 subLoss 4398.3 multi 1.00 import weight 0.00
Epoch 145 Iter 7 subLoss 4191.5 multi 3.98 import weight 0.00
Epoch 145 Iter 8 subLoss 3561.6 multi 3.98 import weight 0.00
Epoch 145 Iter 9 subLoss 4067.3 multi -1.99 import weight 0.00
Epoch 145 Iter 10 subLoss 3261.6 multi 1.00 import weight 0.00
Epoch 145 Iter 11 subLoss 3177.2 multi 12.94 import weight 0.00
Epoch 145 Acc: 98.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 317 train Loss: 3418.9 test Loss: 351.2
Epoch 146 Iter 0 subLoss 2869.4 multi 3.98 import weight 0.00
Epoch 146 Iter 1 subLoss 3238.2 multi 12.94 import weight 0.00
Epoch 146 Iter 2 subLoss 2939.8 multi 3.99 import weight 0.00
Epoch 146 Iter 3 subLoss 2653.9 multi 6.97 import weight 0.00
Epoch 146 Iter 4 subLoss 2669.7 multi -7.96 import weight 0.00
Epoch 146 Iter 5 subLoss 2964.5 multi -4.97 import weight 0.00
Epoch 146 Iter 6 subLoss 3214.7 multi 9.96 import weight 0.00
Epoch 146 Iter 7 subLoss 3425.6 multi -7.96 import weight 0.00
Epoch 146 Iter 8 subLoss 7111.6 multi -7.96 import weight 0.00
Epoch 146 Iter 9 subLoss 144184.5 multi 1.00 import weight 0.00
Epoch 146 Iter 10 subLoss 13998.1 multi 3.99 import weight 0.00
Epoch 146 Iter 11 subLoss 5446.1 multi 9.96 import weight 0.00
Epoch 146 Acc: 97.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 544 train Loss: 3622.6 test Loss: 443.0
Epoch 147 Iter 0 subLoss 3329.4 multi 3.98 import weight 0.00
Epoch 147 Iter 1 subLoss 3279.9 multi -1.99 import weight 0.00
Epoch 147 Iter 2 subLoss 3328.5 multi 6.97 import weight 0.00
Epoch 147 Iter 3 subLoss 3022.6 multi -4.97 import weight 0.00
Epoch 147 Iter 4 subLoss 3559.7 multi -4.97 import weight 0.00
Epoch 147 Iter 5 subLoss 3946.0 multi 3.99 import weight 0.00
Epoch 147 Iter 6 subLoss 3395.9 multi 6.97 import weight 0.00
Epoch 147 Iter 7 subLoss 3501.6 multi 9.96 import weight 0.00
Epoch 147 Iter 8 subLoss 3201.2 multi -1.98 import weight 0.00
Epoch 147 Iter 9 subLoss 2680.7 multi -1.99 import weight 0.00
Epoch 147 Iter 10 subLoss 3190.0 multi -7.96 import weight 0.00
Epoch 147 Iter 11 subLoss 7551.9 multi 6.97 import weight 0.00
Epoch 147 Acc: 95.29 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 755 train Loss: 6765.7 test Loss: 837.7
Epoch 148 Iter 0 subLoss 6917.4 multi 12.94 import weight 0.00
Epoch 148 Iter 1 subLoss 40418.0 multi 1.00 import weight 0.00
Epoch 148 Iter 2 subLoss 8089.0 multi 6.97 import weight 0.00
Epoch 148 Iter 3 subLoss 4022.4 multi -1.99 import weight 0.00
Epoch 148 Iter 4 subLoss 5192.3 multi -4.97 import weight 0.00
Epoch 148 Iter 5 subLoss 15627.9 multi 1.00 import weight 0.00
Epoch 148 Iter 6 subLoss 5892.4 multi -4.97 import weight 0.00
Epoch 148 Iter 7 subLoss 19928.0 multi 1.00 import weight 0.00
Epoch 148 Iter 8 subLoss 8029.0 multi -1.98 import weight 0.00
Epoch 148 Iter 9 subLoss 15767.2 multi 1.00 import weight 0.00
Epoch 148 Iter 10 subLoss 8255.6 multi -1.98 import weight 0.00
Epoch 148 Iter 11 subLoss 15677.1 multi 1.00 import weight 0.00
Epoch 148 Acc: 93.81 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1567 train Loss: 8838.6 test Loss: 1160.0
Epoch 149 Iter 0 subLoss 8635.2 multi 1.00 import weight 0.00
Epoch 149 Iter 1 subLoss 6241.8 multi 3.98 import weight 0.00
Epoch 149 Iter 2 subLoss 4146.0 multi -1.98 import weight 0.00
Epoch 149 Iter 3 subLoss 3819.8 multi 3.98 import weight 0.00
Epoch 149 Iter 4 subLoss 3712.5 multi 6.97 import weight 0.00
Epoch 149 Iter 5 subLoss 2632.1 multi 1.00 import weight 0.00
Epoch 149 Iter 6 subLoss 2943.0 multi -1.98 import weight 0.00
Epoch 149 Iter 7 subLoss 3196.1 multi -7.96 import weight 0.00
Epoch 149 Iter 8 subLoss 3499.2 multi -1.98 import weight 0.00
Epoch 149 Iter 9 subLoss 4086.7 multi 1.00 import weight 0.00
Epoch 149 Iter 10 subLoss 3497.3 multi 1.00 import weight 0.00
Epoch 149 Iter 11 subLoss 3566.1 multi 3.99 import weight 0.00
Epoch 149 Acc: 97.68 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 356 train Loss: 3426.2 test Loss: 368.8
Epoch 150 Iter 0 subLoss 2894.5 multi 6.97 import weight 0.00
Epoch 150 Iter 1 subLoss 3069.1 multi -7.96 import weight 0.00
Epoch 150 Iter 2 subLoss 3513.6 multi -4.97 import weight 0.00
Epoch 150 Iter 3 subLoss 3308.9 multi 12.94 import weight 0.00
Epoch 150 Iter 4 subLoss 3306.8 multi 15.93 import weight 0.00
Epoch 150 Iter 5 subLoss 2500.0 multi 3.99 import weight 0.00
Epoch 150 Iter 6 subLoss 3236.4 multi 15.93 import weight 0.00
Epoch 150 Iter 7 subLoss 3035.4 multi 1.00 import weight 0.00
Epoch 150 Iter 8 subLoss 3041.3 multi 3.99 import weight 0.00
Epoch 150 Iter 9 subLoss 2722.0 multi 1.00 import weight 0.00
Epoch 150 Iter 10 subLoss 2400.4 multi -1.99 import weight 0.00
Epoch 150 Iter 11 subLoss 2886.9 multi -13.93 import weight 0.00
Epoch 150 Acc: 97.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 288 train Loss: 3667.2 test Loss: 434.1
Epoch 151 Iter 0 subLoss 3757.4 multi 3.99 import weight 0.00
Epoch 151 Iter 1 subLoss 2933.5 multi 6.97 import weight 0.00
Epoch 151 Iter 2 subLoss 3100.0 multi 1.00 import weight 0.00
Epoch 151 Iter 3 subLoss 2694.0 multi 1.00 import weight 0.00
Epoch 151 Iter 4 subLoss 2888.9 multi -10.94 import weight 0.00
Epoch 151 Iter 5 subLoss 3072.6 multi -1.99 import weight 0.00
Epoch 151 Iter 6 subLoss 2959.7 multi 6.97 import weight 0.00
Epoch 151 Iter 7 subLoss 2848.6 multi 9.96 import weight 0.00
Epoch 151 Iter 8 subLoss 2783.9 multi 6.97 import weight 0.00
Epoch 151 Iter 9 subLoss 2483.8 multi 1.00 import weight 0.00
Epoch 151 Iter 10 subLoss 2909.3 multi -1.99 import weight 0.00
Epoch 151 Iter 11 subLoss 2425.3 multi 1.00 import weight 0.00
Epoch 151 Acc: 97.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 242 train Loss: 2792.3 test Loss: 301.5
Epoch 152 Iter 0 subLoss 2700.0 multi 3.98 import weight 0.00
Epoch 152 Iter 1 subLoss 2338.4 multi 1.00 import weight 0.00
Epoch 152 Iter 2 subLoss 2482.6 multi 3.99 import weight 0.00
Epoch 152 Iter 3 subLoss 2771.4 multi 3.98 import weight 0.00
Epoch 152 Iter 4 subLoss 2756.7 multi 3.98 import weight 0.00
Epoch 152 Iter 5 subLoss 2262.9 multi 1.00 import weight 0.00
Epoch 152 Iter 6 subLoss 2543.5 multi 3.99 import weight 0.00
Epoch 152 Iter 7 subLoss 2685.0 multi 1.00 import weight 0.00
Epoch 152 Iter 8 subLoss 2416.0 multi -1.99 import weight 0.00
Epoch 152 Iter 9 subLoss 2841.0 multi 12.94 import weight 0.00
Epoch 152 Iter 10 subLoss 2389.2 multi -1.99 import weight 0.00
Epoch 152 Iter 11 subLoss 2671.8 multi 1.00 import weight 0.00
Epoch 152 Acc: 98.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 267 train Loss: 2751.4 test Loss: 283.8
Epoch 153 Iter 0 subLoss 2405.2 multi 1.00 import weight 0.00
Epoch 153 Iter 1 subLoss 2281.9 multi 1.00 import weight 0.00
Epoch 153 Iter 2 subLoss 3021.4 multi -1.99 import weight 0.00
Epoch 153 Iter 3 subLoss 2779.7 multi 6.97 import weight 0.00
Epoch 153 Iter 4 subLoss 2392.9 multi 1.00 import weight 0.00
Epoch 153 Iter 5 subLoss 2433.8 multi 1.00 import weight 0.00
Epoch 153 Iter 6 subLoss 2877.6 multi 9.96 import weight 0.00
Epoch 153 Iter 7 subLoss 2458.2 multi 1.00 import weight 0.00
Epoch 153 Iter 8 subLoss 2398.2 multi 3.98 import weight 0.00
Epoch 153 Iter 9 subLoss 2570.3 multi 1.00 import weight 0.00
Epoch 153 Iter 10 subLoss 2547.3 multi 6.97 import weight 0.00
Epoch 153 Iter 11 subLoss 2442.6 multi -4.97 import weight 0.00
Epoch 153 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 244 train Loss: 2724.1 test Loss: 303.0
Epoch 154 Iter 0 subLoss 2494.0 multi 1.00 import weight 0.00
Epoch 154 Iter 1 subLoss 2620.8 multi -1.99 import weight 0.00
Epoch 154 Iter 2 subLoss 2592.3 multi 1.00 import weight 0.00
Epoch 154 Iter 3 subLoss 2678.4 multi 3.98 import weight 0.00
Epoch 154 Iter 4 subLoss 2326.1 multi 1.00 import weight 0.00
Epoch 154 Iter 5 subLoss 1841.7 multi 1.00 import weight 0.00
Epoch 154 Iter 6 subLoss 2592.6 multi 3.98 import weight 0.00
Epoch 154 Iter 7 subLoss 2419.7 multi -1.98 import weight 0.00
Epoch 154 Iter 8 subLoss 2486.7 multi 6.97 import weight 0.00
Epoch 154 Iter 9 subLoss 2453.9 multi 1.00 import weight 0.00
Epoch 154 Iter 10 subLoss 2580.3 multi -1.98 import weight 0.00
Epoch 154 Iter 11 subLoss 2805.4 multi 3.98 import weight 0.00
Epoch 154 Acc: 98.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 280 train Loss: 2515.3 test Loss: 249.4
Epoch 155 Iter 0 subLoss 2602.8 multi -4.97 import weight 0.00
Epoch 155 Iter 1 subLoss 2792.7 multi -10.94 import weight 0.00
Epoch 155 Iter 2 subLoss 3199.2 multi -4.97 import weight 0.00
Epoch 155 Iter 3 subLoss 5731.8 multi -1.99 import weight 0.00
Epoch 155 Iter 4 subLoss 15389.5 multi 1.00 import weight 0.00
Epoch 155 Iter 5 subLoss 3727.6 multi 1.00 import weight 0.00
Epoch 155 Iter 6 subLoss 3716.1 multi 9.96 import weight 0.00
Epoch 155 Iter 7 subLoss 4111.4 multi -4.97 import weight 0.00
Epoch 155 Iter 8 subLoss 11717.9 multi 6.97 import weight 0.00
Epoch 155 Iter 9 subLoss 11145.2 multi 1.00 import weight 0.00
Epoch 155 Iter 10 subLoss 8123.7 multi -4.97 import weight 0.00
Epoch 155 Iter 11 subLoss 19437.3 multi 1.00 import weight 0.00
Epoch 155 Acc: 83.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1943 train Loss: 12901.9 test Loss: 2390.5
Epoch 156 Iter 0 subLoss 13344.4 multi 1.00 import weight 0.00
Epoch 156 Iter 1 subLoss 10368.9 multi 1.00 import weight 0.00
Epoch 156 Iter 2 subLoss 8041.4 multi 3.99 import weight 0.00
Epoch 156 Iter 3 subLoss 5063.5 multi -4.97 import weight 0.00
Epoch 156 Iter 4 subLoss 7667.6 multi 9.96 import weight 0.00
Epoch 156 Iter 5 subLoss 3486.1 multi 1.00 import weight 0.00
Epoch 156 Iter 6 subLoss 3646.7 multi 15.93 import weight 0.00
Epoch 156 Iter 7 subLoss 3890.7 multi -10.94 import weight 0.00
Epoch 156 Iter 8 subLoss 14354.9 multi 1.00 import weight 0.00
Epoch 156 Iter 9 subLoss 6064.6 multi -1.99 import weight 0.00
Epoch 156 Iter 10 subLoss 9787.2 multi 3.99 import weight 0.00
Epoch 156 Iter 11 subLoss 3434.6 multi 1.00 import weight 0.00
Epoch 156 Acc: 97.68 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 343 train Loss: 3232.9 test Loss: 360.0
Epoch 157 Iter 0 subLoss 3202.3 multi -4.97 import weight 0.00
Epoch 157 Iter 1 subLoss 3382.8 multi -4.97 import weight 0.00
Epoch 157 Iter 2 subLoss 3671.0 multi 1.00 import weight 0.00
Epoch 157 Iter 3 subLoss 3667.8 multi 3.98 import weight 0.00
Epoch 157 Iter 4 subLoss 3292.9 multi 1.00 import weight 0.00
Epoch 157 Iter 5 subLoss 3104.0 multi 3.99 import weight 0.00
Epoch 157 Iter 6 subLoss 3184.3 multi -4.97 import weight 0.00
Epoch 157 Iter 7 subLoss 3258.5 multi -7.96 import weight 0.00
Epoch 157 Iter 8 subLoss 3895.3 multi -7.96 import weight 0.00
Epoch 157 Iter 9 subLoss 6471.2 multi 1.00 import weight 0.00
Epoch 157 Iter 10 subLoss 5178.6 multi -1.98 import weight 0.00
Epoch 157 Iter 11 subLoss 6286.8 multi 6.97 import weight 0.00
Epoch 157 Acc: 97.06 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 628 train Loss: 4099.8 test Loss: 468.7
Epoch 158 Iter 0 subLoss 4836.5 multi -1.99 import weight 0.00
Epoch 158 Iter 1 subLoss 4488.2 multi 6.97 import weight 0.00
Epoch 158 Iter 2 subLoss 3184.6 multi -1.99 import weight 0.00
Epoch 158 Iter 3 subLoss 3983.6 multi 1.00 import weight 0.00
Epoch 158 Iter 4 subLoss 3495.0 multi 1.00 import weight 0.00
Epoch 158 Iter 5 subLoss 3295.5 multi 3.99 import weight 0.00
Epoch 158 Iter 6 subLoss 2926.5 multi -4.97 import weight 0.00
Epoch 158 Iter 7 subLoss 3236.3 multi 18.91 import weight 1.00
Epoch 158 Iter 8 subLoss 4918.9 multi -4.97 import weight 0.00
Epoch 158 Iter 9 subLoss 10477.8 multi -4.97 import weight 0.00
Epoch 158 Iter 10 subLoss 129510.0 multi 1.00 import weight 0.00
Epoch 158 Iter 11 subLoss 9121.6 multi -4.97 import weight 0.00
Epoch 158 Acc: 72.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 912 train Loss: 34117.9 test Loss: 5969.6
Epoch 159 Iter 0 subLoss 35229.6 multi 1.00 import weight 0.00
Epoch 159 Iter 1 subLoss 12679.4 multi 1.00 import weight 0.00
Epoch 159 Iter 2 subLoss 10338.7 multi -1.99 import weight 0.00
Epoch 159 Iter 3 subLoss 14238.8 multi 3.99 import weight 0.00
Epoch 159 Iter 4 subLoss 5950.1 multi -1.98 import weight 0.00
Epoch 159 Iter 5 subLoss 7333.0 multi 1.00 import weight 0.00
Epoch 159 Iter 6 subLoss 6314.2 multi 3.99 import weight 0.00
Epoch 159 Iter 7 subLoss 5003.6 multi 1.00 import weight 0.00
Epoch 159 Iter 8 subLoss 4546.6 multi 6.97 import weight 0.00
Epoch 159 Iter 9 subLoss 3918.8 multi 3.99 import weight 0.00
Epoch 159 Iter 10 subLoss 3203.9 multi -1.99 import weight 0.00
Epoch 159 Iter 11 subLoss 3364.0 multi 12.94 import weight 0.00
Epoch 159 Acc: 97.76 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 336 train Loss: 3096.1 test Loss: 344.4
Epoch 160 Iter 0 subLoss 2929.1 multi -1.98 import weight 0.00
Epoch 160 Iter 1 subLoss 2745.2 multi 3.99 import weight 0.00
Epoch 160 Iter 2 subLoss 2848.6 multi 15.93 import weight 0.00
Epoch 160 Iter 3 subLoss 2661.8 multi -4.97 import weight 0.00
Epoch 160 Iter 4 subLoss 3019.2 multi -1.99 import weight 0.00
Epoch 160 Iter 5 subLoss 3984.0 multi 3.98 import weight 0.00
Epoch 160 Iter 6 subLoss 3272.3 multi 1.00 import weight 0.00
Epoch 160 Iter 7 subLoss 3352.0 multi -1.98 import weight 0.00
Epoch 160 Iter 8 subLoss 2820.6 multi 1.00 import weight 0.00
Epoch 160 Iter 9 subLoss 2704.3 multi 1.00 import weight 0.00
Epoch 160 Iter 10 subLoss 2945.5 multi -1.99 import weight 0.00
Epoch 160 Iter 11 subLoss 2716.7 multi -7.96 import weight 0.00
Epoch 160 Acc: 98.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 271 train Loss: 3150.9 test Loss: 331.9
Epoch 161 Iter 0 subLoss 3063.3 multi -4.97 import weight 0.00
Epoch 161 Iter 1 subLoss 3643.8 multi 18.91 import weight 0.00
Epoch 161 Iter 2 subLoss 3098.1 multi -4.97 import weight 0.00
Epoch 161 Iter 3 subLoss 4259.6 multi 1.00 import weight 0.00
Epoch 161 Iter 4 subLoss 3055.6 multi 1.00 import weight 0.00
Epoch 161 Iter 5 subLoss 3217.4 multi 3.98 import weight 0.00
Epoch 161 Iter 6 subLoss 2761.0 multi -4.97 import weight 0.00
Epoch 161 Iter 7 subLoss 3094.6 multi -1.98 import weight 0.00
Epoch 161 Iter 8 subLoss 3066.8 multi -4.97 import weight 0.00
Epoch 161 Iter 9 subLoss 3285.3 multi -4.97 import weight 0.00
Epoch 161 Iter 10 subLoss 6437.5 multi -4.97 import weight 0.00
Epoch 161 Iter 11 subLoss 66951.9 multi 1.00 import weight 0.00
Epoch 161 Acc: 94.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 6695 train Loss: 5114.4 test Loss: 944.5
Epoch 162 Iter 0 subLoss 4931.8 multi 1.00 import weight 0.00
Epoch 162 Iter 1 subLoss 4563.3 multi 3.98 import weight 0.00
Epoch 162 Iter 2 subLoss 3373.4 multi -7.96 import weight 0.00
Epoch 162 Iter 3 subLoss 4418.2 multi 9.96 import weight 0.00
Epoch 162 Iter 4 subLoss 2969.5 multi -4.97 import weight 0.00
Epoch 162 Iter 5 subLoss 3010.9 multi 1.00 import weight 0.00
Epoch 162 Iter 6 subLoss 2984.8 multi -4.97 import weight 0.00
Epoch 162 Iter 7 subLoss 4366.2 multi 3.99 import weight 0.00
Epoch 162 Iter 8 subLoss 2907.4 multi 1.00 import weight 0.00
Epoch 162 Iter 9 subLoss 3692.4 multi -4.97 import weight 0.00
Epoch 162 Iter 10 subLoss 3156.8 multi -19.90 import weight 0.00
Epoch 162 Iter 11 subLoss 7248.4 multi -4.97 import weight 0.00
Epoch 162 Acc: 86.34 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 724 train Loss: 16061.6 test Loss: 1812.3
Epoch 163 Iter 0 subLoss 16039.6 multi 1.00 import weight 0.00
Epoch 163 Iter 1 subLoss 11930.7 multi 1.00 import weight 0.00
Epoch 163 Iter 2 subLoss 8551.6 multi 1.00 import weight 0.00
Epoch 163 Iter 3 subLoss 7967.4 multi -1.99 import weight 0.00
Epoch 163 Iter 4 subLoss 10111.5 multi 1.00 import weight 0.00
Epoch 163 Iter 5 subLoss 8653.7 multi 6.97 import weight 0.00
Epoch 163 Iter 6 subLoss 3756.7 multi 6.97 import weight 0.00
Epoch 163 Iter 7 subLoss 2431.6 multi 3.98 import weight 0.00
Epoch 163 Iter 8 subLoss 2805.0 multi 3.99 import weight 0.00
Epoch 163 Iter 9 subLoss 3190.5 multi -7.96 import weight 0.00
Epoch 163 Iter 10 subLoss 3505.3 multi 3.98 import weight 0.00
Epoch 163 Iter 11 subLoss 3782.8 multi -4.97 import weight 0.00
Epoch 163 Acc: 97.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 378 train Loss: 3458.6 test Loss: 371.5
Epoch 164 Iter 0 subLoss 3896.2 multi -4.97 import weight 0.00
Epoch 164 Iter 1 subLoss 3277.6 multi 3.99 import weight 0.00
Epoch 164 Iter 2 subLoss 3374.4 multi -4.97 import weight 0.00
Epoch 164 Iter 3 subLoss 3359.0 multi 1.00 import weight 0.00
Epoch 164 Iter 4 subLoss 3084.9 multi 6.97 import weight 0.00
Epoch 164 Iter 5 subLoss 3186.3 multi 1.00 import weight 0.00
Epoch 164 Iter 6 subLoss 3336.2 multi -19.90 import weight 0.00
Epoch 164 Iter 7 subLoss 4626.4 multi 1.00 import weight 0.00
Epoch 164 Iter 8 subLoss 4473.5 multi -1.99 import weight 0.00
Epoch 164 Iter 9 subLoss 4068.7 multi 1.00 import weight 0.00
Epoch 164 Iter 10 subLoss 4853.7 multi 3.99 import weight 0.00
Epoch 164 Iter 11 subLoss 3561.4 multi 6.97 import weight 0.00
Epoch 164 Acc: 97.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 356 train Loss: 3371.3 test Loss: 357.1
Epoch 165 Iter 0 subLoss 3485.5 multi 3.99 import weight 0.00
Epoch 165 Iter 1 subLoss 3505.5 multi 6.97 import weight 0.00
Epoch 165 Iter 2 subLoss 2955.9 multi 6.97 import weight 0.00
Epoch 165 Iter 3 subLoss 2580.3 multi 1.00 import weight 0.00
Epoch 165 Iter 4 subLoss 2579.0 multi 3.98 import weight 0.00
Epoch 165 Iter 5 subLoss 2841.7 multi 18.91 import weight 0.00
Epoch 165 Iter 6 subLoss 3015.1 multi 3.98 import weight 0.00
Epoch 165 Iter 7 subLoss 2496.0 multi 1.00 import weight 0.00
Epoch 165 Iter 8 subLoss 2693.2 multi 3.99 import weight 0.00
Epoch 165 Iter 9 subLoss 3351.1 multi 3.99 import weight 0.00
Epoch 165 Iter 10 subLoss 2850.1 multi -19.90 import weight 0.00
Epoch 165 Iter 11 subLoss 3174.5 multi 15.93 import weight 0.00
Epoch 165 Acc: 97.90 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 317 train Loss: 3572.9 test Loss: 339.4
Epoch 166 Iter 0 subLoss 3639.0 multi -10.94 import weight 0.00
Epoch 166 Iter 1 subLoss 18819.6 multi 1.00 import weight 0.00
Epoch 166 Iter 2 subLoss 4781.7 multi -1.98 import weight 0.00
Epoch 166 Iter 3 subLoss 6708.7 multi 9.96 import weight 0.00
Epoch 166 Iter 4 subLoss 5318.2 multi 3.99 import weight 0.00
Epoch 166 Iter 5 subLoss 3729.7 multi 1.00 import weight 0.00
Epoch 166 Iter 6 subLoss 3304.6 multi 12.94 import weight 1.00
Epoch 166 Iter 7 subLoss 2711.7 multi -4.97 import weight 0.00
Epoch 166 Iter 8 subLoss 3437.2 multi 3.99 import weight 0.00
Epoch 166 Iter 9 subLoss 2892.0 multi 3.99 import weight 0.00
Epoch 166 Iter 10 subLoss 2528.8 multi -1.99 import weight 0.00
Epoch 166 Iter 11 subLoss 3290.7 multi 3.99 import weight 0.00
Epoch 166 Acc: 98.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 329 train Loss: 2849.4 test Loss: 287.2
Epoch 167 Iter 0 subLoss 3056.8 multi 3.99 import weight 0.00
Epoch 167 Iter 1 subLoss 2295.1 multi -1.99 import weight 0.00
Epoch 167 Iter 2 subLoss 2838.7 multi -1.98 import weight 0.00
Epoch 167 Iter 3 subLoss 2384.9 multi 1.00 import weight 0.00
Epoch 167 Iter 4 subLoss 2841.5 multi 18.91 import weight 1.00
Epoch 167 Iter 5 subLoss 3340.0 multi 3.98 import weight 0.00
Epoch 167 Iter 6 subLoss 2086.8 multi 1.00 import weight 0.00
Epoch 167 Iter 7 subLoss 2708.0 multi 1.00 import weight 0.00
Epoch 167 Iter 8 subLoss 2832.1 multi 1.00 import weight 0.00
Epoch 167 Iter 9 subLoss 2628.0 multi 1.00 import weight 0.00
Epoch 167 Iter 10 subLoss 2578.6 multi 6.97 import weight 0.00
Epoch 167 Iter 11 subLoss 2038.3 multi 1.00 import weight 0.00
Epoch 167 Acc: 98.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 203 train Loss: 2557.4 test Loss: 260.5
Epoch 168 Iter 0 subLoss 2352.6 multi 1.00 import weight 0.00
Epoch 168 Iter 1 subLoss 2046.0 multi -1.99 import weight 0.00
Epoch 168 Iter 2 subLoss 2845.3 multi 18.91 import weight 1.00
Epoch 168 Iter 3 subLoss 3129.3 multi -1.99 import weight 0.00
Epoch 168 Iter 4 subLoss 2869.3 multi 3.99 import weight 0.00
Epoch 168 Iter 5 subLoss 2464.6 multi 1.00 import weight 0.00
Epoch 168 Iter 6 subLoss 2046.2 multi 1.00 import weight 0.00
Epoch 168 Iter 7 subLoss 2559.3 multi -7.96 import weight 0.00
Epoch 168 Iter 8 subLoss 2944.5 multi 1.00 import weight 0.00
Epoch 168 Iter 9 subLoss 2469.1 multi 3.99 import weight 0.00
Epoch 168 Iter 10 subLoss 2569.0 multi 1.00 import weight 0.00
Epoch 168 Iter 11 subLoss 2365.6 multi 1.00 import weight 0.00
Epoch 168 Acc: 98.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 236 train Loss: 2507.7 test Loss: 245.8
Epoch 169 Iter 0 subLoss 2495.3 multi 3.99 import weight 0.00
Epoch 169 Iter 1 subLoss 2669.6 multi -1.99 import weight 0.00
Epoch 169 Iter 2 subLoss 2675.8 multi 1.00 import weight 0.00
Epoch 169 Iter 3 subLoss 2135.0 multi 1.00 import weight 0.00
Epoch 169 Iter 4 subLoss 2546.8 multi 9.96 import weight 0.00
Epoch 169 Iter 5 subLoss 2263.7 multi 3.99 import weight 0.00
Epoch 169 Iter 6 subLoss 2736.9 multi -4.97 import weight 0.00
Epoch 169 Iter 7 subLoss 2711.9 multi -4.97 import weight 0.00
Epoch 169 Iter 8 subLoss 2663.4 multi 1.00 import weight 0.00
Epoch 169 Iter 9 subLoss 3125.5 multi 1.00 import weight 0.00
Epoch 169 Iter 10 subLoss 2390.1 multi 3.99 import weight 0.00
Epoch 169 Iter 11 subLoss 2353.2 multi 3.99 import weight 0.00
Epoch 169 Acc: 98.44 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 235 train Loss: 2538.7 test Loss: 241.1
Epoch 170 Iter 0 subLoss 2383.6 multi 3.98 import weight 0.00
Epoch 170 Iter 1 subLoss 2416.7 multi 1.00 import weight 0.00
Epoch 170 Iter 2 subLoss 2184.8 multi 1.00 import weight 0.00
Epoch 170 Iter 3 subLoss 2549.8 multi 12.94 import weight 0.00
Epoch 170 Iter 4 subLoss 2678.4 multi 1.00 import weight 0.00
Epoch 170 Iter 5 subLoss 2212.3 multi 1.00 import weight 0.00
Epoch 170 Iter 6 subLoss 2495.4 multi 6.97 import weight 0.00
Epoch 170 Iter 7 subLoss 2750.2 multi 3.99 import weight 0.00
Epoch 170 Iter 8 subLoss 2138.7 multi 3.99 import weight 0.00
Epoch 170 Iter 9 subLoss 2248.3 multi 3.99 import weight 0.00
Epoch 170 Iter 10 subLoss 2273.4 multi -4.97 import weight 0.00
Epoch 170 Iter 11 subLoss 2168.8 multi 1.00 import weight 0.00
Epoch 170 Acc: 98.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 216 train Loss: 2417.5 test Loss: 251.0
Epoch 171 Iter 0 subLoss 2771.4 multi 6.97 import weight 0.00
Epoch 171 Iter 1 subLoss 2443.9 multi -4.97 import weight 0.00
Epoch 171 Iter 2 subLoss 2276.0 multi -1.98 import weight 0.00
Epoch 171 Iter 3 subLoss 2960.8 multi -4.97 import weight 0.00
Epoch 171 Iter 4 subLoss 5280.7 multi -4.97 import weight 0.00
Epoch 171 Iter 5 subLoss 59790.2 multi 1.00 import weight 0.00
Epoch 171 Iter 6 subLoss 3427.0 multi -4.97 import weight 0.00
Epoch 171 Iter 7 subLoss 4496.4 multi -7.96 import weight 0.00
Epoch 171 Iter 8 subLoss 11859.5 multi 3.99 import weight 0.00
Epoch 171 Iter 9 subLoss 4366.7 multi 6.97 import weight 0.00
Epoch 171 Iter 10 subLoss 2946.0 multi 3.99 import weight 0.00
Epoch 171 Iter 11 subLoss 2676.9 multi 3.98 import weight 0.00
Epoch 171 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 267 train Loss: 2631.3 test Loss: 270.5
Epoch 172 Iter 0 subLoss 2809.5 multi 6.97 import weight 0.00
Epoch 172 Iter 1 subLoss 2394.4 multi 3.99 import weight 0.00
Epoch 172 Iter 2 subLoss 2488.1 multi 9.96 import weight 0.00
Epoch 172 Iter 3 subLoss 2677.5 multi 6.97 import weight 0.00
Epoch 172 Iter 4 subLoss 1937.3 multi 1.00 import weight 0.00
Epoch 172 Iter 5 subLoss 2535.7 multi -1.99 import weight 0.00
Epoch 172 Iter 6 subLoss 2612.8 multi -1.98 import weight 0.00
Epoch 172 Iter 7 subLoss 2049.9 multi 3.98 import weight 0.00
Epoch 172 Iter 8 subLoss 2735.4 multi -1.98 import weight 0.00
Epoch 172 Iter 9 subLoss 2727.9 multi -4.97 import weight 0.00
Epoch 172 Iter 10 subLoss 2337.7 multi 1.00 import weight 0.00
Epoch 172 Iter 11 subLoss 2408.4 multi -7.96 import weight 0.00
Epoch 172 Acc: 98.29 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 240 train Loss: 2764.3 test Loss: 262.4
Epoch 173 Iter 0 subLoss 2599.6 multi 1.00 import weight 0.00
Epoch 173 Iter 1 subLoss 2348.1 multi -4.97 import weight 0.00
Epoch 173 Iter 2 subLoss 3032.9 multi 1.00 import weight 0.00
Epoch 173 Iter 3 subLoss 2931.9 multi 3.99 import weight 0.00
Epoch 173 Iter 4 subLoss 2733.3 multi -1.99 import weight 0.00
Epoch 173 Iter 5 subLoss 2729.1 multi -1.99 import weight 0.00
Epoch 173 Iter 6 subLoss 2434.5 multi 6.97 import weight 0.00
Epoch 173 Iter 7 subLoss 2369.3 multi 1.00 import weight 0.00
Epoch 173 Iter 8 subLoss 2553.2 multi -10.94 import weight 0.00
Epoch 173 Iter 9 subLoss 2451.3 multi 1.00 import weight 0.00
Epoch 173 Iter 10 subLoss 2760.4 multi -4.97 import weight 0.00
Epoch 173 Iter 11 subLoss 2478.0 multi -10.94 import weight 0.00
Epoch 173 Acc: 97.30 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 247 train Loss: 3998.7 test Loss: 396.7
Epoch 174 Iter 0 subLoss 3994.4 multi -13.93 import weight 0.00
Epoch 174 Iter 1 subLoss 18833.7 multi 3.99 import weight 0.00
Epoch 174 Iter 2 subLoss 7018.5 multi -4.97 import weight 0.00
Epoch 174 Iter 3 subLoss 20996.4 multi 1.00 import weight 0.00
Epoch 174 Iter 4 subLoss 8384.0 multi 1.00 import weight 0.00
Epoch 174 Iter 5 subLoss 6494.3 multi 6.97 import weight 0.00
Epoch 174 Iter 6 subLoss 3707.3 multi -4.97 import weight 0.00
Epoch 174 Iter 7 subLoss 4679.9 multi 1.00 import weight 0.00
Epoch 174 Iter 8 subLoss 4563.7 multi 6.97 import weight 0.00
Epoch 174 Iter 9 subLoss 2900.8 multi 1.00 import weight 0.00
Epoch 174 Iter 10 subLoss 3057.6 multi 6.97 import weight 0.00
Epoch 174 Iter 11 subLoss 2680.3 multi -13.93 import weight 0.00
Epoch 174 Acc: 97.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 268 train Loss: 3461.4 test Loss: 407.0
Epoch 175 Iter 0 subLoss 3181.4 multi 1.00 import weight 0.00
Epoch 175 Iter 1 subLoss 3219.5 multi 6.97 import weight 0.00
Epoch 175 Iter 2 subLoss 2578.1 multi 6.97 import weight 0.00
Epoch 175 Iter 3 subLoss 2714.0 multi -1.98 import weight 0.00
Epoch 175 Iter 4 subLoss 2809.2 multi 9.96 import weight 0.00
Epoch 175 Iter 5 subLoss 3072.6 multi -4.97 import weight 0.00
Epoch 175 Iter 6 subLoss 3053.3 multi 9.96 import weight 0.00
Epoch 175 Iter 7 subLoss 2818.2 multi -13.93 import weight 0.00
Epoch 175 Iter 8 subLoss 3181.1 multi 3.99 import weight 1.00
Epoch 175 Iter 9 subLoss 2643.5 multi -1.99 import weight 0.00
Epoch 175 Iter 10 subLoss 2702.4 multi 3.98 import weight 0.00
Epoch 175 Iter 11 subLoss 2593.9 multi 3.99 import weight 0.00
Epoch 175 Acc: 98.21 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 259 train Loss: 2799.5 test Loss: 290.6
Epoch 176 Iter 0 subLoss 1978.4 multi 1.00 import weight 0.00
Epoch 176 Iter 1 subLoss 2569.1 multi 1.00 import weight 0.00
Epoch 176 Iter 2 subLoss 2799.0 multi -7.96 import weight 0.00
Epoch 176 Iter 3 subLoss 2759.3 multi 6.97 import weight 0.00
Epoch 176 Iter 4 subLoss 2924.9 multi 1.00 import weight 0.00
Epoch 176 Iter 5 subLoss 2616.3 multi 1.00 import weight 0.00
Epoch 176 Iter 6 subLoss 2774.3 multi 6.97 import weight 0.00
Epoch 176 Iter 7 subLoss 2803.6 multi 9.96 import weight 0.00
Epoch 176 Iter 8 subLoss 2450.5 multi 3.99 import weight 0.00
Epoch 176 Iter 9 subLoss 2454.5 multi 6.97 import weight 0.00
Epoch 176 Iter 10 subLoss 2202.8 multi 1.00 import weight 0.00
Epoch 176 Iter 11 subLoss 3187.7 multi 6.97 import weight 1.00
Epoch 176 Acc: 98.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 6.97 Pidx 318 train Loss: 2536.9 test Loss: 271.4
Epoch 177 Iter 0 subLoss 3006.2 multi 1.00 import weight 0.00
Epoch 177 Iter 1 subLoss 2560.6 multi 3.99 import weight 0.00
Epoch 177 Iter 2 subLoss 2366.1 multi 3.99 import weight 0.00
Epoch 177 Iter 3 subLoss 2382.9 multi 6.97 import weight 0.00
Epoch 177 Iter 4 subLoss 2123.0 multi 1.00 import weight 0.00
Epoch 177 Iter 5 subLoss 2074.9 multi 1.00 import weight 0.00
Epoch 177 Iter 6 subLoss 2530.1 multi 1.00 import weight 0.00
Epoch 177 Iter 7 subLoss 2243.9 multi 6.97 import weight 0.00
Epoch 177 Iter 8 subLoss 2469.5 multi -1.98 import weight 0.00
Epoch 177 Iter 9 subLoss 2632.7 multi -1.98 import weight 0.00
Epoch 177 Iter 10 subLoss 2189.2 multi 3.99 import weight 0.00
Epoch 177 Iter 11 subLoss 2711.3 multi -1.99 import weight 0.00
Epoch 177 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 271 train Loss: 2477.7 test Loss: 257.8
Epoch 178 Iter 0 subLoss 2651.9 multi 6.97 import weight 0.00
Epoch 178 Iter 1 subLoss 1949.0 multi -1.99 import weight 0.00
Epoch 178 Iter 2 subLoss 2495.7 multi 6.97 import weight 0.00
Epoch 178 Iter 3 subLoss 2960.2 multi -1.99 import weight 0.00
Epoch 178 Iter 4 subLoss 2094.4 multi -1.99 import weight 0.00
Epoch 178 Iter 5 subLoss 2400.5 multi -4.97 import weight 0.00
Epoch 178 Iter 6 subLoss 2635.6 multi 1.00 import weight 0.00
Epoch 178 Iter 7 subLoss 2395.4 multi 3.98 import weight 0.00
Epoch 178 Iter 8 subLoss 2725.4 multi -4.97 import weight 0.00
Epoch 178 Iter 9 subLoss 2184.3 multi 6.97 import weight 0.00
Epoch 178 Iter 10 subLoss 2428.2 multi -4.97 import weight 0.00
Epoch 178 Iter 11 subLoss 2506.8 multi -16.91 import weight 0.00
Epoch 178 Acc: 96.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 250 train Loss: 3216.6 test Loss: 487.7
Epoch 179 Iter 0 subLoss 2967.0 multi 1.00 import weight 0.00
Epoch 179 Iter 1 subLoss 3144.0 multi 18.91 import weight 0.00
Epoch 179 Iter 2 subLoss 2874.1 multi 9.96 import weight 0.00
Epoch 179 Iter 3 subLoss 2584.0 multi -4.97 import weight 0.00
Epoch 179 Iter 4 subLoss 2599.7 multi 3.98 import weight 0.00
Epoch 179 Iter 5 subLoss 2848.5 multi 21.90 import weight 1.00
Epoch 179 Iter 6 subLoss 2998.9 multi 9.96 import weight 0.00
Epoch 179 Iter 7 subLoss 2450.7 multi 9.96 import weight 0.00
Epoch 179 Iter 8 subLoss 2806.5 multi 12.94 import weight 0.00
Epoch 179 Iter 9 subLoss 2555.7 multi -7.96 import weight 0.00
Epoch 179 Iter 10 subLoss 2632.8 multi 3.99 import weight 0.00
Epoch 179 Iter 11 subLoss 2290.2 multi 1.00 import weight 0.00
Epoch 179 Acc: 98.29 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 229 train Loss: 2460.2 test Loss: 273.0
Epoch 180 Iter 0 subLoss 2412.9 multi -1.99 import weight 0.00
Epoch 180 Iter 1 subLoss 2378.4 multi -7.96 import weight 0.00
Epoch 180 Iter 2 subLoss 2703.5 multi 6.97 import weight 0.00
Epoch 180 Iter 3 subLoss 2258.6 multi -7.96 import weight 0.00
Epoch 180 Iter 4 subLoss 2313.8 multi 1.00 import weight 0.00
Epoch 180 Iter 5 subLoss 2254.9 multi -4.97 import weight 0.00
Epoch 180 Iter 6 subLoss 2339.2 multi 3.98 import weight 0.00
Epoch 180 Iter 7 subLoss 2474.7 multi -10.94 import weight 0.00
Epoch 180 Iter 8 subLoss 3416.2 multi 1.00 import weight 0.00
Epoch 180 Iter 9 subLoss 2801.6 multi 15.93 import weight 0.00
Epoch 180 Iter 10 subLoss 3775.3 multi 1.00 import weight 0.00
Epoch 180 Iter 11 subLoss 2844.2 multi 24.88 import weight 1.00
Epoch 180 Acc: 94.42 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 24.88 Pidx 284 train Loss: 7408.0 test Loss: 1126.7
Epoch 181 Iter 0 subLoss 7665.6 multi 12.94 import weight 0.00
Epoch 181 Iter 1 subLoss 34369.3 multi -1.99 import weight 0.00
Epoch 181 Iter 2 subLoss 215159.5 multi 1.00 import weight 0.00
Epoch 181 Iter 3 subLoss 28346.9 multi 3.99 import weight 0.00
Epoch 181 Iter 4 subLoss 11405.8 multi 3.99 import weight 0.00
Epoch 181 Iter 5 subLoss 5595.8 multi 1.00 import weight 0.00
Epoch 181 Iter 6 subLoss 5440.8 multi 12.94 import weight 0.00
Epoch 181 Iter 7 subLoss 4221.2 multi -1.99 import weight 0.00
Epoch 181 Iter 8 subLoss 5101.2 multi -4.97 import weight 0.00
Epoch 181 Iter 9 subLoss 23013.8 multi 1.00 import weight 0.00
Epoch 181 Iter 10 subLoss 4095.8 multi 6.97 import weight 0.00
Epoch 181 Iter 11 subLoss 3538.4 multi -10.94 import weight 0.00
Epoch 181 Acc: 97.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 353 train Loss: 3534.0 test Loss: 385.9
Epoch 182 Iter 0 subLoss 3334.9 multi -16.91 import weight 0.00
Epoch 182 Iter 1 subLoss 16344.3 multi 1.00 import weight 0.00
Epoch 182 Iter 2 subLoss 5993.2 multi 3.98 import weight 0.00
Epoch 182 Iter 3 subLoss 3871.9 multi -1.99 import weight 0.00
Epoch 182 Iter 4 subLoss 3890.7 multi -1.99 import weight 0.00
Epoch 182 Iter 5 subLoss 4125.9 multi 1.00 import weight 0.00
Epoch 182 Iter 6 subLoss 3848.8 multi -4.97 import weight 0.00
Epoch 182 Iter 7 subLoss 4488.2 multi 6.97 import weight 0.00
Epoch 182 Iter 8 subLoss 3702.1 multi -1.99 import weight 0.00
Epoch 182 Iter 9 subLoss 3731.0 multi -13.93 import weight 0.00
Epoch 182 Iter 10 subLoss 5598.9 multi 3.98 import weight 0.00
Epoch 182 Iter 11 subLoss 4744.1 multi 1.00 import weight 0.00
Epoch 182 Acc: 97.55 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 474 train Loss: 4239.8 test Loss: 450.5
Epoch 183 Iter 0 subLoss 4268.5 multi -4.97 import weight 0.00
Epoch 183 Iter 1 subLoss 4945.7 multi 6.97 import weight 0.00
Epoch 183 Iter 2 subLoss 3687.4 multi 3.99 import weight 0.00
Epoch 183 Iter 3 subLoss 3760.6 multi -7.96 import weight 0.00
Epoch 183 Iter 4 subLoss 4210.7 multi 3.99 import weight 0.00
Epoch 183 Iter 5 subLoss 3915.2 multi 6.97 import weight 0.00
Epoch 183 Iter 6 subLoss 3408.1 multi 1.00 import weight 0.00
Epoch 183 Iter 7 subLoss 3413.8 multi 1.00 import weight 0.00
Epoch 183 Iter 8 subLoss 3135.2 multi -1.99 import weight 0.00
Epoch 183 Iter 9 subLoss 3436.1 multi 3.99 import weight 0.00
Epoch 183 Iter 10 subLoss 3298.7 multi 6.97 import weight 0.00
Epoch 183 Iter 11 subLoss 3029.1 multi -7.96 import weight 0.00
Epoch 183 Acc: 97.78 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 302 train Loss: 3545.7 test Loss: 379.9
Epoch 184 Iter 0 subLoss 3716.8 multi 6.97 import weight 0.00
Epoch 184 Iter 1 subLoss 3531.6 multi -7.96 import weight 0.00
Epoch 184 Iter 2 subLoss 3789.0 multi -4.97 import weight 0.00
Epoch 184 Iter 3 subLoss 4878.7 multi 1.00 import weight 0.00
Epoch 184 Iter 4 subLoss 4358.3 multi 1.00 import weight 0.00
Epoch 184 Iter 5 subLoss 4307.5 multi 3.99 import weight 0.00
Epoch 184 Iter 6 subLoss 3475.9 multi -4.97 import weight 0.00
Epoch 184 Iter 7 subLoss 3180.0 multi 9.96 import weight 1.00
Epoch 184 Iter 8 subLoss 2776.9 multi 9.96 import weight 0.00
Epoch 184 Iter 9 subLoss 3022.0 multi -4.97 import weight 0.00
Epoch 184 Iter 10 subLoss 3138.9 multi 1.00 import weight 0.00
Epoch 184 Iter 11 subLoss 3229.6 multi -13.93 import weight 0.00
Epoch 184 Acc: 96.89 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 322 train Loss: 3741.0 test Loss: 429.0
Epoch 185 Iter 0 subLoss 4023.1 multi 1.00 import weight 0.00
Epoch 185 Iter 1 subLoss 3469.7 multi 3.99 import weight 0.00
Epoch 185 Iter 2 subLoss 3592.6 multi 3.98 import weight 0.00
Epoch 185 Iter 3 subLoss 2806.3 multi 18.91 import weight 0.00
Epoch 185 Iter 4 subLoss 3120.1 multi 3.99 import weight 0.00
Epoch 185 Iter 5 subLoss 2604.3 multi -10.94 import weight 0.00
Epoch 185 Iter 6 subLoss 3580.8 multi -1.98 import weight 0.00
Epoch 185 Iter 7 subLoss 3959.4 multi -7.96 import weight 0.00
Epoch 185 Iter 8 subLoss 9244.7 multi 3.98 import weight 0.00
Epoch 185 Iter 9 subLoss 3155.6 multi -19.90 import weight 0.00
Epoch 185 Iter 10 subLoss 6703.1 multi 12.94 import weight 0.00
Epoch 185 Iter 11 subLoss 11837.0 multi 3.98 import weight 0.00
Epoch 185 Acc: 94.38 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 1183 train Loss: 6374.8 test Loss: 794.0
Epoch 186 Iter 0 subLoss 5863.2 multi -10.94 import weight 0.00
Epoch 186 Iter 1 subLoss 13590.3 multi 1.00 import weight 0.00
Epoch 186 Iter 2 subLoss 13074.8 multi -4.97 import weight 0.00
Epoch 186 Iter 3 subLoss 20486.9 multi 3.99 import weight 0.00
Epoch 186 Iter 4 subLoss 13508.5 multi -1.98 import weight 0.00
Epoch 186 Iter 5 subLoss 15346.6 multi 1.00 import weight 0.00
Epoch 186 Iter 6 subLoss 13630.7 multi 3.99 import weight 0.00
Epoch 186 Iter 7 subLoss 11952.9 multi 3.98 import weight 0.00
Epoch 186 Iter 8 subLoss 8433.0 multi -4.97 import weight 0.00
Epoch 186 Iter 9 subLoss 11724.6 multi -7.96 import weight 0.00
Epoch 186 Iter 10 subLoss 17607.0 multi 3.99 import weight 0.00
Epoch 186 Iter 11 subLoss 15172.2 multi 1.00 import weight 0.00
Epoch 186 Acc: 81.32 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1517 train Loss: 14279.0 test Loss: 2144.9
Epoch 187 Iter 0 subLoss 13176.4 multi -4.97 import weight 0.00
Epoch 187 Iter 1 subLoss 19241.1 multi 1.00 import weight 0.00
Epoch 187 Iter 2 subLoss 16727.1 multi 1.00 import weight 0.00
Epoch 187 Iter 3 subLoss 15489.7 multi -4.97 import weight 0.00
Epoch 187 Iter 4 subLoss 20551.0 multi 1.00 import weight 0.00
Epoch 187 Iter 5 subLoss 17008.3 multi 1.00 import weight 0.00
Epoch 187 Iter 6 subLoss 17252.0 multi 1.00 import weight 0.00
Epoch 187 Iter 7 subLoss 17129.8 multi 1.00 import weight 0.00
Epoch 187 Iter 8 subLoss 15987.3 multi 1.00 import weight 0.00
Epoch 187 Iter 9 subLoss 14353.1 multi 3.99 import weight 0.00
Epoch 187 Iter 10 subLoss 13673.1 multi 3.99 import weight 0.00
Epoch 187 Iter 11 subLoss 11582.4 multi 3.98 import weight 0.00
Epoch 187 Acc: 88.89 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 1158 train Loss: 9361.2 test Loss: 1391.9
Epoch 188 Iter 0 subLoss 8839.2 multi -1.99 import weight 0.00
Epoch 188 Iter 1 subLoss 10240.1 multi -1.99 import weight 0.00
Epoch 188 Iter 2 subLoss 11362.8 multi -4.97 import weight 0.00
Epoch 188 Iter 3 subLoss 13992.1 multi 6.97 import weight 0.00
Epoch 188 Iter 4 subLoss 10538.0 multi 1.00 import weight 0.00
Epoch 188 Iter 5 subLoss 10506.3 multi 3.99 import weight 0.00
Epoch 188 Iter 6 subLoss 7793.3 multi -1.99 import weight 0.00
Epoch 188 Iter 7 subLoss 8016.2 multi 3.98 import weight 0.00
Epoch 188 Iter 8 subLoss 7920.3 multi -1.99 import weight 0.00
Epoch 188 Iter 9 subLoss 8399.8 multi -1.99 import weight 0.00
Epoch 188 Iter 10 subLoss 8154.1 multi 3.99 import weight 0.00
Epoch 188 Iter 11 subLoss 7777.5 multi 1.00 import weight 0.00
Epoch 188 Acc: 94.06 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 777 train Loss: 6892.6 test Loss: 952.1
Epoch 189 Iter 0 subLoss 6914.9 multi 15.93 import weight 0.00
Epoch 189 Iter 1 subLoss 4306.0 multi 6.97 import weight 0.00
Epoch 189 Iter 2 subLoss 3389.4 multi -7.96 import weight 0.00
Epoch 189 Iter 3 subLoss 3527.9 multi 12.94 import weight 0.00
Epoch 189 Iter 4 subLoss 3348.7 multi 3.99 import weight 0.00
Epoch 189 Iter 5 subLoss 3319.4 multi -13.93 import weight 0.00
Epoch 189 Iter 6 subLoss 4009.2 multi 6.97 import weight 0.00
Epoch 189 Iter 7 subLoss 3972.0 multi 3.99 import weight 0.00
Epoch 189 Iter 8 subLoss 2869.0 multi 6.97 import weight 0.00
Epoch 189 Iter 9 subLoss 2670.7 multi 9.96 import weight 0.00
Epoch 189 Iter 10 subLoss 2983.0 multi -1.98 import weight 0.00
Epoch 189 Iter 11 subLoss 2582.8 multi -1.98 import weight 0.00
Epoch 189 Acc: 98.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 258 train Loss: 3198.7 test Loss: 315.8
Epoch 190 Iter 0 subLoss 3391.2 multi 3.99 import weight 0.00
Epoch 190 Iter 1 subLoss 3074.4 multi -1.98 import weight 0.00
Epoch 190 Iter 2 subLoss 2707.4 multi 9.96 import weight 0.00
Epoch 190 Iter 3 subLoss 3070.8 multi 1.00 import weight 0.00
Epoch 190 Iter 4 subLoss 2929.1 multi 3.99 import weight 0.00
Epoch 190 Iter 5 subLoss 2636.6 multi 6.97 import weight 0.00
Epoch 190 Iter 6 subLoss 2795.8 multi -4.97 import weight 0.00
Epoch 190 Iter 7 subLoss 2861.8 multi 9.96 import weight 0.00
Epoch 190 Iter 8 subLoss 2511.1 multi -1.98 import weight 0.00
Epoch 190 Iter 9 subLoss 3175.4 multi 18.91 import weight 0.00
Epoch 190 Iter 10 subLoss 3027.0 multi -1.99 import weight 0.00
Epoch 190 Iter 11 subLoss 3001.9 multi 1.00 import weight 0.00
Epoch 190 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 300 train Loss: 3063.5 test Loss: 320.3
Epoch 191 Iter 0 subLoss 2892.5 multi 6.97 import weight 0.00
Epoch 191 Iter 1 subLoss 2621.9 multi -1.99 import weight 0.00
Epoch 191 Iter 2 subLoss 2338.5 multi 6.97 import weight 0.00
Epoch 191 Iter 3 subLoss 2105.9 multi -1.99 import weight 0.00
Epoch 191 Iter 4 subLoss 2636.9 multi 6.97 import weight 0.00
Epoch 191 Iter 5 subLoss 3013.7 multi 1.00 import weight 0.00
Epoch 191 Iter 6 subLoss 2284.7 multi -1.98 import weight 0.00
Epoch 191 Iter 7 subLoss 2697.1 multi 3.99 import weight 0.00
Epoch 191 Iter 8 subLoss 2057.6 multi -7.96 import weight 0.00
Epoch 191 Iter 9 subLoss 2738.0 multi -4.97 import weight 0.00
Epoch 191 Iter 10 subLoss 2679.4 multi 12.94 import weight 0.00
Epoch 191 Iter 11 subLoss 2505.0 multi -13.93 import weight 0.00
Epoch 191 Acc: 98.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 250 train Loss: 2817.1 test Loss: 285.8
Epoch 192 Iter 0 subLoss 2990.0 multi 1.00 import weight 0.00
Epoch 192 Iter 1 subLoss 2912.5 multi -10.94 import weight 0.00
Epoch 192 Iter 2 subLoss 3311.4 multi -10.94 import weight 0.00
Epoch 192 Iter 3 subLoss 12167.3 multi -1.99 import weight 0.00
Epoch 192 Iter 4 subLoss 76220.1 multi 1.00 import weight 0.00
Epoch 192 Iter 5 subLoss 5920.7 multi 6.97 import weight 0.00
Epoch 192 Iter 6 subLoss 2968.0 multi 3.99 import weight 0.00
Epoch 192 Iter 7 subLoss 2296.2 multi 1.00 import weight 0.00
Epoch 192 Iter 8 subLoss 3142.3 multi 15.93 import weight 0.00
Epoch 192 Iter 9 subLoss 2658.9 multi 9.96 import weight 0.00
Epoch 192 Iter 10 subLoss 2128.1 multi 3.99 import weight 0.00
Epoch 192 Iter 11 subLoss 2538.7 multi 3.98 import weight 0.00
Epoch 192 Acc: 98.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 253 train Loss: 2510.9 test Loss: 267.5
Epoch 193 Iter 0 subLoss 2473.2 multi -7.96 import weight 0.00
Epoch 193 Iter 1 subLoss 2625.4 multi 1.00 import weight 0.00
Epoch 193 Iter 2 subLoss 2859.7 multi -28.85 import weight 0.00
Epoch 193 Iter 3 subLoss 9825.6 multi 1.00 import weight 0.00
Epoch 193 Iter 4 subLoss 5268.8 multi -4.97 import weight 0.00
Epoch 193 Iter 5 subLoss 18444.7 multi 1.00 import weight 0.00
Epoch 193 Iter 6 subLoss 5060.6 multi -1.99 import weight 0.00
Epoch 193 Iter 7 subLoss 8146.5 multi 1.00 import weight 0.00
Epoch 193 Iter 8 subLoss 4590.6 multi 6.97 import weight 0.00
Epoch 193 Iter 9 subLoss 2739.7 multi -1.98 import weight 0.00
Epoch 193 Iter 10 subLoss 3431.5 multi 6.97 import weight 0.00
Epoch 193 Iter 11 subLoss 2582.8 multi 1.00 import weight 0.00
Epoch 193 Acc: 98.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 258 train Loss: 2787.0 test Loss: 295.6
Epoch 194 Iter 0 subLoss 2841.1 multi 27.87 import weight 1.00
Epoch 194 Iter 1 subLoss 3173.1 multi 21.90 import weight 0.00
Epoch 194 Iter 2 subLoss 4198.8 multi 6.97 import weight 0.00
Epoch 194 Iter 3 subLoss 2174.1 multi -1.99 import weight 0.00
Epoch 194 Iter 4 subLoss 2445.2 multi -4.97 import weight 0.00
Epoch 194 Iter 5 subLoss 2868.7 multi 9.96 import weight 0.00
Epoch 194 Iter 6 subLoss 2457.8 multi 9.96 import weight 0.00
Epoch 194 Iter 7 subLoss 2101.4 multi 1.00 import weight 0.00
Epoch 194 Iter 8 subLoss 2436.0 multi 6.97 import weight 0.00
Epoch 194 Iter 9 subLoss 2882.7 multi -13.93 import weight 0.00
Epoch 194 Iter 10 subLoss 2908.7 multi 1.00 import weight 0.00
Epoch 194 Iter 11 subLoss 2314.3 multi 3.99 import weight 0.00
Epoch 194 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 231 train Loss: 2584.3 test Loss: 299.0
Epoch 195 Iter 0 subLoss 2414.6 multi 1.00 import weight 0.00
Epoch 195 Iter 1 subLoss 2182.5 multi 6.97 import weight 0.00
Epoch 195 Iter 2 subLoss 2472.3 multi -4.97 import weight 0.00
Epoch 195 Iter 3 subLoss 2731.4 multi 1.00 import weight 0.00
Epoch 195 Iter 4 subLoss 2410.0 multi 3.98 import weight 0.00
Epoch 195 Iter 5 subLoss 2466.4 multi -4.97 import weight 0.00
Epoch 195 Iter 6 subLoss 2494.5 multi 9.96 import weight 0.00
Epoch 195 Iter 7 subLoss 2092.7 multi 1.00 import weight 0.00
Epoch 195 Iter 8 subLoss 2230.7 multi -1.99 import weight 0.00
Epoch 195 Iter 9 subLoss 2949.3 multi 3.98 import weight 0.00
Epoch 195 Iter 10 subLoss 2271.7 multi 1.00 import weight 0.00
Epoch 195 Iter 11 subLoss 2426.7 multi -10.94 import weight 0.00
Epoch 195 Acc: 98.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 242 train Loss: 2592.2 test Loss: 287.5
Epoch 196 Iter 0 subLoss 2412.2 multi 6.97 import weight 0.00
Epoch 196 Iter 1 subLoss 2285.3 multi -1.99 import weight 0.00
Epoch 196 Iter 2 subLoss 2418.7 multi 9.96 import weight 0.00
Epoch 196 Iter 3 subLoss 2554.1 multi -4.97 import weight 0.00
Epoch 196 Iter 4 subLoss 2610.4 multi 1.00 import weight 0.00
Epoch 196 Iter 5 subLoss 2725.0 multi -1.99 import weight 0.00
Epoch 196 Iter 6 subLoss 2870.4 multi 3.98 import weight 0.00
Epoch 196 Iter 7 subLoss 1986.4 multi -1.99 import weight 0.00
Epoch 196 Iter 8 subLoss 2661.3 multi -1.98 import weight 0.00
Epoch 196 Iter 9 subLoss 2383.3 multi 6.97 import weight 0.00
Epoch 196 Iter 10 subLoss 2491.2 multi 12.94 import weight 0.00
Epoch 196 Iter 11 subLoss 2377.0 multi -4.97 import weight 0.00
Epoch 196 Acc: 96.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 237 train Loss: 2980.6 test Loss: 442.3
Epoch 197 Iter 0 subLoss 2484.8 multi 1.00 import weight 0.00
Epoch 197 Iter 1 subLoss 2637.3 multi 6.97 import weight 0.00
Epoch 197 Iter 2 subLoss 2212.5 multi 1.00 import weight 0.00
Epoch 197 Iter 3 subLoss 2398.2 multi 3.99 import weight 0.00
Epoch 197 Iter 4 subLoss 2510.9 multi -1.99 import weight 0.00
Epoch 197 Iter 5 subLoss 2214.5 multi 3.98 import weight 0.00
Epoch 197 Iter 6 subLoss 2430.2 multi 6.97 import weight 0.00
Epoch 197 Iter 7 subLoss 2195.5 multi -10.94 import weight 0.00
Epoch 197 Iter 8 subLoss 2969.5 multi 6.97 import weight 0.00
Epoch 197 Iter 9 subLoss 2212.8 multi 6.97 import weight 0.00
Epoch 197 Iter 10 subLoss 2277.7 multi 3.99 import weight 0.00
Epoch 197 Iter 11 subLoss 2293.3 multi 1.00 import weight 0.00
Epoch 197 Acc: 98.29 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 229 train Loss: 2275.4 test Loss: 253.4
Epoch 198 Iter 0 subLoss 2107.0 multi 1.00 import weight 0.00
Epoch 198 Iter 1 subLoss 2730.8 multi 1.00 import weight 0.00
Epoch 198 Iter 2 subLoss 1979.8 multi 3.99 import weight 0.00
Epoch 198 Iter 3 subLoss 2233.7 multi 1.00 import weight 0.00
Epoch 198 Iter 4 subLoss 1999.7 multi -1.99 import weight 0.00
Epoch 198 Iter 5 subLoss 2929.1 multi 3.99 import weight 0.00
Epoch 198 Iter 6 subLoss 2310.3 multi 6.97 import weight 0.00
Epoch 198 Iter 7 subLoss 2299.2 multi 3.99 import weight 0.00
Epoch 198 Iter 8 subLoss 2255.5 multi -1.99 import weight 0.00
Epoch 198 Iter 9 subLoss 2063.9 multi -1.99 import weight 0.00
Epoch 198 Iter 10 subLoss 2168.9 multi 3.99 import weight 0.00
Epoch 198 Iter 11 subLoss 2118.0 multi -7.96 import weight 0.00
Epoch 198 Acc: 98.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 211 train Loss: 2244.5 test Loss: 250.3
Epoch 199 Iter 0 subLoss 2155.8 multi 1.00 import weight 0.00
Epoch 199 Iter 1 subLoss 2198.0 multi -7.96 import weight 0.00
Epoch 199 Iter 2 subLoss 2644.3 multi -16.91 import weight 0.00
Epoch 199 Iter 3 subLoss 6491.5 multi 9.96 import weight 0.00
Epoch 199 Iter 4 subLoss 29201.0 multi 1.00 import weight 0.00
Epoch 199 Iter 5 subLoss 5764.8 multi 6.97 import weight 0.00
Epoch 199 Iter 6 subLoss 2869.6 multi 12.94 import weight 0.00
Epoch 199 Iter 7 subLoss 2630.9 multi 9.96 import weight 0.00
Epoch 199 Iter 8 subLoss 2466.3 multi -1.99 import weight 0.00
Epoch 199 Iter 9 subLoss 2316.2 multi 9.96 import weight 0.00
Epoch 199 Iter 10 subLoss 2931.0 multi -1.98 import weight 0.00
Epoch 199 Iter 11 subLoss 2502.7 multi -16.91 import weight 0.00
Epoch 199 Acc: 96.67 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 250 train Loss: 4586.8 test Loss: 520.3
Epoch 200 Iter 0 subLoss 4327.5 multi 1.00 import weight 0.00
Epoch 200 Iter 1 subLoss 2596.1 multi 1.00 import weight 0.00
Epoch 200 Iter 2 subLoss 3532.4 multi -7.96 import weight 0.00
Epoch 200 Iter 3 subLoss 10060.6 multi 3.99 import weight 0.00
Epoch 200 Iter 4 subLoss 3327.5 multi 3.99 import weight 0.00
Epoch 200 Iter 5 subLoss 3192.5 multi -19.90 import weight 0.00
Epoch 200 Iter 6 subLoss 6929.8 multi -16.91 import weight 0.00
Epoch 200 Iter 7 subLoss 93279.7 multi 1.00 import weight 0.00
Epoch 200 Iter 8 subLoss 30629.7 multi 1.00 import weight 0.00
Epoch 200 Iter 9 subLoss 19141.0 multi 1.00 import weight 0.00
Epoch 200 Iter 10 subLoss 13157.7 multi 1.00 import weight 0.00
Epoch 200 Iter 11 subLoss 12217.7 multi -1.99 import weight 0.00
Epoch 200 Acc: 80.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1221 train Loss: 16340.7 test Loss: 3344.0
Epoch 201 Iter 0 subLoss 16189.3 multi -1.99 import weight 0.00
Epoch 201 Iter 1 subLoss 22351.0 multi 1.00 import weight 0.00
Epoch 201 Iter 2 subLoss 18859.1 multi -4.97 import weight 0.00
Epoch 201 Iter 3 subLoss 53896.9 multi 3.99 import weight 0.00
Epoch 201 Iter 4 subLoss 15259.8 multi 1.00 import weight 0.00
Epoch 201 Iter 5 subLoss 13034.3 multi 6.97 import weight 0.00
Epoch 201 Iter 6 subLoss 8701.7 multi 1.00 import weight 0.00
Epoch 201 Iter 7 subLoss 7827.2 multi 1.00 import weight 0.00
Epoch 201 Iter 8 subLoss 7419.5 multi 1.00 import weight 0.00
Epoch 201 Iter 9 subLoss 7059.3 multi -4.97 import weight 0.00
Epoch 201 Iter 10 subLoss 9139.0 multi 1.00 import weight 0.00
Epoch 201 Iter 11 subLoss 7822.6 multi 3.99 import weight 0.00
Epoch 201 Acc: 91.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 782 train Loss: 6779.5 test Loss: 1192.3
Epoch 202 Iter 0 subLoss 6490.8 multi 12.94 import weight 0.00
Epoch 202 Iter 1 subLoss 3682.9 multi 6.97 import weight 0.00
Epoch 202 Iter 2 subLoss 3895.2 multi 1.00 import weight 0.00
Epoch 202 Iter 3 subLoss 3300.8 multi 9.96 import weight 0.00
Epoch 202 Iter 4 subLoss 2935.3 multi 1.00 import weight 0.00
Epoch 202 Iter 5 subLoss 2744.9 multi -13.93 import weight 0.00
Epoch 202 Iter 6 subLoss 3278.4 multi 6.97 import weight 0.00
Epoch 202 Iter 7 subLoss 2874.6 multi 3.99 import weight 0.00
Epoch 202 Iter 8 subLoss 2854.8 multi -28.85 import weight 0.00
Epoch 202 Iter 9 subLoss 4410.1 multi 12.94 import weight 0.00
Epoch 202 Iter 10 subLoss 2777.5 multi 12.94 import weight 0.00
Epoch 202 Iter 11 subLoss 2851.6 multi -25.87 import weight 0.00
Epoch 202 Acc: 94.65 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 285 train Loss: 4607.5 test Loss: 803.9
Epoch 203 Iter 0 subLoss 4599.5 multi 9.96 import weight 0.00
Epoch 203 Iter 1 subLoss 3425.8 multi -7.96 import weight 0.00
Epoch 203 Iter 2 subLoss 4004.6 multi 9.96 import weight 0.00
Epoch 203 Iter 3 subLoss 3176.5 multi 24.88 import weight 0.00
Epoch 203 Iter 4 subLoss 3117.4 multi -4.97 import weight 0.00
Epoch 203 Iter 5 subLoss 3133.0 multi 1.00 import weight 0.00
Epoch 203 Iter 6 subLoss 2835.3 multi 3.99 import weight 0.00
Epoch 203 Iter 7 subLoss 2683.3 multi -16.91 import weight 0.00
Epoch 203 Iter 8 subLoss 3773.4 multi 1.00 import weight 0.00
Epoch 203 Iter 9 subLoss 3232.6 multi 18.91 import weight 0.00
Epoch 203 Iter 10 subLoss 3812.7 multi 6.97 import weight 0.00
Epoch 203 Iter 11 subLoss 2605.3 multi -10.94 import weight 0.00
Epoch 203 Acc: 97.88 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 260 train Loss: 3107.4 test Loss: 367.9
Epoch 204 Iter 0 subLoss 3013.0 multi 3.99 import weight 0.00
Epoch 204 Iter 1 subLoss 2621.2 multi 1.00 import weight 0.00
Epoch 204 Iter 2 subLoss 2577.0 multi 3.98 import weight 0.00
Epoch 204 Iter 3 subLoss 2436.8 multi 9.96 import weight 0.00
Epoch 204 Iter 4 subLoss 2404.2 multi -7.96 import weight 0.00
Epoch 204 Iter 5 subLoss 3090.7 multi -1.99 import weight 0.00
Epoch 204 Iter 6 subLoss 2688.9 multi -13.93 import weight 0.00
Epoch 204 Iter 7 subLoss 3575.6 multi -7.96 import weight 0.00
Epoch 204 Iter 8 subLoss 10335.0 multi 1.00 import weight 0.00
Epoch 204 Iter 9 subLoss 5771.7 multi -4.97 import weight 0.00
Epoch 204 Iter 10 subLoss 21563.0 multi 1.00 import weight 0.00
Epoch 204 Iter 11 subLoss 15021.7 multi 1.00 import weight 0.00
Epoch 204 Acc: 89.73 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1502 train Loss: 10096.0 test Loss: 1515.4
Epoch 205 Iter 0 subLoss 9792.6 multi -1.98 import weight 0.00
Epoch 205 Iter 1 subLoss 16220.3 multi 1.00 import weight 0.00
Epoch 205 Iter 2 subLoss 12893.6 multi 1.00 import weight 0.00
Epoch 205 Iter 3 subLoss 10052.8 multi -1.99 import weight 0.00
Epoch 205 Iter 4 subLoss 14463.2 multi 1.00 import weight 0.00
Epoch 205 Iter 5 subLoss 12567.3 multi 1.00 import weight 0.00
Epoch 205 Iter 6 subLoss 10917.4 multi 3.99 import weight 0.00
Epoch 205 Iter 7 subLoss 3675.7 multi 1.00 import weight 0.00
Epoch 205 Iter 8 subLoss 3759.6 multi 9.96 import weight 0.00
Epoch 205 Iter 9 subLoss 2595.7 multi 3.99 import weight 0.00
Epoch 205 Iter 10 subLoss 2935.8 multi 3.99 import weight 0.00
Epoch 205 Iter 11 subLoss 2680.2 multi -10.94 import weight 0.00
Epoch 205 Acc: 97.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 268 train Loss: 2959.7 test Loss: 323.3
Epoch 206 Iter 0 subLoss 2940.4 multi -1.99 import weight 0.00
Epoch 206 Iter 1 subLoss 3015.8 multi 6.97 import weight 0.00
Epoch 206 Iter 2 subLoss 2524.3 multi -4.97 import weight 0.00
Epoch 206 Iter 3 subLoss 3012.9 multi 9.96 import weight 0.00
Epoch 206 Iter 4 subLoss 2892.0 multi 6.97 import weight 0.00
Epoch 206 Iter 5 subLoss 2305.0 multi -13.93 import weight 0.00
Epoch 206 Iter 6 subLoss 3032.6 multi -4.97 import weight 0.00
Epoch 206 Iter 7 subLoss 3076.2 multi 3.99 import weight 0.00
Epoch 206 Iter 8 subLoss 2893.8 multi 9.96 import weight 0.00
Epoch 206 Iter 9 subLoss 3070.0 multi -10.94 import weight 0.00
Epoch 206 Iter 10 subLoss 3000.3 multi 3.99 import weight 0.00
Epoch 206 Iter 11 subLoss 2763.6 multi -4.97 import weight 0.00
Epoch 206 Acc: 97.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 276 train Loss: 3244.3 test Loss: 336.7
Epoch 207 Iter 0 subLoss 3013.7 multi 9.96 import weight 0.00
Epoch 207 Iter 1 subLoss 2423.6 multi -13.93 import weight 0.00
Epoch 207 Iter 2 subLoss 3213.9 multi 9.96 import weight 0.00
Epoch 207 Iter 3 subLoss 2462.4 multi 1.00 import weight 0.00
Epoch 207 Iter 4 subLoss 2479.4 multi -10.94 import weight 0.00
Epoch 207 Iter 5 subLoss 2967.6 multi 9.96 import weight 0.00
Epoch 207 Iter 6 subLoss 2787.7 multi -7.96 import weight 0.00
Epoch 207 Iter 7 subLoss 2916.0 multi -10.94 import weight 0.00
Epoch 207 Iter 8 subLoss 3783.7 multi -4.97 import weight 0.00
Epoch 207 Iter 9 subLoss 4538.3 multi 1.00 import weight 0.00
Epoch 207 Iter 10 subLoss 4022.4 multi 3.99 import weight 0.00
Epoch 207 Iter 11 subLoss 2895.7 multi 12.94 import weight 0.00
Epoch 207 Acc: 98.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 289 train Loss: 2934.2 test Loss: 285.7
Epoch 208 Iter 0 subLoss 2911.7 multi -7.96 import weight 0.00
Epoch 208 Iter 1 subLoss 2748.9 multi -10.94 import weight 0.00
Epoch 208 Iter 2 subLoss 3376.4 multi -1.99 import weight 0.00
Epoch 208 Iter 3 subLoss 4210.6 multi 6.97 import weight 0.00
Epoch 208 Iter 4 subLoss 3070.7 multi 3.99 import weight 0.00
Epoch 208 Iter 5 subLoss 3436.7 multi 6.97 import weight 0.00
Epoch 208 Iter 6 subLoss 2951.5 multi -1.99 import weight 0.00
Epoch 208 Iter 7 subLoss 2645.8 multi -16.91 import weight 0.00
Epoch 208 Iter 8 subLoss 3552.2 multi -1.99 import weight 0.00
Epoch 208 Iter 9 subLoss 3951.1 multi -4.97 import weight 0.00
Epoch 208 Iter 10 subLoss 5307.9 multi 3.98 import weight 0.00
Epoch 208 Iter 11 subLoss 3312.7 multi -10.94 import weight 0.00
Epoch 208 Acc: 95.87 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 331 train Loss: 5194.4 test Loss: 611.6
Epoch 209 Iter 0 subLoss 5269.3 multi -1.99 import weight 0.00
Epoch 209 Iter 1 subLoss 7180.2 multi -4.97 import weight 0.00
Epoch 209 Iter 2 subLoss 14418.1 multi 1.00 import weight 0.00
Epoch 209 Iter 3 subLoss 8776.3 multi 1.00 import weight 0.00
Epoch 209 Iter 4 subLoss 7519.4 multi -1.99 import weight 0.00
Epoch 209 Iter 5 subLoss 10655.0 multi 1.00 import weight 0.00
Epoch 209 Iter 6 subLoss 7214.5 multi 6.97 import weight 0.00
Epoch 209 Iter 7 subLoss 4304.7 multi 9.96 import weight 0.00
Epoch 209 Iter 8 subLoss 3142.3 multi 15.93 import weight 0.00
Epoch 209 Iter 9 subLoss 2958.9 multi 1.00 import weight 0.00
Epoch 209 Iter 10 subLoss 2714.2 multi -4.97 import weight 0.00
Epoch 209 Iter 11 subLoss 3191.7 multi -16.91 import weight 0.00
Epoch 209 Acc: 96.52 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 319 train Loss: 5309.1 test Loss: 607.1
Epoch 210 Iter 0 subLoss 5846.0 multi 6.97 import weight 0.00
Epoch 210 Iter 1 subLoss 2931.4 multi 6.97 import weight 0.00
Epoch 210 Iter 2 subLoss 2716.2 multi -1.98 import weight 0.00
Epoch 210 Iter 3 subLoss 2871.5 multi 6.97 import weight 0.00
Epoch 210 Iter 4 subLoss 3503.3 multi 9.96 import weight 0.00
Epoch 210 Iter 5 subLoss 2481.8 multi 1.00 import weight 0.00
Epoch 210 Iter 6 subLoss 2580.7 multi 1.00 import weight 0.00
Epoch 210 Iter 7 subLoss 3021.0 multi -13.93 import weight 0.00
Epoch 210 Iter 8 subLoss 2831.9 multi 6.97 import weight 0.00
Epoch 210 Iter 9 subLoss 2657.9 multi 6.97 import weight 0.00
Epoch 210 Iter 10 subLoss 2797.0 multi -4.97 import weight 0.00
Epoch 210 Iter 11 subLoss 2270.4 multi 6.97 import weight 0.00
Epoch 210 Acc: 98.21 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 227 train Loss: 2752.1 test Loss: 274.9
Epoch 211 Iter 0 subLoss 3242.7 multi -13.93 import weight 0.00
Epoch 211 Iter 1 subLoss 2749.1 multi -7.96 import weight 0.00
Epoch 211 Iter 2 subLoss 3514.7 multi -10.94 import weight 0.00
Epoch 211 Iter 3 subLoss 5187.2 multi 3.99 import weight 0.00
Epoch 211 Iter 4 subLoss 3202.4 multi -7.96 import weight 0.00
Epoch 211 Iter 5 subLoss 4606.8 multi -1.99 import weight 0.00
Epoch 211 Iter 6 subLoss 5058.3 multi 3.99 import weight 0.00
Epoch 211 Iter 7 subLoss 4310.7 multi -10.94 import weight 0.00
Epoch 211 Iter 8 subLoss 6108.0 multi 9.96 import weight 0.00
Epoch 211 Iter 9 subLoss 3144.4 multi 18.91 import weight 1.00
Epoch 211 Iter 10 subLoss 2798.5 multi -1.99 import weight 0.00
Epoch 211 Iter 11 subLoss 2830.8 multi 9.96 import weight 0.00
Epoch 211 Acc: 98.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 283 train Loss: 2922.7 test Loss: 297.7
Epoch 212 Iter 0 subLoss 2971.9 multi -22.88 import weight 0.00
Epoch 212 Iter 1 subLoss 3974.7 multi 6.97 import weight 0.00
Epoch 212 Iter 2 subLoss 2987.2 multi 1.00 import weight 0.00
Epoch 212 Iter 3 subLoss 3247.3 multi -10.94 import weight 0.00
Epoch 212 Iter 4 subLoss 3778.1 multi 3.99 import weight 0.00
Epoch 212 Iter 5 subLoss 3287.9 multi -7.96 import weight 0.00
Epoch 212 Iter 6 subLoss 3396.4 multi 6.97 import weight 0.00
Epoch 212 Iter 7 subLoss 3254.8 multi -10.94 import weight 0.00
Epoch 212 Iter 8 subLoss 3632.1 multi -7.96 import weight 0.00
Epoch 212 Iter 9 subLoss 5484.8 multi -1.99 import weight 0.00
Epoch 212 Iter 10 subLoss 6301.2 multi -1.99 import weight 0.00
Epoch 212 Iter 11 subLoss 8600.3 multi 1.00 import weight 0.00
Epoch 212 Acc: 96.54 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 860 train Loss: 7157.8 test Loss: 668.3
Epoch 213 Iter 0 subLoss 8041.2 multi 6.97 import weight 0.00
Epoch 213 Iter 1 subLoss 3191.2 multi -13.93 import weight 0.00
Epoch 213 Iter 2 subLoss 4874.4 multi 3.98 import weight 0.00
Epoch 213 Iter 3 subLoss 4084.2 multi 3.98 import weight 0.00
Epoch 213 Iter 4 subLoss 3265.6 multi -1.99 import weight 0.00
Epoch 213 Iter 5 subLoss 3896.2 multi 3.98 import weight 0.00
Epoch 213 Iter 6 subLoss 3788.5 multi -4.97 import weight 0.00
Epoch 213 Iter 7 subLoss 3777.2 multi 6.97 import weight 0.00
Epoch 213 Iter 8 subLoss 3199.9 multi -10.94 import weight 0.00
Epoch 213 Iter 9 subLoss 3888.0 multi 1.00 import weight 0.00
Epoch 213 Iter 10 subLoss 3623.5 multi 12.94 import weight 0.00
Epoch 213 Iter 11 subLoss 3358.4 multi 1.00 import weight 0.00
Epoch 213 Acc: 98.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 335 train Loss: 3193.9 test Loss: 308.6
Epoch 214 Iter 0 subLoss 3017.6 multi 12.94 import weight 0.00
Epoch 214 Iter 1 subLoss 3175.0 multi 27.87 import weight 0.00
Epoch 214 Iter 2 subLoss 2421.9 multi -10.94 import weight 0.00
Epoch 214 Iter 3 subLoss 3726.2 multi 1.00 import weight 0.00
Epoch 214 Iter 4 subLoss 3588.0 multi -1.99 import weight 0.00
Epoch 214 Iter 5 subLoss 3557.9 multi 1.00 import weight 0.00
Epoch 214 Iter 6 subLoss 3388.8 multi -7.96 import weight 0.00
Epoch 214 Iter 7 subLoss 5976.2 multi 6.97 import weight 0.00
Epoch 214 Iter 8 subLoss 3127.5 multi 3.98 import weight 0.00
Epoch 214 Iter 9 subLoss 3129.1 multi 6.97 import weight 0.00
Epoch 214 Iter 10 subLoss 2980.2 multi 3.99 import weight 0.00
Epoch 214 Iter 11 subLoss 3019.1 multi 15.93 import weight 1.00
Epoch 214 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 15.93 Pidx 301 train Loss: 2612.1 test Loss: 267.1
Epoch 215 Iter 0 subLoss 2579.5 multi 6.97 import weight 0.00
Epoch 215 Iter 1 subLoss 2476.4 multi -7.96 import weight 0.00
Epoch 215 Iter 2 subLoss 2829.0 multi 1.00 import weight 0.00
Epoch 215 Iter 3 subLoss 2206.5 multi -1.98 import weight 0.00
Epoch 215 Iter 4 subLoss 2598.0 multi 3.98 import weight 0.00
Epoch 215 Iter 5 subLoss 2573.3 multi 9.96 import weight 0.00
Epoch 215 Iter 6 subLoss 1832.5 multi 1.00 import weight 0.00
Epoch 215 Iter 7 subLoss 2933.2 multi 9.96 import weight 0.00
Epoch 215 Iter 8 subLoss 2791.5 multi 1.00 import weight 0.00
Epoch 215 Iter 9 subLoss 2556.2 multi -1.98 import weight 0.00
Epoch 215 Iter 10 subLoss 1983.3 multi -1.98 import weight 0.00
Epoch 215 Iter 11 subLoss 2630.1 multi 9.96 import weight 0.00
Epoch 215 Acc: 98.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 263 train Loss: 2549.2 test Loss: 249.8
Epoch 216 Iter 0 subLoss 2595.2 multi 6.97 import weight 0.00
Epoch 216 Iter 1 subLoss 2801.8 multi 9.96 import weight 0.00
Epoch 216 Iter 2 subLoss 2283.6 multi -4.97 import weight 0.00
Epoch 216 Iter 3 subLoss 2563.6 multi -1.98 import weight 0.00
Epoch 216 Iter 4 subLoss 2467.5 multi 3.98 import weight 0.00
Epoch 216 Iter 5 subLoss 2310.2 multi 9.96 import weight 0.00
Epoch 216 Iter 6 subLoss 2440.5 multi -10.94 import weight 0.00
Epoch 216 Iter 7 subLoss 2731.1 multi 3.99 import weight 0.00
Epoch 216 Iter 8 subLoss 2545.7 multi 6.97 import weight 0.00
Epoch 216 Iter 9 subLoss 2194.4 multi -4.97 import weight 0.00
Epoch 216 Iter 10 subLoss 2180.3 multi 9.96 import weight 0.00
Epoch 216 Iter 11 subLoss 2396.1 multi 6.97 import weight 0.00
Epoch 216 Acc: 98.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 239 train Loss: 2410.3 test Loss: 239.0
Epoch 217 Iter 0 subLoss 2371.4 multi -1.99 import weight 0.00
Epoch 217 Iter 1 subLoss 1665.1 multi 1.00 import weight 0.00
Epoch 217 Iter 2 subLoss 2389.0 multi 3.98 import weight 0.00
Epoch 217 Iter 3 subLoss 2580.9 multi -1.98 import weight 0.00
Epoch 217 Iter 4 subLoss 2474.3 multi -7.96 import weight 0.00
Epoch 217 Iter 5 subLoss 2240.4 multi 3.99 import weight 0.00
Epoch 217 Iter 6 subLoss 1959.8 multi -1.99 import weight 0.00
Epoch 217 Iter 7 subLoss 2178.3 multi -1.98 import weight 0.00
Epoch 217 Iter 8 subLoss 2479.4 multi -4.97 import weight 0.00
Epoch 217 Iter 9 subLoss 3012.8 multi 18.91 import weight 1.00
Epoch 217 Iter 10 subLoss 3034.8 multi -4.97 import weight 0.00
Epoch 217 Iter 11 subLoss 4355.0 multi 3.99 import weight 0.00
Epoch 217 Acc: 98.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 435 train Loss: 2744.2 test Loss: 306.0
Epoch 218 Iter 0 subLoss 2464.2 multi 6.97 import weight 0.00
Epoch 218 Iter 1 subLoss 1951.3 multi 1.00 import weight 0.00
Epoch 218 Iter 2 subLoss 2434.6 multi 6.97 import weight 0.00
Epoch 218 Iter 3 subLoss 2521.7 multi -1.99 import weight 0.00
Epoch 218 Iter 4 subLoss 2420.8 multi -7.96 import weight 0.00
Epoch 218 Iter 5 subLoss 2616.6 multi 1.00 import weight 0.00
Epoch 218 Iter 6 subLoss 2250.9 multi -1.99 import weight 0.00
Epoch 218 Iter 7 subLoss 2525.7 multi 1.00 import weight 0.00
Epoch 218 Iter 8 subLoss 2807.5 multi 12.94 import weight 0.00
Epoch 218 Iter 9 subLoss 2195.9 multi -4.97 import weight 0.00
Epoch 218 Iter 10 subLoss 2393.8 multi 6.97 import weight 0.00
Epoch 218 Iter 11 subLoss 2221.2 multi -7.96 import weight 0.00
Epoch 218 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 222 train Loss: 2630.6 test Loss: 262.6
Epoch 219 Iter 0 subLoss 2249.6 multi 6.97 import weight 0.00
Epoch 219 Iter 1 subLoss 2143.9 multi -4.97 import weight 0.00
Epoch 219 Iter 2 subLoss 2073.7 multi 1.00 import weight 0.00
Epoch 219 Iter 3 subLoss 2615.9 multi 3.98 import weight 0.00
Epoch 219 Iter 4 subLoss 2115.8 multi -4.97 import weight 0.00
Epoch 219 Iter 5 subLoss 2422.4 multi -4.97 import weight 0.00
Epoch 219 Iter 6 subLoss 2695.4 multi -1.99 import weight 0.00
Epoch 219 Iter 7 subLoss 2855.1 multi -22.88 import weight 0.00
Epoch 219 Iter 8 subLoss 4136.2 multi -1.99 import weight 0.00
Epoch 219 Iter 9 subLoss 6365.2 multi -1.99 import weight 0.00
Epoch 219 Iter 10 subLoss 11379.4 multi -1.99 import weight 0.00
Epoch 219 Iter 11 subLoss 47374.0 multi 1.00 import weight 0.00
Epoch 219 Acc: 93.75 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4737 train Loss: 7717.9 test Loss: 1075.2
Epoch 220 Iter 0 subLoss 8065.5 multi -1.98 import weight 0.00
Epoch 220 Iter 1 subLoss 12772.1 multi -1.99 import weight 0.00
Epoch 220 Iter 2 subLoss 23611.0 multi 1.00 import weight 0.00
Epoch 220 Iter 3 subLoss 15627.3 multi 3.99 import weight 0.00
Epoch 220 Iter 4 subLoss 6979.1 multi -1.99 import weight 0.00
Epoch 220 Iter 5 subLoss 10134.2 multi -1.99 import weight 0.00
Epoch 220 Iter 6 subLoss 14154.0 multi -1.99 import weight 0.00
Epoch 220 Iter 7 subLoss 21576.4 multi -1.99 import weight 0.00
Epoch 220 Iter 8 subLoss 42554.3 multi 3.99 import weight 0.00
Epoch 220 Iter 9 subLoss 13083.9 multi -1.99 import weight 0.00
Epoch 220 Iter 10 subLoss 14874.1 multi 1.00 import weight 0.00
Epoch 220 Iter 11 subLoss 13897.0 multi 1.00 import weight 0.00
Epoch 220 Acc: 89.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1389 train Loss: 12494.6 test Loss: 1489.2
Epoch 221 Iter 0 subLoss 12213.8 multi 1.00 import weight 0.00
Epoch 221 Iter 1 subLoss 10583.3 multi 1.00 import weight 0.00
Epoch 221 Iter 2 subLoss 9526.9 multi 1.00 import weight 0.00
Epoch 221 Iter 3 subLoss 9551.3 multi 1.00 import weight 0.00
Epoch 221 Iter 4 subLoss 7980.5 multi 6.97 import weight 0.00
Epoch 221 Iter 5 subLoss 3553.4 multi 3.99 import weight 0.00
Epoch 221 Iter 6 subLoss 3030.8 multi -1.99 import weight 0.00
Epoch 221 Iter 7 subLoss 3322.9 multi 3.98 import weight 0.00
Epoch 221 Iter 8 subLoss 3463.8 multi 6.97 import weight 0.00
Epoch 221 Iter 9 subLoss 2339.7 multi 9.96 import weight 0.00
Epoch 221 Iter 10 subLoss 3057.7 multi 12.94 import weight 0.00
Epoch 221 Iter 11 subLoss 2441.1 multi -10.94 import weight 0.00
Epoch 221 Acc: 98.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 244 train Loss: 3265.6 test Loss: 298.2
Epoch 222 Iter 0 subLoss 3090.2 multi 1.00 import weight 0.00
Epoch 222 Iter 1 subLoss 3018.2 multi 21.90 import weight 1.00
Epoch 222 Iter 2 subLoss 3339.7 multi -19.90 import weight 0.00
Epoch 222 Iter 3 subLoss 26940.6 multi -1.99 import weight 0.00
Epoch 222 Iter 4 subLoss 261640.8 multi 1.00 import weight 0.00
Epoch 222 Iter 5 subLoss 17198.6 multi 1.00 import weight 0.00
Epoch 222 Iter 6 subLoss 10403.3 multi 3.98 import weight 0.00
Epoch 222 Iter 7 subLoss 6673.0 multi 6.97 import weight 0.00
Epoch 222 Iter 8 subLoss 4821.4 multi -4.97 import weight 0.00
Epoch 222 Iter 9 subLoss 5673.6 multi 6.97 import weight 0.00
Epoch 222 Iter 10 subLoss 5063.2 multi -1.99 import weight 0.00
Epoch 222 Iter 11 subLoss 4491.0 multi -7.96 import weight 0.00
Epoch 222 Acc: 96.26 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 449 train Loss: 7074.8 test Loss: 746.5
Epoch 223 Iter 0 subLoss 6761.3 multi 12.94 import weight 0.00
Epoch 223 Iter 1 subLoss 10386.5 multi -1.99 import weight 0.00
Epoch 223 Iter 2 subLoss 22513.4 multi -1.99 import weight 0.00
Epoch 223 Iter 3 subLoss 130711.0 multi 1.00 import weight 0.00
Epoch 223 Iter 4 subLoss 7746.8 multi -4.97 import weight 0.00
Epoch 223 Iter 5 subLoss 11683.2 multi -1.99 import weight 0.00
Epoch 223 Iter 6 subLoss 14470.5 multi -1.99 import weight 0.00
Epoch 223 Iter 7 subLoss 21566.3 multi 3.99 import weight 0.00
Epoch 223 Iter 8 subLoss 10503.0 multi 6.97 import weight 0.00
Epoch 223 Iter 9 subLoss 6820.9 multi 9.96 import weight 0.00
Epoch 223 Iter 10 subLoss 3674.5 multi 3.99 import weight 0.00
Epoch 223 Iter 11 subLoss 3736.6 multi -13.93 import weight 0.00
Epoch 223 Acc: 97.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 373 train Loss: 4644.1 test Loss: 468.5
Epoch 224 Iter 0 subLoss 4883.5 multi -7.96 import weight 0.00
Epoch 224 Iter 1 subLoss 5668.2 multi -1.99 import weight 0.00
Epoch 224 Iter 2 subLoss 8019.4 multi 6.97 import weight 0.00
Epoch 224 Iter 3 subLoss 5164.3 multi 3.98 import weight 0.00
Epoch 224 Iter 4 subLoss 4442.5 multi 1.00 import weight 0.00
Epoch 224 Iter 5 subLoss 4094.8 multi 6.97 import weight 0.00
Epoch 224 Iter 6 subLoss 3837.0 multi 9.96 import weight 0.00
Epoch 224 Iter 7 subLoss 3666.5 multi 6.97 import weight 0.00
Epoch 224 Iter 8 subLoss 3180.1 multi 1.00 import weight 0.00
Epoch 224 Iter 9 subLoss 2967.1 multi 6.97 import weight 0.00
Epoch 224 Iter 10 subLoss 3109.2 multi -4.97 import weight 0.00
Epoch 224 Iter 11 subLoss 3062.0 multi -10.94 import weight 0.00
Epoch 224 Acc: 97.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 306 train Loss: 3825.0 test Loss: 404.7
Epoch 225 Iter 0 subLoss 3484.1 multi 3.99 import weight 0.00
Epoch 225 Iter 1 subLoss 3109.1 multi -1.99 import weight 0.00
Epoch 225 Iter 2 subLoss 3642.6 multi 15.93 import weight 0.00
Epoch 225 Iter 3 subLoss 3144.0 multi 21.90 import weight 0.00
Epoch 225 Iter 4 subLoss 3862.9 multi 6.97 import weight 0.00
Epoch 225 Iter 5 subLoss 3031.3 multi 1.00 import weight 0.00
Epoch 225 Iter 6 subLoss 2500.0 multi 9.96 import weight 0.00
Epoch 225 Iter 7 subLoss 2530.9 multi -1.99 import weight 0.00
Epoch 225 Iter 8 subLoss 2407.3 multi -10.94 import weight 0.00
Epoch 225 Iter 9 subLoss 3027.9 multi -22.88 import weight 0.00
Epoch 225 Iter 10 subLoss 4808.7 multi -1.98 import weight 0.00
Epoch 225 Iter 11 subLoss 6041.7 multi -4.97 import weight 0.00
Epoch 225 Acc: 83.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 604 train Loss: 23759.8 test Loss: 3405.0
Epoch 226 Iter 0 subLoss 23745.5 multi 1.00 import weight 0.00
Epoch 226 Iter 1 subLoss 5948.7 multi 3.98 import weight 0.00
Epoch 226 Iter 2 subLoss 3498.0 multi -1.98 import weight 0.00
Epoch 226 Iter 3 subLoss 3291.2 multi 6.97 import weight 0.00
Epoch 226 Iter 4 subLoss 3180.5 multi 3.99 import weight 0.00
Epoch 226 Iter 5 subLoss 2740.7 multi -7.96 import weight 0.00
Epoch 226 Iter 6 subLoss 2601.6 multi -16.91 import weight 0.00
Epoch 226 Iter 7 subLoss 3688.8 multi 3.99 import weight 0.00
Epoch 226 Iter 8 subLoss 3437.1 multi 9.96 import weight 0.00
Epoch 226 Iter 9 subLoss 2809.3 multi 15.93 import weight 0.00
Epoch 226 Iter 10 subLoss 2597.5 multi 6.97 import weight 0.00
Epoch 226 Iter 11 subLoss 3364.1 multi 3.99 import weight 0.00
Epoch 226 Acc: 98.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 336 train Loss: 2755.2 test Loss: 258.7
Epoch 227 Iter 0 subLoss 2251.4 multi -1.98 import weight 0.00
Epoch 227 Iter 1 subLoss 2445.1 multi -7.96 import weight 0.00
Epoch 227 Iter 2 subLoss 2754.2 multi -1.99 import weight 0.00
Epoch 227 Iter 3 subLoss 2881.9 multi -19.90 import weight 0.00
Epoch 227 Iter 4 subLoss 5343.9 multi 3.99 import weight 0.00
Epoch 227 Iter 5 subLoss 3278.1 multi 6.97 import weight 0.00
Epoch 227 Iter 6 subLoss 3174.1 multi 30.85 import weight 0.00
Epoch 227 Iter 7 subLoss 2593.3 multi 9.96 import weight 0.00
Epoch 227 Iter 8 subLoss 2866.0 multi 6.97 import weight 0.00
Epoch 227 Iter 9 subLoss 2698.6 multi 1.00 import weight 0.00
Epoch 227 Iter 10 subLoss 2310.8 multi 12.94 import weight 0.00
Epoch 227 Iter 11 subLoss 2681.3 multi -7.96 import weight 0.00
Epoch 227 Acc: 98.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 268 train Loss: 2599.2 test Loss: 246.0
Epoch 228 Iter 0 subLoss 2464.4 multi 9.96 import weight 0.00
Epoch 228 Iter 1 subLoss 2831.5 multi 9.96 import weight 0.00
Epoch 228 Iter 2 subLoss 2185.3 multi 9.96 import weight 0.00
Epoch 228 Iter 3 subLoss 2213.7 multi 6.97 import weight 0.00
Epoch 228 Iter 4 subLoss 2555.5 multi -1.99 import weight 0.00
Epoch 228 Iter 5 subLoss 2282.3 multi -1.98 import weight 0.00
Epoch 228 Iter 6 subLoss 2501.7 multi -16.91 import weight 0.00
Epoch 228 Iter 7 subLoss 3009.1 multi 6.97 import weight 0.00
Epoch 228 Iter 8 subLoss 2458.4 multi 3.99 import weight 0.00
Epoch 228 Iter 9 subLoss 2117.9 multi -1.99 import weight 0.00
Epoch 228 Iter 10 subLoss 2267.9 multi -7.96 import weight 0.00
Epoch 228 Iter 11 subLoss 2300.2 multi -10.94 import weight 0.00
Epoch 228 Acc: 96.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 230 train Loss: 3664.3 test Loss: 450.4
Epoch 229 Iter 0 subLoss 3332.2 multi -16.91 import weight 0.00
Epoch 229 Iter 1 subLoss 33156.8 multi 1.00 import weight 0.00
Epoch 229 Iter 2 subLoss 9702.7 multi 6.97 import weight 0.00
Epoch 229 Iter 3 subLoss 3826.2 multi -4.97 import weight 0.00
Epoch 229 Iter 4 subLoss 5270.6 multi -1.99 import weight 0.00
Epoch 229 Iter 5 subLoss 6874.6 multi -1.98 import weight 0.00
Epoch 229 Iter 6 subLoss 11768.2 multi 1.00 import weight 0.00
Epoch 229 Iter 7 subLoss 7584.9 multi 1.00 import weight 0.00
Epoch 229 Iter 8 subLoss 5717.6 multi 3.98 import weight 0.00
Epoch 229 Iter 9 subLoss 3807.9 multi 3.99 import weight 0.00
Epoch 229 Iter 10 subLoss 3305.6 multi 9.96 import weight 0.00
Epoch 229 Iter 11 subLoss 2533.1 multi 1.00 import weight 0.00
Epoch 229 Acc: 98.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 253 train Loss: 2798.1 test Loss: 292.0
Epoch 230 Iter 0 subLoss 2747.1 multi -4.97 import weight 0.00
Epoch 230 Iter 1 subLoss 2991.5 multi 1.00 import weight 0.00
Epoch 230 Iter 2 subLoss 2771.0 multi 12.94 import weight 0.00
Epoch 230 Iter 3 subLoss 2606.0 multi -19.90 import weight 0.00
Epoch 230 Iter 4 subLoss 3173.3 multi 33.84 import weight 0.00
Epoch 230 Iter 5 subLoss 4608.2 multi 1.00 import weight 0.00
Epoch 230 Iter 6 subLoss 3389.7 multi -4.97 import weight 0.00
Epoch 230 Iter 7 subLoss 5461.5 multi -7.96 import weight 0.00
Epoch 230 Iter 8 subLoss 33890.5 multi 1.00 import weight 0.00
Epoch 230 Iter 9 subLoss 8574.8 multi 1.00 import weight 0.00
Epoch 230 Iter 10 subLoss 6534.9 multi -4.97 import weight 0.00
Epoch 230 Iter 11 subLoss 12517.5 multi -1.98 import weight 0.00
Epoch 230 Acc: 83.52 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 1251 train Loss: 20959.3 test Loss: 2934.6
Epoch 231 Iter 0 subLoss 20192.0 multi -1.99 import weight 0.00
Epoch 231 Iter 1 subLoss 37521.1 multi 1.00 import weight 0.00
Epoch 231 Iter 2 subLoss 21917.2 multi 1.00 import weight 0.00
Epoch 231 Iter 3 subLoss 17317.8 multi 1.00 import weight 0.00
Epoch 231 Iter 4 subLoss 12658.3 multi 1.00 import weight 0.00
Epoch 231 Iter 5 subLoss 9914.2 multi 1.00 import weight 0.00
Epoch 231 Iter 6 subLoss 10180.9 multi -1.99 import weight 0.00
Epoch 231 Iter 7 subLoss 13235.4 multi -4.97 import weight 0.00
Epoch 231 Iter 8 subLoss 32315.8 multi 1.00 import weight 0.00
Epoch 231 Iter 9 subLoss 20262.1 multi -1.99 import weight 0.00
Epoch 231 Iter 10 subLoss 33378.4 multi 1.00 import weight 0.00
Epoch 231 Iter 11 subLoss 21514.0 multi 1.00 import weight 0.00
Epoch 231 Acc: 84.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2151 train Loss: 18847.9 test Loss: 2508.2
Epoch 232 Iter 0 subLoss 18812.9 multi 3.98 import weight 0.00
Epoch 232 Iter 1 subLoss 8364.1 multi -1.99 import weight 0.00
Epoch 232 Iter 2 subLoss 10111.6 multi 3.99 import weight 0.00
Epoch 232 Iter 3 subLoss 7040.0 multi -1.99 import weight 0.00
Epoch 232 Iter 4 subLoss 7863.4 multi -1.98 import weight 0.00
Epoch 232 Iter 5 subLoss 8593.5 multi 1.00 import weight 0.00
Epoch 232 Iter 6 subLoss 8730.6 multi 1.00 import weight 0.00
Epoch 232 Iter 7 subLoss 8062.2 multi 1.00 import weight 0.00
Epoch 232 Iter 8 subLoss 7067.5 multi 3.98 import weight 0.00
Epoch 232 Iter 9 subLoss 6079.6 multi 3.98 import weight 0.00
Epoch 232 Iter 10 subLoss 4561.0 multi 9.96 import weight 0.00
Epoch 232 Iter 11 subLoss 2715.4 multi 1.00 import weight 0.00
Epoch 232 Acc: 98.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 271 train Loss: 2906.6 test Loss: 275.8
Epoch 233 Iter 0 subLoss 3310.2 multi -10.94 import weight 0.00
Epoch 233 Iter 1 subLoss 3471.5 multi -7.96 import weight 0.00
Epoch 233 Iter 2 subLoss 4524.4 multi -1.98 import weight 0.00
Epoch 233 Iter 3 subLoss 5083.3 multi -4.97 import weight 0.00
Epoch 233 Iter 4 subLoss 7596.1 multi 6.97 import weight 0.00
Epoch 233 Iter 5 subLoss 4069.9 multi 3.99 import weight 0.00
Epoch 233 Iter 6 subLoss 3770.9 multi 9.96 import weight 0.00
Epoch 233 Iter 7 subLoss 2976.7 multi -22.88 import weight 0.00
Epoch 233 Iter 8 subLoss 3789.7 multi -7.96 import weight 0.00
Epoch 233 Iter 9 subLoss 4317.8 multi -7.96 import weight 0.00
Epoch 233 Iter 10 subLoss 7572.4 multi 3.99 import weight 0.00
Epoch 233 Iter 11 subLoss 4383.4 multi 1.00 import weight 0.00
Epoch 233 Acc: 97.55 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 438 train Loss: 4630.3 test Loss: 492.4
Epoch 234 Iter 0 subLoss 4559.5 multi -7.96 import weight 0.00
Epoch 234 Iter 1 subLoss 6466.0 multi -4.97 import weight 0.00
Epoch 234 Iter 2 subLoss 10738.3 multi 1.00 import weight 0.00
Epoch 234 Iter 3 subLoss 7896.2 multi 1.00 import weight 0.00
Epoch 234 Iter 4 subLoss 7031.8 multi 6.97 import weight 0.00
Epoch 234 Iter 5 subLoss 4348.6 multi -4.97 import weight 0.00
Epoch 234 Iter 6 subLoss 4966.0 multi -1.99 import weight 0.00
Epoch 234 Iter 7 subLoss 5638.5 multi -1.99 import weight 0.00
Epoch 234 Iter 8 subLoss 6421.3 multi 6.97 import weight 0.00
Epoch 234 Iter 9 subLoss 4692.1 multi 1.00 import weight 0.00
Epoch 234 Iter 10 subLoss 4431.7 multi 1.00 import weight 0.00
Epoch 234 Iter 11 subLoss 3766.8 multi -7.96 import weight 0.00
Epoch 234 Acc: 97.39 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 376 train Loss: 4988.6 test Loss: 571.6
Epoch 235 Iter 0 subLoss 4922.3 multi 1.00 import weight 0.00
Epoch 235 Iter 1 subLoss 4641.4 multi 1.00 import weight 0.00
Epoch 235 Iter 2 subLoss 4756.4 multi 6.97 import weight 0.00
Epoch 235 Iter 3 subLoss 3948.3 multi 6.97 import weight 0.00
Epoch 235 Iter 4 subLoss 3461.0 multi 9.96 import weight 0.00
Epoch 235 Iter 5 subLoss 3098.3 multi 3.99 import weight 0.00
Epoch 235 Iter 6 subLoss 3034.5 multi 1.00 import weight 0.00
Epoch 235 Iter 7 subLoss 2742.0 multi -1.98 import weight 0.00
Epoch 235 Iter 8 subLoss 3268.3 multi 1.00 import weight 0.00
Epoch 235 Iter 9 subLoss 2739.7 multi 6.97 import weight 0.00
Epoch 235 Iter 10 subLoss 2920.7 multi 1.00 import weight 0.00
Epoch 235 Iter 11 subLoss 2794.4 multi 3.99 import weight 0.00
Epoch 235 Acc: 98.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 279 train Loss: 2795.2 test Loss: 263.6
Epoch 236 Iter 0 subLoss 2597.1 multi 12.94 import weight 0.00
Epoch 236 Iter 1 subLoss 2578.6 multi 9.96 import weight 0.00
Epoch 236 Iter 2 subLoss 2605.1 multi -19.90 import weight 0.00
Epoch 236 Iter 3 subLoss 3533.2 multi -4.97 import weight 0.00
Epoch 236 Iter 4 subLoss 4514.7 multi 1.00 import weight 0.00
Epoch 236 Iter 5 subLoss 4026.7 multi 6.97 import weight 0.00
Epoch 236 Iter 6 subLoss 2790.8 multi 6.97 import weight 0.00
Epoch 236 Iter 7 subLoss 2765.8 multi -4.97 import weight 0.00
Epoch 236 Iter 8 subLoss 2832.1 multi 12.94 import weight 0.00
Epoch 236 Iter 9 subLoss 2436.1 multi 3.98 import weight 0.00
Epoch 236 Iter 10 subLoss 2721.5 multi -7.96 import weight 0.00
Epoch 236 Iter 11 subLoss 3040.7 multi -10.94 import weight 0.00
Epoch 236 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 304 train Loss: 3120.9 test Loss: 291.9
Epoch 237 Iter 0 subLoss 3298.9 multi 9.96 import weight 0.00
Epoch 237 Iter 1 subLoss 2682.3 multi -4.97 import weight 0.00
Epoch 237 Iter 2 subLoss 3083.1 multi -4.97 import weight 0.00
Epoch 237 Iter 3 subLoss 3966.4 multi 1.00 import weight 0.00
Epoch 237 Iter 4 subLoss 3275.0 multi 6.97 import weight 0.00
Epoch 237 Iter 5 subLoss 2203.0 multi -4.97 import weight 0.00
Epoch 237 Iter 6 subLoss 2898.2 multi 12.94 import weight 0.00
Epoch 237 Iter 7 subLoss 2898.4 multi 15.93 import weight 0.00
Epoch 237 Iter 8 subLoss 2352.1 multi 3.98 import weight 0.00
Epoch 237 Iter 9 subLoss 2605.6 multi -16.91 import weight 0.00
Epoch 237 Iter 10 subLoss 2178.2 multi 1.00 import weight 0.00
Epoch 237 Iter 11 subLoss 2555.1 multi 1.00 import weight 0.00
Epoch 237 Acc: 98.62 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 255 train Loss: 2679.3 test Loss: 224.7
Epoch 238 Iter 0 subLoss 2538.7 multi 3.98 import weight 0.00
Epoch 238 Iter 1 subLoss 2664.3 multi -1.99 import weight 0.00
Epoch 238 Iter 2 subLoss 2837.7 multi 15.93 import weight 0.00
Epoch 238 Iter 3 subLoss 2415.8 multi 6.97 import weight 0.00
Epoch 238 Iter 4 subLoss 2383.0 multi 6.97 import weight 0.00
Epoch 238 Iter 5 subLoss 2338.3 multi 12.94 import weight 0.00
Epoch 238 Iter 6 subLoss 2638.1 multi 12.94 import weight 0.00
Epoch 238 Iter 7 subLoss 2047.5 multi 6.97 import weight 0.00
Epoch 238 Iter 8 subLoss 2144.5 multi -1.98 import weight 0.00
Epoch 238 Iter 9 subLoss 2089.6 multi -1.98 import weight 0.00
Epoch 238 Iter 10 subLoss 2020.0 multi 1.00 import weight 0.00
Epoch 238 Iter 11 subLoss 2427.9 multi -4.97 import weight 0.00
Epoch 238 Acc: 98.58 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 242 train Loss: 2518.1 test Loss: 214.6
Epoch 239 Iter 0 subLoss 2772.4 multi 12.94 import weight 0.00
Epoch 239 Iter 1 subLoss 2554.9 multi 3.99 import weight 0.00
Epoch 239 Iter 2 subLoss 2286.1 multi 1.00 import weight 0.00
Epoch 239 Iter 3 subLoss 2095.8 multi 1.00 import weight 0.00
Epoch 239 Iter 4 subLoss 2092.4 multi 3.99 import weight 0.00
Epoch 239 Iter 5 subLoss 1974.6 multi 6.97 import weight 0.00
Epoch 239 Iter 6 subLoss 2289.3 multi 3.99 import weight 0.00
Epoch 239 Iter 7 subLoss 1997.5 multi -1.98 import weight 0.00
Epoch 239 Iter 8 subLoss 2236.8 multi 1.00 import weight 0.00
Epoch 239 Iter 9 subLoss 2407.7 multi -7.96 import weight 0.00
Epoch 239 Iter 10 subLoss 2094.4 multi 6.97 import weight 0.00
Epoch 239 Iter 11 subLoss 2244.5 multi 6.97 import weight 0.00
Epoch 239 Acc: 98.66 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 224 train Loss: 2212.6 test Loss: 218.0
Epoch 240 Iter 0 subLoss 1986.2 multi -1.99 import weight 0.00
Epoch 240 Iter 1 subLoss 2058.0 multi -7.96 import weight 0.00
Epoch 240 Iter 2 subLoss 2349.4 multi -13.93 import weight 0.00
Epoch 240 Iter 3 subLoss 2728.6 multi -4.97 import weight 0.00
Epoch 240 Iter 4 subLoss 3772.8 multi 9.96 import weight 0.00
Epoch 240 Iter 5 subLoss 2207.1 multi -1.99 import weight 0.00
Epoch 240 Iter 6 subLoss 2335.4 multi 15.93 import weight 0.00
Epoch 240 Iter 7 subLoss 3142.5 multi 24.88 import weight 1.00
Epoch 240 Iter 8 subLoss 3404.8 multi -1.99 import weight 0.00
Epoch 240 Iter 9 subLoss 4597.5 multi 12.94 import weight 0.00
Epoch 240 Iter 10 subLoss 5222.6 multi 3.99 import weight 0.00
Epoch 240 Iter 11 subLoss 3164.3 multi -4.97 import weight 0.00
Epoch 240 Acc: 97.14 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 316 train Loss: 3619.6 test Loss: 497.8
Epoch 241 Iter 0 subLoss 3710.1 multi 9.96 import weight 0.00
Epoch 241 Iter 1 subLoss 2027.1 multi 3.99 import weight 0.00
Epoch 241 Iter 2 subLoss 2258.3 multi -1.99 import weight 0.00
Epoch 241 Iter 3 subLoss 2359.0 multi 3.99 import weight 0.00
Epoch 241 Iter 4 subLoss 2454.0 multi 6.97 import weight 0.00
Epoch 241 Iter 5 subLoss 2049.7 multi 9.96 import weight 0.00
Epoch 241 Iter 6 subLoss 1947.7 multi 1.00 import weight 0.00
Epoch 241 Iter 7 subLoss 1969.3 multi -4.97 import weight 0.00
Epoch 241 Iter 8 subLoss 2497.1 multi 12.94 import weight 0.00
Epoch 241 Iter 9 subLoss 1986.8 multi 1.00 import weight 0.00
Epoch 241 Iter 10 subLoss 2636.1 multi 15.93 import weight 0.00
Epoch 241 Iter 11 subLoss 2140.8 multi 1.00 import weight 0.00
Epoch 241 Acc: 98.77 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 214 train Loss: 2169.5 test Loss: 200.9
Epoch 242 Iter 0 subLoss 2160.6 multi 3.98 import weight 0.00
Epoch 242 Iter 1 subLoss 2187.9 multi 9.96 import weight 0.00
Epoch 242 Iter 2 subLoss 2262.6 multi -7.96 import weight 0.00
Epoch 242 Iter 3 subLoss 2029.7 multi 6.97 import weight 0.00
Epoch 242 Iter 4 subLoss 1536.3 multi 1.00 import weight 0.00
Epoch 242 Iter 5 subLoss 1829.8 multi 1.00 import weight 0.00
Epoch 242 Iter 6 subLoss 2058.3 multi -7.96 import weight 0.00
Epoch 242 Iter 7 subLoss 1994.2 multi -4.97 import weight 0.00
Epoch 242 Iter 8 subLoss 2366.9 multi 1.00 import weight 0.00
Epoch 242 Iter 9 subLoss 2784.4 multi -10.94 import weight 0.00
Epoch 242 Iter 10 subLoss 2707.0 multi 3.98 import weight 0.00
Epoch 242 Iter 11 subLoss 2071.1 multi 3.98 import weight 0.00
Epoch 242 Acc: 98.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 207 train Loss: 2152.3 test Loss: 225.6
Epoch 243 Iter 0 subLoss 2041.2 multi 12.94 import weight 0.00
Epoch 243 Iter 1 subLoss 2260.9 multi -4.97 import weight 0.00
Epoch 243 Iter 2 subLoss 1977.9 multi 6.97 import weight 0.00
Epoch 243 Iter 3 subLoss 2111.2 multi 1.00 import weight 0.00
Epoch 243 Iter 4 subLoss 2349.9 multi -13.93 import weight 0.00
Epoch 243 Iter 5 subLoss 2135.5 multi 1.00 import weight 0.00
Epoch 243 Iter 6 subLoss 2067.7 multi -4.97 import weight 0.00
Epoch 243 Iter 7 subLoss 2219.7 multi 3.98 import weight 0.00
Epoch 243 Iter 8 subLoss 2309.6 multi -7.96 import weight 0.00
Epoch 243 Iter 9 subLoss 1953.6 multi 1.00 import weight 0.00
Epoch 243 Iter 10 subLoss 2352.2 multi 3.99 import weight 0.00
Epoch 243 Iter 11 subLoss 2122.5 multi -4.97 import weight 0.00
Epoch 243 Acc: 98.68 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 212 train Loss: 2242.4 test Loss: 220.3
Epoch 244 Iter 0 subLoss 1980.0 multi 9.96 import weight 0.00
Epoch 244 Iter 1 subLoss 2301.1 multi -4.97 import weight 0.00
Epoch 244 Iter 2 subLoss 2225.7 multi -10.94 import weight 0.00
Epoch 244 Iter 3 subLoss 2614.2 multi -4.97 import weight 0.00
Epoch 244 Iter 4 subLoss 2973.8 multi -19.90 import weight 0.00
Epoch 244 Iter 5 subLoss 14131.7 multi 1.00 import weight 0.00
Epoch 244 Iter 6 subLoss 4802.0 multi 1.00 import weight 0.00
Epoch 244 Iter 7 subLoss 3618.2 multi -10.94 import weight 0.00
Epoch 244 Iter 8 subLoss 16466.7 multi 1.00 import weight 0.00
Epoch 244 Iter 9 subLoss 6218.2 multi 1.00 import weight 0.00
Epoch 244 Iter 10 subLoss 4186.6 multi 1.00 import weight 0.00
Epoch 244 Iter 11 subLoss 3607.1 multi 3.99 import weight 0.00
Epoch 244 Acc: 97.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 360 train Loss: 2847.8 test Loss: 369.3
Epoch 245 Iter 0 subLoss 2902.0 multi -10.94 import weight 0.00
Epoch 245 Iter 1 subLoss 4740.6 multi 3.98 import weight 0.00
Epoch 245 Iter 2 subLoss 3188.3 multi 1.00 import weight 1.00
Epoch 245 Iter 3 subLoss 2982.5 multi 1.00 import weight 0.00
Epoch 245 Iter 4 subLoss 2432.5 multi 3.99 import weight 0.00
Epoch 245 Iter 5 subLoss 2618.8 multi -1.98 import weight 0.00
Epoch 245 Iter 6 subLoss 2195.8 multi -7.96 import weight 0.00
Epoch 245 Iter 7 subLoss 2846.3 multi 12.94 import weight 0.00
Epoch 245 Iter 8 subLoss 2917.3 multi -7.96 import weight 0.00
Epoch 245 Iter 9 subLoss 2628.0 multi -7.96 import weight 0.00
Epoch 245 Iter 10 subLoss 3741.3 multi -1.99 import weight 0.00
Epoch 245 Iter 11 subLoss 3125.3 multi 9.96 import weight 0.00
Epoch 245 Acc: 98.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 312 train Loss: 2685.5 test Loss: 314.8
Epoch 246 Iter 0 subLoss 2123.6 multi -1.99 import weight 0.00
Epoch 246 Iter 1 subLoss 2572.7 multi 12.94 import weight 0.00
Epoch 246 Iter 2 subLoss 2301.5 multi -1.98 import weight 0.00
Epoch 246 Iter 3 subLoss 2248.1 multi 9.96 import weight 0.00
Epoch 246 Iter 4 subLoss 2869.8 multi 9.96 import weight 0.00
Epoch 246 Iter 5 subLoss 1985.7 multi -1.98 import weight 0.00
Epoch 246 Iter 6 subLoss 2681.9 multi -1.99 import weight 0.00
Epoch 246 Iter 7 subLoss 2267.5 multi -1.99 import weight 0.00
Epoch 246 Iter 8 subLoss 2393.4 multi 6.97 import weight 0.00
Epoch 246 Iter 9 subLoss 2091.4 multi 9.96 import weight 0.00
Epoch 246 Iter 10 subLoss 2108.5 multi -7.96 import weight 0.00
Epoch 246 Iter 11 subLoss 2431.8 multi 6.97 import weight 0.00
Epoch 246 Acc: 98.62 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 243 train Loss: 2223.9 test Loss: 215.9
Epoch 247 Iter 0 subLoss 2384.4 multi 9.96 import weight 0.00
Epoch 247 Iter 1 subLoss 1791.3 multi 1.00 import weight 0.00
Epoch 247 Iter 2 subLoss 1815.5 multi 1.00 import weight 0.00
Epoch 247 Iter 3 subLoss 2211.8 multi 6.97 import weight 0.00
Epoch 247 Iter 4 subLoss 1996.8 multi -4.97 import weight 0.00
Epoch 247 Iter 5 subLoss 2164.0 multi 6.97 import weight 0.00
Epoch 247 Iter 6 subLoss 2142.4 multi 1.00 import weight 0.00
Epoch 247 Iter 7 subLoss 2281.5 multi 6.97 import weight 0.00
Epoch 247 Iter 8 subLoss 2124.5 multi 1.00 import weight 0.00
Epoch 247 Iter 9 subLoss 2231.2 multi 1.00 import weight 0.00
Epoch 247 Iter 10 subLoss 1926.1 multi 1.00 import weight 0.00
Epoch 247 Iter 11 subLoss 2431.6 multi 9.96 import weight 0.00
Epoch 247 Acc: 98.79 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 243 train Loss: 2112.8 test Loss: 215.6
Epoch 248 Iter 0 subLoss 2214.1 multi 9.96 import weight 0.00
Epoch 248 Iter 1 subLoss 1693.2 multi 1.00 import weight 0.00
Epoch 248 Iter 2 subLoss 1913.9 multi 1.00 import weight 0.00
Epoch 248 Iter 3 subLoss 2366.4 multi 1.00 import weight 0.00
Epoch 248 Iter 4 subLoss 2445.2 multi -16.91 import weight 0.00
Epoch 248 Iter 5 subLoss 2104.2 multi -4.97 import weight 0.00
Epoch 248 Iter 6 subLoss 2759.5 multi -4.97 import weight 0.00
Epoch 248 Iter 7 subLoss 2436.0 multi 12.94 import weight 0.00
Epoch 248 Iter 8 subLoss 2180.7 multi 12.94 import weight 0.00
Epoch 248 Iter 9 subLoss 1957.5 multi 3.99 import weight 0.00
Epoch 248 Iter 10 subLoss 1932.8 multi 1.00 import weight 0.00
Epoch 248 Iter 11 subLoss 1862.7 multi 1.00 import weight 0.00
Epoch 248 Acc: 98.75 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 186 train Loss: 2159.0 test Loss: 222.8
Epoch 249 Iter 0 subLoss 1986.5 multi 1.00 import weight 0.00
Epoch 249 Iter 1 subLoss 1823.3 multi 1.00 import weight 0.00
Epoch 249 Iter 2 subLoss 1888.2 multi 1.00 import weight 0.00
Epoch 249 Iter 3 subLoss 2329.6 multi -13.93 import weight 0.00
Epoch 249 Iter 4 subLoss 2270.6 multi -1.99 import weight 0.00
Epoch 249 Iter 5 subLoss 3257.3 multi -7.96 import weight 0.00
Epoch 249 Iter 6 subLoss 2895.4 multi 18.91 import weight 0.00
Epoch 249 Iter 7 subLoss 4796.9 multi 1.00 import weight 0.00
Epoch 249 Iter 8 subLoss 2818.4 multi -31.84 import weight 0.00
Epoch 249 Iter 9 subLoss 199949.3 multi 1.00 import weight 0.00
Epoch 249 Iter 10 subLoss 23658.2 multi 1.00 import weight 0.00
Epoch 249 Iter 11 subLoss 15876.4 multi -1.99 import weight 0.00
Epoch 249 Acc: 76.61 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1587 train Loss: 27187.8 test Loss: 4219.2
Epoch 250 Iter 0 subLoss 26436.8 multi 1.00 import weight 0.00
Epoch 250 Iter 1 subLoss 20703.8 multi 1.00 import weight 0.00
Epoch 250 Iter 2 subLoss 16035.9 multi 3.99 import weight 0.00
Epoch 250 Iter 3 subLoss 5594.8 multi 6.97 import weight 0.00
Epoch 250 Iter 4 subLoss 3313.7 multi -7.96 import weight 0.00
Epoch 250 Iter 5 subLoss 4138.2 multi 1.00 import weight 0.00
Epoch 250 Iter 6 subLoss 4326.3 multi -1.98 import weight 0.00
Epoch 250 Iter 7 subLoss 3736.1 multi -10.94 import weight 0.00
Epoch 250 Iter 8 subLoss 6011.1 multi 3.98 import weight 0.00
Epoch 250 Iter 9 subLoss 4393.9 multi 1.00 import weight 0.00
Epoch 250 Iter 10 subLoss 4104.8 multi -7.96 import weight 0.00
Epoch 250 Iter 11 subLoss 6260.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0127 / 0.10859 / 16.69
Entropy seen (from low to high)
[2054, 395, 270, 189, 163, 115, 106, 133, 143, 116, 104, 106, 74, 66, 80, 82, 81, 70, 75, 69, 55, 64, 40, 62, 47, 44, 40, 31, 23, 29, 19, 29, 25, 22, 27, 26, 12, 15, 10, 12, 4, 5, 3, 4, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 12, 26, 86, 132, 137, 138, 152, 162, 135, 148, 149, 157, 163, 171, 148, 154, 152, 152, 145, 155, 141, 137, 139, 131, 132, 102, 112, 86, 92, 99, 91, 82, 72, 64, 75, 71, 73, 76, 71, 69, 59, 70, 63, 61, 17]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 32.3, 36.8, 40.6, 43.9, 47.5, 50.9, 54.1, 58.0, 61.3, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 39.9, 45.4, 59.0, 51.8, 71.4, 55.9, 79.1, 82.3, 76.1, 82.1, 82.8]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 5, 11, 22, 27, 35, 50, 48, 51, 42, 56, 70]
Epoch 250 Acc: 96.59 BMA: 96.59 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 626 train Loss: 5768.8 test Loss: 723.7
Epoch 251 Iter 0 subLoss 5718.8 multi 6.97 import weight 0.00
Epoch 251 Iter 1 subLoss 3781.6 multi -7.96 import weight 0.00
Epoch 251 Iter 2 subLoss 5182.8 multi 6.97 import weight 0.00
Epoch 251 Iter 3 subLoss 3552.9 multi 6.97 import weight 0.00
Epoch 251 Iter 4 subLoss 3206.9 multi -10.94 import weight 0.00
Epoch 251 Iter 5 subLoss 3920.4 multi -4.97 import weight 0.00
Epoch 251 Iter 6 subLoss 4516.7 multi 3.99 import weight 0.00
Epoch 251 Iter 7 subLoss 3775.9 multi 12.94 import weight 0.00
Epoch 251 Iter 8 subLoss 2841.3 multi 15.93 import weight 1.00
Epoch 251 Iter 9 subLoss 2610.0 multi -13.93 import weight 0.00
Epoch 251 Iter 10 subLoss 2891.5 multi 21.90 import weight 0.00
Epoch 251 Iter 11 subLoss 2851.3 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0146 / 0.10247 / 10.44
Entropy seen (from low to high)
[1547, 645, 384, 243, 238, 190, 170, 130, 126, 121, 107, 99, 87, 66, 73, 55, 61, 70, 42, 44, 69, 65, 46, 46, 39, 41, 31, 29, 29, 39, 33, 18, 20, 21, 22, 27, 17, 12, 7, 12, 7, 3, 5, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 1, 16, 25, 52, 103, 110, 131, 132, 154, 159, 185, 187, 189, 177, 222, 192, 196, 186, 173, 158, 164, 165, 158, 137, 143, 122, 94, 96, 105, 90, 85, 101, 82, 79, 70, 72, 60, 50, 48, 49, 43, 35, 28, 17, 11, 9]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.5, 30.3, 32.2, 37.1, 40.3, 43.7, 47.6, 50.8, 53.9, 57.5, 61.1, 64.4, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 39.9, 24.9, 41.6, 46.4, 58.6, 45.6, 56.0, 62.4, 64.2, 79.3, 81.0, 84.6]
[0, 0, 0, 0, 0, 0, 0, 1, 5, 4, 12, 28, 29, 46, 82, 80, 56, 58, 58, 65]
Epoch 251 Acc: 91.94 BMA: 95.76 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 285 train Loss: 8076.8 test Loss: 1450.6
Epoch 252 Iter 0 subLoss 8219.5 multi 1.00 import weight 0.00
Epoch 252 Iter 1 subLoss 5606.2 multi -1.99 import weight 0.00
Epoch 252 Iter 2 subLoss 8037.4 multi -4.97 import weight 0.00
Epoch 252 Iter 3 subLoss 77105.7 multi 1.00 import weight 0.00
Epoch 252 Iter 4 subLoss 4736.2 multi 1.00 import weight 0.00
Epoch 252 Iter 5 subLoss 3923.0 multi -1.98 import weight 0.00
Epoch 252 Iter 6 subLoss 4369.3 multi 3.98 import weight 0.00
Epoch 252 Iter 7 subLoss 3326.3 multi 1.00 import weight 0.00
Epoch 252 Iter 8 subLoss 3537.6 multi -1.99 import weight 0.00
Epoch 252 Iter 9 subLoss 3184.1 multi 3.99 import weight 1.00
Epoch 252 Iter 10 subLoss 3509.6 multi 9.96 import weight 0.00
Epoch 252 Iter 11 subLoss 2704.2 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0099 / 0.10357 / 19.63
Entropy seen (from low to high)
[1809, 663, 336, 316, 225, 186, 146, 141, 127, 95, 100, 76, 64, 70, 58, 46, 41, 48, 50, 51, 43, 33, 46, 42, 39, 39, 30, 19, 21, 32, 20, 29, 22, 17, 10, 11, 12, 8, 2, 8, 4, 3, 0, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 20, 27, 60, 73, 100, 151, 187, 175, 188, 179, 190, 188, 179, 203, 181, 191, 154, 146, 167, 140, 145, 147, 108, 114, 97, 98, 99, 103, 85, 71, 83, 91, 83, 90, 95, 60, 69, 79, 61, 58, 36, 40, 29, 11, 6]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 33.6, 36.9, 40.3, 43.9, 47.3, 50.7, 54.4, 57.7, 61.4, 64.7, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 39.9, 37.4, 79.9, 64.9, 74.2, 66.0, 57.1, 67.4, 87.3, 91.8, 90.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 8, 10, 20, 35, 53, 42, 40, 63, 61, 63]
Epoch 252 Acc: 98.23 BMA: 97.22 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 270 train Loss: 2776.6 test Loss: 313.3
Epoch 253 Iter 0 subLoss 2433.1 multi 15.93 import weight 0.00
Epoch 253 Iter 1 subLoss 2551.6 multi 6.97 import weight 0.00
Epoch 253 Iter 2 subLoss 2184.6 multi 15.93 import weight 0.00
Epoch 253 Iter 3 subLoss 2334.3 multi 15.93 import weight 0.00
Epoch 253 Iter 4 subLoss 2623.5 multi -4.97 import weight 0.00
Epoch 253 Iter 5 subLoss 2675.4 multi 9.96 import weight 0.00
Epoch 253 Iter 6 subLoss 2387.4 multi 12.94 import weight 0.00
Epoch 253 Iter 7 subLoss 2207.3 multi -1.98 import weight 0.00
Epoch 253 Iter 8 subLoss 2096.9 multi 12.94 import weight 0.00
Epoch 253 Iter 9 subLoss 2365.6 multi 3.99 import weight 0.00
Epoch 253 Iter 10 subLoss 2456.0 multi 6.97 import weight 0.00
Epoch 253 Iter 11 subLoss 2253.5 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0075 / 0.10644 / 18.10
Entropy seen (from low to high)
[2038, 611, 389, 285, 221, 182, 167, 118, 117, 81, 76, 86, 53, 47, 42, 49, 55, 46, 46, 40, 33, 44, 25, 44, 21, 27, 25, 29, 24, 21, 16, 26, 8, 10, 12, 5, 4, 8, 1, 1, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 17, 31, 35, 73, 98, 122, 139, 166, 169, 172, 213, 157, 200, 189, 199, 183, 165, 158, 145, 148, 127, 141, 133, 131, 89, 102, 95, 93, 87, 88, 81, 80, 88, 91, 94, 91, 79, 77, 84, 63, 61, 44, 34, 15, 7]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.5, 35.6, 40.2, 43.5, 47.5, 51.0, 54.9, 57.6, 61.3, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 66.6, 16.6, 46.1, 55.1, 75.8, 69.2, 78.3, 77.2, 83.3, 92.1]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 6, 13, 29, 29, 26, 37, 44, 42, 51]
Epoch 253 Acc: 98.56 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 225 train Loss: 2368.1 test Loss: 224.2
Epoch 254 Iter 0 subLoss 2164.4 multi 9.96 import weight 0.00
Epoch 254 Iter 1 subLoss 2015.1 multi 1.00 import weight 0.00
Epoch 254 Iter 2 subLoss 2132.7 multi -4.97 import weight 0.00
Epoch 254 Iter 3 subLoss 2323.4 multi -10.94 import weight 0.00
Epoch 254 Iter 4 subLoss 2531.9 multi 6.97 import weight 0.00
Epoch 254 Iter 5 subLoss 2212.9 multi 9.96 import weight 0.00
Epoch 254 Iter 6 subLoss 1883.4 multi 3.99 import weight 0.00
Epoch 254 Iter 7 subLoss 2275.4 multi 1.00 import weight 0.00
Epoch 254 Iter 8 subLoss 2246.8 multi 9.96 import weight 0.00
Epoch 254 Iter 9 subLoss 2281.6 multi 3.98 import weight 0.00
Epoch 254 Iter 10 subLoss 2403.7 multi -7.96 import weight 0.00
Epoch 254 Iter 11 subLoss 2519.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10846 / 19.03
Entropy seen (from low to high)
[2163, 595, 432, 277, 206, 198, 135, 126, 95, 89, 70, 57, 43, 53, 57, 49, 47, 40, 36, 35, 40, 32, 31, 28, 33, 23, 18, 22, 22, 21, 7, 17, 14, 3, 8, 2, 6, 1, 4, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 9, 17, 20, 37, 51, 65, 117, 118, 151, 141, 162, 210, 182, 179, 187, 205, 187, 178, 164, 156, 132, 155, 156, 133, 128, 119, 102, 92, 109, 96, 71, 88, 77, 84, 78, 103, 95, 84, 87, 70, 82, 67, 50, 42, 18, 7]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.1, 40.3, 44.0, 47.4, 50.2, 54.2, 58.0, 61.1, 64.7, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 66.6, 79.9, 46.6, 61.9, 83.3, 79.9, 86.2, 83.7, 81.9]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 9, 10, 15, 21, 18, 30, 29, 43, 50]
Epoch 254 Acc: 98.42 BMA: 98.21 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 251 train Loss: 2492.9 test Loss: 287.1
Epoch 255 Iter 0 subLoss 2398.4 multi 3.99 import weight 0.00
Epoch 255 Iter 1 subLoss 2229.0 multi -16.91 import weight 0.00
Epoch 255 Iter 2 subLoss 2658.1 multi 9.96 import weight 0.00
Epoch 255 Iter 3 subLoss 2225.4 multi -13.93 import weight 0.00
Epoch 255 Iter 4 subLoss 2533.8 multi 9.96 import weight 0.00
Epoch 255 Iter 5 subLoss 2401.8 multi -7.96 import weight 0.00
Epoch 255 Iter 6 subLoss 2635.1 multi 12.94 import weight 0.00
Epoch 255 Iter 7 subLoss 2443.9 multi -19.90 import weight 0.00
Epoch 255 Iter 8 subLoss 3429.0 multi -4.97 import weight 0.00
Epoch 255 Iter 9 subLoss 6133.9 multi -4.97 import weight 0.00
Epoch 255 Iter 10 subLoss 31021.9 multi 1.00 import weight 0.00
Epoch 255 Iter 11 subLoss 6651.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10733 / 19.25
Entropy seen (from low to high)
[2263, 568, 395, 268, 224, 170, 128, 132, 108, 74, 63, 57, 69, 58, 60, 38, 51, 38, 36, 35, 38, 28, 33, 22, 29, 21, 21, 15, 21, 17, 9, 15, 9, 8, 3, 4, 3, 4, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 13, 28, 32, 60, 71, 93, 116, 162, 136, 156, 158, 141, 196, 180, 187, 173, 167, 183, 175, 170, 164, 148, 164, 173, 132, 148, 129, 106, 72, 96, 102, 91, 94, 108, 75, 78, 72, 66, 55, 52, 46, 36, 28, 14, 7]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 34.7, 36.5, 40.3, 44.1, 47.3, 50.6, 53.7, 57.9, 61.5, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 62.4, 22.2, 59.9, 55.9, 79.9, 74.9, 85.7, 87.9, 86.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 8, 9, 10, 25, 20, 20, 35, 25, 43]
Epoch 255 Acc: 96.83 BMA: 98.33 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 665 train Loss: 4752.5 test Loss: 488.8
Epoch 256 Iter 0 subLoss 4543.1 multi 6.97 import weight 0.00
Epoch 256 Iter 1 subLoss 2826.2 multi 1.00 import weight 0.00
Epoch 256 Iter 2 subLoss 2487.2 multi -4.97 import weight 0.00
Epoch 256 Iter 3 subLoss 2200.2 multi 1.00 import weight 0.00
Epoch 256 Iter 4 subLoss 2973.1 multi -16.91 import weight 0.00
Epoch 256 Iter 5 subLoss 4341.3 multi -1.99 import weight 0.00
Epoch 256 Iter 6 subLoss 4587.1 multi 1.00 import weight 0.00
Epoch 256 Iter 7 subLoss 4269.0 multi -1.99 import weight 0.00
Epoch 256 Iter 8 subLoss 6270.0 multi -7.96 import weight 0.00
Epoch 256 Iter 9 subLoss 115905.3 multi 1.00 import weight 0.00
Epoch 256 Iter 10 subLoss 3785.5 multi -7.96 import weight 0.00
Epoch 256 Iter 11 subLoss 8126.8 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0072 / 0.10055 / 16.03
Entropy seen (from low to high)
[1815, 489, 339, 215, 218, 176, 183, 178, 169, 160, 150, 167, 116, 86, 85, 66, 43, 45, 56, 37, 46, 43, 24, 22, 35, 24, 22, 23, 14, 20, 20, 11, 11, 9, 7, 2, 2, 6, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 10, 26, 23, 36, 65, 88, 124, 135, 149, 141, 161, 153, 163, 205, 192, 182, 192, 192, 180, 187, 165, 179, 187, 157, 175, 163, 164, 119, 125, 109, 109, 91, 102, 79, 76, 57, 39, 50, 24, 20, 15, 15, 10, 10, 11, 6]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 34.1, 0.0, 40.5, 43.9, 47.1, 50.6, 54.1, 57.8, 61.3, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 44.4, 59.9, 58.3, 53.8, 51.8, 73.0, 79.4, 83.3, 96.7]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 9, 10, 12, 26, 27, 26, 39, 42, 61]
Epoch 256 Acc: 86.24 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 812 train Loss: 15763.7 test Loss: 1929.1
Epoch 257 Iter 0 subLoss 15523.0 multi -1.99 import weight 0.00
Epoch 257 Iter 1 subLoss 56819.8 multi 1.00 import weight 0.00
Epoch 257 Iter 2 subLoss 9971.6 multi 1.00 import weight 0.00
Epoch 257 Iter 3 subLoss 6866.3 multi 3.98 import weight 0.00
Epoch 257 Iter 4 subLoss 4480.5 multi 9.96 import weight 0.00
Epoch 257 Iter 5 subLoss 3256.5 multi -4.97 import weight 0.00
Epoch 257 Iter 6 subLoss 2945.9 multi -4.97 import weight 0.00
Epoch 257 Iter 7 subLoss 3392.6 multi 3.99 import weight 0.00
Epoch 257 Iter 8 subLoss 3134.4 multi -4.97 import weight 0.00
Epoch 257 Iter 9 subLoss 3717.2 multi 12.94 import weight 0.00
Epoch 257 Iter 10 subLoss 3073.3 multi 3.98 import weight 0.00
Epoch 257 Iter 11 subLoss 2697.3 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0068 / 0.10115 / 18.86
Entropy seen (from low to high)
[1796, 536, 349, 235, 212, 203, 184, 203, 166, 161, 172, 127, 96, 88, 66, 42, 53, 46, 43, 45, 34, 28, 31, 30, 27, 19, 21, 24, 17, 16, 15, 17, 10, 5, 6, 1, 5, 3, 2, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 24, 33, 30, 74, 85, 124, 134, 144, 149, 163, 170, 170, 205, 202, 200, 185, 169, 180, 181, 161, 190, 153, 159, 147, 154, 130, 129, 130, 123, 105, 93, 95, 95, 81, 65, 47, 43, 37, 24, 13, 21, 11, 9, 13, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.7, 37.2, 39.6, 43.9, 47.5, 50.8, 54.2, 58.0, 61.6, 65.0, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 49.9, 72.7, 30.7, 42.3, 65.3, 79.9, 82.7, 82.2, 92.9]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 6, 11, 13, 26, 26, 25, 29, 45, 57]
Epoch 257 Acc: 98.19 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 269 train Loss: 3059.4 test Loss: 335.7
Epoch 258 Iter 0 subLoss 2979.7 multi -13.93 import weight 0.00
Epoch 258 Iter 1 subLoss 4216.9 multi 9.96 import weight 0.00
Epoch 258 Iter 2 subLoss 2847.6 multi 18.91 import weight 1.00
Epoch 258 Iter 3 subLoss 2735.4 multi 3.99 import weight 0.00
Epoch 258 Iter 4 subLoss 2535.5 multi 12.94 import weight 0.00
Epoch 258 Iter 5 subLoss 2528.1 multi 1.00 import weight 0.00
Epoch 258 Iter 6 subLoss 2626.9 multi -1.98 import weight 0.00
Epoch 258 Iter 7 subLoss 2038.8 multi -4.97 import weight 0.00
Epoch 258 Iter 8 subLoss 2625.5 multi 1.00 import weight 0.00
Epoch 258 Iter 9 subLoss 2676.0 multi 12.94 import weight 0.00
Epoch 258 Iter 10 subLoss 2351.2 multi 6.97 import weight 0.00
Epoch 258 Iter 11 subLoss 2518.1 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10357 / 16.17
Entropy seen (from low to high)
[1862, 541, 319, 264, 227, 212, 199, 189, 182, 187, 140, 98, 94, 73, 41, 47, 44, 55, 39, 37, 30, 28, 27, 24, 21, 31, 22, 15, 15, 17, 15, 9, 11, 6, 1, 5, 3, 3, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 15, 38, 34, 53, 79, 112, 120, 128, 144, 162, 154, 184, 197, 190, 214, 159, 183, 164, 168, 190, 165, 154, 170, 155, 147, 149, 110, 117, 135, 121, 99, 100, 106, 89, 83, 61, 41, 56, 25, 20, 21, 14, 9, 13, 5]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 38.1, 40.6, 43.5, 47.4, 50.8, 54.0, 57.9, 61.2, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 0.0, 99.9, 39.9, 46.1, 61.1, 47.9, 74.9, 85.1, 79.4, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 7, 10, 13, 18, 25, 24, 27, 34, 50]
Epoch 258 Acc: 98.15 BMA: 98.33 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 251 train Loss: 2445.5 test Loss: 289.1
Epoch 259 Iter 0 subLoss 2203.5 multi 3.99 import weight 0.00
Epoch 259 Iter 1 subLoss 2500.1 multi -16.91 import weight 0.00
Epoch 259 Iter 2 subLoss 2513.2 multi -1.99 import weight 0.00
Epoch 259 Iter 3 subLoss 3043.2 multi -7.96 import weight 0.00
Epoch 259 Iter 4 subLoss 3114.3 multi -7.96 import weight 0.00
Epoch 259 Iter 5 subLoss 7654.8 multi -1.99 import weight 0.00
Epoch 259 Iter 6 subLoss 35430.1 multi 1.00 import weight 0.00
Epoch 259 Iter 7 subLoss 3062.6 multi -7.96 import weight 0.00
Epoch 259 Iter 8 subLoss 4168.4 multi 1.00 import weight 0.00
Epoch 259 Iter 9 subLoss 3545.9 multi -16.91 import weight 0.00
Epoch 259 Iter 10 subLoss 20277.9 multi -1.99 import weight 0.00
Epoch 259 Iter 11 subLoss 72608.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0076 / 0.09957 / 17.22
Entropy seen (from low to high)
[1374, 345, 247, 238, 231, 224, 211, 199, 192, 218, 214, 194, 177, 138, 122, 79, 95, 76, 77, 80, 45, 56, 35, 36, 29, 23, 30, 18, 23, 19, 20, 21, 11, 13, 6, 5, 6, 5, 0, 3, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 13, 19, 28, 52, 69, 99, 105, 170, 149, 178, 189, 203, 207, 209, 213, 224, 206, 171, 180, 174, 154, 154, 142, 156, 139, 126, 121, 84, 111, 100, 97, 97, 80, 80, 72, 67, 59, 36, 36, 28, 20, 8, 16, 11, 6, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 32.6, 37.7, 40.3, 43.4, 47.4, 50.8, 54.3, 57.6, 61.4, 64.4, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 42.8, 38.4, 55.5, 66.6, 67.9, 67.4, 84.8, 91.4, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 7, 13, 18, 21, 25, 40, 33, 47, 58]
Epoch 259 Acc: 76.73 BMA: 98.35 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 7260 train Loss: 20522.0 test Loss: 3428.9
Epoch 260 Iter 0 subLoss 20374.3 multi 1.00 import weight 0.00
Epoch 260 Iter 1 subLoss 12308.6 multi 1.00 import weight 0.00
Epoch 260 Iter 2 subLoss 8784.2 multi -4.97 import weight 0.00
Epoch 260 Iter 3 subLoss 23336.2 multi 1.00 import weight 0.00
Epoch 260 Iter 4 subLoss 16165.5 multi 9.96 import weight 0.00
Epoch 260 Iter 5 subLoss 4898.2 multi -1.99 import weight 0.00
Epoch 260 Iter 6 subLoss 5419.6 multi 1.00 import weight 0.00
Epoch 260 Iter 7 subLoss 5076.0 multi -1.99 import weight 0.00
Epoch 260 Iter 8 subLoss 6571.5 multi 3.98 import weight 0.00
Epoch 260 Iter 9 subLoss 3693.6 multi -10.94 import weight 0.00
Epoch 260 Iter 10 subLoss 5271.0 multi 1.00 import weight 0.00
Epoch 260 Iter 11 subLoss 5179.4 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0075 / 0.09908 / 17.51
Entropy seen (from low to high)
[1317, 365, 282, 245, 245, 213, 220, 219, 218, 226, 231, 201, 151, 132, 90, 97, 71, 74, 83, 51, 58, 42, 43, 32, 25, 20, 31, 21, 19, 23, 19, 16, 12, 12, 15, 3, 3, 7, 0, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 11, 22, 39, 42, 82, 103, 132, 150, 158, 174, 195, 211, 197, 209, 217, 213, 191, 175, 193, 185, 165, 127, 169, 143, 154, 101, 129, 94, 93, 102, 102, 87, 84, 80, 61, 69, 52, 35, 35, 23, 15, 12, 14, 9, 5, 2]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0, 34.6, 37.4, 40.8, 43.8, 47.3, 50.5, 54.2, 58.0, 61.0, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 45.4, 29.9, 63.6, 72.4, 66.6, 67.5, 79.9, 84.6, 91.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 11, 10, 11, 29, 27, 37, 40, 39, 49]
Epoch 260 Acc: 96.44 BMA: 98.29 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 517 train Loss: 6034.6 test Loss: 610.3
Epoch 261 Iter 0 subLoss 5528.7 multi 1.00 import weight 0.00
Epoch 261 Iter 1 subLoss 5392.3 multi -10.94 import weight 0.00
Epoch 261 Iter 2 subLoss 15867.7 multi 1.00 import weight 0.00
Epoch 261 Iter 3 subLoss 10475.3 multi -1.99 import weight 0.00
Epoch 261 Iter 4 subLoss 17061.3 multi 1.00 import weight 0.00
Epoch 261 Iter 5 subLoss 11379.5 multi 1.00 import weight 0.00
Epoch 261 Iter 6 subLoss 8467.8 multi 9.96 import weight 0.00
Epoch 261 Iter 7 subLoss 3721.4 multi -1.98 import weight 0.00
Epoch 261 Iter 8 subLoss 3961.2 multi 3.99 import weight 0.00
Epoch 261 Iter 9 subLoss 3295.1 multi 12.94 import weight 0.00
Epoch 261 Iter 10 subLoss 3281.5 multi -10.94 import weight 0.00
Epoch 261 Iter 11 subLoss 3246.6 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0074 / 0.09869 / 17.70
Entropy seen (from low to high)
[1343, 386, 275, 262, 234, 226, 232, 233, 230, 240, 236, 180, 125, 110, 104, 81, 63, 66, 64, 59, 37, 51, 41, 25, 26, 22, 23, 20, 19, 23, 23, 20, 13, 13, 12, 6, 2, 4, 3, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 13, 19, 37, 43, 74, 112, 147, 158, 172, 177, 191, 217, 200, 216, 204, 235, 187, 180, 182, 171, 156, 145, 151, 127, 138, 124, 112, 101, 88, 93, 100, 91, 86, 76, 57, 63, 56, 43, 34, 28, 16, 9, 16, 9, 5, 2]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 0.0, 37.5, 40.1, 43.8, 47.6, 50.8, 54.2, 58.0, 61.0, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 62.4, 41.6, 76.9, 65.5, 69.9, 70.2, 78.0, 92.1, 86.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 8, 12, 13, 29, 30, 37, 41, 38, 44]
Epoch 261 Acc: 97.08 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 324 train Loss: 3965.8 test Loss: 475.1
Epoch 262 Iter 0 subLoss 3202.2 multi -7.96 import weight 0.00
Epoch 262 Iter 1 subLoss 4810.7 multi 1.00 import weight 0.00
Epoch 262 Iter 2 subLoss 4529.8 multi -4.97 import weight 0.00
Epoch 262 Iter 3 subLoss 5525.0 multi 3.98 import weight 0.00
Epoch 262 Iter 4 subLoss 3818.7 multi 6.97 import weight 0.00
Epoch 262 Iter 5 subLoss 3935.4 multi -7.96 import weight 0.00
Epoch 262 Iter 6 subLoss 3876.9 multi -1.98 import weight 0.00
Epoch 262 Iter 7 subLoss 4715.3 multi 1.00 import weight 0.00
Epoch 262 Iter 8 subLoss 4323.6 multi 1.00 import weight 0.00
Epoch 262 Iter 9 subLoss 4407.2 multi -4.97 import weight 0.00
Epoch 262 Iter 10 subLoss 4937.3 multi 1.00 import weight 0.00
Epoch 262 Iter 11 subLoss 4432.9 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0073 / 0.09849 / 18.29
Entropy seen (from low to high)
[1362, 401, 291, 252, 246, 226, 231, 258, 247, 248, 205, 166, 118, 109, 92, 73, 63, 58, 54, 53, 40, 48, 35, 28, 22, 26, 16, 21, 21, 26, 23, 15, 14, 12, 15, 9, 0, 5, 2, 2, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 11, 21, 32, 48, 77, 111, 153, 158, 177, 204, 193, 206, 204, 216, 206, 213, 206, 165, 188, 174, 137, 169, 124, 115, 139, 128, 108, 103, 97, 82, 91, 104, 88, 71, 59, 61, 51, 51, 31, 33, 13, 11, 13, 12, 4, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 34.9, 36.9, 40.6, 43.8, 47.6, 51.1, 54.3, 57.8, 61.0, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 24.9, 62.4, 33.3, 71.4, 73.0, 59.9, 82.0, 79.4, 83.3, 86.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 8, 9, 21, 26, 35, 39, 34, 42, 37]
Epoch 262 Acc: 96.89 BMA: 98.27 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 443 train Loss: 4122.7 test Loss: 502.0
Epoch 263 Iter 0 subLoss 4345.2 multi 1.00 import weight 0.00
Epoch 263 Iter 1 subLoss 3943.8 multi 6.97 import weight 0.00
Epoch 263 Iter 2 subLoss 3500.6 multi 12.94 import weight 0.00
Epoch 263 Iter 3 subLoss 2998.8 multi 1.00 import weight 0.00
Epoch 263 Iter 4 subLoss 3035.4 multi 3.99 import weight 0.00
Epoch 263 Iter 5 subLoss 2685.9 multi -4.97 import weight 0.00
Epoch 263 Iter 6 subLoss 2671.8 multi 15.93 import weight 0.00
Epoch 263 Iter 7 subLoss 2911.5 multi -4.97 import weight 0.00
Epoch 263 Iter 8 subLoss 3304.9 multi 6.97 import weight 0.00
Epoch 263 Iter 9 subLoss 3124.3 multi 9.96 import weight 0.00
Epoch 263 Iter 10 subLoss 2354.3 multi 9.96 import weight 0.00
Epoch 263 Iter 11 subLoss 2731.6 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0070 / 0.09993 / 17.78
Entropy seen (from low to high)
[1392, 416, 282, 273, 254, 238, 247, 266, 262, 246, 196, 136, 110, 108, 75, 70, 60, 49, 53, 44, 43, 37, 40, 23, 21, 18, 21, 21, 25, 19, 23, 12, 8, 15, 16, 5, 2, 4, 1, 2, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 9, 19, 25, 53, 69, 98, 145, 158, 163, 197, 195, 212, 206, 198, 199, 216, 210, 170, 170, 176, 145, 150, 158, 109, 116, 141, 118, 110, 105, 81, 87, 90, 117, 72, 67, 60, 54, 59, 33, 34, 20, 12, 15, 12, 5, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 34.3, 36.3, 40.3, 43.4, 47.8, 51.0, 54.4, 57.9, 61.3, 64.6, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 42.8, 41.6, 83.3, 47.8, 66.6, 71.0, 87.8, 86.1, 89.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 7, 12, 18, 23, 33, 38, 33, 36, 40]
Epoch 263 Acc: 98.42 BMA: 98.23 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 273 train Loss: 2533.8 test Loss: 264.8
Epoch 264 Iter 0 subLoss 2610.8 multi -1.99 import weight 0.00
Epoch 264 Iter 1 subLoss 2746.3 multi -7.96 import weight 0.00
Epoch 264 Iter 2 subLoss 2597.6 multi 15.93 import weight 0.00
Epoch 264 Iter 3 subLoss 2783.0 multi -7.96 import weight 0.00
Epoch 264 Iter 4 subLoss 2227.0 multi -10.94 import weight 0.00
Epoch 264 Iter 5 subLoss 3457.2 multi 9.96 import weight 0.00
Epoch 264 Iter 6 subLoss 2744.0 multi -4.97 import weight 0.00
Epoch 264 Iter 7 subLoss 2379.4 multi -7.96 import weight 0.00
Epoch 264 Iter 8 subLoss 2911.5 multi -1.98 import weight 0.00
Epoch 264 Iter 9 subLoss 3005.3 multi 3.98 import weight 0.00
Epoch 264 Iter 10 subLoss 2990.3 multi 3.99 import weight 0.00
Epoch 264 Iter 11 subLoss 2861.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0068 / 0.10101 / 16.24
Entropy seen (from low to high)
[1420, 413, 297, 285, 267, 248, 269, 278, 266, 226, 181, 119, 105, 84, 89, 59, 55, 49, 49, 40, 42, 36, 30, 21, 22, 21, 22, 19, 21, 21, 18, 11, 9, 15, 13, 4, 5, 1, 1, 2, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 20, 23, 51, 61, 95, 135, 155, 160, 199, 201, 187, 221, 192, 203, 200, 218, 179, 162, 160, 149, 155, 130, 143, 122, 124, 118, 112, 101, 97, 97, 83, 99, 96, 66, 67, 65, 56, 39, 38, 25, 13, 15, 13, 4, 4]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 34.2, 36.2, 41.0, 43.3, 47.4, 50.5, 54.4, 57.6, 61.3, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 0.0, 49.9, 39.9, 55.5, 68.1, 66.6, 65.6, 86.8, 85.7, 87.1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 6, 10, 18, 22, 30, 32, 38, 35, 39]
Epoch 264 Acc: 98.29 BMA: 98.27 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 286 train Loss: 2612.3 test Loss: 299.2
Epoch 265 Iter 0 subLoss 2750.7 multi -7.96 import weight 0.00
Epoch 265 Iter 1 subLoss 2731.4 multi 9.96 import weight 0.00
Epoch 265 Iter 2 subLoss 2595.1 multi 18.91 import weight 0.00
Epoch 265 Iter 3 subLoss 2539.8 multi 12.94 import weight 0.00
Epoch 265 Iter 4 subLoss 2307.4 multi 1.00 import weight 0.00
Epoch 265 Iter 5 subLoss 2730.7 multi 12.94 import weight 0.00
Epoch 265 Iter 6 subLoss 2327.7 multi -7.96 import weight 0.00
Epoch 265 Iter 7 subLoss 2182.6 multi 18.91 import weight 0.00
Epoch 265 Iter 8 subLoss 2205.2 multi 6.97 import weight 0.00
Epoch 265 Iter 9 subLoss 2200.6 multi 9.96 import weight 0.00
Epoch 265 Iter 10 subLoss 2052.8 multi -7.96 import weight 0.00
Epoch 265 Iter 11 subLoss 2233.9 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0065 / 0.10198 / 17.17
Entropy seen (from low to high)
[1445, 412, 318, 284, 278, 265, 290, 281, 276, 211, 139, 124, 108, 80, 65, 66, 51, 47, 42, 39, 38, 35, 24, 16, 23, 26, 21, 19, 22, 20, 12, 9, 11, 16, 8, 4, 4, 1, 3, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 22, 22, 42, 59, 95, 122, 148, 158, 177, 215, 182, 216, 202, 214, 197, 197, 185, 154, 164, 163, 155, 133, 132, 142, 100, 130, 108, 107, 100, 89, 85, 94, 113, 80, 66, 53, 70, 47, 34, 34, 16, 12, 12, 5, 4]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.3, 34.8, 35.3, 40.4, 43.1, 47.6, 50.9, 54.7, 57.9, 61.5, 64.7, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 33.3, 49.9, 55.5, 57.1, 72.2, 62.4, 69.4, 80.6, 88.5, 92.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 4, 9, 21, 18, 24, 36, 31, 35, 39]
Epoch 265 Acc: 98.52 BMA: 98.29 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 223 train Loss: 2588.0 test Loss: 240.6
Epoch 266 Iter 0 subLoss 2699.4 multi -4.97 import weight 0.00
Epoch 266 Iter 1 subLoss 2711.1 multi -1.99 import weight 0.00
Epoch 266 Iter 2 subLoss 3604.1 multi 6.97 import weight 0.00
Epoch 266 Iter 3 subLoss 2749.3 multi -7.96 import weight 0.00
Epoch 266 Iter 4 subLoss 2158.9 multi -7.96 import weight 0.00
Epoch 266 Iter 5 subLoss 2442.7 multi -16.91 import weight 0.00
Epoch 266 Iter 6 subLoss 3434.6 multi 9.96 import weight 0.00
Epoch 266 Iter 7 subLoss 1945.8 multi 1.00 import weight 0.00
Epoch 266 Iter 8 subLoss 2601.3 multi -16.91 import weight 0.00
Epoch 266 Iter 9 subLoss 2500.6 multi -13.93 import weight 0.00
Epoch 266 Iter 10 subLoss 3767.7 multi -4.97 import weight 0.00
Epoch 266 Iter 11 subLoss 4932.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0065 / 0.10258 / 16.24
Entropy seen (from low to high)
[1470, 420, 325, 292, 278, 280, 305, 292, 253, 203, 135, 112, 91, 70, 76, 51, 53, 44, 33, 40, 36, 40, 16, 20, 26, 26, 20, 18, 19, 24, 11, 7, 16, 9, 11, 3, 4, 2, 1, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 20, 26, 38, 60, 96, 120, 141, 145, 180, 206, 195, 206, 210, 219, 189, 204, 177, 176, 148, 163, 137, 127, 133, 158, 110, 98, 120, 109, 102, 95, 86, 87, 101, 95, 66, 65, 67, 52, 35, 38, 18, 15, 12, 5, 4]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 34.7, 36.2, 40.4, 44.3, 47.0, 50.7, 54.3, 57.9, 61.1, 64.3, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 49.9, 49.9, 44.4, 66.6, 64.9, 70.8, 67.6, 81.4, 81.5, 89.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 6, 9, 18, 20, 24, 34, 27, 38, 38]
Epoch 266 Acc: 97.22 BMA: 98.37 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 493 train Loss: 3563.0 test Loss: 443.8
Epoch 267 Iter 0 subLoss 3378.9 multi -1.99 import weight 0.00
Epoch 267 Iter 1 subLoss 3582.4 multi 1.00 import weight 0.00
Epoch 267 Iter 2 subLoss 3403.2 multi -1.99 import weight 0.00
Epoch 267 Iter 3 subLoss 4440.0 multi -1.98 import weight 0.00
Epoch 267 Iter 4 subLoss 4781.0 multi 1.00 import weight 0.00
Epoch 267 Iter 5 subLoss 4497.7 multi -7.96 import weight 0.00
Epoch 267 Iter 6 subLoss 7471.7 multi -1.99 import weight 0.00
Epoch 267 Iter 7 subLoss 10075.4 multi -4.97 import weight 0.00
Epoch 267 Iter 8 subLoss 43783.4 multi 1.00 import weight 0.00
Epoch 267 Iter 9 subLoss 11542.8 multi 6.97 import weight 0.00
Epoch 267 Iter 10 subLoss 3947.3 multi 9.96 import weight 0.00
Epoch 267 Iter 11 subLoss 3057.6 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10328 / 15.46
Entropy seen (from low to high)
[1491, 425, 344, 295, 291, 296, 321, 290, 240, 175, 128, 110, 83, 64, 71, 55, 42, 46, 29, 46, 33, 29, 13, 22, 36, 16, 20, 17, 23, 19, 10, 6, 16, 12, 7, 4, 5, 1, 0, 4, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 22, 23, 41, 56, 90, 119, 137, 141, 186, 182, 214, 212, 199, 221, 191, 187, 180, 186, 134, 173, 129, 132, 137, 123, 132, 107, 109, 123, 95, 94, 89, 83, 96, 102, 77, 67, 67, 58, 37, 43, 20, 19, 13, 5, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.1, 34.5, 36.4, 40.8, 44.5, 47.3, 50.8, 54.5, 57.6, 61.4, 64.5, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 33.3, 24.9, 49.9, 61.9, 68.7, 70.8, 64.2, 76.6, 86.8, 84.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 10, 21, 16, 24, 28, 30, 38, 33]
Epoch 267 Acc: 98.23 BMA: 98.37 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 305 train Loss: 2700.4 test Loss: 286.9
Epoch 268 Iter 0 subLoss 2731.8 multi 15.93 import weight 0.00
Epoch 268 Iter 1 subLoss 2813.0 multi -28.85 import weight 0.00
Epoch 268 Iter 2 subLoss 3849.0 multi -4.97 import weight 0.00
Epoch 268 Iter 3 subLoss 6387.9 multi -4.97 import weight 0.00
Epoch 268 Iter 4 subLoss 17608.7 multi 6.97 import weight 0.00
Epoch 268 Iter 5 subLoss 3865.2 multi 9.96 import weight 0.00
Epoch 268 Iter 6 subLoss 2926.9 multi -4.97 import weight 0.00
Epoch 268 Iter 7 subLoss 3137.2 multi -4.97 import weight 0.00
Epoch 268 Iter 8 subLoss 3386.8 multi -4.97 import weight 0.00
Epoch 268 Iter 9 subLoss 3558.8 multi 6.97 import weight 0.00
Epoch 268 Iter 10 subLoss 3199.9 multi -19.90 import weight 0.00
Epoch 268 Iter 11 subLoss 4697.0 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10261 / 17.30
Entropy seen (from low to high)
[1484, 452, 331, 299, 310, 305, 313, 292, 218, 177, 129, 100, 75, 72, 56, 54, 50, 38, 32, 38, 42, 30, 19, 21, 28, 27, 12, 21, 21, 19, 10, 9, 14, 15, 9, 3, 5, 1, 0, 5, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 20, 21, 46, 57, 98, 137, 132, 156, 191, 196, 214, 200, 201, 214, 213, 204, 176, 147, 142, 152, 147, 121, 139, 127, 130, 93, 112, 115, 92, 92, 92, 84, 87, 92, 92, 62, 57, 59, 42, 38, 26, 15, 13, 6, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.9, 33.8, 37.2, 39.8, 44.0, 47.0, 50.6, 54.2, 57.6, 61.5, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 49.9, 0.0, 49.9, 74.9, 68.4, 65.5, 69.2, 78.7, 85.2, 88.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 2, 2, 12, 20, 19, 29, 26, 33, 34, 35]
Epoch 268 Acc: 97.18 BMA: 98.35 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 469 train Loss: 3986.1 test Loss: 503.1
Epoch 269 Iter 0 subLoss 3752.2 multi 9.96 import weight 0.00
Epoch 269 Iter 1 subLoss 3230.8 multi 21.90 import weight 0.00
Epoch 269 Iter 2 subLoss 3363.1 multi 6.97 import weight 0.00
Epoch 269 Iter 3 subLoss 2981.3 multi -1.99 import weight 0.00
Epoch 269 Iter 4 subLoss 2350.7 multi 12.94 import weight 0.00
Epoch 269 Iter 5 subLoss 2421.0 multi -1.99 import weight 0.00
Epoch 269 Iter 6 subLoss 2435.4 multi 15.93 import weight 0.00
Epoch 269 Iter 7 subLoss 2952.1 multi 1.00 import weight 0.00
Epoch 269 Iter 8 subLoss 2496.4 multi 12.94 import weight 0.00
Epoch 269 Iter 9 subLoss 2216.3 multi 1.00 import weight 0.00
Epoch 269 Iter 10 subLoss 2735.8 multi 18.91 import weight 0.00
Epoch 269 Iter 11 subLoss 2963.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10365 / 15.77
Entropy seen (from low to high)
[1504, 459, 345, 319, 300, 325, 323, 283, 211, 152, 132, 92, 62, 73, 57, 54, 50, 28, 34, 38, 37, 25, 23, 21, 29, 20, 15, 24, 18, 16, 9, 13, 10, 15, 6, 4, 4, 1, 1, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 19, 20, 45, 51, 95, 127, 130, 140, 196, 174, 227, 196, 206, 207, 207, 198, 175, 162, 150, 157, 130, 134, 129, 129, 129, 111, 103, 116, 90, 95, 99, 88, 82, 95, 94, 71, 59, 67, 42, 44, 23, 18, 15, 6, 4]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.2, 37.0, 41.2, 44.0, 47.4, 50.7, 54.5, 57.8, 61.3, 64.7, 67.9]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 33.3, 0.0, 49.9, 63.9, 76.4, 67.8, 66.6, 72.4, 83.3, 86.2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 2, 8, 25, 17, 28, 21, 29, 36, 29]
Epoch 269 Acc: 98.52 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 296 train Loss: 2542.1 test Loss: 244.9
Epoch 270 Iter 0 subLoss 2342.4 multi -13.93 import weight 0.00
Epoch 270 Iter 1 subLoss 2727.1 multi -4.97 import weight 0.00
Epoch 270 Iter 2 subLoss 3096.1 multi 3.98 import weight 0.00
Epoch 270 Iter 3 subLoss 1928.6 multi 1.00 import weight 0.00
Epoch 270 Iter 4 subLoss 3326.5 multi 3.99 import weight 0.00
Epoch 270 Iter 5 subLoss 2542.0 multi -10.94 import weight 0.00
Epoch 270 Iter 6 subLoss 2772.4 multi 15.93 import weight 0.00
Epoch 270 Iter 7 subLoss 2552.1 multi 6.97 import weight 0.00
Epoch 270 Iter 8 subLoss 2775.7 multi 18.91 import weight 0.00
Epoch 270 Iter 9 subLoss 2120.2 multi 3.98 import weight 0.00
Epoch 270 Iter 10 subLoss 2282.7 multi 6.97 import weight 0.00
Epoch 270 Iter 11 subLoss 2031.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0060 / 0.10469 / 16.11
Entropy seen (from low to high)
[1532, 470, 347, 329, 301, 341, 338, 272, 183, 164, 108, 88, 66, 64, 58, 54, 38, 32, 41, 32, 28, 28, 19, 21, 30, 19, 24, 14, 17, 16, 10, 10, 13, 11, 5, 4, 3, 1, 2, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 19, 18, 44, 49, 84, 130, 116, 147, 172, 188, 201, 197, 212, 201, 190, 213, 190, 150, 161, 141, 148, 120, 147, 121, 127, 123, 93, 118, 99, 98, 91, 97, 93, 88, 92, 81, 68, 63, 46, 47, 29, 18, 16, 6, 4]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 37.0, 40.8, 44.2, 47.5, 50.5, 54.0, 57.6, 61.2, 64.6, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 49.9, 0.0, 33.3, 57.8, 78.9, 63.6, 70.8, 65.5, 83.3, 91.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 9, 19, 19, 22, 24, 29, 30, 37]
Epoch 270 Acc: 98.68 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 203 train Loss: 2257.1 test Loss: 205.7
Epoch 271 Iter 0 subLoss 2708.2 multi 3.99 import weight 0.00
Epoch 271 Iter 1 subLoss 2337.0 multi 12.94 import weight 0.00
Epoch 271 Iter 2 subLoss 2263.3 multi -1.99 import weight 0.00
Epoch 271 Iter 3 subLoss 2566.1 multi -13.93 import weight 0.00
Epoch 271 Iter 4 subLoss 2252.2 multi -1.98 import weight 0.00
Epoch 271 Iter 5 subLoss 2986.9 multi 1.00 import weight 0.00
Epoch 271 Iter 6 subLoss 2293.1 multi -13.93 import weight 0.00
Epoch 271 Iter 7 subLoss 2162.8 multi 9.96 import weight 0.00
Epoch 271 Iter 8 subLoss 2228.7 multi -10.94 import weight 0.00
Epoch 271 Iter 9 subLoss 2028.2 multi 6.97 import weight 0.00
Epoch 271 Iter 10 subLoss 2319.8 multi 1.00 import weight 0.00
Epoch 271 Iter 11 subLoss 2521.0 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0058 / 0.10555 / 16.90
Entropy seen (from low to high)
[1553, 477, 356, 328, 315, 371, 335, 249, 189, 142, 101, 72, 73, 64, 58, 50, 34, 33, 35, 29, 30, 28, 18, 25, 20, 23, 26, 14, 15, 13, 10, 12, 11, 10, 4, 4, 3, 2, 2, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 14, 18, 37, 54, 78, 113, 127, 138, 149, 197, 183, 204, 222, 209, 177, 199, 203, 152, 171, 131, 155, 130, 147, 112, 135, 114, 113, 104, 111, 97, 89, 98, 90, 87, 97, 88, 68, 66, 54, 49, 32, 16, 13, 10, 4]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 34.2, 35.6, 40.7, 44.1, 47.3, 50.7, 54.2, 58.0, 61.1, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 0.0, 24.9, 33.3, 64.2, 78.2, 71.4, 66.6, 63.9, 81.2, 94.2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 4, 9, 14, 23, 21, 24, 25, 32, 35]
Epoch 271 Acc: 98.64 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 252 train Loss: 2465.0 test Loss: 211.1
Epoch 272 Iter 0 subLoss 2289.9 multi 9.96 import weight 0.00
Epoch 272 Iter 1 subLoss 1841.5 multi 1.00 import weight 0.00
Epoch 272 Iter 2 subLoss 2128.9 multi 6.97 import weight 0.00
Epoch 272 Iter 3 subLoss 2328.0 multi -7.96 import weight 0.00
Epoch 272 Iter 4 subLoss 2352.3 multi 12.94 import weight 0.00
Epoch 272 Iter 5 subLoss 2157.4 multi -4.97 import weight 0.00
Epoch 272 Iter 6 subLoss 2487.0 multi -1.98 import weight 0.00
Epoch 272 Iter 7 subLoss 2947.4 multi -1.99 import weight 0.00
Epoch 272 Iter 8 subLoss 2546.0 multi -7.96 import weight 0.00
Epoch 272 Iter 9 subLoss 2579.5 multi 12.94 import weight 0.00
Epoch 272 Iter 10 subLoss 2046.1 multi 9.96 import weight 0.00
Epoch 272 Iter 11 subLoss 2185.5 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0057 / 0.10638 / 13.91
Entropy seen (from low to high)
[1569, 480, 370, 340, 335, 370, 322, 246, 177, 131, 107, 67, 70, 65, 52, 42, 34, 39, 28, 32, 23, 31, 20, 23, 20, 27, 19, 18, 11, 11, 9, 14, 11, 7, 6, 2, 1, 2, 3, 1, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 13, 19, 36, 47, 72, 109, 121, 134, 160, 171, 178, 216, 214, 209, 168, 197, 205, 158, 168, 151, 144, 141, 136, 130, 110, 124, 119, 108, 108, 99, 76, 117, 90, 84, 97, 103, 71, 58, 63, 48, 40, 15, 14, 11, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 34.2, 35.5, 41.3, 43.8, 47.1, 50.9, 54.5, 57.7, 61.3, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 33.3, 33.3, 45.4, 71.9, 68.4, 60.8, 66.6, 82.1, 93.9]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 6, 6, 11, 25, 19, 23, 27, 28, 33]
Epoch 272 Acc: 98.44 BMA: 98.42 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 218 train Loss: 2412.9 test Loss: 238.2
Epoch 273 Iter 0 subLoss 2489.3 multi 1.00 import weight 0.00
Epoch 273 Iter 1 subLoss 2301.2 multi 1.00 import weight 0.00
Epoch 273 Iter 2 subLoss 2527.5 multi 1.00 import weight 0.00
Epoch 273 Iter 3 subLoss 2133.9 multi -7.96 import weight 0.00
Epoch 273 Iter 4 subLoss 2489.8 multi 3.99 import weight 0.00
Epoch 273 Iter 5 subLoss 2277.3 multi 1.00 import weight 0.00
Epoch 273 Iter 6 subLoss 2202.7 multi 12.94 import weight 0.00
Epoch 273 Iter 7 subLoss 1942.9 multi 3.99 import weight 0.00
Epoch 273 Iter 8 subLoss 2487.6 multi 6.97 import weight 0.00
Epoch 273 Iter 9 subLoss 2063.6 multi -4.97 import weight 0.00
Epoch 273 Iter 10 subLoss 2194.0 multi -16.91 import weight 0.00
Epoch 273 Iter 11 subLoss 2723.6 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0056 / 0.10702 / 14.22
Entropy seen (from low to high)
[1585, 482, 378, 356, 357, 374, 315, 221, 173, 126, 98, 73, 60, 65, 52, 34, 35, 35, 31, 29, 28, 25, 21, 21, 25, 24, 17, 21, 11, 11, 5, 13, 14, 4, 7, 2, 3, 0, 4, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 13, 18, 32, 54, 63, 109, 105, 136, 156, 160, 186, 215, 207, 214, 178, 193, 190, 173, 155, 160, 135, 161, 127, 132, 111, 124, 120, 108, 98, 104, 83, 113, 90, 100, 91, 96, 76, 63, 66, 50, 46, 17, 14, 10, 4]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.6, 41.1, 44.0, 47.3, 51.1, 54.1, 57.4, 61.2, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 42.8, 0.0, 53.8, 69.9, 68.4, 63.9, 63.6, 88.8, 86.1]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 7, 5, 13, 20, 19, 25, 22, 27, 36]
Epoch 273 Acc: 98.21 BMA: 98.42 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 272 train Loss: 2725.4 test Loss: 279.9
Epoch 274 Iter 0 subLoss 2547.4 multi -4.97 import weight 0.00
Epoch 274 Iter 1 subLoss 3645.4 multi 18.91 import weight 0.00
Epoch 274 Iter 2 subLoss 3550.4 multi 9.96 import weight 0.00
Epoch 274 Iter 3 subLoss 1899.5 multi -4.97 import weight 0.00
Epoch 274 Iter 4 subLoss 2435.1 multi 18.91 import weight 1.00
Epoch 274 Iter 5 subLoss 1959.5 multi 1.00 import weight 0.00
Epoch 274 Iter 6 subLoss 2565.8 multi -10.94 import weight 0.00
Epoch 274 Iter 7 subLoss 2349.3 multi -13.93 import weight 0.00
Epoch 274 Iter 8 subLoss 2214.8 multi 1.00 import weight 0.00
Epoch 274 Iter 9 subLoss 2045.4 multi 12.94 import weight 0.00
Epoch 274 Iter 10 subLoss 2336.6 multi 12.94 import weight 0.00
Epoch 274 Iter 11 subLoss 2704.5 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0055 / 0.10803 / 16.29
Entropy seen (from low to high)
[1604, 489, 386, 371, 374, 372, 308, 207, 172, 113, 83, 75, 73, 51, 52, 30, 40, 29, 31, 30, 26, 23, 25, 16, 28, 22, 16, 16, 13, 11, 6, 15, 11, 2, 5, 3, 3, 1, 3, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 11, 17, 27, 50, 70, 91, 111, 128, 148, 159, 173, 216, 204, 209, 179, 191, 182, 178, 166, 151, 146, 158, 131, 137, 121, 114, 114, 120, 99, 111, 83, 105, 96, 95, 94, 96, 92, 69, 62, 49, 49, 23, 15, 10, 5]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.8, 40.8, 43.8, 47.4, 50.8, 54.0, 57.8, 61.4, 64.8, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 42.8, 0.0, 62.4, 68.4, 68.1, 67.8, 56.2, 85.7, 90.9]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 7, 6, 8, 19, 22, 28, 16, 28, 33]
Epoch 274 Acc: 98.75 BMA: 98.44 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 270 train Loss: 2199.9 test Loss: 171.9
Epoch 275 Iter 0 subLoss 2701.5 multi 9.96 import weight 0.00
Epoch 275 Iter 1 subLoss 1959.6 multi 3.98 import weight 0.00
Epoch 275 Iter 2 subLoss 1898.8 multi -1.98 import weight 0.00
Epoch 275 Iter 3 subLoss 1905.9 multi -4.97 import weight 0.00
Epoch 275 Iter 4 subLoss 2209.7 multi 12.94 import weight 0.00
Epoch 275 Iter 5 subLoss 2656.2 multi 12.94 import weight 0.00
Epoch 275 Iter 6 subLoss 1916.4 multi 1.00 import weight 0.00
Epoch 275 Iter 7 subLoss 1915.3 multi 3.98 import weight 0.00
Epoch 275 Iter 8 subLoss 2157.9 multi -1.99 import weight 0.00
Epoch 275 Iter 9 subLoss 2191.6 multi -13.93 import weight 0.00
Epoch 275 Iter 10 subLoss 2020.8 multi 9.96 import weight 0.00
Epoch 275 Iter 11 subLoss 2436.1 multi 21.90 import weight 1.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 275 Acc: 98.52 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 21.90 Pidx 243 train Loss: 2300.7 test Loss: 216.7
Epoch 276 Iter 0 subLoss 2235.7 multi -4.97 import weight 0.00
Epoch 276 Iter 1 subLoss 3088.5 multi -4.97 import weight 0.00
Epoch 276 Iter 2 subLoss 4023.0 multi 9.96 import weight 0.00
Epoch 276 Iter 3 subLoss 2779.3 multi 21.90 import weight 0.00
Epoch 276 Iter 4 subLoss 3096.1 multi 3.99 import weight 0.00
Epoch 276 Iter 5 subLoss 2268.4 multi -1.98 import weight 0.00
Epoch 276 Iter 6 subLoss 2426.3 multi 1.00 import weight 0.00
Epoch 276 Iter 7 subLoss 2086.7 multi -1.99 import weight 0.00
Epoch 276 Iter 8 subLoss 2497.7 multi 3.99 import weight 0.00
Epoch 276 Iter 9 subLoss 2153.4 multi 1.00 import weight 0.00
Epoch 276 Iter 10 subLoss 2259.4 multi 1.00 import weight 0.00
Epoch 276 Iter 11 subLoss 2483.6 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 276 Acc: 98.93 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 248 train Loss: 2105.1 test Loss: 162.8
Epoch 277 Iter 0 subLoss 1720.8 multi 1.00 import weight 0.00
Epoch 277 Iter 1 subLoss 2087.4 multi 1.00 import weight 0.00
Epoch 277 Iter 2 subLoss 2083.4 multi 3.99 import weight 0.00
Epoch 277 Iter 3 subLoss 1610.2 multi 1.00 import weight 0.00
Epoch 277 Iter 4 subLoss 1808.0 multi -1.99 import weight 0.00
Epoch 277 Iter 5 subLoss 1612.7 multi 3.99 import weight 0.00
Epoch 277 Iter 6 subLoss 2018.7 multi 3.99 import weight 0.00
Epoch 277 Iter 7 subLoss 2123.7 multi 9.96 import weight 0.00
Epoch 277 Iter 8 subLoss 2285.5 multi 9.96 import weight 0.00
Epoch 277 Iter 9 subLoss 1925.2 multi -1.99 import weight 0.00
Epoch 277 Iter 10 subLoss 2096.3 multi 6.97 import weight 0.00
Epoch 277 Iter 11 subLoss 1954.4 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 277 Acc: 98.93 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 195 train Loss: 1912.8 test Loss: 166.4
Epoch 278 Iter 0 subLoss 1875.7 multi -1.99 import weight 0.00
Epoch 278 Iter 1 subLoss 1844.7 multi 3.98 import weight 0.00
Epoch 278 Iter 2 subLoss 1805.7 multi 1.00 import weight 0.00
Epoch 278 Iter 3 subLoss 2052.8 multi -10.94 import weight 0.00
Epoch 278 Iter 4 subLoss 1845.4 multi 6.97 import weight 0.00
Epoch 278 Iter 5 subLoss 1937.3 multi -1.99 import weight 0.00
Epoch 278 Iter 6 subLoss 1651.4 multi 1.00 import weight 0.00
Epoch 278 Iter 7 subLoss 1832.3 multi -1.98 import weight 0.00
Epoch 278 Iter 8 subLoss 1887.8 multi 3.98 import weight 0.00
Epoch 278 Iter 9 subLoss 1847.6 multi 6.97 import weight 0.00
Epoch 278 Iter 10 subLoss 2003.9 multi -10.94 import weight 0.00
Epoch 278 Iter 11 subLoss 2246.2 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 278 Acc: 98.87 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 224 train Loss: 1978.2 test Loss: 164.1
Epoch 279 Iter 0 subLoss 1415.1 multi 1.00 import weight 0.00
Epoch 279 Iter 1 subLoss 1755.4 multi 1.00 import weight 0.00
Epoch 279 Iter 2 subLoss 1945.7 multi 3.99 import weight 0.00
Epoch 279 Iter 3 subLoss 2282.4 multi 12.94 import weight 0.00
Epoch 279 Iter 4 subLoss 2002.4 multi -7.96 import weight 0.00
Epoch 279 Iter 5 subLoss 2253.3 multi 1.00 import weight 0.00
Epoch 279 Iter 6 subLoss 2169.9 multi 3.99 import weight 0.00
Epoch 279 Iter 7 subLoss 1759.3 multi 3.99 import weight 0.00
Epoch 279 Iter 8 subLoss 2000.1 multi -4.97 import weight 0.00
Epoch 279 Iter 9 subLoss 2047.8 multi 15.93 import weight 0.00
Epoch 279 Iter 10 subLoss 1993.3 multi -4.97 import weight 0.00
Epoch 279 Iter 11 subLoss 1948.4 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 279 Acc: 98.79 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 194 train Loss: 2029.5 test Loss: 184.2
Epoch 280 Iter 0 subLoss 2015.9 multi -1.99 import weight 0.00
Epoch 280 Iter 1 subLoss 2163.6 multi 6.97 import weight 0.00
Epoch 280 Iter 2 subLoss 1784.4 multi 1.00 import weight 0.00
Epoch 280 Iter 3 subLoss 1875.2 multi 1.00 import weight 0.00
Epoch 280 Iter 4 subLoss 1899.5 multi -1.99 import weight 0.00
Epoch 280 Iter 5 subLoss 1784.5 multi 3.99 import weight 0.00
Epoch 280 Iter 6 subLoss 1954.6 multi 3.99 import weight 0.00
Epoch 280 Iter 7 subLoss 1800.5 multi 3.98 import weight 0.00
Epoch 280 Iter 8 subLoss 2252.3 multi 3.99 import weight 0.00
Epoch 280 Iter 9 subLoss 2101.2 multi -7.96 import weight 0.00
Epoch 280 Iter 10 subLoss 1841.6 multi 9.96 import weight 0.00
Epoch 280 Iter 11 subLoss 2053.4 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 280 Acc: 98.83 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 205 train Loss: 1990.4 test Loss: 177.2
Epoch 281 Iter 0 subLoss 1845.2 multi 12.94 import weight 0.00
Epoch 281 Iter 1 subLoss 2026.1 multi 6.97 import weight 0.00
Epoch 281 Iter 2 subLoss 1990.3 multi -1.99 import weight 0.00
Epoch 281 Iter 3 subLoss 2021.2 multi 9.96 import weight 0.00
Epoch 281 Iter 4 subLoss 1589.3 multi 1.00 import weight 0.00
Epoch 281 Iter 5 subLoss 1946.6 multi 9.96 import weight 0.00
Epoch 281 Iter 6 subLoss 1996.5 multi 1.00 import weight 0.00
Epoch 281 Iter 7 subLoss 2288.3 multi 15.93 import weight 0.00
Epoch 281 Iter 8 subLoss 2090.6 multi 9.96 import weight 0.00
Epoch 281 Iter 9 subLoss 1723.7 multi 3.99 import weight 0.00
Epoch 281 Iter 10 subLoss 1656.8 multi 3.99 import weight 0.00
Epoch 281 Iter 11 subLoss 1737.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 281 Acc: 98.95 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 173 train Loss: 1877.2 test Loss: 180.2
Epoch 282 Iter 0 subLoss 1658.1 multi 6.97 import weight 0.00
Epoch 282 Iter 1 subLoss 1700.3 multi -1.99 import weight 0.00
Epoch 282 Iter 2 subLoss 1705.6 multi 1.00 import weight 0.00
Epoch 282 Iter 3 subLoss 1940.8 multi 12.94 import weight 0.00
Epoch 282 Iter 4 subLoss 1700.2 multi 3.98 import weight 0.00
Epoch 282 Iter 5 subLoss 1815.8 multi -4.97 import weight 0.00
Epoch 282 Iter 6 subLoss 1701.4 multi 6.97 import weight 0.00
Epoch 282 Iter 7 subLoss 1695.2 multi 3.99 import weight 0.00
Epoch 282 Iter 8 subLoss 2227.3 multi -10.94 import weight 0.00
Epoch 282 Iter 9 subLoss 1767.3 multi -4.97 import weight 0.00
Epoch 282 Iter 10 subLoss 1860.6 multi 3.99 import weight 0.00
Epoch 282 Iter 11 subLoss 1685.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 282 Acc: 98.87 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 168 train Loss: 1901.3 test Loss: 180.0
Epoch 283 Iter 0 subLoss 1766.3 multi -1.98 import weight 0.00
Epoch 283 Iter 1 subLoss 1870.1 multi 1.00 import weight 0.00
Epoch 283 Iter 2 subLoss 2202.3 multi 12.94 import weight 0.00
Epoch 283 Iter 3 subLoss 1565.0 multi 1.00 import weight 0.00
Epoch 283 Iter 4 subLoss 2265.1 multi -7.96 import weight 0.00
Epoch 283 Iter 5 subLoss 1507.3 multi 1.00 import weight 0.00
Epoch 283 Iter 6 subLoss 2062.9 multi -7.96 import weight 0.00
Epoch 283 Iter 7 subLoss 2350.6 multi 12.94 import weight 0.00
Epoch 283 Iter 8 subLoss 1478.3 multi 1.00 import weight 0.00
Epoch 283 Iter 9 subLoss 1973.0 multi 12.94 import weight 0.00
Epoch 283 Iter 10 subLoss 1533.6 multi 3.99 import weight 0.00
Epoch 283 Iter 11 subLoss 1725.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 283 Acc: 99.01 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 172 train Loss: 1821.3 test Loss: 160.7
Epoch 284 Iter 0 subLoss 1676.2 multi -1.99 import weight 0.00
Epoch 284 Iter 1 subLoss 2083.5 multi 6.97 import weight 0.00
Epoch 284 Iter 2 subLoss 1626.9 multi -4.97 import weight 0.00
Epoch 284 Iter 3 subLoss 2065.1 multi -4.97 import weight 0.00
Epoch 284 Iter 4 subLoss 1920.0 multi 1.00 import weight 0.00
Epoch 284 Iter 5 subLoss 1758.5 multi 6.97 import weight 0.00
Epoch 284 Iter 6 subLoss 2150.6 multi 3.98 import weight 0.00
Epoch 284 Iter 7 subLoss 1478.7 multi 3.99 import weight 0.00
Epoch 284 Iter 8 subLoss 1821.7 multi 1.00 import weight 0.00
Epoch 284 Iter 9 subLoss 1804.7 multi 6.97 import weight 0.00
Epoch 284 Iter 10 subLoss 1724.0 multi 9.96 import weight 0.00
Epoch 284 Iter 11 subLoss 1978.1 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 284 Acc: 98.97 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 197 train Loss: 1850.6 test Loss: 163.6
Epoch 285 Iter 0 subLoss 1323.9 multi 1.00 import weight 0.00
Epoch 285 Iter 1 subLoss 1729.6 multi 12.94 import weight 0.00
Epoch 285 Iter 2 subLoss 1733.3 multi -10.94 import weight 0.00
Epoch 285 Iter 3 subLoss 1805.7 multi 9.96 import weight 0.00
Epoch 285 Iter 4 subLoss 1972.7 multi 18.91 import weight 0.00
Epoch 285 Iter 5 subLoss 1777.9 multi -4.97 import weight 0.00
Epoch 285 Iter 6 subLoss 2160.9 multi 6.97 import weight 0.00
Epoch 285 Iter 7 subLoss 1506.4 multi 3.99 import weight 0.00
Epoch 285 Iter 8 subLoss 1605.3 multi 1.00 import weight 0.00
Epoch 285 Iter 9 subLoss 1341.8 multi 1.00 import weight 0.00
Epoch 285 Iter 10 subLoss 1649.7 multi 1.00 import weight 0.00
Epoch 285 Iter 11 subLoss 1843.3 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 285 Acc: 99.03 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 184 train Loss: 1725.7 test Loss: 163.1
Epoch 286 Iter 0 subLoss 1661.4 multi -4.97 import weight 0.00
Epoch 286 Iter 1 subLoss 1883.6 multi 1.00 import weight 0.00
Epoch 286 Iter 2 subLoss 1848.0 multi 18.91 import weight 0.00
Epoch 286 Iter 3 subLoss 1740.0 multi -7.96 import weight 0.00
Epoch 286 Iter 4 subLoss 1832.1 multi -1.99 import weight 0.00
Epoch 286 Iter 5 subLoss 2416.4 multi 1.00 import weight 0.00
Epoch 286 Iter 6 subLoss 1791.2 multi -1.98 import weight 0.00
Epoch 286 Iter 7 subLoss 2039.1 multi -10.94 import weight 0.00
Epoch 286 Iter 8 subLoss 3965.1 multi 6.97 import weight 0.00
Epoch 286 Iter 9 subLoss 2025.0 multi 12.94 import weight 0.00
Epoch 286 Iter 10 subLoss 1890.1 multi -1.99 import weight 0.00
Epoch 286 Iter 11 subLoss 1454.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 286 Acc: 98.89 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 145 train Loss: 1804.0 test Loss: 171.9
Epoch 287 Iter 0 subLoss 1947.8 multi 15.93 import weight 0.00
Epoch 287 Iter 1 subLoss 1791.9 multi 1.00 import weight 0.00
Epoch 287 Iter 2 subLoss 1683.4 multi 1.00 import weight 0.00
Epoch 287 Iter 3 subLoss 1326.7 multi 3.99 import weight 0.00
Epoch 287 Iter 4 subLoss 1824.8 multi 3.99 import weight 0.00
Epoch 287 Iter 5 subLoss 1917.3 multi 6.97 import weight 0.00
Epoch 287 Iter 6 subLoss 1447.8 multi 1.00 import weight 0.00
Epoch 287 Iter 7 subLoss 1543.0 multi -4.97 import weight 0.00
Epoch 287 Iter 8 subLoss 1303.3 multi 1.00 import weight 0.00
Epoch 287 Iter 9 subLoss 1697.2 multi 1.00 import weight 0.00
Epoch 287 Iter 10 subLoss 1870.0 multi 3.99 import weight 0.00
Epoch 287 Iter 11 subLoss 1975.0 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 287 Acc: 98.91 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 197 train Loss: 1780.1 test Loss: 176.2
Epoch 288 Iter 0 subLoss 1925.8 multi 1.00 import weight 0.00
Epoch 288 Iter 1 subLoss 1698.9 multi 3.99 import weight 0.00
Epoch 288 Iter 2 subLoss 1438.2 multi 1.00 import weight 0.00
Epoch 288 Iter 3 subLoss 1447.8 multi 1.00 import weight 0.00
Epoch 288 Iter 4 subLoss 1902.3 multi -7.96 import weight 0.00
Epoch 288 Iter 5 subLoss 1704.7 multi 1.00 import weight 0.00
Epoch 288 Iter 6 subLoss 1938.8 multi -4.97 import weight 0.00
Epoch 288 Iter 7 subLoss 1872.3 multi 6.97 import weight 0.00
Epoch 288 Iter 8 subLoss 1750.8 multi 9.96 import weight 0.00
Epoch 288 Iter 9 subLoss 1436.9 multi 3.99 import weight 0.00
Epoch 288 Iter 10 subLoss 1912.8 multi 6.97 import weight 0.00
Epoch 288 Iter 11 subLoss 1492.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 288 Acc: 98.91 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 149 train Loss: 1621.7 test Loss: 167.0
Epoch 289 Iter 0 subLoss 1772.3 multi -1.98 import weight 0.00
Epoch 289 Iter 1 subLoss 1463.3 multi -1.99 import weight 0.00
Epoch 289 Iter 2 subLoss 1513.1 multi -4.97 import weight 0.00
Epoch 289 Iter 3 subLoss 1490.1 multi 3.99 import weight 0.00
Epoch 289 Iter 4 subLoss 1503.1 multi 1.00 import weight 0.00
Epoch 289 Iter 5 subLoss 1736.8 multi -4.97 import weight 0.00
Epoch 289 Iter 6 subLoss 1256.1 multi 1.00 import weight 0.00
Epoch 289 Iter 7 subLoss 1449.2 multi 1.00 import weight 0.00
Epoch 289 Iter 8 subLoss 2056.6 multi -7.96 import weight 0.00
Epoch 289 Iter 9 subLoss 2036.4 multi -10.94 import weight 0.00
Epoch 289 Iter 10 subLoss 1853.3 multi -25.87 import weight 0.00
Epoch 289 Iter 11 subLoss 3165.4 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 289 Acc: 97.61 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 316 train Loss: 3969.2 test Loss: 424.5
Epoch 290 Iter 0 subLoss 4863.0 multi -1.98 import weight 0.00
Epoch 290 Iter 1 subLoss 4470.9 multi 1.00 import weight 0.00
Epoch 290 Iter 2 subLoss 4100.6 multi -4.97 import weight 0.00
Epoch 290 Iter 3 subLoss 10273.0 multi 1.00 import weight 0.00
Epoch 290 Iter 4 subLoss 6249.6 multi 6.97 import weight 0.00
Epoch 290 Iter 5 subLoss 2149.2 multi -1.98 import weight 0.00
Epoch 290 Iter 6 subLoss 2444.6 multi -22.88 import weight 0.00
Epoch 290 Iter 7 subLoss 27230.9 multi 1.00 import weight 0.00
Epoch 290 Iter 8 subLoss 6392.4 multi 1.00 import weight 0.00
Epoch 290 Iter 9 subLoss 4097.1 multi 9.96 import weight 0.00
Epoch 290 Iter 10 subLoss 1998.7 multi 3.99 import weight 0.00
Epoch 290 Iter 11 subLoss 2220.8 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 290 Acc: 98.40 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 222 train Loss: 2541.4 test Loss: 283.2
Epoch 291 Iter 0 subLoss 2043.2 multi 12.94 import weight 0.00
Epoch 291 Iter 1 subLoss 1794.7 multi 3.99 import weight 0.00
Epoch 291 Iter 2 subLoss 1880.8 multi -1.98 import weight 0.00
Epoch 291 Iter 3 subLoss 2050.8 multi -7.96 import weight 0.00
Epoch 291 Iter 4 subLoss 2337.7 multi 15.93 import weight 0.00
Epoch 291 Iter 5 subLoss 2280.4 multi 18.91 import weight 0.00
Epoch 291 Iter 6 subLoss 2142.2 multi 1.00 import weight 0.00
Epoch 291 Iter 7 subLoss 1600.7 multi 3.99 import weight 0.00
Epoch 291 Iter 8 subLoss 1813.6 multi -7.96 import weight 0.00
Epoch 291 Iter 9 subLoss 1968.8 multi -19.90 import weight 0.00
Epoch 291 Iter 10 subLoss 2420.1 multi 1.00 import weight 0.00
Epoch 291 Iter 11 subLoss 2114.6 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 291 Acc: 98.11 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 211 train Loss: 2452.9 test Loss: 323.1
Epoch 292 Iter 0 subLoss 2089.6 multi 9.96 import weight 0.00
Epoch 292 Iter 1 subLoss 1965.4 multi -16.91 import weight 0.00
Epoch 292 Iter 2 subLoss 2640.8 multi -25.87 import weight 0.00
Epoch 292 Iter 3 subLoss 19302.4 multi 3.99 import weight 0.00
Epoch 292 Iter 4 subLoss 5874.2 multi 1.00 import weight 0.00
Epoch 292 Iter 5 subLoss 4824.9 multi -4.97 import weight 0.00
Epoch 292 Iter 6 subLoss 13997.7 multi 9.96 import weight 0.00
Epoch 292 Iter 7 subLoss 18199.7 multi 1.00 import weight 0.00
Epoch 292 Iter 8 subLoss 11573.1 multi 1.00 import weight 0.00
Epoch 292 Iter 9 subLoss 9629.1 multi 6.97 import weight 0.00
Epoch 292 Iter 10 subLoss 4259.5 multi 3.99 import weight 0.00
Epoch 292 Iter 11 subLoss 4079.9 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 292 Acc: 98.23 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 407 train Loss: 5926.1 test Loss: 315.3
Epoch 293 Iter 0 subLoss 5526.3 multi 6.97 import weight 0.00
Epoch 293 Iter 1 subLoss 3954.1 multi -10.94 import weight 0.00
Epoch 293 Iter 2 subLoss 7311.8 multi 1.00 import weight 0.00
Epoch 293 Iter 3 subLoss 6983.7 multi 3.99 import weight 0.00
Epoch 293 Iter 4 subLoss 4919.5 multi -1.99 import weight 0.00
Epoch 293 Iter 5 subLoss 5489.5 multi 1.00 import weight 0.00
Epoch 293 Iter 6 subLoss 4651.0 multi 9.96 import weight 0.00
Epoch 293 Iter 7 subLoss 4214.1 multi 12.94 import weight 0.00
Epoch 293 Iter 8 subLoss 2548.5 multi -1.99 import weight 0.00
Epoch 293 Iter 9 subLoss 2796.3 multi 3.99 import weight 0.00
Epoch 293 Iter 10 subLoss 3260.9 multi -1.99 import weight 0.00
Epoch 293 Iter 11 subLoss 3267.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 293 Acc: 98.89 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 326 train Loss: 2959.3 test Loss: 192.6
Epoch 294 Iter 0 subLoss 2685.4 multi -4.97 import weight 0.00
Epoch 294 Iter 1 subLoss 3008.4 multi 3.99 import weight 0.00
Epoch 294 Iter 2 subLoss 2913.3 multi 1.00 import weight 0.00
Epoch 294 Iter 3 subLoss 2608.1 multi -13.93 import weight 0.00
Epoch 294 Iter 4 subLoss 3639.1 multi -7.96 import weight 0.00
Epoch 294 Iter 5 subLoss 4877.0 multi 3.99 import weight 0.00
Epoch 294 Iter 6 subLoss 3476.8 multi -7.96 import weight 0.00
Epoch 294 Iter 7 subLoss 4692.4 multi 6.97 import weight 0.00
Epoch 294 Iter 8 subLoss 3415.7 multi -1.99 import weight 0.00
Epoch 294 Iter 9 subLoss 3879.7 multi -1.99 import weight 0.00
Epoch 294 Iter 10 subLoss 3265.5 multi 3.99 import weight 0.00
Epoch 294 Iter 11 subLoss 3288.6 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 294 Acc: 98.50 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 328 train Loss: 4135.0 test Loss: 248.9
Epoch 295 Iter 0 subLoss 4386.3 multi 3.99 import weight 0.00
Epoch 295 Iter 1 subLoss 3331.6 multi -19.90 import weight 0.00
Epoch 295 Iter 2 subLoss 4979.9 multi -4.97 import weight 0.00
Epoch 295 Iter 3 subLoss 9285.3 multi -4.97 import weight 0.00
Epoch 295 Iter 4 subLoss 56877.6 multi 1.00 import weight 0.00
Epoch 295 Iter 5 subLoss 7430.7 multi 9.96 import weight 0.00
Epoch 295 Iter 6 subLoss 5216.5 multi -4.97 import weight 0.00
Epoch 295 Iter 7 subLoss 5170.1 multi 1.00 import weight 0.00
Epoch 295 Iter 8 subLoss 5548.2 multi 3.99 import weight 0.00
Epoch 295 Iter 9 subLoss 4113.5 multi -7.96 import weight 0.00
Epoch 295 Iter 10 subLoss 4953.1 multi -7.96 import weight 0.00
Epoch 295 Iter 11 subLoss 8162.9 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 295 Acc: 95.17 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 816 train Loss: 11628.2 test Loss: 860.8
Epoch 296 Iter 0 subLoss 11895.1 multi 3.99 import weight 0.00
Epoch 296 Iter 1 subLoss 5760.9 multi 9.96 import weight 0.00
Epoch 296 Iter 2 subLoss 4412.9 multi 12.94 import weight 0.00
Epoch 296 Iter 3 subLoss 3815.8 multi 9.96 import weight 0.00
Epoch 296 Iter 4 subLoss 3054.1 multi 12.94 import weight 0.00
Epoch 296 Iter 5 subLoss 3091.3 multi 6.97 import weight 0.00
Epoch 296 Iter 6 subLoss 2383.2 multi 12.94 import weight 0.00
Epoch 296 Iter 7 subLoss 2581.6 multi -7.96 import weight 0.00
Epoch 296 Iter 8 subLoss 2501.8 multi -16.91 import weight 0.00
Epoch 296 Iter 9 subLoss 3171.0 multi 30.85 import weight 0.00
Epoch 296 Iter 10 subLoss 13855.8 multi 3.99 import weight 0.00
Epoch 296 Iter 11 subLoss 3662.7 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 296 Acc: 98.74 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 366 train Loss: 2623.5 test Loss: 227.0
Epoch 297 Iter 0 subLoss 2314.7 multi 1.00 import weight 0.00
Epoch 297 Iter 1 subLoss 2525.1 multi 3.99 import weight 0.00
Epoch 297 Iter 2 subLoss 2362.4 multi -7.96 import weight 0.00
Epoch 297 Iter 3 subLoss 2393.3 multi 3.98 import weight 0.00
Epoch 297 Iter 4 subLoss 2446.7 multi -19.90 import weight 0.00
Epoch 297 Iter 5 subLoss 4099.1 multi 12.94 import weight 0.00
Epoch 297 Iter 6 subLoss 3068.5 multi -10.94 import weight 0.00
Epoch 297 Iter 7 subLoss 10641.3 multi 1.00 import weight 0.00
Epoch 297 Iter 8 subLoss 5335.6 multi -1.99 import weight 0.00
Epoch 297 Iter 9 subLoss 9715.9 multi -7.96 import weight 0.00
Epoch 297 Iter 10 subLoss 280104.0 multi 1.00 import weight 0.00
Epoch 297 Iter 11 subLoss 40810.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 297 Acc: 70.17 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4081 train Loss: 20550.2 test Loss: 3660.9
Epoch 298 Iter 0 subLoss 19449.2 multi -1.99 import weight 0.00
Epoch 298 Iter 1 subLoss 29810.1 multi 1.00 import weight 0.00
Epoch 298 Iter 2 subLoss 21604.0 multi -1.99 import weight 0.00
Epoch 298 Iter 3 subLoss 29862.6 multi 1.00 import weight 0.00
Epoch 298 Iter 4 subLoss 22792.7 multi -1.99 import weight 0.00
Epoch 298 Iter 5 subLoss 28830.1 multi 1.00 import weight 0.00
Epoch 298 Iter 6 subLoss 24617.7 multi 1.00 import weight 0.00
Epoch 298 Iter 7 subLoss 22110.3 multi 1.00 import weight 0.00
Epoch 298 Iter 8 subLoss 20401.3 multi 1.00 import weight 0.00
Epoch 298 Iter 9 subLoss 18773.8 multi 3.99 import weight 0.00
Epoch 298 Iter 10 subLoss 13680.2 multi -4.97 import weight 0.00
Epoch 298 Iter 11 subLoss 19798.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 298 Acc: 76.53 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1979 train Loss: 16592.6 test Loss: 2654.7
Epoch 299 Iter 0 subLoss 15712.7 multi 1.00 import weight 0.00
Epoch 299 Iter 1 subLoss 15451.3 multi 1.00 import weight 0.00
Epoch 299 Iter 2 subLoss 13383.6 multi 3.99 import weight 0.00
Epoch 299 Iter 3 subLoss 9323.2 multi 1.00 import weight 0.00
Epoch 299 Iter 4 subLoss 8496.5 multi 1.00 import weight 0.00
Epoch 299 Iter 5 subLoss 7801.0 multi -1.99 import weight 0.00
Epoch 299 Iter 6 subLoss 9813.1 multi -1.99 import weight 0.00
Epoch 299 Iter 7 subLoss 11960.1 multi -7.96 import weight 0.00
Epoch 299 Iter 8 subLoss 20368.4 multi -1.99 import weight 0.00
Epoch 299 Iter 9 subLoss 24296.1 multi 1.00 import weight 0.00
Epoch 299 Iter 10 subLoss 21385.5 multi 1.00 import weight 0.00
Epoch 299 Iter 11 subLoss 20418.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 299 Acc: 70.52 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 2041 train Loss: 22504.3 test Loss: 3912.7
Epoch 300 Iter 0 subLoss 22361.2 multi 1.00 import weight 0.00
Epoch 300 Iter 1 subLoss 20517.9 multi 1.00 import weight 0.00
Epoch 300 Iter 2 subLoss 20195.2 multi 1.00 import weight 0.00
Epoch 300 Iter 3 subLoss 18683.9 multi -4.97 import weight 0.00
Epoch 300 Iter 4 subLoss 22205.6 multi 1.00 import weight 0.00
Epoch 300 Iter 5 subLoss 21019.6 multi 3.99 import weight 0.00
Epoch 300 Iter 6 subLoss 18430.4 multi 1.00 import weight 0.00
Epoch 300 Iter 7 subLoss 18471.3 multi 3.99 import weight 0.00
Epoch 300 Iter 8 subLoss 16906.5 multi -4.97 import weight 0.00
Epoch 300 Iter 9 subLoss 19055.8 multi 1.00 import weight 0.00
Epoch 300 Iter 10 subLoss 18921.9 multi 1.00 import weight 0.00
Epoch 300 Iter 11 subLoss 18224.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 300 Acc: 74.31 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1822 train Loss: 17981.8 test Loss: 2858.8
Epoch 301 Iter 0 subLoss 17037.9 multi 1.00 import weight 0.00
Epoch 301 Iter 1 subLoss 16528.0 multi 1.00 import weight 0.00
Epoch 301 Iter 2 subLoss 16413.7 multi 1.00 import weight 0.00
Epoch 301 Iter 3 subLoss 15992.3 multi -1.99 import weight 0.00
Epoch 301 Iter 4 subLoss 16694.3 multi 1.00 import weight 0.00
Epoch 301 Iter 5 subLoss 16221.0 multi 3.99 import weight 0.00
Epoch 301 Iter 6 subLoss 14124.8 multi 1.00 import weight 0.00
Epoch 301 Iter 7 subLoss 13825.2 multi 1.00 import weight 0.00
Epoch 301 Iter 8 subLoss 13438.5 multi -1.98 import weight 0.00
Epoch 301 Iter 9 subLoss 14617.8 multi -1.99 import weight 0.00
Epoch 301 Iter 10 subLoss 16479.9 multi -4.97 import weight 0.00
Epoch 301 Iter 11 subLoss 18168.8 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 301 Acc: 74.24 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1816 train Loss: 19802.2 test Loss: 3036.6
Epoch 302 Iter 0 subLoss 19842.5 multi 1.00 import weight 0.00
Epoch 302 Iter 1 subLoss 18632.9 multi 3.99 import weight 0.00
Epoch 302 Iter 2 subLoss 17484.2 multi 1.00 import weight 0.00
Epoch 302 Iter 3 subLoss 16091.5 multi 1.00 import weight 0.00
Epoch 302 Iter 4 subLoss 16048.5 multi -4.97 import weight 0.00
Epoch 302 Iter 5 subLoss 17834.6 multi 3.99 import weight 0.00
Epoch 302 Iter 6 subLoss 16009.1 multi -1.99 import weight 0.00
Epoch 302 Iter 7 subLoss 17273.2 multi -4.97 import weight 0.00
Epoch 302 Iter 8 subLoss 20934.5 multi -1.99 import weight 0.00
Epoch 302 Iter 9 subLoss 24608.9 multi 1.00 import weight 0.00
Epoch 302 Iter 10 subLoss 20745.9 multi -1.99 import weight 0.00
Epoch 302 Iter 11 subLoss 26178.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 302 Acc: 69.39 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2617 train Loss: 22609.3 test Loss: 3876.5
Epoch 303 Iter 0 subLoss 22891.7 multi 1.00 import weight 0.00
Epoch 303 Iter 1 subLoss 19915.3 multi 1.00 import weight 0.00
Epoch 303 Iter 2 subLoss 18656.4 multi -1.99 import weight 0.00
Epoch 303 Iter 3 subLoss 20882.2 multi -1.99 import weight 0.00
Epoch 303 Iter 4 subLoss 23305.6 multi 1.00 import weight 0.00
Epoch 303 Iter 5 subLoss 21243.3 multi 1.00 import weight 0.00
Epoch 303 Iter 6 subLoss 21023.1 multi -4.97 import weight 0.00
Epoch 303 Iter 7 subLoss 25438.1 multi 1.00 import weight 0.00
Epoch 303 Iter 8 subLoss 22111.3 multi 3.99 import weight 0.00
Epoch 303 Iter 9 subLoss 21090.2 multi -1.99 import weight 0.00
Epoch 303 Iter 10 subLoss 21517.2 multi 3.99 import weight 0.00
Epoch 303 Iter 11 subLoss 19678.3 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 303 Acc: 74.47 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1967 train Loss: 17932.1 test Loss: 2851.3
Epoch 304 Iter 0 subLoss 17664.5 multi 1.00 import weight 0.00
Epoch 304 Iter 1 subLoss 16861.0 multi -1.99 import weight 0.00
Epoch 304 Iter 2 subLoss 18754.9 multi 1.00 import weight 0.00
Epoch 304 Iter 3 subLoss 17901.7 multi 3.99 import weight 0.00
Epoch 304 Iter 4 subLoss 16046.0 multi -1.98 import weight 0.00
Epoch 304 Iter 5 subLoss 16852.3 multi 3.99 import weight 0.00
Epoch 304 Iter 6 subLoss 15299.5 multi -1.99 import weight 0.00
Epoch 304 Iter 7 subLoss 16995.3 multi 1.00 import weight 0.00
Epoch 304 Iter 8 subLoss 15372.4 multi -1.99 import weight 0.00
Epoch 304 Iter 9 subLoss 15857.4 multi 3.99 import weight 0.00
Epoch 304 Iter 10 subLoss 14172.1 multi -4.97 import weight 0.00
Epoch 304 Iter 11 subLoss 17400.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 304 Acc: 74.86 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1740 train Loss: 17055.6 test Loss: 2683.7
Epoch 305 Iter 0 subLoss 17027.6 multi 1.00 import weight 0.00
Epoch 305 Iter 1 subLoss 16374.5 multi 1.00 import weight 0.00
Epoch 305 Iter 2 subLoss 15945.0 multi 3.99 import weight 0.00
Epoch 305 Iter 3 subLoss 13753.9 multi 3.99 import weight 0.00
Epoch 305 Iter 4 subLoss 11808.8 multi 6.97 import weight 0.00
Epoch 305 Iter 5 subLoss 9407.2 multi 3.98 import weight 0.00
Epoch 305 Iter 6 subLoss 7521.4 multi -1.99 import weight 0.00
Epoch 305 Iter 7 subLoss 8867.1 multi 1.00 import weight 0.00
Epoch 305 Iter 8 subLoss 8170.5 multi -4.97 import weight 0.00
Epoch 305 Iter 9 subLoss 9958.3 multi 1.00 import weight 0.00
Epoch 305 Iter 10 subLoss 10166.9 multi 3.99 import weight 0.00
Epoch 305 Iter 11 subLoss 7285.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 305 Acc: 93.35 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 728 train Loss: 7181.7 test Loss: 1211.3
Epoch 306 Iter 0 subLoss 7210.6 multi 9.96 import weight 0.00
Epoch 306 Iter 1 subLoss 4456.8 multi -1.98 import weight 0.00
Epoch 306 Iter 2 subLoss 4350.2 multi -1.98 import weight 0.00
Epoch 306 Iter 3 subLoss 4699.7 multi 9.96 import weight 0.00
Epoch 306 Iter 4 subLoss 3281.9 multi -4.97 import weight 0.00
Epoch 306 Iter 5 subLoss 3939.5 multi -4.97 import weight 0.00
Epoch 306 Iter 6 subLoss 5863.9 multi -7.96 import weight 0.00
Epoch 306 Iter 7 subLoss 24880.8 multi 1.00 import weight 0.00
Epoch 306 Iter 8 subLoss 6999.6 multi -10.94 import weight 0.00
Epoch 306 Iter 9 subLoss 55411.1 multi 1.00 import weight 0.00
Epoch 306 Iter 10 subLoss 9907.4 multi -4.97 import weight 0.00
Epoch 306 Iter 11 subLoss 19589.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 306 Acc: 73.67 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1958 train Loss: 15271.5 test Loss: 3431.1
Epoch 307 Iter 0 subLoss 14824.6 multi 1.00 import weight 0.00
Epoch 307 Iter 1 subLoss 11763.4 multi 3.99 import weight 0.00
Epoch 307 Iter 2 subLoss 7515.2 multi 1.00 import weight 0.00
Epoch 307 Iter 3 subLoss 6958.6 multi 1.00 import weight 0.00
Epoch 307 Iter 4 subLoss 6367.8 multi 1.00 import weight 0.00
Epoch 307 Iter 5 subLoss 5868.5 multi -4.97 import weight 0.00
Epoch 307 Iter 6 subLoss 8160.0 multi 3.98 import weight 0.00
Epoch 307 Iter 7 subLoss 6599.6 multi 6.97 import weight 0.00
Epoch 307 Iter 8 subLoss 4751.0 multi 6.97 import weight 0.00
Epoch 307 Iter 9 subLoss 4365.7 multi 3.99 import weight 0.00
Epoch 307 Iter 10 subLoss 3943.8 multi 9.96 import weight 0.00
Epoch 307 Iter 11 subLoss 3457.9 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 307 Acc: 98.35 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 345 train Loss: 2869.4 test Loss: 316.9
Epoch 308 Iter 0 subLoss 2668.4 multi -4.97 import weight 0.00
Epoch 308 Iter 1 subLoss 3017.7 multi 15.93 import weight 0.00
Epoch 308 Iter 2 subLoss 2517.2 multi -4.97 import weight 0.00
Epoch 308 Iter 3 subLoss 3453.2 multi 15.93 import weight 0.00
Epoch 308 Iter 4 subLoss 2876.2 multi 1.00 import weight 0.00
Epoch 308 Iter 5 subLoss 2678.0 multi 15.93 import weight 0.00
Epoch 308 Iter 6 subLoss 2625.0 multi 1.00 import weight 0.00
Epoch 308 Iter 7 subLoss 2325.1 multi -7.96 import weight 0.00
Epoch 308 Iter 8 subLoss 2987.4 multi 3.98 import weight 0.00
Epoch 308 Iter 9 subLoss 2681.6 multi -4.97 import weight 0.00
Epoch 308 Iter 10 subLoss 3019.2 multi 18.91 import weight 1.00
Epoch 308 Iter 11 subLoss 2821.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 308 Acc: 98.50 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 282 train Loss: 2673.5 test Loss: 249.3
Epoch 309 Iter 0 subLoss 2914.7 multi 3.99 import weight 0.00
Epoch 309 Iter 1 subLoss 2305.8 multi 3.99 import weight 0.00
Epoch 309 Iter 2 subLoss 2464.3 multi 3.98 import weight 0.00
Epoch 309 Iter 3 subLoss 2579.5 multi 12.94 import weight 0.00
Epoch 309 Iter 4 subLoss 2290.5 multi -25.87 import weight 0.00
Epoch 309 Iter 5 subLoss 2148.8 multi 3.99 import weight 0.00
Epoch 309 Iter 6 subLoss 2513.7 multi -1.98 import weight 0.00
Epoch 309 Iter 7 subLoss 2686.4 multi -1.99 import weight 0.00
Epoch 309 Iter 8 subLoss 2505.9 multi -13.93 import weight 0.00
Epoch 309 Iter 9 subLoss 4111.5 multi -4.97 import weight 0.00
Epoch 309 Iter 10 subLoss 10853.5 multi 3.99 import weight 0.00
Epoch 309 Iter 11 subLoss 2931.4 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 309 Acc: 98.56 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 293 train Loss: 2607.4 test Loss: 280.5
Epoch 310 Iter 0 subLoss 2629.5 multi 3.99 import weight 0.00
Epoch 310 Iter 1 subLoss 2807.0 multi 9.96 import weight 0.00
Epoch 310 Iter 2 subLoss 2532.1 multi 6.97 import weight 0.00
Epoch 310 Iter 3 subLoss 2452.8 multi -1.98 import weight 0.00
Epoch 310 Iter 4 subLoss 2429.9 multi 3.98 import weight 0.00
Epoch 310 Iter 5 subLoss 2385.1 multi 15.93 import weight 0.00
Epoch 310 Iter 6 subLoss 2185.7 multi 24.88 import weight 0.00
Epoch 310 Iter 7 subLoss 2272.8 multi -1.99 import weight 0.00
Epoch 310 Iter 8 subLoss 2693.0 multi -10.94 import weight 0.00
Epoch 310 Iter 9 subLoss 5568.7 multi 1.00 import weight 0.00
Epoch 310 Iter 10 subLoss 4426.8 multi -16.91 import weight 0.00
Epoch 310 Iter 11 subLoss 43776.4 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 310 Acc: 27.46 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 4377 train Loss: 833642.2 test Loss: 152982.8
Epoch 311 Iter 0 subLoss 823129.4 multi 1.00 import weight 0.00
Epoch 311 Iter 1 subLoss 133847.7 multi 1.00 import weight 0.00
Epoch 311 Iter 2 subLoss 42310.3 multi 1.00 import weight 0.00
Epoch 311 Iter 3 subLoss 39178.2 multi 1.00 import weight 0.00
Epoch 311 Iter 4 subLoss 37463.5 multi 1.00 import weight 0.00
Epoch 311 Iter 5 subLoss 34584.6 multi 1.00 import weight 0.00
Epoch 311 Iter 6 subLoss 33004.6 multi 3.99 import weight 0.00
Epoch 311 Iter 7 subLoss 28957.2 multi 1.00 import weight 0.00
Epoch 311 Iter 8 subLoss 27720.4 multi 1.00 import weight 0.00
Epoch 311 Iter 9 subLoss 27431.7 multi 1.00 import weight 0.00
Epoch 311 Iter 10 subLoss 26376.4 multi 1.00 import weight 0.00
Epoch 311 Iter 11 subLoss 25307.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 311 Acc: 74.45 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2530 train Loss: 24739.4 test Loss: 3707.2
Epoch 312 Iter 0 subLoss 23906.6 multi 3.99 import weight 0.00
Epoch 312 Iter 1 subLoss 21195.4 multi 3.99 import weight 0.00
Epoch 312 Iter 2 subLoss 17963.9 multi 1.00 import weight 0.00
Epoch 312 Iter 3 subLoss 17330.0 multi -1.99 import weight 0.00
Epoch 312 Iter 4 subLoss 19196.8 multi -1.99 import weight 0.00
Epoch 312 Iter 5 subLoss 21132.1 multi 1.00 import weight 0.00
Epoch 312 Iter 6 subLoss 20365.6 multi 1.00 import weight 0.00
Epoch 312 Iter 7 subLoss 19082.1 multi 1.00 import weight 0.00
Epoch 312 Iter 8 subLoss 19130.0 multi 1.00 import weight 0.00
Epoch 312 Iter 9 subLoss 16691.1 multi 3.99 import weight 0.00
Epoch 312 Iter 10 subLoss 16209.4 multi 1.00 import weight 0.00
Epoch 312 Iter 11 subLoss 15664.0 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 312 Acc: 78.36 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1566 train Loss: 16603.2 test Loss: 2535.1
Epoch 313 Iter 0 subLoss 16924.6 multi -1.99 import weight 0.00
Epoch 313 Iter 1 subLoss 17933.8 multi 1.00 import weight 0.00
Epoch 313 Iter 2 subLoss 17017.7 multi 1.00 import weight 0.00
Epoch 313 Iter 3 subLoss 15875.1 multi -1.98 import weight 0.00
Epoch 313 Iter 4 subLoss 16749.9 multi 6.97 import weight 0.00
Epoch 313 Iter 5 subLoss 14744.2 multi 1.00 import weight 0.00
Epoch 313 Iter 6 subLoss 13084.6 multi 1.00 import weight 0.00
Epoch 313 Iter 7 subLoss 12368.3 multi 1.00 import weight 0.00
Epoch 313 Iter 8 subLoss 13022.0 multi -1.99 import weight 0.00
Epoch 313 Iter 9 subLoss 13881.3 multi 3.99 import weight 0.00
Epoch 313 Iter 10 subLoss 12107.7 multi 1.00 import weight 0.00
Epoch 313 Iter 11 subLoss 11605.2 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 313 Acc: 91.50 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 1160 train Loss: 9211.1 test Loss: 1277.0
Epoch 314 Iter 0 subLoss 9202.0 multi -1.99 import weight 0.00
Epoch 314 Iter 1 subLoss 10269.0 multi 1.00 import weight 0.00
Epoch 314 Iter 2 subLoss 9965.1 multi -1.99 import weight 0.00
Epoch 314 Iter 3 subLoss 10752.2 multi 3.98 import weight 0.00
Epoch 314 Iter 4 subLoss 8026.7 multi -4.97 import weight 0.00
Epoch 314 Iter 5 subLoss 10636.5 multi -1.99 import weight 0.00
Epoch 314 Iter 6 subLoss 12056.2 multi 1.00 import weight 0.00
Epoch 314 Iter 7 subLoss 10864.9 multi -1.98 import weight 0.00
Epoch 314 Iter 8 subLoss 12306.3 multi 3.99 import weight 0.00
Epoch 314 Iter 9 subLoss 10655.7 multi 1.00 import weight 0.00
Epoch 314 Iter 10 subLoss 9500.7 multi 3.99 import weight 0.00
Epoch 314 Iter 11 subLoss 8162.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 314 Acc: 93.44 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 816 train Loss: 8877.5 test Loss: 1175.3
Epoch 315 Iter 0 subLoss 8391.3 multi 1.00 import weight 0.00
Epoch 315 Iter 1 subLoss 8061.2 multi 3.99 import weight 0.00
Epoch 315 Iter 2 subLoss 7187.3 multi -1.98 import weight 0.00
Epoch 315 Iter 3 subLoss 7038.1 multi 9.96 import weight 0.00
Epoch 315 Iter 4 subLoss 5578.9 multi -4.97 import weight 0.00
Epoch 315 Iter 5 subLoss 7508.2 multi -1.98 import weight 0.00
Epoch 315 Iter 6 subLoss 13547.2 multi 1.00 import weight 0.00
Epoch 315 Iter 7 subLoss 6838.1 multi -1.99 import weight 0.00
Epoch 315 Iter 8 subLoss 7760.8 multi -1.99 import weight 0.00
Epoch 315 Iter 9 subLoss 8434.5 multi -1.98 import weight 0.00
Epoch 315 Iter 10 subLoss 11456.6 multi -7.96 import weight 0.00
Epoch 315 Iter 11 subLoss 189798.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 315 Acc: 53.42 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 18979 train Loss: 50681.0 test Loss: 9354.7
Epoch 316 Iter 0 subLoss 48656.5 multi 1.00 import weight 0.00
Epoch 316 Iter 1 subLoss 24212.9 multi 1.00 import weight 0.00
Epoch 316 Iter 2 subLoss 21335.0 multi 1.00 import weight 0.00
Epoch 316 Iter 3 subLoss 17453.0 multi 1.00 import weight 0.00
Epoch 316 Iter 4 subLoss 15736.2 multi -1.99 import weight 0.00
Epoch 316 Iter 5 subLoss 17735.1 multi 1.00 import weight 0.00
Epoch 316 Iter 6 subLoss 17114.9 multi 1.00 import weight 0.00
Epoch 316 Iter 7 subLoss 16049.3 multi 1.00 import weight 0.00
Epoch 316 Iter 8 subLoss 14609.1 multi 3.99 import weight 0.00
Epoch 316 Iter 9 subLoss 11139.6 multi 1.00 import weight 0.00
Epoch 316 Iter 10 subLoss 10528.6 multi 1.00 import weight 0.00
Epoch 316 Iter 11 subLoss 10429.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 316 Acc: 95.89 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1042 train Loss: 8820.7 test Loss: 1112.5
Epoch 317 Iter 0 subLoss 8361.2 multi 1.00 import weight 0.00
Epoch 317 Iter 1 subLoss 7604.0 multi -7.96 import weight 0.00
Epoch 317 Iter 2 subLoss 11335.6 multi 1.00 import weight 0.00
Epoch 317 Iter 3 subLoss 10646.5 multi 1.00 import weight 0.00
Epoch 317 Iter 4 subLoss 9881.5 multi -1.99 import weight 0.00
Epoch 317 Iter 5 subLoss 10734.4 multi 3.98 import weight 0.00
Epoch 317 Iter 6 subLoss 9360.0 multi -1.99 import weight 0.00
Epoch 317 Iter 7 subLoss 10079.3 multi -1.98 import weight 0.00
Epoch 317 Iter 8 subLoss 12134.8 multi 3.99 import weight 0.00
Epoch 317 Iter 9 subLoss 10910.0 multi 6.97 import weight 0.00
Epoch 317 Iter 10 subLoss 14755.9 multi -1.99 import weight 0.00
Epoch 317 Iter 11 subLoss 39211.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 317 Acc: 89.90 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 3921 train Loss: 17139.3 test Loss: 1772.4
Epoch 318 Iter 0 subLoss 17236.7 multi 1.00 import weight 0.00
Epoch 318 Iter 1 subLoss 12599.7 multi 1.00 import weight 0.00
Epoch 318 Iter 2 subLoss 11531.3 multi 1.00 import weight 0.00
Epoch 318 Iter 3 subLoss 10656.6 multi 1.00 import weight 0.00
Epoch 318 Iter 4 subLoss 9576.7 multi 1.00 import weight 0.00
Epoch 318 Iter 5 subLoss 9820.0 multi 1.00 import weight 0.00
Epoch 318 Iter 6 subLoss 9163.1 multi -1.99 import weight 0.00
Epoch 318 Iter 7 subLoss 9560.2 multi 6.97 import weight 0.00
Epoch 318 Iter 8 subLoss 7673.7 multi -10.94 import weight 0.00
Epoch 318 Iter 9 subLoss 11879.2 multi -1.99 import weight 0.00
Epoch 318 Iter 10 subLoss 14812.2 multi 1.00 import weight 0.00
Epoch 318 Iter 11 subLoss 10952.5 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 318 Acc: 96.40 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 1095 train Loss: 9118.1 test Loss: 805.9
Epoch 319 Iter 0 subLoss 9290.6 multi -1.98 import weight 0.00
Epoch 319 Iter 1 subLoss 9759.0 multi 3.99 import weight 0.00
Epoch 319 Iter 2 subLoss 7836.9 multi -1.98 import weight 0.00
Epoch 319 Iter 3 subLoss 8619.4 multi -1.99 import weight 0.00
Epoch 319 Iter 4 subLoss 9594.3 multi -4.97 import weight 0.00
Epoch 319 Iter 5 subLoss 13791.4 multi 6.97 import weight 0.00
Epoch 319 Iter 6 subLoss 12681.3 multi -1.99 import weight 0.00
Epoch 319 Iter 7 subLoss 24761.7 multi 1.00 import weight 0.00
Epoch 319 Iter 8 subLoss 10970.2 multi -1.99 import weight 0.00
Epoch 319 Iter 9 subLoss 14405.5 multi 3.99 import weight 0.00
Epoch 319 Iter 10 subLoss 9278.4 multi 9.96 import weight 0.00
Epoch 319 Iter 11 subLoss 7291.2 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 319 Acc: 96.44 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 729 train Loss: 7829.6 test Loss: 629.8
Epoch 320 Iter 0 subLoss 7592.2 multi 9.96 import weight 0.00
Epoch 320 Iter 1 subLoss 6284.7 multi 6.97 import weight 0.00
Epoch 320 Iter 2 subLoss 5693.6 multi -4.97 import weight 0.00
Epoch 320 Iter 3 subLoss 5906.4 multi -4.97 import weight 0.00
Epoch 320 Iter 4 subLoss 7922.9 multi 1.00 import weight 0.00
Epoch 320 Iter 5 subLoss 6739.8 multi 1.00 import weight 0.00
Epoch 320 Iter 6 subLoss 6771.4 multi -7.96 import weight 0.00
Epoch 320 Iter 7 subLoss 10828.4 multi 3.99 import weight 0.00
Epoch 320 Iter 8 subLoss 6578.8 multi 6.97 import weight 0.00
Epoch 320 Iter 9 subLoss 5821.7 multi -1.98 import weight 0.00
Epoch 320 Iter 10 subLoss 6159.9 multi -7.96 import weight 0.00
Epoch 320 Iter 11 subLoss 7432.9 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 320 Acc: 96.63 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 743 train Loss: 8078.3 test Loss: 662.6
Epoch 321 Iter 0 subLoss 7994.7 multi -4.97 import weight 0.00
Epoch 321 Iter 1 subLoss 41224.9 multi 1.00 import weight 0.00
Epoch 321 Iter 2 subLoss 6108.7 multi 12.94 import weight 0.00
Epoch 321 Iter 3 subLoss 5099.6 multi 3.99 import weight 0.00
Epoch 321 Iter 4 subLoss 5134.5 multi -1.99 import weight 0.00
Epoch 321 Iter 5 subLoss 5032.5 multi 24.88 import weight 0.00
Epoch 321 Iter 6 subLoss 5532.7 multi -10.94 import weight 0.00
Epoch 321 Iter 7 subLoss 10435.9 multi -1.98 import weight 0.00
Epoch 321 Iter 8 subLoss 17290.1 multi 1.00 import weight 0.00
Epoch 321 Iter 9 subLoss 11593.8 multi -4.97 import weight 0.00
Epoch 321 Iter 10 subLoss 37911.5 multi 1.00 import weight 0.00
Epoch 321 Iter 11 subLoss 19479.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 321 Acc: 84.10 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1947 train Loss: 14540.7 test Loss: 1670.9
Epoch 322 Iter 0 subLoss 14018.3 multi 1.00 import weight 0.00
Epoch 322 Iter 1 subLoss 11261.6 multi -4.97 import weight 0.00
Epoch 322 Iter 2 subLoss 27183.9 multi 1.00 import weight 0.00
Epoch 322 Iter 3 subLoss 18028.9 multi 3.99 import weight 0.00
Epoch 322 Iter 4 subLoss 9142.7 multi -1.98 import weight 0.00
Epoch 322 Iter 5 subLoss 9455.9 multi 1.00 import weight 0.00
Epoch 322 Iter 6 subLoss 9905.2 multi -1.99 import weight 0.00
Epoch 322 Iter 7 subLoss 11591.1 multi -1.99 import weight 0.00
Epoch 322 Iter 8 subLoss 13439.0 multi 1.00 import weight 0.00
Epoch 322 Iter 9 subLoss 12460.9 multi 3.99 import weight 0.00
Epoch 322 Iter 10 subLoss 8409.9 multi -4.97 import weight 0.00
Epoch 322 Iter 11 subLoss 10598.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 322 Acc: 96.21 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1059 train Loss: 12419.9 test Loss: 803.0
Epoch 323 Iter 0 subLoss 11992.4 multi 3.98 import weight 0.00
Epoch 323 Iter 1 subLoss 9557.9 multi 3.99 import weight 0.00
Epoch 323 Iter 2 subLoss 7652.2 multi 1.00 import weight 0.00
Epoch 323 Iter 3 subLoss 7825.5 multi 6.97 import weight 0.00
Epoch 323 Iter 4 subLoss 6070.6 multi 6.97 import weight 0.00
Epoch 323 Iter 5 subLoss 5542.1 multi 3.98 import weight 0.00
Epoch 323 Iter 6 subLoss 5385.2 multi 9.96 import weight 0.00
Epoch 323 Iter 7 subLoss 4547.0 multi 9.96 import weight 0.00
Epoch 323 Iter 8 subLoss 4576.5 multi -13.93 import weight 0.00
Epoch 323 Iter 9 subLoss 10014.3 multi 3.99 import weight 0.00
Epoch 323 Iter 10 subLoss 4201.0 multi -10.94 import weight 0.00
Epoch 323 Iter 11 subLoss 8505.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 323 Acc: 81.49 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 850 train Loss: 17198.4 test Loss: 2697.6
Epoch 324 Iter 0 subLoss 17036.7 multi 1.00 import weight 0.00
Epoch 324 Iter 1 subLoss 12957.1 multi 3.99 import weight 0.00
Epoch 324 Iter 2 subLoss 6703.2 multi 15.93 import weight 0.00
Epoch 324 Iter 3 subLoss 7490.6 multi 6.97 import weight 0.00
Epoch 324 Iter 4 subLoss 5382.5 multi 12.94 import weight 0.00
Epoch 324 Iter 5 subLoss 4443.0 multi 1.00 import weight 0.00
Epoch 324 Iter 6 subLoss 4427.7 multi -13.93 import weight 0.00
Epoch 324 Iter 7 subLoss 6833.1 multi 1.00 import weight 0.00
Epoch 324 Iter 8 subLoss 5037.1 multi 27.87 import weight 0.00
Epoch 324 Iter 9 subLoss 22298.7 multi 1.00 import weight 0.00
Epoch 324 Iter 10 subLoss 7928.3 multi 3.98 import weight 0.00
Epoch 324 Iter 11 subLoss 4480.8 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 324 Acc: 98.02 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 448 train Loss: 3853.6 test Loss: 337.1
Epoch 325 Iter 0 subLoss 3459.0 multi 18.91 import weight 0.00
Epoch 325 Iter 1 subLoss 4648.1 multi 3.99 import weight 0.00
Epoch 325 Iter 2 subLoss 3274.8 multi 1.00 import weight 0.00
Epoch 325 Iter 3 subLoss 3444.5 multi -25.87 import weight 0.00
Epoch 325 Iter 4 subLoss 4463.2 multi -1.98 import weight 0.00
Epoch 325 Iter 5 subLoss 5507.2 multi 6.97 import weight 0.00
Epoch 325 Iter 6 subLoss 3668.7 multi 12.94 import weight 0.00
Epoch 325 Iter 7 subLoss 3806.0 multi 6.97 import weight 0.00
Epoch 325 Iter 8 subLoss 3234.4 multi 24.88 import weight 0.00
Epoch 325 Iter 9 subLoss 3176.8 multi 33.84 import weight 0.00
Epoch 325 Iter 10 subLoss 13070.0 multi -1.98 import weight 0.00
Epoch 325 Iter 11 subLoss 29435.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 325 Acc: 91.09 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2943 train Loss: 11771.4 test Loss: 1561.3
Epoch 326 Iter 0 subLoss 13180.7 multi 1.00 import weight 0.00
Epoch 326 Iter 1 subLoss 7431.2 multi 15.93 import weight 0.00
Epoch 326 Iter 2 subLoss 10220.3 multi 1.00 import weight 0.00
Epoch 326 Iter 3 subLoss 6775.7 multi -4.97 import weight 0.00
Epoch 326 Iter 4 subLoss 18567.3 multi 1.00 import weight 0.00
Epoch 326 Iter 5 subLoss 9749.2 multi -1.99 import weight 0.00
Epoch 326 Iter 6 subLoss 15721.6 multi 1.00 import weight 0.00
Epoch 326 Iter 7 subLoss 10356.5 multi -1.99 import weight 0.00
Epoch 326 Iter 8 subLoss 16766.2 multi 3.99 import weight 0.00
Epoch 326 Iter 9 subLoss 6057.1 multi 1.00 import weight 0.00
Epoch 326 Iter 10 subLoss 5364.5 multi 9.96 import weight 0.00
Epoch 326 Iter 11 subLoss 3484.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 326 Acc: 98.05 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 348 train Loss: 3577.2 test Loss: 323.5
Epoch 327 Iter 0 subLoss 3195.3 multi -16.91 import weight 0.00
Epoch 327 Iter 1 subLoss 4702.1 multi -10.94 import weight 0.00
Epoch 327 Iter 2 subLoss 10407.4 multi 6.97 import weight 0.00
Epoch 327 Iter 3 subLoss 5196.5 multi -7.96 import weight 0.00
Epoch 327 Iter 4 subLoss 10500.3 multi 9.96 import weight 0.00
Epoch 327 Iter 5 subLoss 5549.7 multi 6.97 import weight 0.00
Epoch 327 Iter 6 subLoss 3498.5 multi -1.99 import weight 0.00
Epoch 327 Iter 7 subLoss 3523.3 multi 12.94 import weight 0.00
Epoch 327 Iter 8 subLoss 3390.3 multi 3.99 import weight 0.00
Epoch 327 Iter 9 subLoss 3134.9 multi -1.99 import weight 0.00
Epoch 327 Iter 10 subLoss 3026.1 multi -25.87 import weight 0.00
Epoch 327 Iter 11 subLoss 4707.2 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 327 Acc: 93.36 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 470 train Loss: 8407.9 test Loss: 1031.1
Epoch 328 Iter 0 subLoss 7410.2 multi 3.99 import weight 0.00
Epoch 328 Iter 1 subLoss 4300.2 multi 12.94 import weight 0.00
Epoch 328 Iter 2 subLoss 3697.6 multi -7.96 import weight 0.00
Epoch 328 Iter 3 subLoss 3342.9 multi -1.98 import weight 0.00
Epoch 328 Iter 4 subLoss 3586.7 multi 3.99 import weight 0.00
Epoch 328 Iter 5 subLoss 3607.7 multi 9.96 import weight 0.00
Epoch 328 Iter 6 subLoss 3182.1 multi 1.00 import weight 1.00
Epoch 328 Iter 7 subLoss 3200.4 multi -10.94 import weight 0.00
Epoch 328 Iter 8 subLoss 3460.1 multi 1.00 import weight 0.00
Epoch 328 Iter 9 subLoss 3560.7 multi -7.96 import weight 0.00
Epoch 328 Iter 10 subLoss 3618.3 multi -16.91 import weight 0.00
Epoch 328 Iter 11 subLoss 5045.4 multi -22.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 328 Acc: 65.36 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -22.88 Pidx 504 train Loss: 63290.2 test Loss: 16598.8
Epoch 329 Iter 0 subLoss 62468.4 multi 1.00 import weight 0.00
Epoch 329 Iter 1 subLoss 13292.7 multi 1.00 import weight 0.00
Epoch 329 Iter 2 subLoss 10822.4 multi 6.97 import weight 0.00
Epoch 329 Iter 3 subLoss 5508.1 multi 9.96 import weight 0.00
Epoch 329 Iter 4 subLoss 4267.9 multi -1.99 import weight 0.00
Epoch 329 Iter 5 subLoss 4877.8 multi 6.97 import weight 0.00
Epoch 329 Iter 6 subLoss 3863.0 multi 12.94 import weight 0.00
Epoch 329 Iter 7 subLoss 3482.9 multi 3.99 import weight 0.00
Epoch 329 Iter 8 subLoss 4192.9 multi 6.97 import weight 0.00
Epoch 329 Iter 9 subLoss 3371.0 multi -1.98 import weight 0.00
Epoch 329 Iter 10 subLoss 3686.5 multi 6.97 import weight 0.00
Epoch 329 Iter 11 subLoss 3542.8 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 329 Acc: 98.00 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 354 train Loss: 3876.2 test Loss: 352.9
Epoch 330 Iter 0 subLoss 3760.8 multi -4.97 import weight 0.00
Epoch 330 Iter 1 subLoss 4109.8 multi -7.96 import weight 0.00
Epoch 330 Iter 2 subLoss 6354.7 multi 1.00 import weight 0.00
Epoch 330 Iter 3 subLoss 5266.8 multi 1.00 import weight 0.00
Epoch 330 Iter 4 subLoss 4948.3 multi 3.99 import weight 0.00
Epoch 330 Iter 5 subLoss 4394.6 multi 1.00 import weight 0.00
Epoch 330 Iter 6 subLoss 3377.2 multi 1.00 import weight 0.00
Epoch 330 Iter 7 subLoss 4167.8 multi 3.98 import weight 0.00
Epoch 330 Iter 8 subLoss 3345.7 multi 1.00 import weight 0.00
Epoch 330 Iter 9 subLoss 3820.8 multi -7.96 import weight 0.00
Epoch 330 Iter 10 subLoss 3858.5 multi -7.96 import weight 0.00
Epoch 330 Iter 11 subLoss 5845.4 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 330 Acc: 97.78 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 584 train Loss: 3828.7 test Loss: 378.7
Epoch 331 Iter 0 subLoss 3938.6 multi -1.98 import weight 0.00
Epoch 331 Iter 1 subLoss 3863.8 multi 12.94 import weight 0.00
Epoch 331 Iter 2 subLoss 3101.0 multi -10.94 import weight 0.00
Epoch 331 Iter 3 subLoss 3495.5 multi -1.99 import weight 0.00
Epoch 331 Iter 4 subLoss 3969.2 multi 6.97 import weight 0.00
Epoch 331 Iter 5 subLoss 3593.2 multi -4.97 import weight 0.00
Epoch 331 Iter 6 subLoss 3411.8 multi 1.00 import weight 0.00
Epoch 331 Iter 7 subLoss 3858.6 multi -4.97 import weight 0.00
Epoch 331 Iter 8 subLoss 4250.9 multi 6.97 import weight 0.00
Epoch 331 Iter 9 subLoss 3680.9 multi 9.96 import weight 0.00
Epoch 331 Iter 10 subLoss 3178.0 multi 36.82 import weight 1.00
Epoch 331 Iter 11 subLoss 3504.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 331 Acc: 98.54 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 350 train Loss: 3005.4 test Loss: 259.3
Epoch 332 Iter 0 subLoss 3012.1 multi 21.90 import weight 1.00
Epoch 332 Iter 1 subLoss 2875.4 multi 3.99 import weight 0.00
Epoch 332 Iter 2 subLoss 2577.0 multi 15.93 import weight 0.00
Epoch 332 Iter 3 subLoss 2539.2 multi 9.96 import weight 0.00
Epoch 332 Iter 4 subLoss 2428.7 multi 6.97 import weight 0.00
Epoch 332 Iter 5 subLoss 2600.1 multi -10.94 import weight 0.00
Epoch 332 Iter 6 subLoss 2926.9 multi -7.96 import weight 0.00
Epoch 332 Iter 7 subLoss 3877.5 multi -4.97 import weight 0.00
Epoch 332 Iter 8 subLoss 4758.4 multi 9.96 import weight 0.00
Epoch 332 Iter 9 subLoss 3844.5 multi -1.99 import weight 0.00
Epoch 332 Iter 10 subLoss 3893.8 multi 3.99 import weight 0.00
Epoch 332 Iter 11 subLoss 3337.8 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 332 Acc: 96.79 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 333 train Loss: 5414.2 test Loss: 537.8
Epoch 333 Iter 0 subLoss 5152.6 multi 1.00 import weight 0.00
Epoch 333 Iter 1 subLoss 4798.3 multi 1.00 import weight 0.00
Epoch 333 Iter 2 subLoss 4282.1 multi -4.97 import weight 0.00
Epoch 333 Iter 3 subLoss 6397.2 multi 3.98 import weight 0.00
Epoch 333 Iter 4 subLoss 3562.9 multi -4.97 import weight 0.00
Epoch 333 Iter 5 subLoss 4141.1 multi -4.97 import weight 0.00
Epoch 333 Iter 6 subLoss 5833.8 multi -4.97 import weight 0.00
Epoch 333 Iter 7 subLoss 12852.6 multi 1.00 import weight 0.00
Epoch 333 Iter 8 subLoss 7939.0 multi -4.97 import weight 0.00
Epoch 333 Iter 9 subLoss 26544.1 multi 1.00 import weight 0.00
Epoch 333 Iter 10 subLoss 10575.9 multi -1.99 import weight 0.00
Epoch 333 Iter 11 subLoss 20044.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 333 Acc: 92.00 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2004 train Loss: 11552.8 test Loss: 1272.4
Epoch 334 Iter 0 subLoss 11892.8 multi 6.97 import weight 0.00
Epoch 334 Iter 1 subLoss 3086.8 multi -1.99 import weight 0.00
Epoch 334 Iter 2 subLoss 3399.2 multi 6.97 import weight 0.00
Epoch 334 Iter 3 subLoss 3199.9 multi -16.91 import weight 0.00
Epoch 334 Iter 4 subLoss 3688.8 multi 12.94 import weight 0.00
Epoch 334 Iter 5 subLoss 3281.5 multi -4.97 import weight 0.00
Epoch 334 Iter 6 subLoss 3174.4 multi 39.81 import weight 1.00
Epoch 334 Iter 7 subLoss 4528.2 multi -1.99 import weight 0.00
Epoch 334 Iter 8 subLoss 4669.4 multi -10.94 import weight 0.00
Epoch 334 Iter 9 subLoss 33327.6 multi 1.00 import weight 0.00
Epoch 334 Iter 10 subLoss 8858.2 multi -4.97 import weight 0.00
Epoch 334 Iter 11 subLoss 34558.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 334 Acc: 96.22 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 3455 train Loss: 8432.4 test Loss: 708.0
Epoch 335 Iter 0 subLoss 8148.3 multi 3.99 import weight 0.00
Epoch 335 Iter 1 subLoss 5502.3 multi 12.94 import weight 0.00
Epoch 335 Iter 2 subLoss 3481.9 multi 6.97 import weight 0.00
Epoch 335 Iter 3 subLoss 3163.4 multi 1.00 import weight 0.00
Epoch 335 Iter 4 subLoss 2907.6 multi -13.93 import weight 0.00
Epoch 335 Iter 5 subLoss 3289.5 multi -1.99 import weight 0.00
Epoch 335 Iter 6 subLoss 3680.2 multi 15.93 import weight 0.00
Epoch 335 Iter 7 subLoss 3382.2 multi -7.96 import weight 0.00
Epoch 335 Iter 8 subLoss 4936.5 multi 6.97 import weight 0.00
Epoch 335 Iter 9 subLoss 3143.7 multi 18.91 import weight 0.00
Epoch 335 Iter 10 subLoss 2651.9 multi 12.94 import weight 0.00
Epoch 335 Iter 11 subLoss 2713.8 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 335 Acc: 98.46 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 271 train Loss: 2953.4 test Loss: 262.4
Epoch 336 Iter 0 subLoss 3106.6 multi -7.96 import weight 0.00
Epoch 336 Iter 1 subLoss 3429.0 multi -7.96 import weight 0.00
Epoch 336 Iter 2 subLoss 4182.0 multi 3.98 import weight 0.00
Epoch 336 Iter 3 subLoss 3214.1 multi 1.00 import weight 0.00
Epoch 336 Iter 4 subLoss 3019.1 multi 24.88 import weight 1.00
Epoch 336 Iter 5 subLoss 3435.6 multi 9.96 import weight 0.00
Epoch 336 Iter 6 subLoss 2598.2 multi 18.91 import weight 0.00
Epoch 336 Iter 7 subLoss 3408.4 multi -4.97 import weight 0.00
Epoch 336 Iter 8 subLoss 2629.0 multi 6.97 import weight 0.00
Epoch 336 Iter 9 subLoss 2236.5 multi -7.96 import weight 0.00
Epoch 336 Iter 10 subLoss 2552.6 multi 1.00 import weight 0.00
Epoch 336 Iter 11 subLoss 3044.7 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 336 Acc: 98.40 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 304 train Loss: 3050.5 test Loss: 272.2
Epoch 337 Iter 0 subLoss 3023.1 multi -28.85 import weight 0.00
Epoch 337 Iter 1 subLoss 11966.0 multi -4.97 import weight 0.00
Epoch 337 Iter 2 subLoss 131623.4 multi 1.00 import weight 0.00
Epoch 337 Iter 3 subLoss 22703.5 multi -1.99 import weight 0.00
Epoch 337 Iter 4 subLoss 53256.5 multi 1.00 import weight 0.00
Epoch 337 Iter 5 subLoss 19006.0 multi 1.00 import weight 0.00
Epoch 337 Iter 6 subLoss 15561.3 multi 3.99 import weight 0.00
Epoch 337 Iter 7 subLoss 10310.0 multi -1.99 import weight 0.00
Epoch 337 Iter 8 subLoss 12363.0 multi 3.98 import weight 0.00
Epoch 337 Iter 9 subLoss 9442.1 multi -1.99 import weight 0.00
Epoch 337 Iter 10 subLoss 10328.4 multi 1.00 import weight 0.00
Epoch 337 Iter 11 subLoss 9721.0 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 337 Acc: 80.07 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 972 train Loss: 11321.8 test Loss: 2107.9
Epoch 338 Iter 0 subLoss 11507.7 multi -4.97 import weight 0.00
Epoch 338 Iter 1 subLoss 14634.4 multi 1.00 import weight 0.00
Epoch 338 Iter 2 subLoss 13815.0 multi 3.99 import weight 0.00
Epoch 338 Iter 3 subLoss 11418.9 multi -1.98 import weight 0.00
Epoch 338 Iter 4 subLoss 13028.4 multi 1.00 import weight 0.00
Epoch 338 Iter 5 subLoss 11419.3 multi 1.00 import weight 0.00
Epoch 338 Iter 6 subLoss 11199.5 multi 3.98 import weight 0.00
Epoch 338 Iter 7 subLoss 9099.6 multi 6.97 import weight 0.00
Epoch 338 Iter 8 subLoss 7491.5 multi 9.96 import weight 0.00
Epoch 338 Iter 9 subLoss 4225.8 multi -10.94 import weight 0.00
Epoch 338 Iter 10 subLoss 6505.6 multi -13.93 import weight 0.00
Epoch 338 Iter 11 subLoss 23612.8 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 338 Acc: 94.26 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 2361 train Loss: 9746.9 test Loss: 1732.8
Epoch 339 Iter 0 subLoss 9507.5 multi 6.97 import weight 0.00
Epoch 339 Iter 1 subLoss 5800.6 multi 3.99 import weight 0.00
Epoch 339 Iter 2 subLoss 5377.5 multi -4.97 import weight 0.00
Epoch 339 Iter 3 subLoss 6282.1 multi 9.96 import weight 0.00
Epoch 339 Iter 4 subLoss 4814.7 multi 3.99 import weight 0.00
Epoch 339 Iter 5 subLoss 4580.8 multi 1.00 import weight 0.00
Epoch 339 Iter 6 subLoss 5329.8 multi -7.96 import weight 0.00
Epoch 339 Iter 7 subLoss 5252.2 multi 9.96 import weight 0.00
Epoch 339 Iter 8 subLoss 4714.7 multi -1.99 import weight 0.00
Epoch 339 Iter 9 subLoss 4080.9 multi 3.99 import weight 0.00
Epoch 339 Iter 10 subLoss 4413.0 multi 15.93 import weight 0.00
Epoch 339 Iter 11 subLoss 3734.2 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 339 Acc: 97.04 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 373 train Loss: 4528.9 test Loss: 589.7
Epoch 340 Iter 0 subLoss 4140.4 multi -1.99 import weight 0.00
Epoch 340 Iter 1 subLoss 4432.2 multi 1.00 import weight 0.00
Epoch 340 Iter 2 subLoss 4763.0 multi -16.91 import weight 0.00
Epoch 340 Iter 3 subLoss 11507.4 multi -1.98 import weight 0.00
Epoch 340 Iter 4 subLoss 37422.6 multi 1.00 import weight 0.00
Epoch 340 Iter 5 subLoss 6347.8 multi -4.97 import weight 0.00
Epoch 340 Iter 6 subLoss 8954.9 multi 1.00 import weight 0.00
Epoch 340 Iter 7 subLoss 6893.8 multi -4.97 import weight 0.00
Epoch 340 Iter 8 subLoss 12381.0 multi -1.98 import weight 0.00
Epoch 340 Iter 9 subLoss 27537.7 multi -1.99 import weight 0.00
Epoch 340 Iter 10 subLoss 276666.4 multi 1.00 import weight 0.00
Epoch 340 Iter 11 subLoss 27280.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 340 Acc: 84.69 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2728 train Loss: 16146.9 test Loss: 2400.4
Epoch 341 Iter 0 subLoss 15873.5 multi 1.00 import weight 0.00
Epoch 341 Iter 1 subLoss 12900.7 multi 1.00 import weight 0.00
Epoch 341 Iter 2 subLoss 10870.1 multi -4.97 import weight 0.00
Epoch 341 Iter 3 subLoss 20950.7 multi 1.00 import weight 0.00
Epoch 341 Iter 4 subLoss 16824.7 multi 3.99 import weight 0.00
Epoch 341 Iter 5 subLoss 9598.3 multi -1.98 import weight 0.00
Epoch 341 Iter 6 subLoss 10737.1 multi 6.97 import weight 0.00
Epoch 341 Iter 7 subLoss 6619.2 multi 1.00 import weight 0.00
Epoch 341 Iter 8 subLoss 6243.9 multi 9.96 import weight 0.00
Epoch 341 Iter 9 subLoss 5109.8 multi -4.97 import weight 0.00
Epoch 341 Iter 10 subLoss 5941.1 multi 6.97 import weight 0.00
Epoch 341 Iter 11 subLoss 4824.7 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 341 Acc: 97.37 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 482 train Loss: 5260.8 test Loss: 686.9
Epoch 342 Iter 0 subLoss 4994.1 multi 1.00 import weight 0.00
Epoch 342 Iter 1 subLoss 5099.8 multi 6.97 import weight 0.00
Epoch 342 Iter 2 subLoss 4300.7 multi 15.93 import weight 0.00
Epoch 342 Iter 3 subLoss 3901.9 multi -13.93 import weight 0.00
Epoch 342 Iter 4 subLoss 4414.1 multi 18.91 import weight 0.00
Epoch 342 Iter 5 subLoss 3898.9 multi 6.97 import weight 0.00
Epoch 342 Iter 6 subLoss 3654.0 multi -22.88 import weight 0.00
Epoch 342 Iter 7 subLoss 4112.6 multi -4.97 import weight 0.00
Epoch 342 Iter 8 subLoss 6738.6 multi 3.99 import weight 0.00
Epoch 342 Iter 9 subLoss 4950.7 multi -7.96 import weight 0.00
Epoch 342 Iter 10 subLoss 5688.1 multi 1.00 import weight 0.00
Epoch 342 Iter 11 subLoss 5752.6 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 342 Acc: 97.00 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 575 train Loss: 6338.2 test Loss: 554.9
Epoch 343 Iter 0 subLoss 5657.0 multi 3.99 import weight 0.00
Epoch 343 Iter 1 subLoss 4845.3 multi -1.99 import weight 0.00
Epoch 343 Iter 2 subLoss 5269.9 multi 1.00 import weight 0.00
Epoch 343 Iter 3 subLoss 5057.3 multi 3.99 import weight 0.00
Epoch 343 Iter 4 subLoss 4010.4 multi -7.96 import weight 0.00
Epoch 343 Iter 5 subLoss 4734.0 multi 3.98 import weight 0.00
Epoch 343 Iter 6 subLoss 4857.7 multi 3.98 import weight 0.00
Epoch 343 Iter 7 subLoss 4199.6 multi 6.97 import weight 0.00
Epoch 343 Iter 8 subLoss 3720.4 multi 1.00 import weight 0.00
Epoch 343 Iter 9 subLoss 4353.1 multi 1.00 import weight 0.00
Epoch 343 Iter 10 subLoss 3132.5 multi 1.00 import weight 0.00
Epoch 343 Iter 11 subLoss 3996.7 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 343 Acc: 97.47 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 399 train Loss: 4322.8 test Loss: 504.8
Epoch 344 Iter 0 subLoss 4284.2 multi -1.99 import weight 0.00
Epoch 344 Iter 1 subLoss 4406.4 multi -4.97 import weight 0.00
Epoch 344 Iter 2 subLoss 5897.6 multi -1.99 import weight 0.00
Epoch 344 Iter 3 subLoss 10652.0 multi 3.99 import weight 0.00
Epoch 344 Iter 4 subLoss 4037.5 multi -7.96 import weight 0.00
Epoch 344 Iter 5 subLoss 5393.8 multi -13.93 import weight 0.00
Epoch 344 Iter 6 subLoss 21204.7 multi -4.97 import weight 0.00
Epoch 344 Iter 7 subLoss 239874.2 multi 1.00 import weight 0.00
Epoch 344 Iter 8 subLoss 28867.4 multi 1.00 import weight 0.00
Epoch 344 Iter 9 subLoss 22787.2 multi 3.99 import weight 0.00
Epoch 344 Iter 10 subLoss 12226.2 multi -1.98 import weight 0.00
Epoch 344 Iter 11 subLoss 13751.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 344 Acc: 90.45 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 1375 train Loss: 8527.8 test Loss: 1469.9
Epoch 345 Iter 0 subLoss 8883.9 multi 1.00 import weight 0.00
Epoch 345 Iter 1 subLoss 7512.6 multi 1.00 import weight 0.00
Epoch 345 Iter 2 subLoss 7582.5 multi 1.00 import weight 0.00
Epoch 345 Iter 3 subLoss 7413.5 multi 6.97 import weight 0.00
Epoch 345 Iter 4 subLoss 5734.9 multi 1.00 import weight 0.00
Epoch 345 Iter 5 subLoss 5225.3 multi 3.98 import weight 0.00
Epoch 345 Iter 6 subLoss 4721.9 multi -4.97 import weight 0.00
Epoch 345 Iter 7 subLoss 5604.5 multi 1.00 import weight 0.00
Epoch 345 Iter 8 subLoss 5213.4 multi -1.98 import weight 0.00
Epoch 345 Iter 9 subLoss 5407.7 multi -7.96 import weight 0.00
Epoch 345 Iter 10 subLoss 6370.4 multi 1.00 import weight 0.00
Epoch 345 Iter 11 subLoss 7037.8 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 345 Acc: 97.37 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 703 train Loss: 4787.8 test Loss: 518.5
Epoch 346 Iter 0 subLoss 4390.7 multi 3.99 import weight 0.00
Epoch 346 Iter 1 subLoss 4643.9 multi 6.97 import weight 0.00
Epoch 346 Iter 2 subLoss 3954.1 multi -10.94 import weight 0.00
Epoch 346 Iter 3 subLoss 3859.4 multi -4.97 import weight 0.00
Epoch 346 Iter 4 subLoss 4805.3 multi -1.99 import weight 0.00
Epoch 346 Iter 5 subLoss 4641.4 multi 9.96 import weight 0.00
Epoch 346 Iter 6 subLoss 4536.4 multi -4.97 import weight 0.00
Epoch 346 Iter 7 subLoss 4202.4 multi -13.93 import weight 0.00
Epoch 346 Iter 8 subLoss 5946.7 multi 9.96 import weight 0.00
Epoch 346 Iter 9 subLoss 4691.3 multi 12.94 import weight 0.00
Epoch 346 Iter 10 subLoss 4278.8 multi -1.99 import weight 0.00
Epoch 346 Iter 11 subLoss 3923.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 346 Acc: 97.78 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 392 train Loss: 4102.6 test Loss: 399.9
Epoch 347 Iter 0 subLoss 3941.5 multi 9.96 import weight 0.00
Epoch 347 Iter 1 subLoss 3760.3 multi -1.99 import weight 0.00
Epoch 347 Iter 2 subLoss 4132.2 multi 3.99 import weight 0.00
Epoch 347 Iter 3 subLoss 4057.7 multi 3.99 import weight 0.00
Epoch 347 Iter 4 subLoss 3595.9 multi -1.98 import weight 0.00
Epoch 347 Iter 5 subLoss 3260.4 multi 6.97 import weight 0.00
Epoch 347 Iter 6 subLoss 3310.0 multi 9.96 import weight 0.00
Epoch 347 Iter 7 subLoss 3259.2 multi -4.97 import weight 0.00
Epoch 347 Iter 8 subLoss 3507.1 multi 12.94 import weight 0.00
Epoch 347 Iter 9 subLoss 3202.1 multi -10.94 import weight 0.00
Epoch 347 Iter 10 subLoss 3408.7 multi -1.99 import weight 0.00
Epoch 347 Iter 11 subLoss 3643.7 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 347 Acc: 97.57 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 364 train Loss: 3843.1 test Loss: 385.2
Epoch 348 Iter 0 subLoss 3593.5 multi 1.00 import weight 0.00
Epoch 348 Iter 1 subLoss 2916.4 multi 3.99 import weight 0.00
Epoch 348 Iter 2 subLoss 3515.4 multi -19.90 import weight 0.00
Epoch 348 Iter 3 subLoss 5781.4 multi -1.98 import weight 0.00
Epoch 348 Iter 4 subLoss 8922.1 multi 6.97 import weight 0.00
Epoch 348 Iter 5 subLoss 4077.1 multi -7.96 import weight 0.00
Epoch 348 Iter 6 subLoss 5551.2 multi -7.96 import weight 0.00
Epoch 348 Iter 7 subLoss 14053.0 multi 1.00 import weight 0.00
Epoch 348 Iter 8 subLoss 9800.0 multi 1.00 import weight 0.00
Epoch 348 Iter 9 subLoss 8750.4 multi 1.00 import weight 0.00
Epoch 348 Iter 10 subLoss 6688.2 multi -4.97 import weight 0.00
Epoch 348 Iter 11 subLoss 12459.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 348 Acc: 76.10 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1245 train Loss: 20961.1 test Loss: 2553.2
Epoch 349 Iter 0 subLoss 21740.7 multi 1.00 import weight 0.00
Epoch 349 Iter 1 subLoss 13899.4 multi 1.00 import weight 0.00
Epoch 349 Iter 2 subLoss 11029.7 multi -1.99 import weight 0.00
Epoch 349 Iter 3 subLoss 15311.3 multi 1.00 import weight 0.00
Epoch 349 Iter 4 subLoss 11973.4 multi -4.97 import weight 0.00
Epoch 349 Iter 5 subLoss 22028.1 multi 1.00 import weight 0.00
Epoch 349 Iter 6 subLoss 17769.9 multi 3.99 import weight 0.00
Epoch 349 Iter 7 subLoss 12988.1 multi 3.99 import weight 0.00
Epoch 349 Iter 8 subLoss 10092.1 multi 1.00 import weight 0.00
Epoch 349 Iter 9 subLoss 9457.3 multi 1.00 import weight 0.00
Epoch 349 Iter 10 subLoss 8747.3 multi 1.00 import weight 0.00
Epoch 349 Iter 11 subLoss 7059.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 349 Acc: 87.10 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 705 train Loss: 10802.9 test Loss: 1610.2
Epoch 350 Iter 0 subLoss 10391.1 multi 1.00 import weight 0.00
Epoch 350 Iter 1 subLoss 10356.6 multi 1.00 import weight 0.00
Epoch 350 Iter 2 subLoss 9757.3 multi 3.98 import weight 0.00
Epoch 350 Iter 3 subLoss 7015.2 multi -1.99 import weight 0.00
Epoch 350 Iter 4 subLoss 7993.0 multi -1.99 import weight 0.00
Epoch 350 Iter 5 subLoss 8883.9 multi 3.99 import weight 0.00
Epoch 350 Iter 6 subLoss 7297.8 multi 1.00 import weight 0.00
Epoch 350 Iter 7 subLoss 6058.9 multi 3.98 import weight 0.00
Epoch 350 Iter 8 subLoss 5148.4 multi 1.00 import weight 0.00
Epoch 350 Iter 9 subLoss 5258.2 multi 12.94 import weight 0.00
Epoch 350 Iter 10 subLoss 3442.3 multi -25.87 import weight 0.00
Epoch 350 Iter 11 subLoss 7327.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 350 Acc: 90.00 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 732 train Loss: 13985.2 test Loss: 1334.4
Epoch 351 Iter 0 subLoss 13386.9 multi 6.97 import weight 0.00
Epoch 351 Iter 1 subLoss 4663.2 multi -7.96 import weight 0.00
Epoch 351 Iter 2 subLoss 5532.6 multi -7.96 import weight 0.00
Epoch 351 Iter 3 subLoss 11572.4 multi 3.98 import weight 0.00
Epoch 351 Iter 4 subLoss 5503.3 multi 15.93 import weight 0.00
Epoch 351 Iter 5 subLoss 3108.5 multi -4.97 import weight 0.00
Epoch 351 Iter 6 subLoss 3391.6 multi 6.97 import weight 0.00
Epoch 351 Iter 7 subLoss 3862.5 multi 9.96 import weight 0.00
Epoch 351 Iter 8 subLoss 3061.3 multi -7.96 import weight 0.00
Epoch 351 Iter 9 subLoss 3300.7 multi 12.94 import weight 0.00
Epoch 351 Iter 10 subLoss 3502.3 multi 15.93 import weight 0.00
Epoch 351 Iter 11 subLoss 3598.7 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 351 Acc: 98.37 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 359 train Loss: 3010.9 test Loss: 276.7
Epoch 352 Iter 0 subLoss 2931.9 multi 6.97 import weight 0.00
Epoch 352 Iter 1 subLoss 2677.7 multi 18.91 import weight 0.00
Epoch 352 Iter 2 subLoss 3161.9 multi 3.98 import weight 0.00
Epoch 352 Iter 3 subLoss 2934.9 multi 9.96 import weight 0.00
Epoch 352 Iter 4 subLoss 3250.1 multi -1.98 import weight 0.00
Epoch 352 Iter 5 subLoss 2700.1 multi 9.96 import weight 0.00
Epoch 352 Iter 6 subLoss 2918.8 multi 6.97 import weight 0.00
Epoch 352 Iter 7 subLoss 2320.9 multi -4.97 import weight 0.00
Epoch 352 Iter 8 subLoss 2833.3 multi 12.94 import weight 0.00
Epoch 352 Iter 9 subLoss 2131.2 multi -7.96 import weight 0.00
Epoch 352 Iter 10 subLoss 2207.1 multi 15.93 import weight 0.00
Epoch 352 Iter 11 subLoss 2285.1 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 352 Acc: 98.75 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 228 train Loss: 2621.1 test Loss: 217.5
Epoch 353 Iter 0 subLoss 2573.5 multi 18.91 import weight 0.00
Epoch 353 Iter 1 subLoss 2782.8 multi -13.93 import weight 0.00
Epoch 353 Iter 2 subLoss 3665.1 multi 12.94 import weight 0.00
Epoch 353 Iter 3 subLoss 3440.3 multi -22.88 import weight 0.00
Epoch 353 Iter 4 subLoss 13428.2 multi 6.97 import weight 0.00
Epoch 353 Iter 5 subLoss 4104.3 multi -4.97 import weight 0.00
Epoch 353 Iter 6 subLoss 7099.6 multi 6.97 import weight 0.00
Epoch 353 Iter 7 subLoss 2966.6 multi 9.96 import weight 0.00
Epoch 353 Iter 8 subLoss 2475.1 multi -10.94 import weight 0.00
Epoch 353 Iter 9 subLoss 3186.1 multi -1.99 import weight 0.00
Epoch 353 Iter 10 subLoss 3482.6 multi 9.96 import weight 0.00
Epoch 353 Iter 11 subLoss 2215.4 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 353 Acc: 98.46 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 221 train Loss: 2750.4 test Loss: 278.3
Epoch 354 Iter 0 subLoss 3103.2 multi -1.98 import weight 0.00
Epoch 354 Iter 1 subLoss 3304.8 multi 15.93 import weight 0.00
Epoch 354 Iter 2 subLoss 2688.4 multi -1.98 import weight 0.00
Epoch 354 Iter 3 subLoss 2810.5 multi -28.85 import weight 0.00
Epoch 354 Iter 4 subLoss 4400.1 multi -4.97 import weight 0.00
Epoch 354 Iter 5 subLoss 9555.5 multi 6.97 import weight 0.00
Epoch 354 Iter 6 subLoss 4497.3 multi -7.96 import weight 0.00
Epoch 354 Iter 7 subLoss 8015.6 multi 9.96 import weight 0.00
Epoch 354 Iter 8 subLoss 3973.9 multi -1.99 import weight 0.00
Epoch 354 Iter 9 subLoss 3904.6 multi -13.93 import weight 0.00
Epoch 354 Iter 10 subLoss 21308.6 multi 1.00 import weight 0.00
Epoch 354 Iter 11 subLoss 14112.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 354 Acc: 92.99 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1411 train Loss: 9636.5 test Loss: 1235.2
Epoch 355 Iter 0 subLoss 9223.6 multi 3.99 import weight 0.00
Epoch 355 Iter 1 subLoss 4080.8 multi 3.99 import weight 0.00
Epoch 355 Iter 2 subLoss 3236.1 multi 27.87 import weight 0.00
Epoch 355 Iter 3 subLoss 3419.9 multi -1.99 import weight 0.00
Epoch 355 Iter 4 subLoss 3995.9 multi -7.96 import weight 0.00
Epoch 355 Iter 5 subLoss 12622.3 multi 1.00 import weight 0.00
Epoch 355 Iter 6 subLoss 4232.3 multi 3.99 import weight 0.00
Epoch 355 Iter 7 subLoss 2943.1 multi -7.96 import weight 0.00
Epoch 355 Iter 8 subLoss 4872.9 multi 9.96 import weight 0.00
Epoch 355 Iter 9 subLoss 3048.3 multi -4.97 import weight 0.00
Epoch 355 Iter 10 subLoss 2845.2 multi 18.91 import weight 0.00
Epoch 355 Iter 11 subLoss 2510.0 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 355 Acc: 98.11 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 250 train Loss: 3237.6 test Loss: 329.1
Epoch 356 Iter 0 subLoss 2845.4 multi 21.90 import weight 0.00
Epoch 356 Iter 1 subLoss 3070.1 multi -1.99 import weight 0.00
Epoch 356 Iter 2 subLoss 3211.5 multi 1.00 import weight 0.00
Epoch 356 Iter 3 subLoss 3046.0 multi -1.99 import weight 0.00
Epoch 356 Iter 4 subLoss 3259.1 multi 1.00 import weight 0.00
Epoch 356 Iter 5 subLoss 3275.3 multi 1.00 import weight 0.00
Epoch 356 Iter 6 subLoss 3040.7 multi 1.00 import weight 0.00
Epoch 356 Iter 7 subLoss 2542.8 multi -4.97 import weight 0.00
Epoch 356 Iter 8 subLoss 3159.7 multi -34.82 import weight 0.00
Epoch 356 Iter 9 subLoss 28509.6 multi 1.00 import weight 0.00
Epoch 356 Iter 10 subLoss 12323.2 multi -4.97 import weight 0.00
Epoch 356 Iter 11 subLoss 54204.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 356 Acc: 88.21 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 5420 train Loss: 14739.9 test Loss: 2142.7
Epoch 357 Iter 0 subLoss 14551.1 multi -1.99 import weight 0.00
Epoch 357 Iter 1 subLoss 25510.5 multi 1.00 import weight 0.00
Epoch 357 Iter 2 subLoss 16288.6 multi -4.97 import weight 0.00
Epoch 357 Iter 3 subLoss 59355.5 multi 1.00 import weight 0.00
Epoch 357 Iter 4 subLoss 26067.7 multi 1.00 import weight 0.00
Epoch 357 Iter 5 subLoss 20578.1 multi 3.99 import weight 0.00
Epoch 357 Iter 6 subLoss 8652.8 multi 9.96 import weight 0.00
Epoch 357 Iter 7 subLoss 3381.2 multi -4.97 import weight 0.00
Epoch 357 Iter 8 subLoss 5038.9 multi 30.85 import weight 0.00
Epoch 357 Iter 9 subLoss 3160.4 multi 3.99 import weight 0.00
Epoch 357 Iter 10 subLoss 3300.7 multi 18.91 import weight 0.00
Epoch 357 Iter 11 subLoss 3749.3 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 357 Acc: 97.84 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 374 train Loss: 3362.5 test Loss: 371.0
Epoch 358 Iter 0 subLoss 3321.8 multi 6.97 import weight 0.00
Epoch 358 Iter 1 subLoss 3032.3 multi 1.00 import weight 0.00
Epoch 358 Iter 2 subLoss 2571.7 multi 21.90 import weight 0.00
Epoch 358 Iter 3 subLoss 2776.7 multi 24.88 import weight 0.00
Epoch 358 Iter 4 subLoss 2858.8 multi -31.84 import weight 0.00
Epoch 358 Iter 5 subLoss 3222.2 multi -19.90 import weight 0.00
Epoch 358 Iter 6 subLoss 41303.8 multi 1.00 import weight 0.00
Epoch 358 Iter 7 subLoss 10204.8 multi 1.00 import weight 0.00
Epoch 358 Iter 8 subLoss 8350.0 multi 3.99 import weight 0.00
Epoch 358 Iter 9 subLoss 4410.9 multi 15.93 import weight 0.00
Epoch 358 Iter 10 subLoss 2899.6 multi 24.88 import weight 0.00
Epoch 358 Iter 11 subLoss 3422.6 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 358 Acc: 96.63 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 342 train Loss: 3917.0 test Loss: 500.6
Epoch 359 Iter 0 subLoss 4041.5 multi -7.96 import weight 0.00
Epoch 359 Iter 1 subLoss 7976.5 multi -1.99 import weight 0.00
Epoch 359 Iter 2 subLoss 15711.8 multi 3.99 import weight 0.00
Epoch 359 Iter 3 subLoss 3476.4 multi -7.96 import weight 0.00
Epoch 359 Iter 4 subLoss 6028.4 multi 1.00 import weight 0.00
Epoch 359 Iter 5 subLoss 5294.3 multi 1.00 import weight 0.00
Epoch 359 Iter 6 subLoss 4428.2 multi -19.90 import weight 0.00
Epoch 359 Iter 7 subLoss 16263.3 multi 1.00 import weight 0.00
Epoch 359 Iter 8 subLoss 14137.3 multi 1.00 import weight 0.00
Epoch 359 Iter 9 subLoss 12684.3 multi 1.00 import weight 0.00
Epoch 359 Iter 10 subLoss 11146.0 multi 1.00 import weight 0.00
Epoch 359 Iter 11 subLoss 10041.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 359 Acc: 89.59 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1004 train Loss: 7485.4 test Loss: 1275.4
Epoch 360 Iter 0 subLoss 7336.2 multi 1.00 import weight 0.00
Epoch 360 Iter 1 subLoss 7485.7 multi -1.99 import weight 0.00
Epoch 360 Iter 2 subLoss 8117.3 multi 6.97 import weight 0.00
Epoch 360 Iter 3 subLoss 5382.0 multi 12.94 import weight 0.00
Epoch 360 Iter 4 subLoss 4412.4 multi 18.91 import weight 0.00
Epoch 360 Iter 5 subLoss 2728.5 multi -1.98 import weight 0.00
Epoch 360 Iter 6 subLoss 2971.9 multi -16.91 import weight 0.00
Epoch 360 Iter 7 subLoss 4046.5 multi -4.97 import weight 0.00
Epoch 360 Iter 8 subLoss 4440.1 multi 1.00 import weight 0.00
Epoch 360 Iter 9 subLoss 4679.2 multi -1.99 import weight 0.00
Epoch 360 Iter 10 subLoss 4503.4 multi -4.97 import weight 0.00
Epoch 360 Iter 11 subLoss 5157.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 360 Acc: 94.86 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 515 train Loss: 5214.5 test Loss: 903.4
Epoch 361 Iter 0 subLoss 5568.2 multi 1.00 import weight 0.00
Epoch 361 Iter 1 subLoss 4406.1 multi -1.99 import weight 0.00
Epoch 361 Iter 2 subLoss 5230.1 multi -1.99 import weight 0.00
Epoch 361 Iter 3 subLoss 6535.4 multi -1.99 import weight 0.00
Epoch 361 Iter 4 subLoss 8140.5 multi 6.97 import weight 0.00
Epoch 361 Iter 5 subLoss 3960.5 multi 6.97 import weight 0.00
Epoch 361 Iter 6 subLoss 4033.8 multi -4.97 import weight 0.00
Epoch 361 Iter 7 subLoss 4562.5 multi 9.96 import weight 0.00
Epoch 361 Iter 8 subLoss 3690.4 multi -16.91 import weight 0.00
Epoch 361 Iter 9 subLoss 4391.2 multi 6.97 import weight 0.00
Epoch 361 Iter 10 subLoss 4676.3 multi 1.00 import weight 0.00
Epoch 361 Iter 11 subLoss 3383.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0042 / 0.14263 / 8.43
Entropy seen (from low to high)
[3840, 498, 216, 110, 60, 41, 43, 30, 24, 26, 23, 17, 16, 19, 17, 15, 9, 13, 9, 18, 21, 13, 12, 5, 4, 5, 6, 0, 6, 4, 6, 0, 4, 0, 1, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 2, 3, 9, 23, 21, 26, 31, 40, 51, 54, 71, 98, 71, 90, 81, 89, 94, 82, 98, 88, 109, 108, 138, 179, 189, 159, 120, 126, 111, 112, 110, 124, 106, 129, 113, 127, 137, 150, 156, 169, 184, 206, 263, 414]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.7, 36.2, 40.1, 43.7, 46.4, 50.9, 53.8, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 0.0, 0.0, 49.9, 36.3, 49.9, 61.1, 64.9, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 1, 3, 18, 11, 6, 18, 20, 9]
Epoch 361 Acc: 96.58 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 338 train Loss: 4083.1 test Loss: 685.5
Epoch 362 Iter 0 subLoss 3847.6 multi 1.00 import weight 0.00
Epoch 362 Iter 1 subLoss 3804.3 multi 9.96 import weight 0.00
Epoch 362 Iter 2 subLoss 4197.0 multi 9.96 import weight 0.00
Epoch 362 Iter 3 subLoss 3760.0 multi 1.00 import weight 0.00
Epoch 362 Iter 4 subLoss 3670.5 multi -4.97 import weight 0.00
Epoch 362 Iter 5 subLoss 3240.6 multi -13.93 import weight 0.00
Epoch 362 Iter 6 subLoss 3528.9 multi 12.94 import weight 0.00
Epoch 362 Iter 7 subLoss 3633.3 multi -4.97 import weight 0.00
Epoch 362 Iter 8 subLoss 3481.1 multi 9.96 import weight 0.00
Epoch 362 Iter 9 subLoss 2836.5 multi 15.93 import weight 0.00
Epoch 362 Iter 10 subLoss 2619.9 multi -7.96 import weight 0.00
Epoch 362 Iter 11 subLoss 3186.2 multi 1.00 import weight 1.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 362 Acc: 98.11 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 318 train Loss: 2859.8 test Loss: 381.4
Epoch 363 Iter 0 subLoss 2798.1 multi 3.99 import weight 0.00
Epoch 363 Iter 1 subLoss 2569.5 multi -10.94 import weight 0.00
Epoch 363 Iter 2 subLoss 2817.7 multi -25.87 import weight 0.00
Epoch 363 Iter 3 subLoss 4606.9 multi 1.00 import weight 0.00
Epoch 363 Iter 4 subLoss 4728.8 multi -1.99 import weight 0.00
Epoch 363 Iter 5 subLoss 4931.7 multi 9.96 import weight 0.00
Epoch 363 Iter 6 subLoss 3265.0 multi 1.00 import weight 0.00
Epoch 363 Iter 7 subLoss 3407.9 multi -1.99 import weight 0.00
Epoch 363 Iter 8 subLoss 3298.9 multi 1.00 import weight 0.00
Epoch 363 Iter 9 subLoss 3913.7 multi 3.98 import weight 0.00
Epoch 363 Iter 10 subLoss 3182.0 multi 3.99 import weight 1.00
Epoch 363 Iter 11 subLoss 3345.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 363 Acc: 98.15 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 334 train Loss: 3145.5 test Loss: 430.4
Epoch 364 Iter 0 subLoss 2783.2 multi -13.93 import weight 0.00
Epoch 364 Iter 1 subLoss 3327.7 multi 9.96 import weight 0.00
Epoch 364 Iter 2 subLoss 3350.2 multi -4.97 import weight 0.00
Epoch 364 Iter 3 subLoss 3338.7 multi -19.90 import weight 0.00
Epoch 364 Iter 4 subLoss 4149.9 multi -1.98 import weight 0.00
Epoch 364 Iter 5 subLoss 3862.0 multi 12.94 import weight 0.00
Epoch 364 Iter 6 subLoss 3054.7 multi 3.99 import weight 0.00
Epoch 364 Iter 7 subLoss 3496.9 multi -7.96 import weight 0.00
Epoch 364 Iter 8 subLoss 4087.4 multi 6.97 import weight 0.00
Epoch 364 Iter 9 subLoss 3313.2 multi -19.90 import weight 0.00
Epoch 364 Iter 10 subLoss 3892.8 multi 9.96 import weight 0.00
Epoch 364 Iter 11 subLoss 4031.8 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 364 Acc: 96.77 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 403 train Loss: 3810.2 test Loss: 587.1
Epoch 365 Iter 0 subLoss 3698.2 multi -13.93 import weight 0.00
Epoch 365 Iter 1 subLoss 4134.4 multi 6.97 import weight 0.00
Epoch 365 Iter 2 subLoss 3532.8 multi -4.97 import weight 0.00
Epoch 365 Iter 3 subLoss 4131.7 multi 9.96 import weight 0.00
Epoch 365 Iter 4 subLoss 3536.3 multi -1.98 import weight 0.00
Epoch 365 Iter 5 subLoss 4159.5 multi -10.94 import weight 0.00
Epoch 365 Iter 6 subLoss 5093.4 multi 9.96 import weight 0.00
Epoch 365 Iter 7 subLoss 3495.4 multi -4.97 import weight 0.00
Epoch 365 Iter 8 subLoss 3798.3 multi -19.90 import weight 0.00
Epoch 365 Iter 9 subLoss 4648.7 multi 12.94 import weight 0.00
Epoch 365 Iter 10 subLoss 4113.9 multi -4.97 import weight 0.00
Epoch 365 Iter 11 subLoss 4327.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 365 Acc: 96.63 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 432 train Loss: 4059.4 test Loss: 644.5
Epoch 366 Iter 0 subLoss 4184.2 multi 6.97 import weight 0.00
Epoch 366 Iter 1 subLoss 3379.1 multi 3.99 import weight 0.00
Epoch 366 Iter 2 subLoss 3154.1 multi -31.84 import weight 0.00
Epoch 366 Iter 3 subLoss 5070.7 multi 1.00 import weight 0.00
Epoch 366 Iter 4 subLoss 4342.2 multi 3.98 import weight 0.00
Epoch 366 Iter 5 subLoss 4381.3 multi 6.97 import weight 0.00
Epoch 366 Iter 6 subLoss 3838.5 multi 6.97 import weight 0.00
Epoch 366 Iter 7 subLoss 3727.2 multi 3.99 import weight 0.00
Epoch 366 Iter 8 subLoss 4038.6 multi 1.00 import weight 0.00
Epoch 366 Iter 9 subLoss 4053.2 multi 1.00 import weight 0.00
Epoch 366 Iter 10 subLoss 4022.8 multi 9.96 import weight 0.00
Epoch 366 Iter 11 subLoss 4227.7 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 366 Acc: 97.12 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 422 train Loss: 3822.0 test Loss: 601.8
Epoch 367 Iter 0 subLoss 3968.3 multi 9.96 import weight 0.00
Epoch 367 Iter 1 subLoss 3413.4 multi -1.99 import weight 0.00
Epoch 367 Iter 2 subLoss 3519.8 multi -19.90 import weight 0.00
Epoch 367 Iter 3 subLoss 4518.8 multi 3.99 import weight 0.00
Epoch 367 Iter 4 subLoss 4234.7 multi 3.99 import weight 0.00
Epoch 367 Iter 5 subLoss 3871.1 multi -7.96 import weight 0.00
Epoch 367 Iter 6 subLoss 4807.4 multi 1.00 import weight 0.00
Epoch 367 Iter 7 subLoss 3968.2 multi 12.94 import weight 0.00
Epoch 367 Iter 8 subLoss 3197.1 multi -22.88 import weight 0.00
Epoch 367 Iter 9 subLoss 4445.3 multi 3.99 import weight 0.00
Epoch 367 Iter 10 subLoss 3944.6 multi 12.94 import weight 0.00
Epoch 367 Iter 11 subLoss 3473.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 367 Acc: 97.04 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 347 train Loss: 3869.7 test Loss: 627.0
Epoch 368 Iter 0 subLoss 3616.8 multi -13.93 import weight 0.00
Epoch 368 Iter 1 subLoss 4362.5 multi 3.99 import weight 0.00
Epoch 368 Iter 2 subLoss 3821.7 multi -4.97 import weight 0.00
Epoch 368 Iter 3 subLoss 4786.1 multi 3.99 import weight 0.00
Epoch 368 Iter 4 subLoss 3847.0 multi 1.00 import weight 0.00
Epoch 368 Iter 5 subLoss 3917.8 multi 6.97 import weight 0.00
Epoch 368 Iter 6 subLoss 3748.8 multi -1.98 import weight 0.00
Epoch 368 Iter 7 subLoss 4311.5 multi -10.94 import weight 0.00
Epoch 368 Iter 8 subLoss 3974.2 multi -7.96 import weight 0.00
Epoch 368 Iter 9 subLoss 4528.5 multi -1.98 import weight 0.00
Epoch 368 Iter 10 subLoss 4788.4 multi 6.97 import weight 0.00
Epoch 368 Iter 11 subLoss 4814.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 368 Acc: 95.10 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 481 train Loss: 4506.0 test Loss: 780.8
Epoch 369 Iter 0 subLoss 4892.4 multi 1.00 import weight 0.00
Epoch 369 Iter 1 subLoss 4374.3 multi -22.88 import weight 0.00
Epoch 369 Iter 2 subLoss 5504.3 multi 18.91 import weight 0.00
Epoch 369 Iter 3 subLoss 4385.0 multi 6.97 import weight 0.00
Epoch 369 Iter 4 subLoss 4485.1 multi 12.94 import weight 0.00
Epoch 369 Iter 5 subLoss 3585.9 multi 6.97 import weight 0.00
Epoch 369 Iter 6 subLoss 3730.0 multi 6.97 import weight 0.00
Epoch 369 Iter 7 subLoss 3300.3 multi 18.91 import weight 0.00
Epoch 369 Iter 8 subLoss 3250.3 multi 1.00 import weight 0.00
Epoch 369 Iter 9 subLoss 2781.1 multi -10.94 import weight 0.00
Epoch 369 Iter 10 subLoss 3852.7 multi -7.96 import weight 0.00
Epoch 369 Iter 11 subLoss 4872.0 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 369 Acc: 98.00 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 487 train Loss: 3523.0 test Loss: 495.0
Epoch 370 Iter 0 subLoss 3591.4 multi 3.99 import weight 0.00
Epoch 370 Iter 1 subLoss 3708.4 multi -10.94 import weight 0.00
Epoch 370 Iter 2 subLoss 3865.0 multi 12.94 import weight 0.00
Epoch 370 Iter 3 subLoss 3263.8 multi 1.00 import weight 0.00
Epoch 370 Iter 4 subLoss 2918.5 multi 9.96 import weight 0.00
Epoch 370 Iter 5 subLoss 3409.9 multi 1.00 import weight 0.00
Epoch 370 Iter 6 subLoss 2721.1 multi 1.00 import weight 0.00
Epoch 370 Iter 7 subLoss 3047.2 multi 1.00 import weight 0.00
Epoch 370 Iter 8 subLoss 3189.9 multi 6.97 import weight 1.00
Epoch 370 Iter 9 subLoss 2676.3 multi 21.90 import weight 0.00
Epoch 370 Iter 10 subLoss 2578.7 multi 21.90 import weight 0.00
Epoch 370 Iter 11 subLoss 2375.6 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 370 Acc: 98.21 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 237 train Loss: 2982.8 test Loss: 325.0
Epoch 371 Iter 0 subLoss 2403.1 multi -7.96 import weight 0.00
Epoch 371 Iter 1 subLoss 3668.4 multi 15.93 import weight 0.00
Epoch 371 Iter 2 subLoss 3364.6 multi 6.97 import weight 0.00
Epoch 371 Iter 3 subLoss 2430.6 multi 12.94 import weight 0.00
Epoch 371 Iter 4 subLoss 2761.7 multi -7.96 import weight 0.00
Epoch 371 Iter 5 subLoss 2536.1 multi 12.94 import weight 0.00
Epoch 371 Iter 6 subLoss 2464.7 multi 3.99 import weight 0.00
Epoch 371 Iter 7 subLoss 2422.9 multi 9.96 import weight 0.00
Epoch 371 Iter 8 subLoss 2297.4 multi -25.87 import weight 0.00
Epoch 371 Iter 9 subLoss 2882.6 multi -22.88 import weight 0.00
Epoch 371 Iter 10 subLoss 3605.8 multi -1.98 import weight 0.00
Epoch 371 Iter 11 subLoss 3982.0 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 371 Acc: 92.24 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 398 train Loss: 6405.7 test Loss: 1164.2
Epoch 372 Iter 0 subLoss 6482.8 multi -4.97 import weight 0.00
Epoch 372 Iter 1 subLoss 17942.1 multi -1.99 import weight 0.00
Epoch 372 Iter 2 subLoss 50430.8 multi 1.00 import weight 0.00
Epoch 372 Iter 3 subLoss 16829.6 multi 6.97 import weight 0.00
Epoch 372 Iter 4 subLoss 4596.5 multi 9.96 import weight 0.00
Epoch 372 Iter 5 subLoss 3398.1 multi 3.99 import weight 0.00
Epoch 372 Iter 6 subLoss 3151.9 multi -28.85 import weight 0.00
Epoch 372 Iter 7 subLoss 4407.3 multi -1.98 import weight 0.00
Epoch 372 Iter 8 subLoss 4423.7 multi -19.90 import weight 0.00
Epoch 372 Iter 9 subLoss 9210.1 multi -1.99 import weight 0.00
Epoch 372 Iter 10 subLoss 11436.9 multi 3.99 import weight 0.00
Epoch 372 Iter 11 subLoss 6406.0 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 372 Acc: 91.36 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 640 train Loss: 10689.8 test Loss: 1483.5
Epoch 373 Iter 0 subLoss 10120.5 multi -1.98 import weight 0.00
Epoch 373 Iter 1 subLoss 13431.9 multi 1.00 import weight 0.00
Epoch 373 Iter 2 subLoss 11545.7 multi 6.97 import weight 0.00
Epoch 373 Iter 3 subLoss 5719.0 multi 9.96 import weight 0.00
Epoch 373 Iter 4 subLoss 3729.4 multi 9.96 import weight 0.00
Epoch 373 Iter 5 subLoss 4070.6 multi -4.97 import weight 0.00
Epoch 373 Iter 6 subLoss 4142.2 multi -4.97 import weight 0.00
Epoch 373 Iter 7 subLoss 4331.4 multi 1.00 import weight 0.00
Epoch 373 Iter 8 subLoss 4088.5 multi 6.97 import weight 0.00
Epoch 373 Iter 9 subLoss 4111.0 multi -1.99 import weight 0.00
Epoch 373 Iter 10 subLoss 4024.1 multi 12.94 import weight 0.00
Epoch 373 Iter 11 subLoss 3604.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 373 Acc: 98.02 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 360 train Loss: 3628.2 test Loss: 420.0
Epoch 374 Iter 0 subLoss 3735.4 multi -19.90 import weight 0.00
Epoch 374 Iter 1 subLoss 4351.6 multi 1.00 import weight 0.00
Epoch 374 Iter 2 subLoss 4220.2 multi -4.97 import weight 0.00
Epoch 374 Iter 3 subLoss 4784.8 multi 9.96 import weight 0.00
Epoch 374 Iter 4 subLoss 3979.7 multi -4.97 import weight 0.00
Epoch 374 Iter 5 subLoss 4146.0 multi -1.99 import weight 0.00
Epoch 374 Iter 6 subLoss 4492.9 multi -7.96 import weight 0.00
Epoch 374 Iter 7 subLoss 4219.9 multi 9.96 import weight 0.00
Epoch 374 Iter 8 subLoss 4164.7 multi 3.99 import weight 0.00
Epoch 374 Iter 9 subLoss 4117.6 multi 1.00 import weight 0.00
Epoch 374 Iter 10 subLoss 4071.6 multi -1.98 import weight 0.00
Epoch 374 Iter 11 subLoss 3592.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 374 Acc: 97.68 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 359 train Loss: 3872.4 test Loss: 469.5
Epoch 375 Iter 0 subLoss 4342.9 multi 3.99 import weight 0.00
Epoch 375 Iter 1 subLoss 3350.9 multi -1.99 import weight 0.00
Epoch 375 Iter 2 subLoss 3623.4 multi 6.97 import weight 0.00
Epoch 375 Iter 3 subLoss 3206.4 multi -10.94 import weight 0.00
Epoch 375 Iter 4 subLoss 3546.6 multi -16.91 import weight 0.00
Epoch 375 Iter 5 subLoss 4990.1 multi 3.98 import weight 0.00
Epoch 375 Iter 6 subLoss 4585.4 multi 3.98 import weight 0.00
Epoch 375 Iter 7 subLoss 4154.6 multi -13.93 import weight 0.00
Epoch 375 Iter 8 subLoss 5096.9 multi 12.94 import weight 0.00
Epoch 375 Iter 9 subLoss 4760.8 multi -13.93 import weight 0.00
Epoch 375 Iter 10 subLoss 5572.0 multi -4.97 import weight 0.00
Epoch 375 Iter 11 subLoss 7561.9 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 375 Acc: 77.54 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 756 train Loss: 29645.0 test Loss: 4872.3
Epoch 376 Iter 0 subLoss 29168.1 multi 1.00 import weight 0.00
Epoch 376 Iter 1 subLoss 10451.6 multi 3.99 import weight 0.00
Epoch 376 Iter 2 subLoss 5428.9 multi 1.00 import weight 0.00
Epoch 376 Iter 3 subLoss 5498.6 multi -7.96 import weight 0.00
Epoch 376 Iter 4 subLoss 6892.7 multi -1.98 import weight 0.00
Epoch 376 Iter 5 subLoss 7669.6 multi 9.96 import weight 0.00
Epoch 376 Iter 6 subLoss 4361.9 multi 3.98 import weight 0.00
Epoch 376 Iter 7 subLoss 4721.5 multi 1.00 import weight 0.00
Epoch 376 Iter 8 subLoss 4873.7 multi 15.93 import weight 0.00
Epoch 376 Iter 9 subLoss 4534.9 multi -4.97 import weight 0.00
Epoch 376 Iter 10 subLoss 3947.3 multi 15.93 import weight 0.00
Epoch 376 Iter 11 subLoss 3676.7 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 376 Acc: 97.68 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 367 train Loss: 4012.8 test Loss: 472.2
Epoch 377 Iter 0 subLoss 3956.8 multi -16.91 import weight 0.00
Epoch 377 Iter 1 subLoss 5157.0 multi 3.99 import weight 0.00
Epoch 377 Iter 2 subLoss 4803.3 multi 3.98 import weight 0.00
Epoch 377 Iter 3 subLoss 3637.2 multi -4.97 import weight 0.00
Epoch 377 Iter 4 subLoss 4615.0 multi -13.93 import weight 0.00
Epoch 377 Iter 5 subLoss 5494.8 multi -4.97 import weight 0.00
Epoch 377 Iter 6 subLoss 6998.5 multi -7.96 import weight 0.00
Epoch 377 Iter 7 subLoss 16461.7 multi 3.98 import weight 0.00
Epoch 377 Iter 8 subLoss 5394.7 multi -13.93 import weight 0.00
Epoch 377 Iter 9 subLoss 7349.7 multi -7.96 import weight 0.00
Epoch 377 Iter 10 subLoss 14578.8 multi 1.00 import weight 0.00
Epoch 377 Iter 11 subLoss 9904.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 377 Acc: 89.67 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 990 train Loss: 9314.9 test Loss: 1588.8
Epoch 378 Iter 0 subLoss 9606.8 multi -1.98 import weight 0.00
Epoch 378 Iter 1 subLoss 12407.0 multi 3.99 import weight 0.00
Epoch 378 Iter 2 subLoss 6745.1 multi -4.97 import weight 0.00
Epoch 378 Iter 3 subLoss 7066.7 multi 3.99 import weight 0.00
Epoch 378 Iter 4 subLoss 6525.7 multi 6.97 import weight 0.00
Epoch 378 Iter 5 subLoss 5361.0 multi 12.94 import weight 0.00
Epoch 378 Iter 6 subLoss 4789.9 multi 12.94 import weight 0.00
Epoch 378 Iter 7 subLoss 4077.0 multi 1.00 import weight 0.00
Epoch 378 Iter 8 subLoss 4388.1 multi 9.96 import weight 0.00
Epoch 378 Iter 9 subLoss 3482.0 multi 9.96 import weight 0.00
Epoch 378 Iter 10 subLoss 3684.3 multi 12.94 import weight 0.00
Epoch 378 Iter 11 subLoss 3587.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 378 Acc: 98.19 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 358 train Loss: 3367.3 test Loss: 356.7
Epoch 379 Iter 0 subLoss 3444.9 multi -19.90 import weight 0.00
Epoch 379 Iter 1 subLoss 3354.7 multi 1.00 import weight 0.00
Epoch 379 Iter 2 subLoss 3847.0 multi 3.98 import weight 0.00
Epoch 379 Iter 3 subLoss 3154.2 multi -25.87 import weight 0.00
Epoch 379 Iter 4 subLoss 4506.6 multi -4.97 import weight 0.00
Epoch 379 Iter 5 subLoss 4717.7 multi 1.00 import weight 0.00
Epoch 379 Iter 6 subLoss 4839.5 multi -7.96 import weight 0.00
Epoch 379 Iter 7 subLoss 6591.4 multi 9.96 import weight 0.00
Epoch 379 Iter 8 subLoss 3884.0 multi -7.96 import weight 0.00
Epoch 379 Iter 9 subLoss 4380.2 multi 12.94 import weight 0.00
Epoch 379 Iter 10 subLoss 3532.1 multi 1.00 import weight 0.00
Epoch 379 Iter 11 subLoss 3954.8 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 379 Acc: 97.16 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 395 train Loss: 4428.1 test Loss: 587.8
Epoch 380 Iter 0 subLoss 4173.5 multi -7.96 import weight 0.00
Epoch 380 Iter 1 subLoss 4997.1 multi 6.97 import weight 0.00
Epoch 380 Iter 2 subLoss 3897.8 multi 9.96 import weight 0.00
Epoch 380 Iter 3 subLoss 3587.3 multi 12.94 import weight 0.00
Epoch 380 Iter 4 subLoss 3495.8 multi -4.97 import weight 0.00
Epoch 380 Iter 5 subLoss 4040.9 multi -10.94 import weight 0.00
Epoch 380 Iter 6 subLoss 5088.5 multi -7.96 import weight 0.00
Epoch 380 Iter 7 subLoss 5172.8 multi 3.99 import weight 0.00
Epoch 380 Iter 8 subLoss 4978.3 multi -1.99 import weight 0.00
Epoch 380 Iter 9 subLoss 4414.6 multi 15.93 import weight 0.00
Epoch 380 Iter 10 subLoss 3688.8 multi 15.93 import weight 0.00
Epoch 380 Iter 11 subLoss 3522.0 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 380 Acc: 98.11 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 352 train Loss: 3455.3 test Loss: 350.8
Epoch 381 Iter 0 subLoss 3576.0 multi -10.94 import weight 0.00
Epoch 381 Iter 1 subLoss 3465.7 multi 3.98 import weight 0.00
Epoch 381 Iter 2 subLoss 3454.7 multi 9.96 import weight 0.00
Epoch 381 Iter 3 subLoss 3115.0 multi -16.91 import weight 0.00
Epoch 381 Iter 4 subLoss 3620.3 multi 9.96 import weight 0.00
Epoch 381 Iter 5 subLoss 3716.7 multi 12.94 import weight 0.00
Epoch 381 Iter 6 subLoss 3117.5 multi -13.93 import weight 0.00
Epoch 381 Iter 7 subLoss 3394.4 multi 6.97 import weight 0.00
Epoch 381 Iter 8 subLoss 3400.3 multi -1.99 import weight 0.00
Epoch 381 Iter 9 subLoss 3604.3 multi 1.00 import weight 0.00
Epoch 381 Iter 10 subLoss 3329.0 multi 9.96 import weight 0.00
Epoch 381 Iter 11 subLoss 3378.7 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 381 Acc: 98.03 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 337 train Loss: 3195.6 test Loss: 345.4
Epoch 382 Iter 0 subLoss 2726.5 multi 3.99 import weight 0.00
Epoch 382 Iter 1 subLoss 2914.7 multi 12.94 import weight 0.00
Epoch 382 Iter 2 subLoss 2899.3 multi 24.88 import weight 0.00
Epoch 382 Iter 3 subLoss 3025.3 multi -25.87 import weight 0.00
Epoch 382 Iter 4 subLoss 3273.0 multi -1.98 import weight 0.00
Epoch 382 Iter 5 subLoss 2988.5 multi 3.99 import weight 0.00
Epoch 382 Iter 6 subLoss 2918.3 multi 15.93 import weight 0.00
Epoch 382 Iter 7 subLoss 2938.7 multi 12.94 import weight 0.00
Epoch 382 Iter 8 subLoss 3025.3 multi -22.88 import weight 0.00
Epoch 382 Iter 9 subLoss 4309.4 multi 18.91 import weight 0.00
Epoch 382 Iter 10 subLoss 3797.6 multi -16.91 import weight 0.00
Epoch 382 Iter 11 subLoss 9198.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 382 Acc: 98.03 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 919 train Loss: 3620.0 test Loss: 374.2
Epoch 383 Iter 0 subLoss 3300.0 multi 3.99 import weight 0.00
Epoch 383 Iter 1 subLoss 3346.9 multi 1.00 import weight 0.00
Epoch 383 Iter 2 subLoss 3608.0 multi 3.99 import weight 0.00
Epoch 383 Iter 3 subLoss 3181.6 multi 9.96 import weight 1.00
Epoch 383 Iter 4 subLoss 2766.0 multi -4.97 import weight 0.00
Epoch 383 Iter 5 subLoss 2879.8 multi 6.97 import weight 0.00
Epoch 383 Iter 6 subLoss 3200.0 multi -7.96 import weight 0.00
Epoch 383 Iter 7 subLoss 2788.8 multi -7.96 import weight 0.00
Epoch 383 Iter 8 subLoss 2874.1 multi 9.96 import weight 0.00
Epoch 383 Iter 9 subLoss 2938.3 multi 15.93 import weight 0.00
Epoch 383 Iter 10 subLoss 3049.3 multi 3.99 import weight 0.00
Epoch 383 Iter 11 subLoss 2514.7 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 383 Acc: 98.60 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 251 train Loss: 2890.9 test Loss: 264.5
Epoch 384 Iter 0 subLoss 2858.8 multi -28.85 import weight 0.00
Epoch 384 Iter 1 subLoss 4952.1 multi -4.97 import weight 0.00
Epoch 384 Iter 2 subLoss 9801.2 multi -4.97 import weight 0.00
Epoch 384 Iter 3 subLoss 75570.3 multi 1.00 import weight 0.00
Epoch 384 Iter 4 subLoss 9751.3 multi 6.97 import weight 0.00
Epoch 384 Iter 5 subLoss 3389.0 multi -4.97 import weight 0.00
Epoch 384 Iter 6 subLoss 3787.4 multi -4.97 import weight 0.00
Epoch 384 Iter 7 subLoss 4342.9 multi 6.97 import weight 0.00
Epoch 384 Iter 8 subLoss 3201.6 multi -4.97 import weight 0.00
Epoch 384 Iter 9 subLoss 3645.0 multi 15.93 import weight 0.00
Epoch 384 Iter 10 subLoss 3333.4 multi -19.90 import weight 0.00
Epoch 384 Iter 11 subLoss 3378.6 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 384 Acc: 98.09 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 337 train Loss: 3378.8 test Loss: 324.2
Epoch 385 Iter 0 subLoss 3353.8 multi 1.00 import weight 0.00
Epoch 385 Iter 1 subLoss 3416.8 multi -4.97 import weight 0.00
Epoch 385 Iter 2 subLoss 3172.3 multi 33.84 import weight 0.00
Epoch 385 Iter 3 subLoss 3390.2 multi 6.97 import weight 0.00
Epoch 385 Iter 4 subLoss 3143.2 multi 18.91 import weight 0.00
Epoch 385 Iter 5 subLoss 3188.9 multi 9.96 import weight 1.00
Epoch 385 Iter 6 subLoss 2521.4 multi -1.99 import weight 0.00
Epoch 385 Iter 7 subLoss 2339.6 multi 12.94 import weight 0.00
Epoch 385 Iter 8 subLoss 2632.8 multi 1.00 import weight 0.00
Epoch 385 Iter 9 subLoss 2944.4 multi -10.94 import weight 0.00
Epoch 385 Iter 10 subLoss 2386.5 multi 15.93 import weight 0.00
Epoch 385 Iter 11 subLoss 2753.3 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 385 Acc: 98.72 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 275 train Loss: 2678.5 test Loss: 221.8
Epoch 386 Iter 0 subLoss 2965.4 multi 12.94 import weight 0.00
Epoch 386 Iter 1 subLoss 2868.1 multi 6.97 import weight 0.00
Epoch 386 Iter 2 subLoss 2445.1 multi -19.90 import weight 0.00
Epoch 386 Iter 3 subLoss 2634.5 multi 3.99 import weight 0.00
Epoch 386 Iter 4 subLoss 2024.4 multi 15.93 import weight 0.00
Epoch 386 Iter 5 subLoss 2531.3 multi 12.94 import weight 0.00
Epoch 386 Iter 6 subLoss 2099.8 multi 6.97 import weight 0.00
Epoch 386 Iter 7 subLoss 2310.2 multi 1.00 import weight 0.00
Epoch 386 Iter 8 subLoss 2743.9 multi -10.94 import weight 0.00
Epoch 386 Iter 9 subLoss 2249.9 multi 6.97 import weight 0.00
Epoch 386 Iter 10 subLoss 2388.2 multi 18.91 import weight 0.00
Epoch 386 Iter 11 subLoss 2719.2 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 386 Acc: 98.75 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 271 train Loss: 2588.6 test Loss: 222.1
Epoch 387 Iter 0 subLoss 2292.2 multi -22.88 import weight 0.00
Epoch 387 Iter 1 subLoss 2521.0 multi 1.00 import weight 0.00
Epoch 387 Iter 2 subLoss 2791.9 multi -1.99 import weight 0.00
Epoch 387 Iter 3 subLoss 3116.3 multi -10.94 import weight 0.00
Epoch 387 Iter 4 subLoss 5876.6 multi -1.99 import weight 0.00
Epoch 387 Iter 5 subLoss 9483.5 multi 3.99 import weight 0.00
Epoch 387 Iter 6 subLoss 2270.7 multi 1.00 import weight 0.00
Epoch 387 Iter 7 subLoss 2847.7 multi 21.90 import weight 0.00
Epoch 387 Iter 8 subLoss 2656.0 multi 15.93 import weight 0.00
Epoch 387 Iter 9 subLoss 2364.4 multi -4.97 import weight 0.00
Epoch 387 Iter 10 subLoss 2611.5 multi -4.97 import weight 0.00
Epoch 387 Iter 11 subLoss 2236.8 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 387 Acc: 98.66 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 223 train Loss: 2614.5 test Loss: 246.5
Epoch 388 Iter 0 subLoss 2894.4 multi 27.87 import weight 0.00
Epoch 388 Iter 1 subLoss 2804.6 multi 6.97 import weight 0.00
Epoch 388 Iter 2 subLoss 2166.5 multi 9.96 import weight 0.00
Epoch 388 Iter 3 subLoss 2508.2 multi -7.96 import weight 0.00
Epoch 388 Iter 4 subLoss 2143.3 multi 3.99 import weight 0.00
Epoch 388 Iter 5 subLoss 2852.0 multi -28.85 import weight 0.00
Epoch 388 Iter 6 subLoss 2779.3 multi 21.90 import weight 0.00
Epoch 388 Iter 7 subLoss 2598.5 multi 21.90 import weight 0.00
Epoch 388 Iter 8 subLoss 2360.2 multi -1.99 import weight 0.00
Epoch 388 Iter 9 subLoss 2642.0 multi -28.85 import weight 0.00
Epoch 388 Iter 10 subLoss 4614.3 multi -10.94 import weight 0.00
Epoch 388 Iter 11 subLoss 31176.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 388 Acc: 93.36 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3117 train Loss: 8338.8 test Loss: 911.5
Epoch 389 Iter 0 subLoss 7887.5 multi -1.98 import weight 0.00
Epoch 389 Iter 1 subLoss 14075.2 multi 1.00 import weight 0.00
Epoch 389 Iter 2 subLoss 9291.0 multi 1.00 import weight 0.00
Epoch 389 Iter 3 subLoss 5756.6 multi 1.00 import weight 0.00
Epoch 389 Iter 4 subLoss 4951.4 multi -1.99 import weight 0.00
Epoch 389 Iter 5 subLoss 6877.8 multi -1.99 import weight 0.00
Epoch 389 Iter 6 subLoss 10125.5 multi 1.00 import weight 0.00
Epoch 389 Iter 7 subLoss 8347.4 multi -4.97 import weight 0.00
Epoch 389 Iter 8 subLoss 25858.2 multi 3.99 import weight 0.00
Epoch 389 Iter 9 subLoss 4989.3 multi -4.97 import weight 0.00
Epoch 389 Iter 10 subLoss 7949.8 multi -1.98 import weight 0.00
Epoch 389 Iter 11 subLoss 15338.7 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 389 Acc: 98.19 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1533 train Loss: 3690.0 test Loss: 394.9
Epoch 390 Iter 0 subLoss 3710.6 multi 15.93 import weight 0.00
Epoch 390 Iter 1 subLoss 2983.0 multi 6.97 import weight 0.00
Epoch 390 Iter 2 subLoss 2512.2 multi -4.97 import weight 0.00
Epoch 390 Iter 3 subLoss 2387.3 multi 21.90 import weight 0.00
Epoch 390 Iter 4 subLoss 2857.2 multi -25.87 import weight 0.00
Epoch 390 Iter 5 subLoss 3708.6 multi -7.96 import weight 0.00
Epoch 390 Iter 6 subLoss 5689.8 multi 3.99 import weight 0.00
Epoch 390 Iter 7 subLoss 3820.0 multi 6.97 import weight 0.00
Epoch 390 Iter 8 subLoss 2278.0 multi 3.99 import weight 0.00
Epoch 390 Iter 9 subLoss 2370.0 multi 1.00 import weight 0.00
Epoch 390 Iter 10 subLoss 2531.4 multi 12.94 import weight 0.00
Epoch 390 Iter 11 subLoss 2444.6 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 390 Acc: 98.40 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 244 train Loss: 2928.2 test Loss: 249.1
Epoch 391 Iter 0 subLoss 3012.8 multi 27.87 import weight 0.00
Epoch 391 Iter 1 subLoss 2625.0 multi 3.99 import weight 0.00
Epoch 391 Iter 2 subLoss 2616.0 multi -1.99 import weight 0.00
Epoch 391 Iter 3 subLoss 2678.0 multi 24.88 import weight 0.00
Epoch 391 Iter 4 subLoss 2042.9 multi 15.93 import weight 0.00
Epoch 391 Iter 5 subLoss 2276.4 multi 6.97 import weight 0.00
Epoch 391 Iter 6 subLoss 2461.3 multi 6.97 import weight 0.00
Epoch 391 Iter 7 subLoss 2763.8 multi -4.97 import weight 0.00
Epoch 391 Iter 8 subLoss 2207.6 multi 18.91 import weight 0.00
Epoch 391 Iter 9 subLoss 2128.0 multi 9.96 import weight 0.00
Epoch 391 Iter 10 subLoss 2236.1 multi -1.99 import weight 0.00
Epoch 391 Iter 11 subLoss 2230.0 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 391 Acc: 98.79 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 222 train Loss: 2286.9 test Loss: 194.7
Epoch 392 Iter 0 subLoss 2137.9 multi -7.96 import weight 0.00
Epoch 392 Iter 1 subLoss 2472.7 multi -13.93 import weight 0.00
Epoch 392 Iter 2 subLoss 3066.6 multi -7.96 import weight 0.00
Epoch 392 Iter 3 subLoss 4739.9 multi -1.99 import weight 0.00
Epoch 392 Iter 4 subLoss 6406.7 multi -4.97 import weight 0.00
Epoch 392 Iter 5 subLoss 20959.5 multi 3.99 import weight 0.00
Epoch 392 Iter 6 subLoss 2477.7 multi -10.94 import weight 0.00
Epoch 392 Iter 7 subLoss 2628.8 multi 3.99 import weight 0.00
Epoch 392 Iter 8 subLoss 2807.5 multi 9.96 import weight 0.00
Epoch 392 Iter 9 subLoss 2281.4 multi 12.94 import weight 0.00
Epoch 392 Iter 10 subLoss 2833.7 multi 18.91 import weight 0.00
Epoch 392 Iter 11 subLoss 2320.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 392 Acc: 98.75 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 231 train Loss: 2495.7 test Loss: 202.3
Epoch 393 Iter 0 subLoss 2825.8 multi -1.99 import weight 0.00
Epoch 393 Iter 1 subLoss 2249.7 multi 3.99 import weight 0.00
Epoch 393 Iter 2 subLoss 2050.2 multi -7.96 import weight 0.00
Epoch 393 Iter 3 subLoss 2150.0 multi 3.98 import weight 0.00
Epoch 393 Iter 4 subLoss 2297.8 multi -22.88 import weight 0.00
Epoch 393 Iter 5 subLoss 2587.5 multi -19.90 import weight 0.00
Epoch 393 Iter 6 subLoss 3233.3 multi 27.87 import weight 0.00
Epoch 393 Iter 7 subLoss 3410.4 multi -1.99 import weight 0.00
Epoch 393 Iter 8 subLoss 3231.8 multi 30.85 import weight 0.00
Epoch 393 Iter 9 subLoss 2740.1 multi -7.96 import weight 0.00
Epoch 393 Iter 10 subLoss 5896.4 multi 1.00 import weight 0.00
Epoch 393 Iter 11 subLoss 4535.1 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 393 Acc: 95.00 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 453 train Loss: 6422.7 test Loss: 760.9
Epoch 394 Iter 0 subLoss 5962.9 multi -1.98 import weight 0.00
Epoch 394 Iter 1 subLoss 11441.0 multi 3.99 import weight 0.00
Epoch 394 Iter 2 subLoss 2773.8 multi 21.90 import weight 0.00
Epoch 394 Iter 3 subLoss 2522.3 multi 1.00 import weight 0.00
Epoch 394 Iter 4 subLoss 2195.3 multi -13.93 import weight 0.00
Epoch 394 Iter 5 subLoss 2331.1 multi 15.93 import weight 0.00
Epoch 394 Iter 6 subLoss 2183.6 multi 27.87 import weight 0.00
Epoch 394 Iter 7 subLoss 2408.3 multi -4.97 import weight 0.00
Epoch 394 Iter 8 subLoss 2412.6 multi -1.98 import weight 0.00
Epoch 394 Iter 9 subLoss 2122.0 multi 12.94 import weight 0.00
Epoch 394 Iter 10 subLoss 2057.5 multi -4.97 import weight 0.00
Epoch 394 Iter 11 subLoss 2663.6 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 394 Acc: 98.74 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 266 train Loss: 2408.6 test Loss: 193.1
Epoch 395 Iter 0 subLoss 2367.7 multi 3.98 import weight 0.00
Epoch 395 Iter 1 subLoss 2147.8 multi 6.97 import weight 0.00
Epoch 395 Iter 2 subLoss 2290.5 multi -19.90 import weight 0.00
Epoch 395 Iter 3 subLoss 2561.4 multi -7.96 import weight 0.00
Epoch 395 Iter 4 subLoss 2162.1 multi 12.94 import weight 0.00
Epoch 395 Iter 5 subLoss 2576.7 multi 21.90 import weight 0.00
Epoch 395 Iter 6 subLoss 2126.4 multi 15.93 import weight 0.00
Epoch 395 Iter 7 subLoss 2160.0 multi 15.93 import weight 0.00
Epoch 395 Iter 8 subLoss 2095.8 multi 9.96 import weight 0.00
Epoch 395 Iter 9 subLoss 1896.7 multi -1.98 import weight 0.00
Epoch 395 Iter 10 subLoss 2214.2 multi -4.97 import weight 0.00
Epoch 395 Iter 11 subLoss 2345.7 multi -22.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 395 Acc: 98.50 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -22.88 Pidx 234 train Loss: 2466.8 test Loss: 204.0
Epoch 396 Iter 0 subLoss 2570.9 multi 24.88 import weight 0.00
Epoch 396 Iter 1 subLoss 2544.5 multi -10.94 import weight 0.00
Epoch 396 Iter 2 subLoss 2312.4 multi 6.97 import weight 0.00
Epoch 396 Iter 3 subLoss 2622.3 multi 6.97 import weight 0.00
Epoch 396 Iter 4 subLoss 2333.2 multi 18.91 import weight 0.00
Epoch 396 Iter 5 subLoss 1745.0 multi -10.94 import weight 0.00
Epoch 396 Iter 6 subLoss 2257.6 multi 1.00 import weight 0.00
Epoch 396 Iter 7 subLoss 2130.9 multi -10.94 import weight 0.00
Epoch 396 Iter 8 subLoss 2134.1 multi -7.96 import weight 0.00
Epoch 396 Iter 9 subLoss 2647.1 multi -25.87 import weight 0.00
Epoch 396 Iter 10 subLoss 3249.5 multi -16.91 import weight 0.00
Epoch 396 Iter 11 subLoss 22262.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 396 Acc: 95.41 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2226 train Loss: 6295.6 test Loss: 718.4
Epoch 397 Iter 0 subLoss 5982.4 multi -7.96 import weight 0.00
Epoch 397 Iter 1 subLoss 31334.0 multi 1.00 import weight 0.00
Epoch 397 Iter 2 subLoss 7330.9 multi 3.99 import weight 0.00
Epoch 397 Iter 3 subLoss 3085.5 multi -1.99 import weight 0.00
Epoch 397 Iter 4 subLoss 3865.9 multi 15.93 import weight 0.00
Epoch 397 Iter 5 subLoss 2513.6 multi -1.98 import weight 0.00
Epoch 397 Iter 6 subLoss 2369.0 multi 6.97 import weight 0.00
Epoch 397 Iter 7 subLoss 1908.5 multi -7.96 import weight 0.00
Epoch 397 Iter 8 subLoss 2721.3 multi 3.99 import weight 0.00
Epoch 397 Iter 9 subLoss 2380.0 multi -19.90 import weight 0.00
Epoch 397 Iter 10 subLoss 3373.6 multi 9.96 import weight 0.00
Epoch 397 Iter 11 subLoss 2065.2 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 397 Acc: 98.58 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 206 train Loss: 2823.3 test Loss: 285.6
Epoch 398 Iter 0 subLoss 2672.7 multi 24.88 import weight 0.00
Epoch 398 Iter 1 subLoss 2336.2 multi 21.90 import weight 0.00
Epoch 398 Iter 2 subLoss 2014.3 multi 1.00 import weight 0.00
Epoch 398 Iter 3 subLoss 1806.4 multi 3.98 import weight 0.00
Epoch 398 Iter 4 subLoss 2669.6 multi -4.97 import weight 0.00
Epoch 398 Iter 5 subLoss 2660.4 multi -1.99 import weight 0.00
Epoch 398 Iter 6 subLoss 1973.2 multi 18.91 import weight 0.00
Epoch 398 Iter 7 subLoss 2071.7 multi -7.96 import weight 0.00
Epoch 398 Iter 8 subLoss 2320.1 multi -10.94 import weight 0.00
Epoch 398 Iter 9 subLoss 3104.3 multi 1.00 import weight 0.00
Epoch 398 Iter 10 subLoss 2812.4 multi -28.85 import weight 0.00
Epoch 398 Iter 11 subLoss 14347.8 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 398 Acc: 37.56 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 1434 train Loss: 294289.0 test Loss: 52524.3
Epoch 399 Iter 0 subLoss 281223.8 multi 1.00 import weight 0.00
Epoch 399 Iter 1 subLoss 16699.0 multi 6.97 import weight 0.00
Epoch 399 Iter 2 subLoss 3109.4 multi 3.99 import weight 0.00
Epoch 399 Iter 3 subLoss 3130.2 multi 3.99 import weight 0.00
Epoch 399 Iter 4 subLoss 2703.0 multi 12.94 import weight 0.00
Epoch 399 Iter 5 subLoss 2440.0 multi -13.93 import weight 0.00
Epoch 399 Iter 6 subLoss 2607.0 multi -13.93 import weight 0.00
Epoch 399 Iter 7 subLoss 3412.0 multi 1.00 import weight 0.00
Epoch 399 Iter 8 subLoss 3779.1 multi 3.99 import weight 0.00
Epoch 399 Iter 9 subLoss 2810.7 multi -25.87 import weight 0.00
Epoch 399 Iter 10 subLoss 6579.3 multi 9.96 import weight 0.00
Epoch 399 Iter 11 subLoss 3330.5 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 399 Acc: 97.90 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 333 train Loss: 4501.0 test Loss: 376.8
Epoch 400 Iter 0 subLoss 4591.7 multi 9.96 import weight 0.00
Epoch 400 Iter 1 subLoss 2742.0 multi -4.97 import weight 0.00
Epoch 400 Iter 2 subLoss 2893.1 multi 30.85 import weight 0.00
Epoch 400 Iter 3 subLoss 2963.1 multi 15.93 import weight 0.00
Epoch 400 Iter 4 subLoss 2302.1 multi -7.96 import weight 0.00
Epoch 400 Iter 5 subLoss 2435.7 multi 12.94 import weight 0.00
Epoch 400 Iter 6 subLoss 2633.3 multi -1.99 import weight 0.00
Epoch 400 Iter 7 subLoss 2563.1 multi -4.97 import weight 0.00
Epoch 400 Iter 8 subLoss 2766.6 multi -1.99 import weight 0.00
Epoch 400 Iter 9 subLoss 2558.0 multi -1.99 import weight 0.00
Epoch 400 Iter 10 subLoss 2384.5 multi 21.90 import weight 0.00
Epoch 400 Iter 11 subLoss 2178.7 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 400 Acc: 97.70 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 217 train Loss: 3552.4 test Loss: 383.5
Epoch 401 Iter 0 subLoss 3114.3 multi -13.93 import weight 0.00
Epoch 401 Iter 1 subLoss 7528.9 multi -4.97 import weight 0.00
Epoch 401 Iter 2 subLoss 37043.3 multi 1.00 import weight 0.00
Epoch 401 Iter 3 subLoss 9063.1 multi -10.94 import weight 0.00
Epoch 401 Iter 4 subLoss 146969.3 multi 1.00 import weight 0.00
Epoch 401 Iter 5 subLoss 13473.5 multi -1.99 import weight 0.00
Epoch 401 Iter 6 subLoss 20095.4 multi 1.00 import weight 0.00
Epoch 401 Iter 7 subLoss 14373.1 multi 1.00 import weight 0.00
Epoch 401 Iter 8 subLoss 11254.1 multi 6.97 import weight 0.00
Epoch 401 Iter 9 subLoss 4355.9 multi -1.98 import weight 0.00
Epoch 401 Iter 10 subLoss 4454.8 multi -7.96 import weight 0.00
Epoch 401 Iter 11 subLoss 5411.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 401 Acc: 95.31 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 541 train Loss: 5656.4 test Loss: 874.6
Epoch 402 Iter 0 subLoss 4931.1 multi 12.94 import weight 0.00
Epoch 402 Iter 1 subLoss 3324.1 multi 12.94 import weight 0.00
Epoch 402 Iter 2 subLoss 2663.2 multi 1.00 import weight 0.00
Epoch 402 Iter 3 subLoss 2839.0 multi 18.91 import weight 0.00
Epoch 402 Iter 4 subLoss 2517.4 multi 1.00 import weight 0.00
Epoch 402 Iter 5 subLoss 2584.7 multi -22.88 import weight 0.00
Epoch 402 Iter 6 subLoss 3515.5 multi -16.91 import weight 0.00
Epoch 402 Iter 7 subLoss 6367.4 multi 1.00 import weight 0.00
Epoch 402 Iter 8 subLoss 5396.2 multi -10.94 import weight 0.00
Epoch 402 Iter 9 subLoss 20909.4 multi 1.00 import weight 0.00
Epoch 402 Iter 10 subLoss 9906.9 multi 3.99 import weight 0.00
Epoch 402 Iter 11 subLoss 4285.3 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 402 Acc: 95.87 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 428 train Loss: 5036.5 test Loss: 696.9
Epoch 403 Iter 0 subLoss 5032.4 multi 33.84 import weight 0.00
Epoch 403 Iter 1 subLoss 6918.5 multi 18.91 import weight 0.00
Epoch 403 Iter 2 subLoss 5505.6 multi 15.93 import weight 0.00
Epoch 403 Iter 3 subLoss 2606.1 multi -10.94 import weight 0.00
Epoch 403 Iter 4 subLoss 3401.0 multi -1.99 import weight 0.00
Epoch 403 Iter 5 subLoss 4078.8 multi 3.99 import weight 0.00
Epoch 403 Iter 6 subLoss 3378.5 multi 12.94 import weight 0.00
Epoch 403 Iter 7 subLoss 3004.8 multi 6.97 import weight 0.00
Epoch 403 Iter 8 subLoss 2482.2 multi 3.99 import weight 0.00
Epoch 403 Iter 9 subLoss 2522.3 multi -1.99 import weight 0.00
Epoch 403 Iter 10 subLoss 2682.7 multi -7.96 import weight 0.00
Epoch 403 Iter 11 subLoss 2805.3 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 403 Acc: 98.52 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 280 train Loss: 2590.5 test Loss: 251.9
Epoch 404 Iter 0 subLoss 2560.6 multi -4.97 import weight 0.00
Epoch 404 Iter 1 subLoss 2576.6 multi 21.90 import weight 0.00
Epoch 404 Iter 2 subLoss 2526.7 multi 1.00 import weight 0.00
Epoch 404 Iter 3 subLoss 2320.5 multi -7.96 import weight 0.00
Epoch 404 Iter 4 subLoss 2734.2 multi 3.99 import weight 0.00
Epoch 404 Iter 5 subLoss 2577.7 multi 24.88 import weight 0.00
Epoch 404 Iter 6 subLoss 2474.1 multi -7.96 import weight 0.00
Epoch 404 Iter 7 subLoss 2096.5 multi 12.94 import weight 0.00
Epoch 404 Iter 8 subLoss 2376.4 multi -16.91 import weight 0.00
Epoch 404 Iter 9 subLoss 2841.7 multi 18.91 import weight 0.00
Epoch 404 Iter 10 subLoss 2575.7 multi 27.87 import weight 0.00
Epoch 404 Iter 11 subLoss 2310.3 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 404 Acc: 98.44 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 231 train Loss: 2268.0 test Loss: 239.2
Epoch 405 Iter 0 subLoss 2252.1 multi 3.99 import weight 0.00
Epoch 405 Iter 1 subLoss 2508.7 multi -4.97 import weight 0.00
Epoch 405 Iter 2 subLoss 2047.5 multi 18.91 import weight 0.00
Epoch 405 Iter 3 subLoss 2550.4 multi 1.00 import weight 0.00
Epoch 405 Iter 4 subLoss 2193.5 multi -13.93 import weight 0.00
Epoch 405 Iter 5 subLoss 2108.0 multi -16.91 import weight 0.00
Epoch 405 Iter 6 subLoss 2495.3 multi 1.00 import weight 0.00
Epoch 405 Iter 7 subLoss 2231.8 multi -1.99 import weight 0.00
Epoch 405 Iter 8 subLoss 2631.5 multi 1.00 import weight 0.00
Epoch 405 Iter 9 subLoss 1778.8 multi 1.00 import weight 0.00
Epoch 405 Iter 10 subLoss 2450.4 multi -7.96 import weight 0.00
Epoch 405 Iter 11 subLoss 2731.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 405 Acc: 98.40 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 273 train Loss: 2439.3 test Loss: 253.4
Epoch 406 Iter 0 subLoss 2550.8 multi 3.99 import weight 0.00
Epoch 406 Iter 1 subLoss 2838.5 multi 21.90 import weight 0.00
Epoch 406 Iter 2 subLoss 2764.7 multi 1.00 import weight 0.00
Epoch 406 Iter 3 subLoss 2224.0 multi -7.96 import weight 0.00
Epoch 406 Iter 4 subLoss 2169.2 multi 18.91 import weight 0.00
Epoch 406 Iter 5 subLoss 2408.4 multi -1.99 import weight 0.00
Epoch 406 Iter 6 subLoss 1926.5 multi 1.00 import weight 0.00
Epoch 406 Iter 7 subLoss 2530.5 multi 6.97 import weight 0.00
Epoch 406 Iter 8 subLoss 2048.5 multi 21.90 import weight 0.00
Epoch 406 Iter 9 subLoss 1988.7 multi -10.94 import weight 0.00
Epoch 406 Iter 10 subLoss 2291.4 multi -16.91 import weight 0.00
Epoch 406 Iter 11 subLoss 4372.0 multi -22.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 406 Acc: 79.45 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -22.88 Pidx 437 train Loss: 39662.2 test Loss: 6553.8
Epoch 407 Iter 0 subLoss 42136.9 multi 1.00 import weight 0.00
Epoch 407 Iter 1 subLoss 18835.1 multi 6.97 import weight 0.00
Epoch 407 Iter 2 subLoss 5010.4 multi 6.97 import weight 0.00
Epoch 407 Iter 3 subLoss 2529.9 multi 3.99 import weight 0.00
Epoch 407 Iter 4 subLoss 2824.9 multi -4.97 import weight 0.00
Epoch 407 Iter 5 subLoss 3193.6 multi -28.85 import weight 0.00
Epoch 407 Iter 6 subLoss 6634.3 multi 1.00 import weight 0.00
Epoch 407 Iter 7 subLoss 6734.3 multi 6.97 import weight 0.00
Epoch 407 Iter 8 subLoss 3930.4 multi -1.99 import weight 0.00
Epoch 407 Iter 9 subLoss 5275.8 multi -1.98 import weight 0.00
Epoch 407 Iter 10 subLoss 4830.7 multi -4.97 import weight 0.00
Epoch 407 Iter 11 subLoss 5822.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 407 Acc: 94.28 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 582 train Loss: 5749.0 test Loss: 900.7
Epoch 408 Iter 0 subLoss 5727.9 multi -10.94 import weight 0.00
Epoch 408 Iter 1 subLoss 8404.4 multi -1.98 import weight 0.00
Epoch 408 Iter 2 subLoss 9935.5 multi 1.00 import weight 0.00
Epoch 408 Iter 3 subLoss 9097.1 multi 9.96 import weight 0.00
Epoch 408 Iter 4 subLoss 5825.4 multi 3.99 import weight 0.00
Epoch 408 Iter 5 subLoss 4683.7 multi -7.96 import weight 0.00
Epoch 408 Iter 6 subLoss 6126.3 multi 3.98 import weight 0.00
Epoch 408 Iter 7 subLoss 5455.7 multi -1.98 import weight 0.00
Epoch 408 Iter 8 subLoss 5578.7 multi -1.99 import weight 0.00
Epoch 408 Iter 9 subLoss 6253.6 multi -7.96 import weight 0.00
Epoch 408 Iter 10 subLoss 8907.5 multi 3.99 import weight 0.00
Epoch 408 Iter 11 subLoss 6161.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 408 Acc: 93.83 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 616 train Loss: 6452.5 test Loss: 1033.8
Epoch 409 Iter 0 subLoss 6137.8 multi -4.97 import weight 0.00
Epoch 409 Iter 1 subLoss 7414.2 multi 9.96 import weight 0.00
Epoch 409 Iter 2 subLoss 5462.9 multi -7.96 import weight 0.00
Epoch 409 Iter 3 subLoss 6470.7 multi 1.00 import weight 0.00
Epoch 409 Iter 4 subLoss 6271.0 multi -4.97 import weight 0.00
Epoch 409 Iter 5 subLoss 7013.0 multi 1.00 import weight 0.00
Epoch 409 Iter 6 subLoss 6618.3 multi 3.98 import weight 0.00
Epoch 409 Iter 7 subLoss 5548.7 multi 6.97 import weight 0.00
Epoch 409 Iter 8 subLoss 5055.7 multi 6.97 import weight 0.00
Epoch 409 Iter 9 subLoss 4250.4 multi 9.96 import weight 0.00
Epoch 409 Iter 10 subLoss 3365.2 multi 1.00 import weight 0.00
Epoch 409 Iter 11 subLoss 3264.3 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 409 Acc: 98.27 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 326 train Loss: 3061.3 test Loss: 373.7
Epoch 410 Iter 0 subLoss 3133.8 multi 6.97 import weight 0.00
Epoch 410 Iter 1 subLoss 2686.0 multi -4.97 import weight 0.00
Epoch 410 Iter 2 subLoss 2863.6 multi 3.99 import weight 0.00
Epoch 410 Iter 3 subLoss 2629.7 multi 9.96 import weight 0.00
Epoch 410 Iter 4 subLoss 2662.3 multi 3.98 import weight 0.00
Epoch 410 Iter 5 subLoss 2232.5 multi -1.98 import weight 0.00
Epoch 410 Iter 6 subLoss 2665.4 multi 6.97 import weight 0.00
Epoch 410 Iter 7 subLoss 2436.2 multi 15.93 import weight 0.00
Epoch 410 Iter 8 subLoss 2167.5 multi 21.90 import weight 0.00
Epoch 410 Iter 9 subLoss 2165.2 multi 24.88 import weight 0.00
Epoch 410 Iter 10 subLoss 2170.9 multi -31.84 import weight 0.00
Epoch 410 Iter 11 subLoss 2878.9 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 410 Acc: 98.48 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 287 train Loss: 2516.1 test Loss: 268.3
Epoch 411 Iter 0 subLoss 2503.4 multi -4.97 import weight 0.00
Epoch 411 Iter 1 subLoss 2527.7 multi 6.97 import weight 0.00
Epoch 411 Iter 2 subLoss 2802.3 multi 15.93 import weight 0.00
Epoch 411 Iter 3 subLoss 2052.5 multi -7.96 import weight 0.00
Epoch 411 Iter 4 subLoss 1902.6 multi -4.97 import weight 0.00
Epoch 411 Iter 5 subLoss 2752.6 multi -13.93 import weight 0.00
Epoch 411 Iter 6 subLoss 2881.5 multi -28.85 import weight 0.00
Epoch 411 Iter 7 subLoss 5412.8 multi 3.98 import weight 0.00
Epoch 411 Iter 8 subLoss 3135.5 multi 9.96 import weight 0.00
Epoch 411 Iter 9 subLoss 2614.7 multi -4.97 import weight 0.00
Epoch 411 Iter 10 subLoss 2707.0 multi 15.93 import weight 0.00
Epoch 411 Iter 11 subLoss 2022.1 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 411 Acc: 98.62 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 202 train Loss: 2469.7 test Loss: 224.8
Epoch 412 Iter 0 subLoss 1889.4 multi 1.00 import weight 0.00
Epoch 412 Iter 1 subLoss 2508.8 multi -1.98 import weight 0.00
Epoch 412 Iter 2 subLoss 2339.6 multi 18.91 import weight 0.00
Epoch 412 Iter 3 subLoss 2160.3 multi 27.87 import weight 0.00
Epoch 412 Iter 4 subLoss 2102.5 multi -13.93 import weight 0.00
Epoch 412 Iter 5 subLoss 2042.0 multi 24.88 import weight 0.00
Epoch 412 Iter 6 subLoss 1784.3 multi -1.99 import weight 0.00
Epoch 412 Iter 7 subLoss 2335.8 multi 21.90 import weight 0.00
Epoch 412 Iter 8 subLoss 1717.3 multi -13.93 import weight 0.00
Epoch 412 Iter 9 subLoss 2694.7 multi -16.91 import weight 0.00
Epoch 412 Iter 10 subLoss 3851.9 multi -7.96 import weight 0.00
Epoch 412 Iter 11 subLoss 8530.9 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 412 Acc: 98.13 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 853 train Loss: 3136.2 test Loss: 311.4
Epoch 413 Iter 0 subLoss 2840.7 multi 18.91 import weight 0.00
Epoch 413 Iter 1 subLoss 2323.9 multi -7.96 import weight 0.00
Epoch 413 Iter 2 subLoss 2655.1 multi 12.94 import weight 0.00
Epoch 413 Iter 3 subLoss 1990.5 multi 3.98 import weight 0.00
Epoch 413 Iter 4 subLoss 2347.1 multi -31.84 import weight 0.00
Epoch 413 Iter 5 subLoss 3011.2 multi 27.87 import weight 0.00
Epoch 413 Iter 6 subLoss 2252.4 multi 6.97 import weight 0.00
Epoch 413 Iter 7 subLoss 2140.1 multi 3.99 import weight 0.00
Epoch 413 Iter 8 subLoss 2026.7 multi 18.91 import weight 0.00
Epoch 413 Iter 9 subLoss 2183.1 multi 24.88 import weight 0.00
Epoch 413 Iter 10 subLoss 2146.4 multi 6.97 import weight 0.00
Epoch 413 Iter 11 subLoss 2283.8 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 413 Acc: 98.91 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 228 train Loss: 2084.6 test Loss: 193.5
Epoch 414 Iter 0 subLoss 2341.2 multi -28.85 import weight 0.00
Epoch 414 Iter 1 subLoss 3029.0 multi -25.87 import weight 0.00
Epoch 414 Iter 2 subLoss 14050.1 multi 3.99 import weight 0.00
Epoch 414 Iter 3 subLoss 2438.7 multi 18.91 import weight 0.00
Epoch 414 Iter 4 subLoss 2483.6 multi 3.99 import weight 0.00
Epoch 414 Iter 5 subLoss 2238.4 multi 1.00 import weight 0.00
Epoch 414 Iter 6 subLoss 2176.3 multi -31.84 import weight 0.00
Epoch 414 Iter 7 subLoss 3282.9 multi -4.97 import weight 0.00
Epoch 414 Iter 8 subLoss 3462.6 multi 3.99 import weight 0.00
Epoch 414 Iter 9 subLoss 2828.3 multi -1.98 import weight 0.00
Epoch 414 Iter 10 subLoss 2742.5 multi -7.96 import weight 0.00
Epoch 414 Iter 11 subLoss 4819.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 414 Acc: 97.10 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 481 train Loss: 4139.7 test Loss: 443.0
Epoch 415 Iter 0 subLoss 4412.4 multi 18.91 import weight 0.00
Epoch 415 Iter 1 subLoss 3756.1 multi 6.97 import weight 0.00
Epoch 415 Iter 2 subLoss 2155.6 multi -16.91 import weight 0.00
Epoch 415 Iter 3 subLoss 2599.1 multi 18.91 import weight 0.00
Epoch 415 Iter 4 subLoss 2492.2 multi 1.00 import weight 0.00
Epoch 415 Iter 5 subLoss 2283.8 multi 18.91 import weight 0.00
Epoch 415 Iter 6 subLoss 2304.4 multi -7.96 import weight 0.00
Epoch 415 Iter 7 subLoss 1923.7 multi 3.99 import weight 0.00
Epoch 415 Iter 8 subLoss 2255.4 multi 9.96 import weight 0.00
Epoch 415 Iter 9 subLoss 2412.2 multi -1.99 import weight 0.00
Epoch 415 Iter 10 subLoss 2578.2 multi 30.85 import weight 0.00
Epoch 415 Iter 11 subLoss 2137.5 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 415 Acc: 98.66 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 213 train Loss: 2533.9 test Loss: 215.6
Epoch 416 Iter 0 subLoss 2204.7 multi 15.93 import weight 0.00
Epoch 416 Iter 1 subLoss 2257.9 multi 12.94 import weight 0.00
Epoch 416 Iter 2 subLoss 2218.9 multi -4.97 import weight 0.00
Epoch 416 Iter 3 subLoss 1795.5 multi 3.99 import weight 0.00
Epoch 416 Iter 4 subLoss 1992.4 multi 6.97 import weight 0.00
Epoch 416 Iter 5 subLoss 1795.2 multi 6.97 import weight 0.00
Epoch 416 Iter 6 subLoss 2393.5 multi -7.96 import weight 0.00
Epoch 416 Iter 7 subLoss 1949.6 multi 15.93 import weight 0.00
Epoch 416 Iter 8 subLoss 1552.2 multi -1.99 import weight 0.00
Epoch 416 Iter 9 subLoss 1916.0 multi 3.98 import weight 0.00
Epoch 416 Iter 10 subLoss 2047.5 multi 27.87 import weight 0.00
Epoch 416 Iter 11 subLoss 1859.1 multi -22.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 416 Acc: 98.87 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -22.88 Pidx 185 train Loss: 2139.0 test Loss: 181.6
Epoch 417 Iter 0 subLoss 1939.2 multi -7.96 import weight 0.00
Epoch 417 Iter 1 subLoss 2279.9 multi 9.96 import weight 0.00
Epoch 417 Iter 2 subLoss 2284.9 multi 18.91 import weight 0.00
Epoch 417 Iter 3 subLoss 2164.9 multi 27.87 import weight 0.00
Epoch 417 Iter 4 subLoss 1958.2 multi -4.97 import weight 0.00
Epoch 417 Iter 5 subLoss 1674.3 multi -1.98 import weight 0.00
Epoch 417 Iter 6 subLoss 1809.9 multi 1.00 import weight 0.00
Epoch 417 Iter 7 subLoss 2333.3 multi 21.90 import weight 0.00
Epoch 417 Iter 8 subLoss 1650.3 multi 6.97 import weight 0.00
Epoch 417 Iter 9 subLoss 2058.2 multi -10.94 import weight 0.00
Epoch 417 Iter 10 subLoss 1662.1 multi -4.97 import weight 0.00
Epoch 417 Iter 11 subLoss 1889.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 417 Acc: 98.81 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 188 train Loss: 2057.1 test Loss: 207.8
Epoch 418 Iter 0 subLoss 2154.0 multi -13.93 import weight 0.00
Epoch 418 Iter 1 subLoss 2063.3 multi -16.91 import weight 0.00
Epoch 418 Iter 2 subLoss 3967.7 multi 9.96 import weight 0.00
Epoch 418 Iter 3 subLoss 1989.4 multi -7.96 import weight 0.00
Epoch 418 Iter 4 subLoss 2053.6 multi -7.96 import weight 0.00
Epoch 418 Iter 5 subLoss 1921.3 multi 3.99 import weight 0.00
Epoch 418 Iter 6 subLoss 2098.4 multi 15.93 import weight 0.00
Epoch 418 Iter 7 subLoss 1733.7 multi -1.98 import weight 0.00
Epoch 418 Iter 8 subLoss 2515.4 multi -4.97 import weight 0.00
Epoch 418 Iter 9 subLoss 1992.9 multi 6.97 import weight 0.00
Epoch 418 Iter 10 subLoss 1932.6 multi -7.96 import weight 0.00
Epoch 418 Iter 11 subLoss 1972.5 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 418 Acc: 98.85 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 197 train Loss: 1995.3 test Loss: 184.3
Epoch 419 Iter 0 subLoss 2050.5 multi -4.97 import weight 0.00
Epoch 419 Iter 1 subLoss 2180.5 multi 24.88 import weight 0.00
Epoch 419 Iter 2 subLoss 1879.2 multi 9.96 import weight 0.00
Epoch 419 Iter 3 subLoss 1986.9 multi -7.96 import weight 0.00
Epoch 419 Iter 4 subLoss 2346.4 multi -28.85 import weight 0.00
Epoch 419 Iter 5 subLoss 4227.3 multi -4.97 import weight 0.00
Epoch 419 Iter 6 subLoss 17867.7 multi 3.99 import weight 0.00
Epoch 419 Iter 7 subLoss 3666.2 multi 18.91 import weight 0.00
Epoch 419 Iter 8 subLoss 2687.4 multi -1.98 import weight 0.00
Epoch 419 Iter 9 subLoss 2926.6 multi -19.90 import weight 0.00
Epoch 419 Iter 10 subLoss 14054.9 multi 6.97 import weight 0.00
Epoch 419 Iter 11 subLoss 4665.5 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 419 Acc: 91.98 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 466 train Loss: 9171.9 test Loss: 1265.0
Epoch 420 Iter 0 subLoss 9080.7 multi -1.99 import weight 0.00
Epoch 420 Iter 1 subLoss 24958.6 multi 1.00 import weight 0.00
Epoch 420 Iter 2 subLoss 6792.7 multi 12.94 import weight 0.00
Epoch 420 Iter 3 subLoss 2716.8 multi -10.94 import weight 0.00
Epoch 420 Iter 4 subLoss 4431.5 multi -1.99 import weight 0.00
Epoch 420 Iter 5 subLoss 4711.9 multi 3.99 import weight 0.00
Epoch 420 Iter 6 subLoss 2821.6 multi 1.00 import weight 0.00
Epoch 420 Iter 7 subLoss 3147.0 multi 12.94 import weight 0.00
Epoch 420 Iter 8 subLoss 2750.7 multi -13.93 import weight 0.00
Epoch 420 Iter 9 subLoss 2626.8 multi 9.96 import weight 0.00
Epoch 420 Iter 10 subLoss 2452.3 multi -4.97 import weight 0.00
Epoch 420 Iter 11 subLoss 2424.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 420 Acc: 98.37 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 242 train Loss: 2501.6 test Loss: 255.4
Epoch 421 Iter 0 subLoss 2865.5 multi 6.97 import weight 0.00
Epoch 421 Iter 1 subLoss 2636.5 multi -1.98 import weight 0.00
Epoch 421 Iter 2 subLoss 2030.7 multi -16.91 import weight 0.00
Epoch 421 Iter 3 subLoss 2416.2 multi 1.00 import weight 0.00
Epoch 421 Iter 4 subLoss 2475.4 multi -4.97 import weight 0.00
Epoch 421 Iter 5 subLoss 2295.7 multi -22.88 import weight 0.00
Epoch 421 Iter 6 subLoss 3453.2 multi 12.94 import weight 0.00
Epoch 421 Iter 7 subLoss 2982.1 multi 9.96 import weight 0.00
Epoch 421 Iter 8 subLoss 2268.0 multi -19.90 import weight 0.00
Epoch 421 Iter 9 subLoss 2948.3 multi -7.96 import weight 0.00
Epoch 421 Iter 10 subLoss 2807.7 multi 18.91 import weight 0.00
Epoch 421 Iter 11 subLoss 3344.8 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 421 Acc: 98.50 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 334 train Loss: 2566.4 test Loss: 262.9
Epoch 422 Iter 0 subLoss 3057.4 multi 1.00 import weight 0.00
Epoch 422 Iter 1 subLoss 2394.1 multi -4.97 import weight 0.00
Epoch 422 Iter 2 subLoss 2013.8 multi 3.99 import weight 0.00
Epoch 422 Iter 3 subLoss 2359.6 multi 3.99 import weight 0.00
Epoch 422 Iter 4 subLoss 2819.2 multi -31.84 import weight 0.00
Epoch 422 Iter 5 subLoss 3028.7 multi -22.88 import weight 0.00
Epoch 422 Iter 6 subLoss 6916.7 multi 21.90 import weight 0.00
Epoch 422 Iter 7 subLoss 6236.8 multi 3.99 import weight 0.00
Epoch 422 Iter 8 subLoss 2977.7 multi -19.90 import weight 0.00
Epoch 422 Iter 9 subLoss 8492.3 multi 3.98 import weight 0.00
Epoch 422 Iter 10 subLoss 4400.5 multi 1.00 import weight 0.00
Epoch 422 Iter 11 subLoss 4353.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 422 Acc: 97.86 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 435 train Loss: 3655.0 test Loss: 409.4
Epoch 423 Iter 0 subLoss 3780.5 multi -4.97 import weight 0.00
Epoch 423 Iter 1 subLoss 3405.7 multi 1.00 import weight 0.00
Epoch 423 Iter 2 subLoss 4406.7 multi 3.99 import weight 0.00
Epoch 423 Iter 3 subLoss 3059.2 multi 3.98 import weight 0.00
Epoch 423 Iter 4 subLoss 3245.7 multi -13.93 import weight 0.00
Epoch 423 Iter 5 subLoss 4461.0 multi -1.99 import weight 0.00
Epoch 423 Iter 6 subLoss 5318.8 multi 3.99 import weight 0.00
Epoch 423 Iter 7 subLoss 3522.7 multi 12.94 import weight 0.00
Epoch 423 Iter 8 subLoss 3028.5 multi -19.90 import weight 0.00
Epoch 423 Iter 9 subLoss 3584.6 multi 12.94 import weight 0.00
Epoch 423 Iter 10 subLoss 3485.7 multi 12.94 import weight 0.00
Epoch 423 Iter 11 subLoss 2948.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 423 Acc: 98.03 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 294 train Loss: 2942.1 test Loss: 324.4
Epoch 424 Iter 0 subLoss 2624.1 multi 12.94 import weight 0.00
Epoch 424 Iter 1 subLoss 2210.3 multi -1.99 import weight 0.00
Epoch 424 Iter 2 subLoss 2840.7 multi 21.90 import weight 0.00
Epoch 424 Iter 3 subLoss 2755.7 multi -10.94 import weight 0.00
Epoch 424 Iter 4 subLoss 2564.6 multi -7.96 import weight 0.00
Epoch 424 Iter 5 subLoss 2593.8 multi 21.90 import weight 0.00
Epoch 424 Iter 6 subLoss 2701.7 multi 15.93 import weight 0.00
Epoch 424 Iter 7 subLoss 2077.1 multi -7.96 import weight 0.00
Epoch 424 Iter 8 subLoss 2579.7 multi 30.85 import weight 1.00
Epoch 424 Iter 9 subLoss 2402.5 multi -4.97 import weight 0.00
Epoch 424 Iter 10 subLoss 2921.0 multi -16.91 import weight 0.00
Epoch 424 Iter 11 subLoss 6127.2 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 424 Acc: 98.38 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 612 train Loss: 2638.4 test Loss: 252.5
Epoch 425 Iter 0 subLoss 2444.8 multi -19.90 import weight 0.00
Epoch 425 Iter 1 subLoss 7194.0 multi -1.98 import weight 0.00
Epoch 425 Iter 2 subLoss 8089.5 multi 9.96 import weight 0.00
Epoch 425 Iter 3 subLoss 4392.2 multi -1.99 import weight 0.00
Epoch 425 Iter 4 subLoss 5375.4 multi -4.97 import weight 0.00
Epoch 425 Iter 5 subLoss 9131.5 multi 3.98 import weight 0.00
Epoch 425 Iter 6 subLoss 3943.6 multi 15.93 import weight 0.00
Epoch 425 Iter 7 subLoss 2661.3 multi 6.97 import weight 0.00
Epoch 425 Iter 8 subLoss 2294.8 multi -19.90 import weight 0.00
Epoch 425 Iter 9 subLoss 2729.3 multi 3.98 import weight 0.00
Epoch 425 Iter 10 subLoss 2670.3 multi 9.96 import weight 0.00
Epoch 425 Iter 11 subLoss 2881.6 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 425 Acc: 97.86 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 288 train Loss: 3127.5 test Loss: 389.7
Epoch 426 Iter 0 subLoss 2924.9 multi -13.93 import weight 0.00
Epoch 426 Iter 1 subLoss 4464.3 multi 1.00 import weight 0.00
Epoch 426 Iter 2 subLoss 3589.7 multi 15.93 import weight 0.00
Epoch 426 Iter 3 subLoss 2232.0 multi 3.99 import weight 0.00
Epoch 426 Iter 4 subLoss 2716.3 multi -10.94 import weight 0.00
Epoch 426 Iter 5 subLoss 2678.5 multi 12.94 import weight 0.00
Epoch 426 Iter 6 subLoss 3163.2 multi -1.98 import weight 0.00
Epoch 426 Iter 7 subLoss 2597.5 multi 24.88 import weight 0.00
Epoch 426 Iter 8 subLoss 2137.8 multi -1.98 import weight 0.00
Epoch 426 Iter 9 subLoss 2689.4 multi -4.97 import weight 0.00
Epoch 426 Iter 10 subLoss 2475.2 multi -1.98 import weight 0.00
Epoch 426 Iter 11 subLoss 2175.0 multi -31.84 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 426 Acc: 97.74 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -31.84 Pidx 217 train Loss: 3334.1 test Loss: 385.6
Epoch 427 Iter 0 subLoss 3453.2 multi 15.93 import weight 0.00
Epoch 427 Iter 1 subLoss 2455.2 multi -4.97 import weight 0.00
Epoch 427 Iter 2 subLoss 2825.1 multi 1.00 import weight 0.00
Epoch 427 Iter 3 subLoss 2140.4 multi 3.99 import weight 0.00
Epoch 427 Iter 4 subLoss 2761.9 multi -4.97 import weight 0.00
Epoch 427 Iter 5 subLoss 2523.8 multi 6.97 import weight 0.00
Epoch 427 Iter 6 subLoss 2141.3 multi 6.97 import weight 0.00
Epoch 427 Iter 7 subLoss 2364.6 multi 6.97 import weight 0.00
Epoch 427 Iter 8 subLoss 2510.8 multi -1.98 import weight 0.00
Epoch 427 Iter 9 subLoss 2324.9 multi -4.97 import weight 0.00
Epoch 427 Iter 10 subLoss 2517.3 multi 1.00 import weight 0.00
Epoch 427 Iter 11 subLoss 2720.7 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 427 Acc: 98.77 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 272 train Loss: 2314.4 test Loss: 234.4
Epoch 428 Iter 0 subLoss 2098.6 multi 18.91 import weight 0.00
Epoch 428 Iter 1 subLoss 1923.6 multi 6.97 import weight 0.00
Epoch 428 Iter 2 subLoss 2278.4 multi 9.96 import weight 0.00
Epoch 428 Iter 3 subLoss 1937.9 multi -7.96 import weight 0.00
Epoch 428 Iter 4 subLoss 2377.2 multi -16.91 import weight 0.00
Epoch 428 Iter 5 subLoss 2166.8 multi 27.87 import weight 0.00
Epoch 428 Iter 6 subLoss 2161.5 multi 30.85 import weight 0.00
Epoch 428 Iter 7 subLoss 2117.0 multi -7.96 import weight 0.00
Epoch 428 Iter 8 subLoss 2442.2 multi -16.91 import weight 0.00
Epoch 428 Iter 9 subLoss 2978.7 multi -16.91 import weight 0.00
Epoch 428 Iter 10 subLoss 9530.5 multi -1.98 import weight 0.00
Epoch 428 Iter 11 subLoss 30574.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 428 Acc: 81.07 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 3057 train Loss: 10541.8 test Loss: 2147.3
Epoch 429 Iter 0 subLoss 10865.9 multi 1.00 import weight 0.00
Epoch 429 Iter 1 subLoss 8809.3 multi 6.97 import weight 0.00
Epoch 429 Iter 2 subLoss 3684.5 multi 18.91 import weight 0.00
Epoch 429 Iter 3 subLoss 2655.6 multi 15.93 import weight 0.00
Epoch 429 Iter 4 subLoss 2235.2 multi 6.97 import weight 0.00
Epoch 429 Iter 5 subLoss 1830.7 multi -1.99 import weight 0.00
Epoch 429 Iter 6 subLoss 2153.4 multi -16.91 import weight 0.00
Epoch 429 Iter 7 subLoss 2327.9 multi -1.99 import weight 0.00
Epoch 429 Iter 8 subLoss 2607.8 multi -16.91 import weight 0.00
Epoch 429 Iter 9 subLoss 3276.0 multi -1.99 import weight 0.00
Epoch 429 Iter 10 subLoss 2767.9 multi -1.99 import weight 0.00
Epoch 429 Iter 11 subLoss 3108.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 429 Acc: 98.27 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 310 train Loss: 2713.7 test Loss: 289.0
Epoch 430 Iter 0 subLoss 2430.4 multi 18.91 import weight 0.00
Epoch 430 Iter 1 subLoss 2355.3 multi 6.97 import weight 0.00
Epoch 430 Iter 2 subLoss 2603.9 multi -13.93 import weight 0.00
Epoch 430 Iter 3 subLoss 2687.2 multi -1.99 import weight 0.00
Epoch 430 Iter 4 subLoss 2953.1 multi -10.94 import weight 0.00
Epoch 430 Iter 5 subLoss 2756.5 multi -7.96 import weight 0.00
Epoch 430 Iter 6 subLoss 3138.8 multi 12.94 import weight 0.00
Epoch 430 Iter 7 subLoss 2340.8 multi -25.87 import weight 0.00
Epoch 430 Iter 8 subLoss 3110.3 multi -13.93 import weight 0.00
Epoch 430 Iter 9 subLoss 4864.3 multi -1.99 import weight 0.00
Epoch 430 Iter 10 subLoss 4567.1 multi 12.94 import weight 0.00
Epoch 430 Iter 11 subLoss 2455.5 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 430 Acc: 98.07 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 245 train Loss: 2939.6 test Loss: 336.9
Epoch 431 Iter 0 subLoss 3188.2 multi 12.94 import weight 1.00
Epoch 431 Iter 1 subLoss 2162.9 multi 30.85 import weight 0.00
Epoch 431 Iter 2 subLoss 2213.2 multi 1.00 import weight 0.00
Epoch 431 Iter 3 subLoss 2202.0 multi 18.91 import weight 0.00
Epoch 431 Iter 4 subLoss 2163.3 multi 33.84 import weight 0.00
Epoch 431 Iter 5 subLoss 2084.2 multi 6.97 import weight 0.00
Epoch 431 Iter 6 subLoss 2455.2 multi -1.99 import weight 0.00
Epoch 431 Iter 7 subLoss 2077.9 multi -4.97 import weight 0.00
Epoch 431 Iter 8 subLoss 2360.9 multi 6.97 import weight 0.00
Epoch 431 Iter 9 subLoss 1810.1 multi -10.94 import weight 0.00
Epoch 431 Iter 10 subLoss 2506.0 multi -1.99 import weight 0.00
Epoch 431 Iter 11 subLoss 1815.0 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 431 Acc: 98.38 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 181 train Loss: 2516.3 test Loss: 237.1
Epoch 432 Iter 0 subLoss 2424.5 multi 6.97 import weight 0.00
Epoch 432 Iter 1 subLoss 2438.4 multi 18.91 import weight 0.00
Epoch 432 Iter 2 subLoss 2101.6 multi -16.91 import weight 0.00
Epoch 432 Iter 3 subLoss 2930.4 multi 9.96 import weight 0.00
Epoch 432 Iter 4 subLoss 2221.1 multi -13.93 import weight 0.00
Epoch 432 Iter 5 subLoss 2427.4 multi 9.96 import weight 0.00
Epoch 432 Iter 6 subLoss 1972.7 multi 24.88 import weight 0.00
Epoch 432 Iter 7 subLoss 2592.4 multi 27.87 import weight 0.00
Epoch 432 Iter 8 subLoss 2340.6 multi -22.88 import weight 0.00
Epoch 432 Iter 9 subLoss 4701.4 multi -7.96 import weight 0.00
Epoch 432 Iter 10 subLoss 30591.9 multi 1.00 import weight 0.00
Epoch 432 Iter 11 subLoss 5863.2 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 432 Acc: 88.71 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 586 train Loss: 9887.4 test Loss: 1772.3
Epoch 433 Iter 0 subLoss 9182.1 multi -1.99 import weight 0.00
Epoch 433 Iter 1 subLoss 23957.6 multi 3.99 import weight 0.00
Epoch 433 Iter 2 subLoss 3912.5 multi 9.96 import weight 0.00
Epoch 433 Iter 3 subLoss 2806.2 multi 21.90 import weight 0.00
Epoch 433 Iter 4 subLoss 2090.6 multi 18.91 import weight 0.00
Epoch 433 Iter 5 subLoss 2173.6 multi -40.79 import weight 0.00
Epoch 433 Iter 6 subLoss 2511.4 multi 1.00 import weight 0.00
Epoch 433 Iter 7 subLoss 2623.7 multi 15.93 import weight 0.00
Epoch 433 Iter 8 subLoss 2194.6 multi -16.91 import weight 0.00
Epoch 433 Iter 9 subLoss 2535.7 multi 1.00 import weight 0.00
Epoch 433 Iter 10 subLoss 2651.8 multi 18.91 import weight 0.00
Epoch 433 Iter 11 subLoss 2423.5 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 433 Acc: 98.85 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 242 train Loss: 2098.5 test Loss: 188.7
Epoch 434 Iter 0 subLoss 2039.2 multi -13.93 import weight 0.00
Epoch 434 Iter 1 subLoss 2009.0 multi -22.88 import weight 0.00
Epoch 434 Iter 2 subLoss 2263.2 multi -16.91 import weight 0.00
Epoch 434 Iter 3 subLoss 2639.4 multi -4.97 import weight 0.00
Epoch 434 Iter 4 subLoss 2827.0 multi 3.99 import weight 0.00
Epoch 434 Iter 5 subLoss 2691.1 multi -22.88 import weight 0.00
Epoch 434 Iter 6 subLoss 3960.0 multi 12.94 import weight 0.00
Epoch 434 Iter 7 subLoss 2836.9 multi 9.96 import weight 0.00
Epoch 434 Iter 8 subLoss 2510.6 multi 3.99 import weight 0.00
Epoch 434 Iter 9 subLoss 2543.9 multi -13.93 import weight 0.00
Epoch 434 Iter 10 subLoss 2380.2 multi 18.91 import weight 0.00
Epoch 434 Iter 11 subLoss 2507.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 434 Acc: 98.64 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 250 train Loss: 2430.7 test Loss: 236.3
Epoch 435 Iter 0 subLoss 2367.4 multi 9.96 import weight 0.00
Epoch 435 Iter 1 subLoss 2222.3 multi -10.94 import weight 0.00
Epoch 435 Iter 2 subLoss 2866.9 multi 9.96 import weight 0.00
Epoch 435 Iter 3 subLoss 2282.8 multi 18.91 import weight 0.00
Epoch 435 Iter 4 subLoss 2303.9 multi -10.94 import weight 0.00
Epoch 435 Iter 5 subLoss 2629.4 multi 18.91 import weight 0.00
Epoch 435 Iter 6 subLoss 2367.6 multi 12.94 import weight 0.00
Epoch 435 Iter 7 subLoss 1829.1 multi -1.98 import weight 0.00
Epoch 435 Iter 8 subLoss 2424.8 multi 15.93 import weight 0.00
Epoch 435 Iter 9 subLoss 2268.7 multi -13.93 import weight 0.00
Epoch 435 Iter 10 subLoss 2225.6 multi -7.96 import weight 0.00
Epoch 435 Iter 11 subLoss 2147.5 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 435 Acc: 98.79 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 214 train Loss: 2208.7 test Loss: 198.3
Epoch 436 Iter 0 subLoss 2004.5 multi -19.90 import weight 0.00
Epoch 436 Iter 1 subLoss 2573.5 multi 33.84 import weight 1.00
Epoch 436 Iter 2 subLoss 2177.1 multi -37.81 import weight 0.00
Epoch 436 Iter 3 subLoss 4499.4 multi -4.97 import weight 0.00
Epoch 436 Iter 4 subLoss 8365.1 multi 1.00 import weight 0.00
Epoch 436 Iter 5 subLoss 6636.0 multi 3.99 import weight 0.00
Epoch 436 Iter 6 subLoss 3090.3 multi 3.98 import weight 0.00
Epoch 436 Iter 7 subLoss 2351.0 multi 3.99 import weight 0.00
Epoch 436 Iter 8 subLoss 2364.7 multi 12.94 import weight 0.00
Epoch 436 Iter 9 subLoss 2525.9 multi -1.98 import weight 0.00
Epoch 436 Iter 10 subLoss 2442.8 multi -19.90 import weight 0.00
Epoch 436 Iter 11 subLoss 2081.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 436 Acc: 98.87 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 208 train Loss: 2318.6 test Loss: 202.2
Epoch 437 Iter 0 subLoss 2529.0 multi 1.00 import weight 0.00
Epoch 437 Iter 1 subLoss 2311.1 multi 3.99 import weight 0.00
Epoch 437 Iter 2 subLoss 1926.0 multi 9.96 import weight 0.00
Epoch 437 Iter 3 subLoss 2221.8 multi -4.97 import weight 0.00
Epoch 437 Iter 4 subLoss 2202.9 multi 18.91 import weight 0.00
Epoch 437 Iter 5 subLoss 1653.7 multi 9.96 import weight 0.00
Epoch 437 Iter 6 subLoss 1807.2 multi 3.99 import weight 0.00
Epoch 437 Iter 7 subLoss 2008.6 multi -16.91 import weight 0.00
Epoch 437 Iter 8 subLoss 2166.4 multi 36.82 import weight 0.00
Epoch 437 Iter 9 subLoss 2178.6 multi -37.81 import weight 0.00
Epoch 437 Iter 10 subLoss 3246.2 multi -10.94 import weight 0.00
Epoch 437 Iter 11 subLoss 7840.9 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 437 Acc: 98.56 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 784 train Loss: 3017.6 test Loss: 227.3
Epoch 438 Iter 0 subLoss 2587.3 multi -37.81 import weight 0.00
Epoch 438 Iter 1 subLoss 10719.1 multi 1.00 import weight 0.00
Epoch 438 Iter 2 subLoss 7783.7 multi 1.00 import weight 0.00
Epoch 438 Iter 3 subLoss 6384.7 multi -4.97 import weight 0.00
Epoch 438 Iter 4 subLoss 14319.1 multi 3.99 import weight 0.00
Epoch 438 Iter 5 subLoss 5504.2 multi 18.91 import weight 0.00
Epoch 438 Iter 6 subLoss 3193.6 multi -28.85 import weight 0.00
Epoch 438 Iter 7 subLoss 22281.5 multi 1.00 import weight 0.00
Epoch 438 Iter 8 subLoss 10185.0 multi 1.00 import weight 0.00
Epoch 438 Iter 9 subLoss 5909.6 multi -7.96 import weight 0.00
Epoch 438 Iter 10 subLoss 33723.6 multi 1.00 import weight 0.00
Epoch 438 Iter 11 subLoss 15641.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 438 Acc: 95.74 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1564 train Loss: 11050.7 test Loss: 646.5
Epoch 439 Iter 0 subLoss 11773.1 multi -7.96 import weight 0.00
Epoch 439 Iter 1 subLoss 58751.3 multi 1.00 import weight 0.00
Epoch 439 Iter 2 subLoss 28668.7 multi 1.00 import weight 0.00
Epoch 439 Iter 3 subLoss 20443.6 multi 1.00 import weight 0.00
Epoch 439 Iter 4 subLoss 16053.3 multi -7.96 import weight 0.00
Epoch 439 Iter 5 subLoss 58859.8 multi 1.00 import weight 0.00
Epoch 439 Iter 6 subLoss 40021.8 multi 1.00 import weight 0.00
Epoch 439 Iter 7 subLoss 29828.5 multi -1.99 import weight 0.00
Epoch 439 Iter 8 subLoss 40435.4 multi 1.00 import weight 0.00
Epoch 439 Iter 9 subLoss 32661.9 multi 1.00 import weight 0.00
Epoch 439 Iter 10 subLoss 29117.8 multi 1.00 import weight 0.00
Epoch 439 Iter 11 subLoss 25109.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 439 Acc: 76.98 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2510 train Loss: 23635.5 test Loss: 3127.0
Epoch 440 Iter 0 subLoss 23283.6 multi 1.00 import weight 0.00
Epoch 440 Iter 1 subLoss 21621.3 multi 1.00 import weight 0.00
Epoch 440 Iter 2 subLoss 19692.6 multi 1.00 import weight 0.00
Epoch 440 Iter 3 subLoss 17383.2 multi -1.99 import weight 0.00
Epoch 440 Iter 4 subLoss 21555.7 multi 1.00 import weight 0.00
Epoch 440 Iter 5 subLoss 19209.6 multi -1.99 import weight 0.00
Epoch 440 Iter 6 subLoss 23051.3 multi -1.99 import weight 0.00
Epoch 440 Iter 7 subLoss 26109.2 multi 1.00 import weight 0.00
Epoch 440 Iter 8 subLoss 23807.0 multi -1.99 import weight 0.00
Epoch 440 Iter 9 subLoss 28695.2 multi 1.00 import weight 0.00
Epoch 440 Iter 10 subLoss 27360.3 multi 3.99 import weight 0.00
Epoch 440 Iter 11 subLoss 19370.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 440 Acc: 78.40 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1937 train Loss: 18296.2 test Loss: 2387.2
Epoch 441 Iter 0 subLoss 17673.5 multi 1.00 import weight 0.00
Epoch 441 Iter 1 subLoss 16605.5 multi -1.98 import weight 0.00
Epoch 441 Iter 2 subLoss 19433.9 multi 3.99 import weight 0.00
Epoch 441 Iter 3 subLoss 15714.9 multi 6.97 import weight 0.00
Epoch 441 Iter 4 subLoss 7620.1 multi 3.99 import weight 0.00
Epoch 441 Iter 5 subLoss 6997.5 multi -4.97 import weight 0.00
Epoch 441 Iter 6 subLoss 7874.4 multi 1.00 import weight 0.00
Epoch 441 Iter 7 subLoss 7421.8 multi -16.91 import weight 0.00
Epoch 441 Iter 8 subLoss 13646.6 multi -4.97 import weight 0.00
Epoch 441 Iter 9 subLoss 40322.4 multi 1.00 import weight 0.00
Epoch 441 Iter 10 subLoss 16413.2 multi 3.99 import weight 0.00
Epoch 441 Iter 11 subLoss 12084.7 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 441 Acc: 97.66 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1208 train Loss: 9194.8 test Loss: 504.7
Epoch 442 Iter 0 subLoss 9231.4 multi -1.98 import weight 0.00
Epoch 442 Iter 1 subLoss 9658.8 multi 1.00 import weight 0.00
Epoch 442 Iter 2 subLoss 9446.0 multi 1.00 import weight 0.00
Epoch 442 Iter 3 subLoss 8443.7 multi -1.98 import weight 0.00
Epoch 442 Iter 4 subLoss 9372.2 multi 1.00 import weight 0.00
Epoch 442 Iter 5 subLoss 9410.1 multi -7.96 import weight 0.00
Epoch 442 Iter 6 subLoss 13428.5 multi 9.96 import weight 0.00
Epoch 442 Iter 7 subLoss 8210.3 multi 3.99 import weight 0.00
Epoch 442 Iter 8 subLoss 7256.7 multi 1.00 import weight 0.00
Epoch 442 Iter 9 subLoss 6234.0 multi 6.97 import weight 0.00
Epoch 442 Iter 10 subLoss 5875.5 multi -1.98 import weight 0.00
Epoch 442 Iter 11 subLoss 5759.9 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 442 Acc: 98.23 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 575 train Loss: 5593.8 test Loss: 297.2
Epoch 443 Iter 0 subLoss 5528.9 multi 9.96 import weight 0.00
Epoch 443 Iter 1 subLoss 4732.8 multi 1.00 import weight 0.00
Epoch 443 Iter 2 subLoss 4405.4 multi 3.99 import weight 0.00
Epoch 443 Iter 3 subLoss 5297.1 multi 3.98 import weight 0.00
Epoch 443 Iter 4 subLoss 3591.3 multi -1.99 import weight 0.00
Epoch 443 Iter 5 subLoss 4222.6 multi -1.99 import weight 0.00
Epoch 443 Iter 6 subLoss 4515.6 multi 3.98 import weight 0.00
Epoch 443 Iter 7 subLoss 3810.2 multi 9.96 import weight 0.00
Epoch 443 Iter 8 subLoss 3676.9 multi -4.97 import weight 0.00
Epoch 443 Iter 9 subLoss 3753.6 multi 9.96 import weight 0.00
Epoch 443 Iter 10 subLoss 3323.2 multi 15.93 import weight 0.00
Epoch 443 Iter 11 subLoss 2667.4 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 443 Acc: 98.93 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 266 train Loss: 2900.3 test Loss: 190.2
Epoch 444 Iter 0 subLoss 2710.7 multi -7.96 import weight 0.00
Epoch 444 Iter 1 subLoss 3151.8 multi -28.85 import weight 0.00
Epoch 444 Iter 2 subLoss 5339.5 multi -1.98 import weight 0.00
Epoch 444 Iter 3 subLoss 6101.1 multi 15.93 import weight 0.00
Epoch 444 Iter 4 subLoss 4638.8 multi -4.97 import weight 0.00
Epoch 444 Iter 5 subLoss 5690.2 multi -7.96 import weight 0.00
Epoch 444 Iter 6 subLoss 13247.6 multi -1.99 import weight 0.00
Epoch 444 Iter 7 subLoss 22485.5 multi 1.00 import weight 0.00
Epoch 444 Iter 8 subLoss 14865.3 multi -1.99 import weight 0.00
Epoch 444 Iter 9 subLoss 22154.2 multi 1.00 import weight 0.00
Epoch 444 Iter 10 subLoss 16292.5 multi -1.99 import weight 0.00
Epoch 444 Iter 11 subLoss 23004.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 444 Acc: 80.44 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2300 train Loss: 18096.5 test Loss: 2540.1
Epoch 445 Iter 0 subLoss 17974.4 multi -1.99 import weight 0.00
Epoch 445 Iter 1 subLoss 25160.5 multi 1.00 import weight 0.00
Epoch 445 Iter 2 subLoss 20107.8 multi -1.99 import weight 0.00
Epoch 445 Iter 3 subLoss 27317.9 multi 3.99 import weight 0.00
Epoch 445 Iter 4 subLoss 15547.4 multi 1.00 import weight 0.00
Epoch 445 Iter 5 subLoss 14477.8 multi 1.00 import weight 0.00
Epoch 445 Iter 6 subLoss 12441.4 multi 3.99 import weight 0.00
Epoch 445 Iter 7 subLoss 8911.7 multi -4.97 import weight 0.00
Epoch 445 Iter 8 subLoss 12845.8 multi 1.00 import weight 0.00
Epoch 445 Iter 9 subLoss 11192.5 multi 6.97 import weight 0.00
Epoch 445 Iter 10 subLoss 6265.4 multi 1.00 import weight 0.00
Epoch 445 Iter 11 subLoss 6098.1 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 445 Acc: 98.03 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 609 train Loss: 6343.0 test Loss: 503.9
Epoch 446 Iter 0 subLoss 6344.3 multi -1.99 import weight 0.00
Epoch 446 Iter 1 subLoss 7665.7 multi 12.94 import weight 0.00
Epoch 446 Iter 2 subLoss 3660.7 multi 21.90 import weight 0.00
Epoch 446 Iter 3 subLoss 3586.5 multi 18.91 import weight 0.00
Epoch 446 Iter 4 subLoss 3059.4 multi 6.97 import weight 0.00
Epoch 446 Iter 5 subLoss 2306.2 multi -7.96 import weight 0.00
Epoch 446 Iter 6 subLoss 2932.5 multi 12.94 import weight 0.00
Epoch 446 Iter 7 subLoss 2160.0 multi -16.91 import weight 0.00
Epoch 446 Iter 8 subLoss 2437.2 multi 12.94 import weight 1.00
Epoch 446 Iter 9 subLoss 2401.2 multi -1.98 import weight 0.00
Epoch 446 Iter 10 subLoss 2510.3 multi 3.98 import weight 0.00
Epoch 446 Iter 11 subLoss 2003.2 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 446 Acc: 98.62 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 200 train Loss: 2620.6 test Loss: 220.6
Epoch 447 Iter 0 subLoss 2604.3 multi -13.93 import weight 0.00
Epoch 447 Iter 1 subLoss 2731.5 multi 3.98 import weight 0.00
Epoch 447 Iter 2 subLoss 3031.8 multi -10.94 import weight 0.00
Epoch 447 Iter 3 subLoss 2902.0 multi -22.88 import weight 0.00
Epoch 447 Iter 4 subLoss 4866.5 multi 1.00 import weight 0.00
Epoch 447 Iter 5 subLoss 3935.4 multi 1.00 import weight 0.00
Epoch 447 Iter 6 subLoss 4193.7 multi 9.96 import weight 0.00
Epoch 447 Iter 7 subLoss 2828.2 multi 6.97 import weight 0.00
Epoch 447 Iter 8 subLoss 2875.1 multi 3.99 import weight 0.00
Epoch 447 Iter 9 subLoss 2607.9 multi -10.94 import weight 0.00
Epoch 447 Iter 10 subLoss 2671.3 multi 12.94 import weight 0.00
Epoch 447 Iter 11 subLoss 2889.2 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 447 Acc: 98.42 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 288 train Loss: 3378.3 test Loss: 296.9
Epoch 448 Iter 0 subLoss 3048.2 multi 3.99 import weight 0.00
Epoch 448 Iter 1 subLoss 3240.7 multi -7.96 import weight 0.00
Epoch 448 Iter 2 subLoss 3585.7 multi 21.90 import weight 0.00
Epoch 448 Iter 3 subLoss 2362.2 multi 15.93 import weight 0.00
Epoch 448 Iter 4 subLoss 3031.8 multi -7.96 import weight 0.00
Epoch 448 Iter 5 subLoss 2626.2 multi 21.90 import weight 0.00
Epoch 448 Iter 6 subLoss 2623.9 multi 24.88 import weight 0.00
Epoch 448 Iter 7 subLoss 2308.1 multi -4.97 import weight 0.00
Epoch 448 Iter 8 subLoss 2929.4 multi -10.94 import weight 0.00
Epoch 448 Iter 9 subLoss 3996.9 multi -7.96 import weight 0.00
Epoch 448 Iter 10 subLoss 11909.5 multi -7.96 import weight 0.00
Epoch 448 Iter 11 subLoss 286836.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 448 Acc: 89.34 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 28683 train Loss: 16547.2 test Loss: 1838.5
Epoch 449 Iter 0 subLoss 16618.1 multi -4.97 import weight 0.00
Epoch 449 Iter 1 subLoss 34332.1 multi 1.00 import weight 0.00
Epoch 449 Iter 2 subLoss 30918.8 multi 3.99 import weight 0.00
Epoch 449 Iter 3 subLoss 18702.6 multi 6.97 import weight 0.00
Epoch 449 Iter 4 subLoss 6502.6 multi -10.94 import weight 0.00
Epoch 449 Iter 5 subLoss 13760.7 multi -1.99 import weight 0.00
Epoch 449 Iter 6 subLoss 15831.8 multi 1.00 import weight 0.00
Epoch 449 Iter 7 subLoss 15481.5 multi -1.98 import weight 0.00
Epoch 449 Iter 8 subLoss 17644.8 multi 1.00 import weight 0.00
Epoch 449 Iter 9 subLoss 16601.9 multi 1.00 import weight 0.00
Epoch 449 Iter 10 subLoss 15462.4 multi -1.99 import weight 0.00
Epoch 449 Iter 11 subLoss 17874.9 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 449 Acc: 79.68 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 1787 train Loss: 24865.4 test Loss: 3145.1
Epoch 450 Iter 0 subLoss 24411.2 multi 1.00 import weight 0.00
Epoch 450 Iter 1 subLoss 23067.8 multi -1.99 import weight 0.00
Epoch 450 Iter 2 subLoss 26217.9 multi 1.00 import weight 0.00
Epoch 450 Iter 3 subLoss 24522.0 multi 1.00 import weight 0.00
Epoch 450 Iter 4 subLoss 20816.3 multi -1.99 import weight 0.00
Epoch 450 Iter 5 subLoss 26004.9 multi 1.00 import weight 0.00
Epoch 450 Iter 6 subLoss 23467.9 multi 1.00 import weight 0.00
Epoch 450 Iter 7 subLoss 20835.2 multi 3.99 import weight 0.00
Epoch 450 Iter 8 subLoss 16384.3 multi 1.00 import weight 0.00
Epoch 450 Iter 9 subLoss 15734.2 multi -1.98 import weight 0.00
Epoch 450 Iter 10 subLoss 16303.4 multi -1.99 import weight 0.00
Epoch 450 Iter 11 subLoss 18786.5 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 450 Acc: 78.11 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 1878 train Loss: 24974.2 test Loss: 3175.0
Epoch 451 Iter 0 subLoss 23406.5 multi 1.00 import weight 0.00
Epoch 451 Iter 1 subLoss 23909.7 multi 6.97 import weight 0.00
Epoch 451 Iter 2 subLoss 15649.4 multi 3.99 import weight 0.00
Epoch 451 Iter 3 subLoss 12977.3 multi -1.99 import weight 0.00
Epoch 451 Iter 4 subLoss 14578.8 multi 3.99 import weight 0.00
Epoch 451 Iter 5 subLoss 11845.8 multi -7.96 import weight 0.00
Epoch 451 Iter 6 subLoss 16650.4 multi 1.00 import weight 0.00
Epoch 451 Iter 7 subLoss 15465.6 multi 1.00 import weight 0.00
Epoch 451 Iter 8 subLoss 14781.0 multi 1.00 import weight 0.00
Epoch 451 Iter 9 subLoss 14256.1 multi 1.00 import weight 0.00
Epoch 451 Iter 10 subLoss 14011.7 multi 3.99 import weight 0.00
Epoch 451 Iter 11 subLoss 12481.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 451 Acc: 95.19 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1248 train Loss: 11792.5 test Loss: 1078.2
Epoch 452 Iter 0 subLoss 11057.6 multi -1.98 import weight 0.00
Epoch 452 Iter 1 subLoss 12123.9 multi 1.00 import weight 0.00
Epoch 452 Iter 2 subLoss 11813.6 multi -4.97 import weight 0.00
Epoch 452 Iter 3 subLoss 13802.9 multi -7.96 import weight 0.00
Epoch 452 Iter 4 subLoss 19507.6 multi 1.00 import weight 0.00
Epoch 452 Iter 5 subLoss 17533.1 multi 1.00 import weight 0.00
Epoch 452 Iter 6 subLoss 16727.8 multi 3.99 import weight 0.00
Epoch 452 Iter 7 subLoss 15286.5 multi 3.99 import weight 0.00
Epoch 452 Iter 8 subLoss 13238.8 multi -1.98 import weight 0.00
Epoch 452 Iter 9 subLoss 13977.2 multi -1.99 import weight 0.00
Epoch 452 Iter 10 subLoss 14771.1 multi -4.97 import weight 0.00
Epoch 452 Iter 11 subLoss 16842.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 452 Acc: 91.85 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1684 train Loss: 16763.1 test Loss: 1784.8
Epoch 453 Iter 0 subLoss 16627.4 multi -1.99 import weight 0.00
Epoch 453 Iter 1 subLoss 18332.4 multi 1.00 import weight 0.00
Epoch 453 Iter 2 subLoss 16915.3 multi 1.00 import weight 0.00
Epoch 453 Iter 3 subLoss 15812.8 multi -1.99 import weight 0.00
Epoch 453 Iter 4 subLoss 17686.3 multi -4.97 import weight 0.00
Epoch 453 Iter 5 subLoss 18911.6 multi 3.99 import weight 0.00
Epoch 453 Iter 6 subLoss 16589.8 multi 3.99 import weight 0.00
Epoch 453 Iter 7 subLoss 15468.5 multi 3.98 import weight 0.00
Epoch 453 Iter 8 subLoss 13694.3 multi 1.00 import weight 0.00
Epoch 453 Iter 9 subLoss 13187.5 multi 3.98 import weight 0.00
Epoch 453 Iter 10 subLoss 11578.2 multi 6.97 import weight 0.00
Epoch 453 Iter 11 subLoss 10363.8 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 453 Acc: 95.37 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 1036 train Loss: 10979.7 test Loss: 978.8
Epoch 454 Iter 0 subLoss 11226.8 multi -1.99 import weight 0.00
Epoch 454 Iter 1 subLoss 11520.6 multi -1.99 import weight 0.00
Epoch 454 Iter 2 subLoss 11680.2 multi 1.00 import weight 0.00
Epoch 454 Iter 3 subLoss 11225.1 multi 1.00 import weight 0.00
Epoch 454 Iter 4 subLoss 11373.3 multi 3.98 import weight 0.00
Epoch 454 Iter 5 subLoss 10671.9 multi -4.97 import weight 0.00
Epoch 454 Iter 6 subLoss 11377.4 multi 6.97 import weight 0.00
Epoch 454 Iter 7 subLoss 9464.5 multi -1.98 import weight 0.00
Epoch 454 Iter 8 subLoss 10107.1 multi -1.99 import weight 0.00
Epoch 454 Iter 9 subLoss 10049.9 multi 6.97 import weight 0.00
Epoch 454 Iter 10 subLoss 8855.1 multi -1.98 import weight 0.00
Epoch 454 Iter 11 subLoss 9133.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 454 Acc: 96.28 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 913 train Loss: 8071.4 test Loss: 702.2
Epoch 455 Iter 0 subLoss 7974.9 multi 1.00 import weight 0.00
Epoch 455 Iter 1 subLoss 7612.7 multi -1.99 import weight 0.00
Epoch 455 Iter 2 subLoss 8530.2 multi 9.96 import weight 0.00
Epoch 455 Iter 3 subLoss 6810.0 multi 1.00 import weight 0.00
Epoch 455 Iter 4 subLoss 5886.8 multi -1.98 import weight 0.00
Epoch 455 Iter 5 subLoss 7037.2 multi 15.93 import weight 0.00
Epoch 455 Iter 6 subLoss 4883.1 multi -19.90 import weight 0.00
Epoch 455 Iter 7 subLoss 8839.1 multi 1.00 import weight 0.00
Epoch 455 Iter 8 subLoss 7651.2 multi 3.98 import weight 0.00
Epoch 455 Iter 9 subLoss 6453.7 multi 6.97 import weight 0.00
Epoch 455 Iter 10 subLoss 5211.2 multi 1.00 import weight 0.00
Epoch 455 Iter 11 subLoss 5216.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 455 Acc: 96.81 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 521 train Loss: 4750.2 test Loss: 522.5
Epoch 456 Iter 0 subLoss 3985.1 multi -4.97 import weight 0.00
Epoch 456 Iter 1 subLoss 4965.9 multi -10.94 import weight 0.00
Epoch 456 Iter 2 subLoss 6307.1 multi 1.00 import weight 0.00
Epoch 456 Iter 3 subLoss 5641.3 multi -1.99 import weight 0.00
Epoch 456 Iter 4 subLoss 5893.7 multi 1.00 import weight 0.00
Epoch 456 Iter 5 subLoss 6011.5 multi 6.97 import weight 0.00
Epoch 456 Iter 6 subLoss 5542.2 multi 9.96 import weight 0.00
Epoch 456 Iter 7 subLoss 4807.1 multi 6.97 import weight 0.00
Epoch 456 Iter 8 subLoss 3728.4 multi 6.97 import weight 0.00
Epoch 456 Iter 9 subLoss 3747.4 multi -1.99 import weight 0.00
Epoch 456 Iter 10 subLoss 3920.3 multi -4.97 import weight 0.00
Epoch 456 Iter 11 subLoss 4676.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 456 Acc: 97.26 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 467 train Loss: 4071.2 test Loss: 447.4
Epoch 457 Iter 0 subLoss 3620.4 multi 12.94 import weight 0.00
Epoch 457 Iter 1 subLoss 3528.2 multi 15.93 import weight 0.00
Epoch 457 Iter 2 subLoss 3423.2 multi -16.91 import weight 0.00
Epoch 457 Iter 3 subLoss 3612.9 multi -22.88 import weight 0.00
Epoch 457 Iter 4 subLoss 6487.9 multi -4.97 import weight 0.00
Epoch 457 Iter 5 subLoss 18472.9 multi 6.97 import weight 0.00
Epoch 457 Iter 6 subLoss 8436.8 multi 1.00 import weight 0.00
Epoch 457 Iter 7 subLoss 8282.7 multi 3.99 import weight 0.00
Epoch 457 Iter 8 subLoss 5876.8 multi 1.00 import weight 0.00
Epoch 457 Iter 9 subLoss 5727.8 multi -7.96 import weight 0.00
Epoch 457 Iter 10 subLoss 8844.4 multi 1.00 import weight 0.00
Epoch 457 Iter 11 subLoss 8365.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 457 Acc: 94.12 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 836 train Loss: 5720.6 test Loss: 865.6
Epoch 458 Iter 0 subLoss 5623.8 multi 1.00 import weight 0.00
Epoch 458 Iter 1 subLoss 6017.6 multi 9.96 import weight 0.00
Epoch 458 Iter 2 subLoss 4038.3 multi -1.99 import weight 0.00
Epoch 458 Iter 3 subLoss 3980.3 multi -1.99 import weight 0.00
Epoch 458 Iter 4 subLoss 4097.4 multi 3.98 import weight 0.00
Epoch 458 Iter 5 subLoss 3806.6 multi 6.97 import weight 0.00
Epoch 458 Iter 6 subLoss 3423.2 multi -13.93 import weight 0.00
Epoch 458 Iter 7 subLoss 3331.1 multi -19.90 import weight 0.00
Epoch 458 Iter 8 subLoss 5864.6 multi 1.00 import weight 0.00
Epoch 458 Iter 9 subLoss 5785.5 multi 1.00 import weight 0.00
Epoch 458 Iter 10 subLoss 5653.9 multi 3.98 import weight 0.00
Epoch 458 Iter 11 subLoss 4377.8 multi -19.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 458 Acc: 94.67 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -19.90 Pidx 437 train Loss: 8456.4 test Loss: 950.0
Epoch 459 Iter 0 subLoss 8325.4 multi 1.00 import weight 0.00
Epoch 459 Iter 1 subLoss 7336.6 multi 6.97 import weight 0.00
Epoch 459 Iter 2 subLoss 5162.2 multi -1.99 import weight 0.00
Epoch 459 Iter 3 subLoss 5413.9 multi 6.97 import weight 0.00
Epoch 459 Iter 4 subLoss 4481.0 multi 15.93 import weight 0.00
Epoch 459 Iter 5 subLoss 4127.6 multi -13.93 import weight 0.00
Epoch 459 Iter 6 subLoss 5309.5 multi 1.00 import weight 0.00
Epoch 459 Iter 7 subLoss 4706.8 multi -4.97 import weight 0.00
Epoch 459 Iter 8 subLoss 6509.5 multi -7.96 import weight 0.00
Epoch 459 Iter 9 subLoss 20425.6 multi -1.99 import weight 0.00
Epoch 459 Iter 10 subLoss 54281.1 multi 1.00 import weight 0.00
Epoch 459 Iter 11 subLoss 17830.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 459 Acc: 96.13 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 1783 train Loss: 5804.4 test Loss: 676.8
Epoch 460 Iter 0 subLoss 5997.9 multi 3.99 import weight 0.00
Epoch 460 Iter 1 subLoss 4668.9 multi -1.98 import weight 0.00
Epoch 460 Iter 2 subLoss 5363.8 multi 15.93 import weight 0.00
Epoch 460 Iter 3 subLoss 3511.8 multi -13.93 import weight 0.00
Epoch 460 Iter 4 subLoss 4489.4 multi 18.91 import weight 0.00
Epoch 460 Iter 5 subLoss 3842.2 multi 6.97 import weight 0.00
Epoch 460 Iter 6 subLoss 3404.2 multi 3.98 import weight 0.00
Epoch 460 Iter 7 subLoss 2899.6 multi 24.88 import weight 0.00
Epoch 460 Iter 8 subLoss 2723.7 multi 3.99 import weight 0.00
Epoch 460 Iter 9 subLoss 2426.3 multi 18.91 import weight 0.00
Epoch 460 Iter 10 subLoss 2532.9 multi -1.99 import weight 0.00
Epoch 460 Iter 11 subLoss 3283.0 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 460 Acc: 98.09 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 328 train Loss: 2973.6 test Loss: 306.6
Epoch 461 Iter 0 subLoss 2510.4 multi 6.97 import weight 0.00
Epoch 461 Iter 1 subLoss 2636.3 multi -10.94 import weight 0.00
Epoch 461 Iter 2 subLoss 3267.5 multi 6.97 import weight 0.00
Epoch 461 Iter 3 subLoss 3113.3 multi -10.94 import weight 0.00
Epoch 461 Iter 4 subLoss 2910.0 multi -22.88 import weight 0.00
Epoch 461 Iter 5 subLoss 5199.8 multi -4.97 import weight 0.00
Epoch 461 Iter 6 subLoss 8926.3 multi 6.97 import weight 0.00
Epoch 461 Iter 7 subLoss 4079.7 multi 6.97 import weight 0.00
Epoch 461 Iter 8 subLoss 2619.1 multi -13.93 import weight 0.00
Epoch 461 Iter 9 subLoss 3439.2 multi 3.99 import weight 0.00
Epoch 461 Iter 10 subLoss 2977.9 multi -13.93 import weight 0.00
Epoch 461 Iter 11 subLoss 3822.5 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 461 Acc: 95.60 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 382 train Loss: 4842.3 test Loss: 690.8
Epoch 462 Iter 0 subLoss 4565.0 multi 15.93 import weight 0.00
Epoch 462 Iter 1 subLoss 3367.1 multi 3.99 import weight 0.00
Epoch 462 Iter 2 subLoss 2477.7 multi 1.00 import weight 0.00
Epoch 462 Iter 3 subLoss 3095.7 multi 6.97 import weight 0.00
Epoch 462 Iter 4 subLoss 2944.0 multi -7.96 import weight 0.00
Epoch 462 Iter 5 subLoss 2741.3 multi -7.96 import weight 0.00
Epoch 462 Iter 6 subLoss 2673.2 multi 15.93 import weight 0.00
Epoch 462 Iter 7 subLoss 3039.9 multi -4.97 import weight 0.00
Epoch 462 Iter 8 subLoss 2707.0 multi 15.93 import weight 0.00
Epoch 462 Iter 9 subLoss 2393.5 multi -4.97 import weight 0.00
Epoch 462 Iter 10 subLoss 3206.6 multi -7.96 import weight 0.00
Epoch 462 Iter 11 subLoss 3259.6 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 462 Acc: 98.48 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 325 train Loss: 3120.3 test Loss: 271.4
Epoch 463 Iter 0 subLoss 2944.7 multi -4.97 import weight 0.00
Epoch 463 Iter 1 subLoss 3891.1 multi 12.94 import weight 0.00
Epoch 463 Iter 2 subLoss 3443.4 multi -19.90 import weight 0.00
Epoch 463 Iter 3 subLoss 2939.5 multi 12.94 import weight 0.00
Epoch 463 Iter 4 subLoss 2628.3 multi 24.88 import weight 0.00
Epoch 463 Iter 5 subLoss 2398.5 multi -1.99 import weight 0.00
Epoch 463 Iter 6 subLoss 2621.2 multi 27.87 import weight 1.00
Epoch 463 Iter 7 subLoss 2578.1 multi 36.82 import weight 1.00
Epoch 463 Iter 8 subLoss 2731.5 multi 3.99 import weight 0.00
Epoch 463 Iter 9 subLoss 2425.7 multi 21.90 import weight 0.00
Epoch 463 Iter 10 subLoss 2077.8 multi -1.99 import weight 0.00
Epoch 463 Iter 11 subLoss 2796.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 463 Acc: 98.44 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 279 train Loss: 2466.7 test Loss: 255.0
Epoch 464 Iter 0 subLoss 2860.0 multi 12.94 import weight 0.00
Epoch 464 Iter 1 subLoss 2452.2 multi -1.98 import weight 0.00
Epoch 464 Iter 2 subLoss 1999.7 multi 6.97 import weight 0.00
Epoch 464 Iter 3 subLoss 2093.4 multi 18.91 import weight 0.00
Epoch 464 Iter 4 subLoss 2293.7 multi -19.90 import weight 0.00
Epoch 464 Iter 5 subLoss 2669.9 multi 6.97 import weight 0.00
Epoch 464 Iter 6 subLoss 2296.7 multi -16.91 import weight 0.00
Epoch 464 Iter 7 subLoss 2573.8 multi 39.81 import weight 1.00
Epoch 464 Iter 8 subLoss 3024.1 multi -16.91 import weight 0.00
Epoch 464 Iter 9 subLoss 10359.6 multi 3.98 import weight 0.00
Epoch 464 Iter 10 subLoss 3173.6 multi 33.84 import weight 0.00
Epoch 464 Iter 11 subLoss 3236.1 multi 33.84 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 464 Acc: 97.94 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 33.84 Pidx 323 train Loss: 3659.9 test Loss: 366.5
Epoch 465 Iter 0 subLoss 3795.8 multi -19.90 import weight 0.00
Epoch 465 Iter 1 subLoss 25677.9 multi 1.00 import weight 0.00
Epoch 465 Iter 2 subLoss 8822.1 multi 3.99 import weight 0.00
Epoch 465 Iter 3 subLoss 3669.6 multi 24.88 import weight 0.00
Epoch 465 Iter 4 subLoss 3066.8 multi -13.93 import weight 0.00
Epoch 465 Iter 5 subLoss 4221.8 multi 1.00 import weight 0.00
Epoch 465 Iter 6 subLoss 4253.5 multi 12.94 import weight 0.00
Epoch 465 Iter 7 subLoss 2341.5 multi -19.90 import weight 0.00
Epoch 465 Iter 8 subLoss 2439.4 multi 9.96 import weight 0.00
Epoch 465 Iter 9 subLoss 2360.6 multi 18.91 import weight 0.00
Epoch 465 Iter 10 subLoss 2155.5 multi -13.93 import weight 0.00
Epoch 465 Iter 11 subLoss 2704.5 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 465 Acc: 98.52 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 270 train Loss: 2273.1 test Loss: 223.4
Epoch 466 Iter 0 subLoss 1918.3 multi 6.97 import weight 0.00
Epoch 466 Iter 1 subLoss 2128.6 multi 15.93 import weight 0.00
Epoch 466 Iter 2 subLoss 1949.2 multi 9.96 import weight 0.00
Epoch 466 Iter 3 subLoss 2181.9 multi 15.93 import weight 0.00
Epoch 466 Iter 4 subLoss 2105.1 multi -19.90 import weight 0.00
Epoch 466 Iter 5 subLoss 2629.5 multi 30.85 import weight 0.00
Epoch 466 Iter 6 subLoss 2474.9 multi 3.99 import weight 0.00
Epoch 466 Iter 7 subLoss 2003.2 multi -13.93 import weight 0.00
Epoch 466 Iter 8 subLoss 2225.7 multi -1.99 import weight 0.00
Epoch 466 Iter 9 subLoss 2705.6 multi 21.90 import weight 0.00
Epoch 466 Iter 10 subLoss 2466.5 multi -7.96 import weight 0.00
Epoch 466 Iter 11 subLoss 2263.3 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 466 Acc: 97.10 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 226 train Loss: 3281.7 test Loss: 443.7
Epoch 467 Iter 0 subLoss 3129.5 multi -4.97 import weight 0.00
Epoch 467 Iter 1 subLoss 4674.9 multi 1.00 import weight 0.00
Epoch 467 Iter 2 subLoss 4267.2 multi -7.96 import weight 0.00
Epoch 467 Iter 3 subLoss 8652.9 multi 12.94 import weight 0.00
Epoch 467 Iter 4 subLoss 3135.0 multi 12.94 import weight 0.00
Epoch 467 Iter 5 subLoss 2242.8 multi -7.96 import weight 0.00
Epoch 467 Iter 6 subLoss 2609.7 multi -7.96 import weight 0.00
Epoch 467 Iter 7 subLoss 2346.0 multi -16.91 import weight 0.00
Epoch 467 Iter 8 subLoss 2709.1 multi 24.88 import weight 0.00
Epoch 467 Iter 9 subLoss 2098.2 multi 21.90 import weight 0.00
Epoch 467 Iter 10 subLoss 1800.4 multi 6.97 import weight 0.00
Epoch 467 Iter 11 subLoss 1963.5 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 467 Acc: 98.72 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 196 train Loss: 2261.7 test Loss: 216.2
Epoch 468 Iter 0 subLoss 2729.5 multi 6.97 import weight 0.00
Epoch 468 Iter 1 subLoss 2272.5 multi 3.98 import weight 0.00
Epoch 468 Iter 2 subLoss 2091.8 multi 24.88 import weight 0.00
Epoch 468 Iter 3 subLoss 2238.7 multi -4.97 import weight 0.00
Epoch 468 Iter 4 subLoss 2244.5 multi -7.96 import weight 0.00
Epoch 468 Iter 5 subLoss 1953.0 multi -4.97 import weight 0.00
Epoch 468 Iter 6 subLoss 2395.1 multi 1.00 import weight 0.00
Epoch 468 Iter 7 subLoss 2181.6 multi 18.91 import weight 0.00
Epoch 468 Iter 8 subLoss 2036.0 multi -10.94 import weight 0.00
Epoch 468 Iter 9 subLoss 2127.3 multi 18.91 import weight 0.00
Epoch 468 Iter 10 subLoss 1784.1 multi 1.00 import weight 0.00
Epoch 468 Iter 11 subLoss 2050.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 468 Acc: 98.72 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 205 train Loss: 2079.8 test Loss: 214.5
Epoch 469 Iter 0 subLoss 1803.4 multi 9.96 import weight 0.00
Epoch 469 Iter 1 subLoss 1903.3 multi -1.98 import weight 0.00
Epoch 469 Iter 2 subLoss 2190.8 multi -19.90 import weight 0.00
Epoch 469 Iter 3 subLoss 1965.0 multi -16.91 import weight 0.00
Epoch 469 Iter 4 subLoss 2362.9 multi 21.90 import weight 0.00
Epoch 469 Iter 5 subLoss 1985.6 multi -7.96 import weight 0.00
Epoch 469 Iter 6 subLoss 2308.0 multi -7.96 import weight 0.00
Epoch 469 Iter 7 subLoss 2779.5 multi 12.94 import weight 0.00
Epoch 469 Iter 8 subLoss 1916.7 multi 6.97 import weight 0.00
Epoch 469 Iter 9 subLoss 2455.0 multi 1.00 import weight 0.00
Epoch 469 Iter 10 subLoss 1954.6 multi -1.98 import weight 0.00
Epoch 469 Iter 11 subLoss 2217.9 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 469 Acc: 98.81 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 221 train Loss: 2134.9 test Loss: 197.9
Epoch 470 Iter 0 subLoss 1805.2 multi 12.94 import weight 0.00
Epoch 470 Iter 1 subLoss 2079.3 multi 1.00 import weight 0.00
Epoch 470 Iter 2 subLoss 2467.7 multi -7.96 import weight 0.00
Epoch 470 Iter 3 subLoss 2276.1 multi 6.97 import weight 0.00
Epoch 470 Iter 4 subLoss 1638.4 multi -1.99 import weight 0.00
Epoch 470 Iter 5 subLoss 2136.1 multi -4.97 import weight 0.00
Epoch 470 Iter 6 subLoss 2208.6 multi 18.91 import weight 0.00
Epoch 470 Iter 7 subLoss 2489.3 multi -4.97 import weight 0.00
Epoch 470 Iter 8 subLoss 2029.5 multi 18.91 import weight 0.00
Epoch 470 Iter 9 subLoss 1725.6 multi 12.94 import weight 0.00
Epoch 470 Iter 10 subLoss 1759.7 multi 9.96 import weight 0.00
Epoch 470 Iter 11 subLoss 1766.9 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 470 Acc: 98.83 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 176 train Loss: 2046.8 test Loss: 193.2
Epoch 471 Iter 0 subLoss 2218.9 multi -1.99 import weight 0.00
Epoch 471 Iter 1 subLoss 1957.1 multi 1.00 import weight 0.00
Epoch 471 Iter 2 subLoss 1679.8 multi -1.99 import weight 0.00
Epoch 471 Iter 3 subLoss 1835.2 multi -1.98 import weight 0.00
Epoch 471 Iter 4 subLoss 2145.5 multi 9.96 import weight 0.00
Epoch 471 Iter 5 subLoss 1984.7 multi -4.97 import weight 0.00
Epoch 471 Iter 6 subLoss 2014.8 multi -7.96 import weight 0.00
Epoch 471 Iter 7 subLoss 2223.1 multi -4.97 import weight 0.00
Epoch 471 Iter 8 subLoss 2305.6 multi -4.97 import weight 0.00
Epoch 471 Iter 9 subLoss 2050.4 multi 1.00 import weight 0.00
Epoch 471 Iter 10 subLoss 2162.2 multi 33.84 import weight 0.00
Epoch 471 Iter 11 subLoss 2316.6 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 471 Acc: 97.88 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 231 train Loss: 2550.8 test Loss: 343.9
Epoch 472 Iter 0 subLoss 2088.3 multi 3.99 import weight 0.00
Epoch 472 Iter 1 subLoss 2908.1 multi -19.90 import weight 0.00
Epoch 472 Iter 2 subLoss 4679.8 multi 3.99 import weight 0.00
Epoch 472 Iter 3 subLoss 2281.9 multi 15.93 import weight 0.00
Epoch 472 Iter 4 subLoss 1635.8 multi 1.00 import weight 0.00
Epoch 472 Iter 5 subLoss 2259.2 multi 9.96 import weight 0.00
Epoch 472 Iter 6 subLoss 2042.3 multi 21.90 import weight 0.00
Epoch 472 Iter 7 subLoss 1837.2 multi 1.00 import weight 0.00
Epoch 472 Iter 8 subLoss 2063.4 multi -25.87 import weight 0.00
Epoch 472 Iter 9 subLoss 2219.9 multi 1.00 import weight 0.00
Epoch 472 Iter 10 subLoss 1990.1 multi 3.99 import weight 0.00
Epoch 472 Iter 11 subLoss 1862.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 472 Acc: 98.87 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 186 train Loss: 2121.1 test Loss: 188.4
Epoch 473 Iter 0 subLoss 1952.8 multi 3.99 import weight 0.00
Epoch 473 Iter 1 subLoss 1851.0 multi -19.90 import weight 0.00
Epoch 473 Iter 2 subLoss 2003.2 multi -13.93 import weight 0.00
Epoch 473 Iter 3 subLoss 2141.9 multi 12.94 import weight 0.00
Epoch 473 Iter 4 subLoss 2088.4 multi 6.97 import weight 0.00
Epoch 473 Iter 5 subLoss 2300.6 multi -1.99 import weight 0.00
Epoch 473 Iter 6 subLoss 2032.9 multi -10.94 import weight 0.00
Epoch 473 Iter 7 subLoss 2635.3 multi -16.91 import weight 0.00
Epoch 473 Iter 8 subLoss 2703.1 multi 27.87 import weight 0.00
Epoch 473 Iter 9 subLoss 2230.7 multi -4.97 import weight 0.00
Epoch 473 Iter 10 subLoss 2496.2 multi 1.00 import weight 0.00
Epoch 473 Iter 11 subLoss 2469.8 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 473 Acc: 97.78 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 246 train Loss: 3115.1 test Loss: 371.2
Epoch 474 Iter 0 subLoss 3159.7 multi -25.87 import weight 0.00
Epoch 474 Iter 1 subLoss 67948.7 multi 1.00 import weight 0.00
Epoch 474 Iter 2 subLoss 2579.2 multi 42.79 import weight 1.00
Epoch 474 Iter 3 subLoss 2370.5 multi -34.82 import weight 0.00
Epoch 474 Iter 4 subLoss 21167.0 multi 1.00 import weight 0.00
Epoch 474 Iter 5 subLoss 9865.7 multi -1.98 import weight 0.00
Epoch 474 Iter 6 subLoss 17849.7 multi -4.97 import weight 0.00
Epoch 474 Iter 7 subLoss 185867.8 multi 1.00 import weight 0.00
Epoch 474 Iter 8 subLoss 16715.5 multi 1.00 import weight 0.00
Epoch 474 Iter 9 subLoss 12863.5 multi 1.00 import weight 0.00
Epoch 474 Iter 10 subLoss 10230.5 multi 1.00 import weight 0.00
Epoch 474 Iter 11 subLoss 8127.5 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 474 Acc: 90.89 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 812 train Loss: 11229.8 test Loss: 1623.5
Epoch 475 Iter 0 subLoss 11939.9 multi 3.99 import weight 0.00
Epoch 475 Iter 1 subLoss 5556.2 multi -10.94 import weight 0.00
Epoch 475 Iter 2 subLoss 11495.8 multi 1.00 import weight 0.00
Epoch 475 Iter 3 subLoss 9442.2 multi 3.98 import weight 0.00
Epoch 475 Iter 4 subLoss 6534.1 multi -1.99 import weight 0.00
Epoch 475 Iter 5 subLoss 7085.7 multi 1.00 import weight 0.00
Epoch 475 Iter 6 subLoss 6786.1 multi -10.94 import weight 0.00
Epoch 475 Iter 7 subLoss 16342.6 multi 3.98 import weight 0.00
Epoch 475 Iter 8 subLoss 8373.2 multi -10.94 import weight 0.00
Epoch 475 Iter 9 subLoss 15678.6 multi 1.00 import weight 0.00
Epoch 475 Iter 10 subLoss 13769.3 multi 1.00 import weight 0.00
Epoch 475 Iter 11 subLoss 12172.9 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0047 / 0.12879 / 21.79
Entropy seen (from low to high)
[3464, 495, 217, 131, 115, 70, 68, 58, 51, 29, 37, 41, 11, 14, 24, 20, 16, 28, 16, 31, 38, 45, 22, 18, 9, 13, 8, 3, 7, 8, 4, 2, 2, 4, 5, 3, 2, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 10, 34, 19, 41, 47, 51, 75, 59, 97, 91, 110, 93, 136, 115, 117, 118, 123, 104, 121, 117, 122, 110, 139, 131, 185, 156, 115, 113, 93, 115, 93, 100, 99, 95, 98, 112, 111, 119, 123, 143, 143, 144, 162, 167, 195]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 32.0, 38.1, 40.7, 43.9, 47.3, 51.1, 54.1, 57.8, 61.2, 63.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 0.0, 74.9, 66.6, 62.4, 24.9, 87.4, 63.6, 84.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 6, 4, 6, 16, 16, 16, 11, 20, 25]
Epoch 475 Acc: 90.33 BMA: 98.64 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 1217 train Loss: 9149.5 test Loss: 1466.4
Epoch 476 Iter 0 subLoss 9646.4 multi 1.00 import weight 0.00
Epoch 476 Iter 1 subLoss 8410.9 multi -4.97 import weight 0.00
Epoch 476 Iter 2 subLoss 10539.2 multi 1.00 import weight 0.00
Epoch 476 Iter 3 subLoss 10296.0 multi 1.00 import weight 0.00
Epoch 476 Iter 4 subLoss 10553.1 multi -1.99 import weight 0.00
Epoch 476 Iter 5 subLoss 10346.8 multi -1.98 import weight 0.00
Epoch 476 Iter 6 subLoss 12911.6 multi -4.97 import weight 0.00
Epoch 476 Iter 7 subLoss 18209.2 multi -1.99 import weight 0.00
Epoch 476 Iter 8 subLoss 23900.7 multi 9.96 import weight 0.00
Epoch 476 Iter 9 subLoss 10977.3 multi 1.00 import weight 0.00
Epoch 476 Iter 10 subLoss 9397.4 multi 3.99 import weight 0.00
Epoch 476 Iter 11 subLoss 6493.3 multi 9.96 import weight 0.00
