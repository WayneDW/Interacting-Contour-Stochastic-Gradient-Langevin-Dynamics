Namespace(N=50000, T=0.1, batch=5000, c='csghmc', classes=5, div=10, filters=16, gpu=0, hidden=10, ifprint=1.0, ifsave=1.0, lr=1e-06, part=1000000, seed=39332, sn=500, stepsize=0.01, warm=0.5, wdecay=25, zeta=30000.0)
adjust the learning rate 2.000e-06 weight decay 1.200e+01
(16, 1, 5, 5)
(16,)
(32, 16, 5, 5)
(32,)
(10, 1568)
(10,)
(5, 10)
(5,)
Current Theta
tensor([1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06, 1.0000e-06,
        1.0000e-06], device='cuda:0')
Epoch 0 Iter 0 subLoss 48404.8 multi 1.00 import weight 1.00
Epoch 0 Iter 1 subLoss 48406.0 multi 3.99 import weight 1.00
Epoch 0 Iter 2 subLoss 48105.9 multi 1.00 import weight 0.00
Epoch 0 Iter 3 subLoss 47826.8 multi 1.00 import weight 0.00
Epoch 0 Iter 4 subLoss 47884.3 multi 1.00 import weight 0.00
Epoch 0 Iter 5 subLoss 47589.7 multi 1.00 import weight 0.00
Epoch 0 Iter 6 subLoss 47631.2 multi 1.00 import weight 0.00
Epoch 0 Iter 7 subLoss 47314.3 multi 1.00 import weight 0.00
Epoch 0 Iter 8 subLoss 47186.9 multi 1.00 import weight 0.00
Epoch 0 Iter 9 subLoss 46996.0 multi 1.00 import weight 0.00
Epoch 0 Iter 10 subLoss 46792.9 multi 1.00 import weight 0.00
Epoch 0 Iter 11 subLoss 46694.6 multi 1.00 import weight 0.00
Epoch 0 Acc: 38.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4669 train Loss: 47314.2 test Loss: 7847.3
Epoch 1 Iter 0 subLoss 46383.8 multi 1.00 import weight 0.00
Epoch 1 Iter 1 subLoss 46013.3 multi 1.00 import weight 0.00
Epoch 1 Iter 2 subLoss 45787.2 multi 1.00 import weight 0.00
Epoch 1 Iter 3 subLoss 45605.6 multi 1.00 import weight 0.00
Epoch 1 Iter 4 subLoss 45387.6 multi 1.00 import weight 0.00
Epoch 1 Iter 5 subLoss 45216.8 multi 1.00 import weight 0.00
Epoch 1 Iter 6 subLoss 44869.7 multi 1.00 import weight 0.00
Epoch 1 Iter 7 subLoss 44297.2 multi 1.00 import weight 0.00
Epoch 1 Iter 8 subLoss 44106.0 multi 1.00 import weight 0.00
Epoch 1 Iter 9 subLoss 43561.6 multi 1.00 import weight 0.00
Epoch 1 Iter 10 subLoss 43240.0 multi 1.00 import weight 0.00
Epoch 1 Iter 11 subLoss 42609.6 multi 1.00 import weight 0.00
Epoch 1 Acc: 48.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4260 train Loss: 42995.0 test Loss: 6820.8
Epoch 2 Iter 0 subLoss 42109.2 multi 1.00 import weight 0.00
Epoch 2 Iter 1 subLoss 41631.2 multi 1.00 import weight 0.00
Epoch 2 Iter 2 subLoss 41283.8 multi 1.00 import weight 0.00
Epoch 2 Iter 3 subLoss 40236.9 multi 1.00 import weight 0.00
Epoch 2 Iter 4 subLoss 39516.5 multi 1.00 import weight 0.00
Epoch 2 Iter 5 subLoss 39725.7 multi 1.00 import weight 0.00
Epoch 2 Iter 6 subLoss 40889.3 multi 1.00 import weight 0.00
Epoch 2 Iter 7 subLoss 46384.3 multi 3.99 import weight 1.00
Epoch 2 Iter 8 subLoss 71554.7 multi 1.00 import weight 0.00
Epoch 2 Iter 9 subLoss 47354.6 multi 1.00 import weight 0.00
Epoch 2 Iter 10 subLoss 47010.3 multi 1.00 import weight 0.00
Epoch 2 Iter 11 subLoss 46813.2 multi 1.00 import weight 0.00
Epoch 2 Acc: 38.55 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4681 train Loss: 47542.4 test Loss: 7802.1
Epoch 3 Iter 0 subLoss 46649.0 multi 1.00 import weight 0.00
Epoch 3 Iter 1 subLoss 45969.2 multi 1.00 import weight 0.00
Epoch 3 Iter 2 subLoss 45796.1 multi -1.99 import weight 0.00
Epoch 3 Iter 3 subLoss 46494.0 multi 1.00 import weight 0.00
Epoch 3 Iter 4 subLoss 46226.4 multi 1.00 import weight 0.00
Epoch 3 Iter 5 subLoss 46177.4 multi 1.00 import weight 0.00
Epoch 3 Iter 6 subLoss 45619.7 multi -1.99 import weight 0.00
Epoch 3 Iter 7 subLoss 46651.1 multi -1.99 import weight 0.00
Epoch 3 Iter 8 subLoss 47156.9 multi 1.00 import weight 0.00
Epoch 3 Iter 9 subLoss 46948.2 multi 1.00 import weight 0.00
Epoch 3 Iter 10 subLoss 46699.0 multi 3.99 import weight 1.00
Epoch 3 Iter 11 subLoss 45285.6 multi 1.00 import weight 0.00
Epoch 3 Acc: 51.14 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4528 train Loss: 45611.0 test Loss: 7350.3
Epoch 4 Iter 0 subLoss 44554.2 multi 1.00 import weight 0.00
Epoch 4 Iter 1 subLoss 43677.7 multi 1.00 import weight 0.00
Epoch 4 Iter 2 subLoss 43082.9 multi 1.00 import weight 0.00
Epoch 4 Iter 3 subLoss 42266.4 multi 1.00 import weight 0.00
Epoch 4 Iter 4 subLoss 41560.3 multi 1.00 import weight 0.00
Epoch 4 Iter 5 subLoss 40121.4 multi 1.00 import weight 0.00
Epoch 4 Iter 6 subLoss 40262.5 multi 1.00 import weight 0.00
Epoch 4 Iter 7 subLoss 38950.1 multi 1.00 import weight 0.00
Epoch 4 Iter 8 subLoss 38308.1 multi 1.00 import weight 0.00
Epoch 4 Iter 9 subLoss 37104.3 multi 1.00 import weight 0.00
Epoch 4 Iter 10 subLoss 36888.3 multi 1.00 import weight 0.00
Epoch 4 Iter 11 subLoss 36134.0 multi 1.00 import weight 0.00
Epoch 4 Acc: 60.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3613 train Loss: 35970.7 test Loss: 5334.6
Epoch 5 Iter 0 subLoss 34944.0 multi 1.00 import weight 0.00
Epoch 5 Iter 1 subLoss 34535.7 multi 1.00 import weight 0.00
Epoch 5 Iter 2 subLoss 34139.6 multi 1.00 import weight 0.00
Epoch 5 Iter 3 subLoss 32016.1 multi 1.00 import weight 0.00
Epoch 5 Iter 4 subLoss 32667.3 multi 1.00 import weight 0.00
Epoch 5 Iter 5 subLoss 30965.3 multi 1.00 import weight 0.00
Epoch 5 Iter 6 subLoss 30581.9 multi 1.00 import weight 0.00
Epoch 5 Iter 7 subLoss 31361.9 multi 1.00 import weight 0.00
Epoch 5 Iter 8 subLoss 35101.9 multi 1.00 import weight 0.00
Epoch 5 Iter 9 subLoss 48520.4 multi 1.00 import weight 0.00
Epoch 5 Iter 10 subLoss 33500.1 multi 1.00 import weight 0.00
Epoch 5 Iter 11 subLoss 32296.0 multi 1.00 import weight 0.00
Epoch 5 Acc: 74.92 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3229 train Loss: 30337.9 test Loss: 4067.2
Epoch 6 Iter 0 subLoss 29845.2 multi 1.00 import weight 0.00
Epoch 6 Iter 1 subLoss 28057.5 multi 1.00 import weight 0.00
Epoch 6 Iter 2 subLoss 28817.9 multi 1.00 import weight 0.00
Epoch 6 Iter 3 subLoss 29936.7 multi 1.00 import weight 0.00
Epoch 6 Iter 4 subLoss 30818.2 multi 1.00 import weight 0.00
Epoch 6 Iter 5 subLoss 37955.0 multi 1.00 import weight 0.00
Epoch 6 Iter 6 subLoss 31806.4 multi 1.00 import weight 0.00
Epoch 6 Iter 7 subLoss 31628.6 multi 1.00 import weight 0.00
Epoch 6 Iter 8 subLoss 26552.3 multi 1.00 import weight 0.00
Epoch 6 Iter 9 subLoss 25801.1 multi 1.00 import weight 0.00
Epoch 6 Iter 10 subLoss 25067.6 multi 1.00 import weight 0.00
Epoch 6 Iter 11 subLoss 25727.4 multi 1.00 import weight 0.00
Epoch 6 Acc: 59.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2572 train Loss: 32914.6 test Loss: 4555.3
Epoch 7 Iter 0 subLoss 31553.5 multi 1.00 import weight 0.00
Epoch 7 Iter 1 subLoss 40740.3 multi 1.00 import weight 0.00
Epoch 7 Iter 2 subLoss 31545.6 multi 1.00 import weight 0.00
Epoch 7 Iter 3 subLoss 28406.3 multi 1.00 import weight 0.00
Epoch 7 Iter 4 subLoss 25567.4 multi 1.00 import weight 0.00
Epoch 7 Iter 5 subLoss 24195.6 multi 1.00 import weight 0.00
Epoch 7 Iter 6 subLoss 22988.1 multi 1.00 import weight 0.00
Epoch 7 Iter 7 subLoss 22367.5 multi 1.00 import weight 0.00
Epoch 7 Iter 8 subLoss 22072.2 multi 1.00 import weight 0.00
Epoch 7 Iter 9 subLoss 24520.2 multi 1.00 import weight 0.00
Epoch 7 Iter 10 subLoss 31528.9 multi 1.00 import weight 0.00
Epoch 7 Iter 11 subLoss 36552.1 multi 1.00 import weight 0.00
Epoch 7 Acc: 60.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3655 train Loss: 33372.7 test Loss: 4495.7
Epoch 8 Iter 0 subLoss 32445.3 multi 1.00 import weight 0.00
Epoch 8 Iter 1 subLoss 25076.9 multi -1.99 import weight 0.00
Epoch 8 Iter 2 subLoss 58043.9 multi 1.00 import weight 0.00
Epoch 8 Iter 3 subLoss 31774.3 multi 1.00 import weight 0.00
Epoch 8 Iter 4 subLoss 27653.0 multi 1.00 import weight 0.00
Epoch 8 Iter 5 subLoss 25970.5 multi 1.00 import weight 0.00
Epoch 8 Iter 6 subLoss 24550.4 multi 1.00 import weight 0.00
Epoch 8 Iter 7 subLoss 22718.4 multi 1.00 import weight 0.00
Epoch 8 Iter 8 subLoss 22223.1 multi 1.00 import weight 0.00
Epoch 8 Iter 9 subLoss 21232.5 multi 1.00 import weight 0.00
Epoch 8 Iter 10 subLoss 20571.0 multi 1.00 import weight 0.00
Epoch 8 Iter 11 subLoss 19721.0 multi 1.00 import weight 0.00
Epoch 8 Acc: 87.95 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1972 train Loss: 19927.9 test Loss: 2135.5
Epoch 9 Iter 0 subLoss 19449.1 multi 1.00 import weight 0.00
Epoch 9 Iter 1 subLoss 19167.3 multi 1.00 import weight 0.00
Epoch 9 Iter 2 subLoss 19889.8 multi 1.00 import weight 0.00
Epoch 9 Iter 3 subLoss 20808.2 multi 1.00 import weight 0.00
Epoch 9 Iter 4 subLoss 27063.8 multi 1.00 import weight 0.00
Epoch 9 Iter 5 subLoss 34450.7 multi 1.00 import weight 0.00
Epoch 9 Iter 6 subLoss 27347.2 multi 1.00 import weight 0.00
Epoch 9 Iter 7 subLoss 23026.0 multi 1.00 import weight 0.00
Epoch 9 Iter 8 subLoss 20190.7 multi 1.00 import weight 0.00
Epoch 9 Iter 9 subLoss 19554.8 multi 1.00 import weight 0.00
Epoch 9 Iter 10 subLoss 18319.4 multi 1.00 import weight 0.00
Epoch 9 Iter 11 subLoss 17857.1 multi 1.00 import weight 0.00
Epoch 9 Acc: 91.26 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1785 train Loss: 17934.8 test Loss: 1870.5
Epoch 10 Iter 0 subLoss 17677.7 multi 1.00 import weight 0.00
Epoch 10 Iter 1 subLoss 17203.0 multi 1.00 import weight 0.00
Epoch 10 Iter 2 subLoss 16589.0 multi 1.00 import weight 0.00
Epoch 10 Iter 3 subLoss 18817.0 multi 1.00 import weight 0.00
Epoch 10 Iter 4 subLoss 24377.6 multi 1.00 import weight 0.00
Epoch 10 Iter 5 subLoss 40041.8 multi 1.00 import weight 0.00
Epoch 10 Iter 6 subLoss 29977.1 multi 1.00 import weight 0.00
Epoch 10 Iter 7 subLoss 22445.8 multi 1.00 import weight 0.00
Epoch 10 Iter 8 subLoss 20143.2 multi 1.00 import weight 0.00
Epoch 10 Iter 9 subLoss 17775.0 multi 1.00 import weight 0.00
Epoch 10 Iter 10 subLoss 16522.0 multi 1.00 import weight 0.00
Epoch 10 Iter 11 subLoss 15452.6 multi 1.00 import weight 0.00
Epoch 10 Acc: 91.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1545 train Loss: 15663.1 test Loss: 1613.3
Epoch 11 Iter 0 subLoss 15677.4 multi 1.00 import weight 0.00
Epoch 11 Iter 1 subLoss 15314.6 multi 1.00 import weight 0.00
Epoch 11 Iter 2 subLoss 14216.1 multi 1.00 import weight 0.00
Epoch 11 Iter 3 subLoss 14204.8 multi 1.00 import weight 0.00
Epoch 11 Iter 4 subLoss 14918.2 multi 1.00 import weight 0.00
Epoch 11 Iter 5 subLoss 15901.1 multi 1.00 import weight 0.00
Epoch 11 Iter 6 subLoss 18008.5 multi 1.00 import weight 0.00
Epoch 11 Iter 7 subLoss 27535.0 multi 1.00 import weight 0.00
Epoch 11 Iter 8 subLoss 30829.9 multi -1.99 import weight 0.00
Epoch 11 Iter 9 subLoss 320245.2 multi 1.00 import weight 0.00
Epoch 11 Iter 10 subLoss 55570.5 multi 1.00 import weight 0.00
Epoch 11 Iter 11 subLoss 46721.6 multi 1.00 import weight 0.00
Epoch 11 Acc: 36.74 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4672 train Loss: 46653.7 test Loss: 7808.5
Epoch 12 Iter 0 subLoss 45607.4 multi 3.99 import weight 1.00
Epoch 12 Iter 1 subLoss 39937.9 multi 1.00 import weight 0.00
Epoch 12 Iter 2 subLoss 38030.9 multi 1.00 import weight 0.00
Epoch 12 Iter 3 subLoss 36790.4 multi 1.00 import weight 0.00
Epoch 12 Iter 4 subLoss 35544.0 multi 1.00 import weight 0.00
Epoch 12 Iter 5 subLoss 33656.5 multi 1.00 import weight 0.00
Epoch 12 Iter 6 subLoss 32140.3 multi 1.00 import weight 0.00
Epoch 12 Iter 7 subLoss 31040.5 multi 1.00 import weight 0.00
Epoch 12 Iter 8 subLoss 29985.5 multi -1.99 import weight 0.00
Epoch 12 Iter 9 subLoss 31909.2 multi 1.00 import weight 0.00
Epoch 12 Iter 10 subLoss 31023.9 multi 1.00 import weight 0.00
Epoch 12 Iter 11 subLoss 30144.9 multi 1.00 import weight 0.00
Epoch 12 Acc: 72.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3014 train Loss: 29791.0 test Loss: 4436.3
Epoch 13 Iter 0 subLoss 29741.9 multi 1.00 import weight 0.00
Epoch 13 Iter 1 subLoss 28441.5 multi 1.00 import weight 0.00
Epoch 13 Iter 2 subLoss 27453.9 multi 1.00 import weight 0.00
Epoch 13 Iter 3 subLoss 25974.7 multi 3.99 import weight 1.00
Epoch 13 Iter 4 subLoss 25180.2 multi 1.00 import weight 0.00
Epoch 13 Iter 5 subLoss 24732.2 multi 1.00 import weight 0.00
Epoch 13 Iter 6 subLoss 25712.3 multi 1.00 import weight 0.00
Epoch 13 Iter 7 subLoss 28014.8 multi 1.00 import weight 0.00
Epoch 13 Iter 8 subLoss 32444.7 multi 3.99 import weight 1.00
Epoch 13 Iter 9 subLoss 110177.9 multi 1.00 import weight 0.00
Epoch 13 Iter 10 subLoss 44999.4 multi 1.00 import weight 0.00
Epoch 13 Iter 11 subLoss 44713.4 multi 1.00 import weight 0.00
Epoch 13 Acc: 34.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4471 train Loss: 45237.0 test Loss: 7510.8
Epoch 14 Iter 0 subLoss 44706.6 multi 1.00 import weight 0.00
Epoch 14 Iter 1 subLoss 43718.5 multi 1.00 import weight 0.00
Epoch 14 Iter 2 subLoss 43384.1 multi 1.00 import weight 0.00
Epoch 14 Iter 3 subLoss 43141.8 multi 1.00 import weight 0.00
Epoch 14 Iter 4 subLoss 43200.1 multi 1.00 import weight 0.00
Epoch 14 Iter 5 subLoss 42808.2 multi 1.00 import weight 0.00
Epoch 14 Iter 6 subLoss 42575.8 multi 1.00 import weight 0.00
Epoch 14 Iter 7 subLoss 42160.2 multi 1.00 import weight 0.00
Epoch 14 Iter 8 subLoss 41878.7 multi 1.00 import weight 0.00
Epoch 14 Iter 9 subLoss 41626.7 multi 1.00 import weight 0.00
Epoch 14 Iter 10 subLoss 40883.9 multi 3.99 import weight 1.00
Epoch 14 Iter 11 subLoss 39935.0 multi 3.99 import weight 1.00
Epoch 14 Acc: 41.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 3.99 Pidx 3993 train Loss: 40225.2 test Loss: 6625.6
Epoch 15 Iter 0 subLoss 39725.5 multi 3.99 import weight 1.00
Epoch 15 Iter 1 subLoss 38777.9 multi 1.00 import weight 0.00
Epoch 15 Iter 2 subLoss 37201.8 multi 1.00 import weight 0.00
Epoch 15 Iter 3 subLoss 36712.1 multi 1.00 import weight 0.00
Epoch 15 Iter 4 subLoss 35811.4 multi 1.00 import weight 0.00
Epoch 15 Iter 5 subLoss 35316.7 multi 1.00 import weight 0.00
Epoch 15 Iter 6 subLoss 34883.6 multi 1.00 import weight 0.00
Epoch 15 Iter 7 subLoss 34044.5 multi 1.00 import weight 0.00
Epoch 15 Iter 8 subLoss 32794.1 multi 1.00 import weight 0.00
Epoch 15 Iter 9 subLoss 33094.8 multi 1.00 import weight 0.00
Epoch 15 Iter 10 subLoss 32245.0 multi 1.00 import weight 0.00
Epoch 15 Iter 11 subLoss 34361.9 multi 1.00 import weight 0.00
Epoch 15 Acc: 47.60 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3436 train Loss: 39101.2 test Loss: 6194.6
Epoch 16 Iter 0 subLoss 38323.8 multi 1.00 import weight 0.00
Epoch 16 Iter 1 subLoss 31884.2 multi 1.00 import weight 0.00
Epoch 16 Iter 2 subLoss 31138.1 multi 1.00 import weight 0.00
Epoch 16 Iter 3 subLoss 31503.0 multi 1.00 import weight 0.00
Epoch 16 Iter 4 subLoss 30706.8 multi 1.00 import weight 0.00
Epoch 16 Iter 5 subLoss 33971.5 multi 1.00 import weight 0.00
Epoch 16 Iter 6 subLoss 30798.8 multi 1.00 import weight 0.00
Epoch 16 Iter 7 subLoss 34817.6 multi 1.00 import weight 0.00
Epoch 16 Iter 8 subLoss 29926.9 multi 1.00 import weight 0.00
Epoch 16 Iter 9 subLoss 31086.0 multi 1.00 import weight 0.00
Epoch 16 Iter 10 subLoss 29713.3 multi 1.00 import weight 0.00
Epoch 16 Iter 11 subLoss 32199.8 multi 1.00 import weight 0.00
Epoch 16 Acc: 65.58 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3219 train Loss: 29407.3 test Loss: 4171.0
Epoch 17 Iter 0 subLoss 28853.8 multi 1.00 import weight 0.00
Epoch 17 Iter 1 subLoss 29966.5 multi 1.00 import weight 0.00
Epoch 17 Iter 2 subLoss 28940.4 multi 1.00 import weight 0.00
Epoch 17 Iter 3 subLoss 32872.7 multi 1.00 import weight 0.00
Epoch 17 Iter 4 subLoss 27063.8 multi 3.99 import weight 1.00
Epoch 17 Iter 5 subLoss 50149.9 multi 1.00 import weight 0.00
Epoch 17 Iter 6 subLoss 39142.0 multi 1.00 import weight 0.00
Epoch 17 Iter 7 subLoss 35486.4 multi 1.00 import weight 0.00
Epoch 17 Iter 8 subLoss 33129.7 multi 1.00 import weight 0.00
Epoch 17 Iter 9 subLoss 30645.0 multi 1.00 import weight 0.00
Epoch 17 Iter 10 subLoss 27827.1 multi 1.00 import weight 0.00
Epoch 17 Iter 11 subLoss 26900.6 multi 1.00 import weight 0.00
Epoch 17 Acc: 77.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2690 train Loss: 26593.2 test Loss: 3505.3
Epoch 18 Iter 0 subLoss 26002.0 multi 1.00 import weight 0.00
Epoch 18 Iter 1 subLoss 26329.1 multi 1.00 import weight 0.00
Epoch 18 Iter 2 subLoss 25712.2 multi 3.99 import weight 1.00
Epoch 18 Iter 3 subLoss 134331.5 multi 1.00 import weight 0.00
Epoch 18 Iter 4 subLoss 34573.4 multi 1.00 import weight 0.00
Epoch 18 Iter 5 subLoss 33596.4 multi 1.00 import weight 0.00
Epoch 18 Iter 6 subLoss 31766.3 multi 1.00 import weight 0.00
Epoch 18 Iter 7 subLoss 30396.8 multi 1.00 import weight 0.00
Epoch 18 Iter 8 subLoss 29203.1 multi 1.00 import weight 0.00
Epoch 18 Iter 9 subLoss 27992.9 multi 1.00 import weight 0.00
Epoch 18 Iter 10 subLoss 26265.5 multi 1.00 import weight 0.00
Epoch 18 Iter 11 subLoss 24932.9 multi 1.00 import weight 0.00
Epoch 18 Acc: 86.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2493 train Loss: 23626.5 test Loss: 3094.2
Epoch 19 Iter 0 subLoss 22711.4 multi 3.99 import weight 1.00
Epoch 19 Iter 1 subLoss 18885.8 multi 1.00 import weight 0.00
Epoch 19 Iter 2 subLoss 17537.4 multi 1.00 import weight 0.00
Epoch 19 Iter 3 subLoss 16659.1 multi 1.00 import weight 0.00
Epoch 19 Iter 4 subLoss 16240.0 multi 1.00 import weight 0.00
Epoch 19 Iter 5 subLoss 15792.5 multi 1.00 import weight 0.00
Epoch 19 Iter 6 subLoss 15589.1 multi 1.00 import weight 0.00
Epoch 19 Iter 7 subLoss 15183.6 multi 1.00 import weight 0.00
Epoch 19 Iter 8 subLoss 14840.2 multi 1.00 import weight 0.00
Epoch 19 Iter 9 subLoss 15013.8 multi 1.00 import weight 0.00
Epoch 19 Iter 10 subLoss 16252.8 multi 1.00 import weight 0.00
Epoch 19 Iter 11 subLoss 15453.7 multi 3.99 import weight 1.00
Epoch 19 Acc: 34.38 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 3.99 Pidx 1545 train Loss: 100998.8 test Loss: 14853.9
Epoch 20 Iter 0 subLoss 100492.1 multi 1.00 import weight 0.00
Epoch 20 Iter 1 subLoss 48889.3 multi 1.00 import weight 0.00
Epoch 20 Iter 2 subLoss 32867.0 multi 1.00 import weight 0.00
Epoch 20 Iter 3 subLoss 26990.9 multi 1.00 import weight 0.00
Epoch 20 Iter 4 subLoss 25912.6 multi 1.00 import weight 0.00
Epoch 20 Iter 5 subLoss 24643.9 multi 1.00 import weight 0.00
Epoch 20 Iter 6 subLoss 22604.1 multi 1.00 import weight 0.00
Epoch 20 Iter 7 subLoss 22818.1 multi 1.00 import weight 0.00
Epoch 20 Iter 8 subLoss 21271.9 multi 1.00 import weight 0.00
Epoch 20 Iter 9 subLoss 20013.8 multi 1.00 import weight 0.00
Epoch 20 Iter 10 subLoss 19984.4 multi 1.00 import weight 0.00
Epoch 20 Iter 11 subLoss 19257.4 multi 1.00 import weight 0.00
Epoch 20 Acc: 89.14 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1925 train Loss: 18604.8 test Loss: 2246.0
Epoch 21 Iter 0 subLoss 18625.1 multi 1.00 import weight 0.00
Epoch 21 Iter 1 subLoss 17602.0 multi 1.00 import weight 0.00
Epoch 21 Iter 2 subLoss 17328.8 multi 1.00 import weight 0.00
Epoch 21 Iter 3 subLoss 16217.0 multi 1.00 import weight 0.00
Epoch 21 Iter 4 subLoss 16999.9 multi 1.00 import weight 0.00
Epoch 21 Iter 5 subLoss 15400.6 multi 1.00 import weight 0.00
Epoch 21 Iter 6 subLoss 14533.0 multi 1.00 import weight 0.00
Epoch 21 Iter 7 subLoss 14332.2 multi 1.00 import weight 0.00
Epoch 21 Iter 8 subLoss 13909.5 multi 1.00 import weight 0.00
Epoch 21 Iter 9 subLoss 13437.1 multi 1.00 import weight 0.00
Epoch 21 Iter 10 subLoss 13893.7 multi 1.00 import weight 0.00
Epoch 21 Iter 11 subLoss 12826.0 multi 1.00 import weight 0.00
Epoch 21 Acc: 92.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1282 train Loss: 13316.1 test Loss: 1332.1
Epoch 22 Iter 0 subLoss 13121.2 multi 1.00 import weight 0.00
Epoch 22 Iter 1 subLoss 12527.4 multi 1.00 import weight 0.00
Epoch 22 Iter 2 subLoss 12578.1 multi 1.00 import weight 0.00
Epoch 22 Iter 3 subLoss 12330.6 multi 1.00 import weight 0.00
Epoch 22 Iter 4 subLoss 12388.1 multi 1.00 import weight 0.00
Epoch 22 Iter 5 subLoss 12341.6 multi -1.99 import weight 0.00
Epoch 22 Iter 6 subLoss 46492.4 multi 3.99 import weight 1.00
Epoch 22 Iter 7 subLoss 241755.5 multi 1.00 import weight 0.00
Epoch 22 Iter 8 subLoss 41373.7 multi 1.00 import weight 0.00
Epoch 22 Iter 9 subLoss 38553.7 multi 1.00 import weight 0.00
Epoch 22 Iter 10 subLoss 37011.7 multi 1.00 import weight 0.00
Epoch 22 Iter 11 subLoss 34516.4 multi 1.00 import weight 0.00
Epoch 22 Acc: 72.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3451 train Loss: 33740.3 test Loss: 4746.0
Epoch 23 Iter 0 subLoss 33229.8 multi 1.00 import weight 0.00
Epoch 23 Iter 1 subLoss 31821.3 multi 1.00 import weight 0.00
Epoch 23 Iter 2 subLoss 29384.6 multi 1.00 import weight 0.00
Epoch 23 Iter 3 subLoss 27858.4 multi 1.00 import weight 0.00
Epoch 23 Iter 4 subLoss 26121.4 multi 1.00 import weight 0.00
Epoch 23 Iter 5 subLoss 24687.3 multi 1.00 import weight 0.00
Epoch 23 Iter 6 subLoss 23019.7 multi 1.00 import weight 0.00
Epoch 23 Iter 7 subLoss 22321.5 multi 1.00 import weight 0.00
Epoch 23 Iter 8 subLoss 21698.9 multi 1.00 import weight 0.00
Epoch 23 Iter 9 subLoss 20505.8 multi 1.00 import weight 0.00
Epoch 23 Iter 10 subLoss 20790.2 multi 1.00 import weight 0.00
Epoch 23 Iter 11 subLoss 19062.8 multi 1.00 import weight 0.00
Epoch 23 Acc: 89.18 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1906 train Loss: 19227.3 test Loss: 1909.5
Epoch 24 Iter 0 subLoss 18663.0 multi 1.00 import weight 0.00
Epoch 24 Iter 1 subLoss 18375.0 multi 1.00 import weight 0.00
Epoch 24 Iter 2 subLoss 18524.0 multi 1.00 import weight 0.00
Epoch 24 Iter 3 subLoss 18594.0 multi 1.00 import weight 0.00
Epoch 24 Iter 4 subLoss 18396.0 multi 1.00 import weight 0.00
Epoch 24 Iter 5 subLoss 19504.5 multi 1.00 import weight 0.00
Epoch 24 Iter 6 subLoss 19064.5 multi 3.99 import weight 1.00
Epoch 24 Iter 7 subLoss 95286.3 multi 1.00 import weight 0.00
Epoch 24 Iter 8 subLoss 49284.0 multi 1.00 import weight 0.00
Epoch 24 Iter 9 subLoss 34348.6 multi 1.00 import weight 0.00
Epoch 24 Iter 10 subLoss 30142.5 multi 3.99 import weight 1.00
Epoch 24 Iter 11 subLoss 25829.2 multi 1.00 import weight 0.00
Epoch 24 Acc: 77.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2582 train Loss: 25317.1 test Loss: 3393.0
Epoch 25 Iter 0 subLoss 24815.8 multi 1.00 import weight 0.00
Epoch 25 Iter 1 subLoss 22734.2 multi 1.00 import weight 0.00
Epoch 25 Iter 2 subLoss 22215.3 multi 1.00 import weight 0.00
Epoch 25 Iter 3 subLoss 19519.7 multi -1.99 import weight 0.00
Epoch 25 Iter 4 subLoss 22994.5 multi -1.99 import weight 0.00
Epoch 25 Iter 5 subLoss 27860.1 multi -1.99 import weight 0.00
Epoch 25 Iter 6 subLoss 44735.3 multi 1.00 import weight 0.00
Epoch 25 Iter 7 subLoss 28413.3 multi -1.99 import weight 0.00
Epoch 25 Iter 8 subLoss 31817.8 multi -1.99 import weight 0.00
Epoch 25 Iter 9 subLoss 36253.5 multi 1.00 import weight 0.00
Epoch 25 Iter 10 subLoss 33662.0 multi -1.99 import weight 0.00
Epoch 25 Iter 11 subLoss 39646.5 multi 1.00 import weight 0.00
Epoch 25 Acc: 61.57 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3964 train Loss: 36854.8 test Loss: 5538.7
Epoch 26 Iter 0 subLoss 36089.4 multi 1.00 import weight 0.00
Epoch 26 Iter 1 subLoss 32847.3 multi 1.00 import weight 0.00
Epoch 26 Iter 2 subLoss 31140.6 multi -1.99 import weight 0.00
Epoch 26 Iter 3 subLoss 34045.5 multi 3.99 import weight 1.00
Epoch 26 Iter 4 subLoss 28381.9 multi 1.00 import weight 0.00
Epoch 26 Iter 5 subLoss 28442.7 multi 3.99 import weight 1.00
Epoch 26 Iter 6 subLoss 24799.6 multi 1.00 import weight 0.00
Epoch 26 Iter 7 subLoss 23195.7 multi 1.00 import weight 0.00
Epoch 26 Iter 8 subLoss 22560.1 multi 1.00 import weight 0.00
Epoch 26 Iter 9 subLoss 20541.5 multi 1.00 import weight 0.00
Epoch 26 Iter 10 subLoss 19995.9 multi -1.99 import weight 0.00
Epoch 26 Iter 11 subLoss 22176.9 multi 1.00 import weight 0.00
Epoch 26 Acc: 77.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2217 train Loss: 21588.5 test Loss: 2591.4
Epoch 27 Iter 0 subLoss 20891.6 multi 1.00 import weight 0.00
Epoch 27 Iter 1 subLoss 19453.0 multi -1.99 import weight 0.00
Epoch 27 Iter 2 subLoss 22972.7 multi 1.00 import weight 0.00
Epoch 27 Iter 3 subLoss 21530.7 multi 1.00 import weight 0.00
Epoch 27 Iter 4 subLoss 19418.2 multi 1.00 import weight 0.00
Epoch 27 Iter 5 subLoss 18234.5 multi 1.00 import weight 0.00
Epoch 27 Iter 6 subLoss 17241.8 multi 1.00 import weight 0.00
Epoch 27 Iter 7 subLoss 16328.3 multi 1.00 import weight 0.00
Epoch 27 Iter 8 subLoss 15719.7 multi 1.00 import weight 0.00
Epoch 27 Iter 9 subLoss 15032.9 multi 1.00 import weight 0.00
Epoch 27 Iter 10 subLoss 15150.0 multi 1.00 import weight 0.00
Epoch 27 Iter 11 subLoss 14069.9 multi 1.00 import weight 0.00
Epoch 27 Acc: 94.78 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1406 train Loss: 13930.2 test Loss: 1264.2
Epoch 28 Iter 0 subLoss 12922.6 multi 1.00 import weight 0.00
Epoch 28 Iter 1 subLoss 13401.9 multi 1.00 import weight 0.00
Epoch 28 Iter 2 subLoss 13200.1 multi 1.00 import weight 0.00
Epoch 28 Iter 3 subLoss 12735.6 multi 1.00 import weight 0.00
Epoch 28 Iter 4 subLoss 12394.8 multi -1.99 import weight 0.00
Epoch 28 Iter 5 subLoss 13544.8 multi 1.00 import weight 0.00
Epoch 28 Iter 6 subLoss 12291.8 multi 1.00 import weight 0.00
Epoch 28 Iter 7 subLoss 13045.7 multi 1.00 import weight 0.00
Epoch 28 Iter 8 subLoss 12465.8 multi 1.00 import weight 0.00
Epoch 28 Iter 9 subLoss 11372.8 multi 1.00 import weight 0.00
Epoch 28 Iter 10 subLoss 12091.4 multi 1.00 import weight 0.00
Epoch 28 Iter 11 subLoss 11414.4 multi 1.00 import weight 0.00
Epoch 28 Acc: 95.45 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1141 train Loss: 11856.1 test Loss: 984.2
Epoch 29 Iter 0 subLoss 11087.6 multi 1.00 import weight 0.00
Epoch 29 Iter 1 subLoss 10870.9 multi 1.00 import weight 0.00
Epoch 29 Iter 2 subLoss 11352.7 multi 1.00 import weight 0.00
Epoch 29 Iter 3 subLoss 11569.5 multi 1.00 import weight 0.00
Epoch 29 Iter 4 subLoss 9871.7 multi 1.00 import weight 0.00
Epoch 29 Iter 5 subLoss 10504.9 multi 1.00 import weight 0.00
Epoch 29 Iter 6 subLoss 10972.5 multi 1.00 import weight 0.00
Epoch 29 Iter 7 subLoss 9915.7 multi 1.00 import weight 0.00
Epoch 29 Iter 8 subLoss 10603.0 multi 1.00 import weight 0.00
Epoch 29 Iter 9 subLoss 10517.6 multi -1.99 import weight 0.00
Epoch 29 Iter 10 subLoss 12727.3 multi 1.00 import weight 0.00
Epoch 29 Iter 11 subLoss 10713.0 multi 1.00 import weight 0.00
Epoch 29 Acc: 94.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1071 train Loss: 10859.3 test Loss: 950.8
Epoch 30 Iter 0 subLoss 10759.2 multi 1.00 import weight 0.00
Epoch 30 Iter 1 subLoss 9994.9 multi 1.00 import weight 0.00
Epoch 30 Iter 2 subLoss 10117.0 multi 1.00 import weight 0.00
Epoch 30 Iter 3 subLoss 10062.7 multi 1.00 import weight 0.00
Epoch 30 Iter 4 subLoss 9424.3 multi 1.00 import weight 0.00
Epoch 30 Iter 5 subLoss 9165.1 multi 1.00 import weight 0.00
Epoch 30 Iter 6 subLoss 9040.4 multi 1.00 import weight 0.00
Epoch 30 Iter 7 subLoss 9214.7 multi 1.00 import weight 0.00
Epoch 30 Iter 8 subLoss 8660.4 multi 1.00 import weight 0.00
Epoch 30 Iter 9 subLoss 9100.9 multi 1.00 import weight 0.00
Epoch 30 Iter 10 subLoss 8992.5 multi 1.00 import weight 0.00
Epoch 30 Iter 11 subLoss 8987.4 multi 1.00 import weight 0.00
Epoch 30 Acc: 96.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 898 train Loss: 8888.1 test Loss: 736.1
Epoch 31 Iter 0 subLoss 8794.2 multi 1.00 import weight 0.00
Epoch 31 Iter 1 subLoss 8977.2 multi 1.00 import weight 0.00
Epoch 31 Iter 2 subLoss 8533.0 multi 1.00 import weight 0.00
Epoch 31 Iter 3 subLoss 8058.7 multi 1.00 import weight 0.00
Epoch 31 Iter 4 subLoss 9095.2 multi 1.00 import weight 0.00
Epoch 31 Iter 5 subLoss 9055.1 multi -1.99 import weight 0.00
Epoch 31 Iter 6 subLoss 20507.9 multi 3.99 import weight 1.00
Epoch 31 Iter 7 subLoss 158212.2 multi 1.00 import weight 0.00
Epoch 31 Iter 8 subLoss 60805.7 multi 1.00 import weight 0.00
Epoch 31 Iter 9 subLoss 34136.9 multi 3.99 import weight 1.00
Epoch 31 Iter 10 subLoss 29552.8 multi 1.00 import weight 0.00
Epoch 31 Iter 11 subLoss 28223.9 multi 1.00 import weight 0.00
Epoch 31 Acc: 60.61 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2822 train Loss: 27628.7 test Loss: 4194.6
Epoch 32 Iter 0 subLoss 27280.6 multi 1.00 import weight 0.00
Epoch 32 Iter 1 subLoss 26064.7 multi 1.00 import weight 0.00
Epoch 32 Iter 2 subLoss 25170.5 multi 1.00 import weight 0.00
Epoch 32 Iter 3 subLoss 24323.1 multi 1.00 import weight 0.00
Epoch 32 Iter 4 subLoss 24012.3 multi 1.00 import weight 0.00
Epoch 32 Iter 5 subLoss 22834.8 multi 1.00 import weight 0.00
Epoch 32 Iter 6 subLoss 20560.4 multi 1.00 import weight 0.00
Epoch 32 Iter 7 subLoss 20145.4 multi 3.99 import weight 1.00
Epoch 32 Iter 8 subLoss 16135.1 multi 1.00 import weight 0.00
Epoch 32 Iter 9 subLoss 14190.5 multi 1.00 import weight 0.00
Epoch 32 Iter 10 subLoss 13740.1 multi 1.00 import weight 0.00
Epoch 32 Iter 11 subLoss 13422.1 multi 1.00 import weight 0.00
Epoch 32 Acc: 92.99 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1342 train Loss: 13096.1 test Loss: 1503.1
Epoch 33 Iter 0 subLoss 13374.1 multi 1.00 import weight 0.00
Epoch 33 Iter 1 subLoss 12540.3 multi 1.00 import weight 0.00
Epoch 33 Iter 2 subLoss 12265.3 multi 1.00 import weight 0.00
Epoch 33 Iter 3 subLoss 11362.2 multi -1.99 import weight 0.00
Epoch 33 Iter 4 subLoss 12883.7 multi 1.00 import weight 0.00
Epoch 33 Iter 5 subLoss 12176.0 multi 1.00 import weight 0.00
Epoch 33 Iter 6 subLoss 11444.3 multi 1.00 import weight 0.00
Epoch 33 Iter 7 subLoss 11360.1 multi 1.00 import weight 1.00
Epoch 33 Iter 8 subLoss 11248.8 multi 1.00 import weight 0.00
Epoch 33 Iter 9 subLoss 10807.3 multi 1.00 import weight 0.00
Epoch 33 Iter 10 subLoss 11049.9 multi 1.00 import weight 0.00
Epoch 33 Iter 11 subLoss 11157.6 multi 1.00 import weight 0.00
Epoch 33 Acc: 94.59 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1115 train Loss: 10722.3 test Loss: 1074.4
Epoch 34 Iter 0 subLoss 10607.6 multi 3.99 import weight 1.00
Epoch 34 Iter 1 subLoss 10573.7 multi 1.00 import weight 0.00
Epoch 34 Iter 2 subLoss 9194.8 multi 1.00 import weight 0.00
Epoch 34 Iter 3 subLoss 9696.0 multi 1.00 import weight 0.00
Epoch 34 Iter 4 subLoss 9194.4 multi 3.99 import weight 1.00
Epoch 34 Iter 5 subLoss 8465.8 multi 1.00 import weight 0.00
Epoch 34 Iter 6 subLoss 8684.5 multi 1.00 import weight 0.00
Epoch 34 Iter 7 subLoss 9215.5 multi 3.99 import weight 1.00
Epoch 34 Iter 8 subLoss 9559.9 multi 1.00 import weight 0.00
Epoch 34 Iter 9 subLoss 8489.2 multi 1.00 import weight 0.00
Epoch 34 Iter 10 subLoss 8247.8 multi 1.00 import weight 0.00
Epoch 34 Iter 11 subLoss 8335.9 multi 1.00 import weight 0.00
Epoch 34 Acc: 95.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 833 train Loss: 8261.1 test Loss: 732.5
Epoch 35 Iter 0 subLoss 8187.6 multi 1.00 import weight 0.00
Epoch 35 Iter 1 subLoss 7722.1 multi 1.00 import weight 0.00
Epoch 35 Iter 2 subLoss 8475.5 multi -1.99 import weight 0.00
Epoch 35 Iter 3 subLoss 7968.2 multi 1.00 import weight 0.00
Epoch 35 Iter 4 subLoss 7578.6 multi 1.00 import weight 0.00
Epoch 35 Iter 5 subLoss 8310.4 multi 1.00 import weight 0.00
Epoch 35 Iter 6 subLoss 7985.4 multi 1.00 import weight 0.00
Epoch 35 Iter 7 subLoss 7637.0 multi 1.00 import weight 0.00
Epoch 35 Iter 8 subLoss 7843.7 multi 1.00 import weight 0.00
Epoch 35 Iter 9 subLoss 7665.4 multi 1.00 import weight 0.00
Epoch 35 Iter 10 subLoss 8006.0 multi 1.00 import weight 0.00
Epoch 35 Iter 11 subLoss 7189.2 multi 1.00 import weight 0.00
Epoch 35 Acc: 96.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 718 train Loss: 7677.1 test Loss: 676.8
Epoch 36 Iter 0 subLoss 7961.2 multi 3.99 import weight 1.00
Epoch 36 Iter 1 subLoss 8019.2 multi -1.99 import weight 0.00
Epoch 36 Iter 2 subLoss 16038.2 multi 1.00 import weight 0.00
Epoch 36 Iter 3 subLoss 8876.8 multi 1.00 import weight 0.00
Epoch 36 Iter 4 subLoss 6905.4 multi 1.00 import weight 0.00
Epoch 36 Iter 5 subLoss 7564.5 multi 1.00 import weight 0.00
Epoch 36 Iter 6 subLoss 7217.1 multi 1.00 import weight 0.00
Epoch 36 Iter 7 subLoss 7270.5 multi 1.00 import weight 0.00
Epoch 36 Iter 8 subLoss 7305.8 multi 1.00 import weight 0.00
Epoch 36 Iter 9 subLoss 6931.1 multi 1.00 import weight 0.00
Epoch 36 Iter 10 subLoss 6840.2 multi 1.00 import weight 0.00
Epoch 36 Iter 11 subLoss 6926.3 multi 1.00 import weight 0.00
Epoch 36 Acc: 96.40 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 692 train Loss: 6882.5 test Loss: 626.2
Epoch 37 Iter 0 subLoss 6750.2 multi 1.00 import weight 0.00
Epoch 37 Iter 1 subLoss 6587.2 multi 1.00 import weight 0.00
Epoch 37 Iter 2 subLoss 6723.8 multi 1.00 import weight 0.00
Epoch 37 Iter 3 subLoss 6723.9 multi 3.99 import weight 1.00
Epoch 37 Iter 4 subLoss 16695.3 multi 1.00 import weight 0.00
Epoch 37 Iter 5 subLoss 39792.5 multi 1.00 import weight 0.00
Epoch 37 Iter 6 subLoss 33406.3 multi 1.00 import weight 0.00
Epoch 37 Iter 7 subLoss 21812.9 multi 1.00 import weight 0.00
Epoch 37 Iter 8 subLoss 9935.2 multi 1.00 import weight 0.00
Epoch 37 Iter 9 subLoss 8759.5 multi 1.00 import weight 0.00
Epoch 37 Iter 10 subLoss 7541.9 multi 1.00 import weight 0.00
Epoch 37 Iter 11 subLoss 7916.9 multi 1.00 import weight 0.00
Epoch 37 Acc: 96.44 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 791 train Loss: 7653.7 test Loss: 747.9
Epoch 38 Iter 0 subLoss 7931.3 multi 1.00 import weight 0.00
Epoch 38 Iter 1 subLoss 7003.6 multi 1.00 import weight 0.00
Epoch 38 Iter 2 subLoss 7310.3 multi -1.99 import weight 0.00
Epoch 38 Iter 3 subLoss 7573.0 multi 1.00 import weight 1.00
Epoch 38 Iter 4 subLoss 6909.4 multi 3.99 import weight 1.00
Epoch 38 Iter 5 subLoss 7333.0 multi 1.00 import weight 0.00
Epoch 38 Iter 6 subLoss 6153.8 multi 1.00 import weight 0.00
Epoch 38 Iter 7 subLoss 6705.3 multi 1.00 import weight 0.00
Epoch 38 Iter 8 subLoss 6161.7 multi -1.99 import weight 0.00
Epoch 38 Iter 9 subLoss 7088.8 multi 1.00 import weight 0.00
Epoch 38 Iter 10 subLoss 6547.4 multi 1.00 import weight 0.00
Epoch 38 Iter 11 subLoss 6714.2 multi -1.99 import weight 0.00
Epoch 38 Acc: 96.58 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 671 train Loss: 6816.5 test Loss: 628.9
Epoch 39 Iter 0 subLoss 6326.8 multi 1.00 import weight 0.00
Epoch 39 Iter 1 subLoss 7074.4 multi 1.00 import weight 0.00
Epoch 39 Iter 2 subLoss 6323.2 multi 3.99 import weight 1.00
Epoch 39 Iter 3 subLoss 6260.5 multi 1.00 import weight 0.00
Epoch 39 Iter 4 subLoss 6345.4 multi 1.00 import weight 0.00
Epoch 39 Iter 5 subLoss 6254.7 multi 1.00 import weight 0.00
Epoch 39 Iter 6 subLoss 6599.2 multi -1.99 import weight 0.00
Epoch 39 Iter 7 subLoss 6382.5 multi 1.00 import weight 0.00
Epoch 39 Iter 8 subLoss 5922.6 multi 1.00 import weight 0.00
Epoch 39 Iter 9 subLoss 6365.1 multi 1.00 import weight 0.00
Epoch 39 Iter 10 subLoss 6157.7 multi 3.99 import weight 1.00
Epoch 39 Iter 11 subLoss 6184.7 multi 1.00 import weight 0.00
Epoch 39 Acc: 96.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 618 train Loss: 5837.2 test Loss: 536.5
Epoch 40 Iter 0 subLoss 6286.6 multi 1.00 import weight 0.00
Epoch 40 Iter 1 subLoss 5492.7 multi 1.00 import weight 0.00
Epoch 40 Iter 2 subLoss 6036.1 multi 1.00 import weight 0.00
Epoch 40 Iter 3 subLoss 5934.1 multi -1.99 import weight 0.00
Epoch 40 Iter 4 subLoss 6570.5 multi 1.00 import weight 0.00
Epoch 40 Iter 5 subLoss 5752.8 multi 1.00 import weight 0.00
Epoch 40 Iter 6 subLoss 6029.7 multi 1.00 import weight 0.00
Epoch 40 Iter 7 subLoss 5628.2 multi 1.00 import weight 0.00
Epoch 40 Iter 8 subLoss 5448.4 multi 1.00 import weight 0.00
Epoch 40 Iter 9 subLoss 5846.8 multi 1.00 import weight 0.00
Epoch 40 Iter 10 subLoss 5406.9 multi 1.00 import weight 0.00
Epoch 40 Iter 11 subLoss 5272.4 multi 1.00 import weight 0.00
Epoch 40 Acc: 97.04 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 527 train Loss: 5608.6 test Loss: 494.9
Epoch 41 Iter 0 subLoss 6162.8 multi -1.98 import weight 1.00
Epoch 41 Iter 1 subLoss 5468.2 multi 1.00 import weight 0.00
Epoch 41 Iter 2 subLoss 5462.6 multi 3.99 import weight 1.00
Epoch 41 Iter 3 subLoss 6825.9 multi 1.00 import weight 0.00
Epoch 41 Iter 4 subLoss 5172.8 multi 1.00 import weight 0.00
Epoch 41 Iter 5 subLoss 5325.8 multi 1.00 import weight 0.00
Epoch 41 Iter 6 subLoss 5850.2 multi -1.99 import weight 0.00
Epoch 41 Iter 7 subLoss 6185.0 multi 3.99 import weight 1.00
Epoch 41 Iter 8 subLoss 25719.2 multi 6.97 import weight 1.00
Epoch 41 Iter 9 subLoss 189683.0 multi 1.00 import weight 0.00
Epoch 41 Iter 10 subLoss 47091.1 multi 1.00 import weight 0.00
Epoch 41 Iter 11 subLoss 29829.8 multi 1.00 import weight 0.00
Epoch 41 Acc: 63.69 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2982 train Loss: 28682.9 test Loss: 4537.7
Epoch 42 Iter 0 subLoss 28279.3 multi 1.00 import weight 0.00
Epoch 42 Iter 1 subLoss 27072.1 multi -4.97 import weight 0.00
Epoch 42 Iter 2 subLoss 33634.5 multi 1.00 import weight 0.00
Epoch 42 Iter 3 subLoss 32209.8 multi -1.99 import weight 0.00
Epoch 42 Iter 4 subLoss 37655.4 multi 1.00 import weight 0.00
Epoch 42 Iter 5 subLoss 32459.9 multi -4.97 import weight 0.00
Epoch 42 Iter 6 subLoss 41459.3 multi 1.00 import weight 0.00
Epoch 42 Iter 7 subLoss 38214.4 multi 1.00 import weight 0.00
Epoch 42 Iter 8 subLoss 36220.5 multi 1.00 import weight 0.00
Epoch 42 Iter 9 subLoss 35151.5 multi 1.00 import weight 0.00
Epoch 42 Iter 10 subLoss 33874.8 multi 1.00 import weight 0.00
Epoch 42 Iter 11 subLoss 33111.3 multi 1.00 import weight 0.00
Epoch 42 Acc: 55.22 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3311 train Loss: 32478.0 test Loss: 5229.8
Epoch 43 Iter 0 subLoss 31930.0 multi 1.00 import weight 0.00
Epoch 43 Iter 1 subLoss 31495.3 multi 1.00 import weight 0.00
Epoch 43 Iter 2 subLoss 29941.9 multi -1.99 import weight 0.00
Epoch 43 Iter 3 subLoss 31960.7 multi 1.00 import weight 0.00
Epoch 43 Iter 4 subLoss 30913.8 multi 1.00 import weight 0.00
Epoch 43 Iter 5 subLoss 30642.9 multi 3.99 import weight 0.00
Epoch 43 Iter 6 subLoss 27348.5 multi 3.99 import weight 0.00
Epoch 43 Iter 7 subLoss 29022.1 multi 1.00 import weight 0.00
Epoch 43 Iter 8 subLoss 24908.4 multi 1.00 import weight 0.00
Epoch 43 Iter 9 subLoss 23585.6 multi 1.00 import weight 0.00
Epoch 43 Iter 10 subLoss 23295.2 multi 1.00 import weight 0.00
Epoch 43 Iter 11 subLoss 21645.6 multi 1.00 import weight 0.00
Epoch 43 Acc: 76.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2164 train Loss: 21595.4 test Loss: 3057.0
Epoch 44 Iter 0 subLoss 21204.9 multi 1.00 import weight 0.00
Epoch 44 Iter 1 subLoss 20219.3 multi 1.00 import weight 0.00
Epoch 44 Iter 2 subLoss 20164.1 multi 1.00 import weight 0.00
Epoch 44 Iter 3 subLoss 19519.3 multi 1.00 import weight 0.00
Epoch 44 Iter 4 subLoss 19615.4 multi 1.00 import weight 0.00
Epoch 44 Iter 5 subLoss 18586.0 multi 1.00 import weight 0.00
Epoch 44 Iter 6 subLoss 18575.6 multi 1.00 import weight 0.00
Epoch 44 Iter 7 subLoss 18075.7 multi 1.00 import weight 0.00
Epoch 44 Iter 8 subLoss 17745.8 multi 1.00 import weight 0.00
Epoch 44 Iter 9 subLoss 17899.8 multi 1.00 import weight 0.00
Epoch 44 Iter 10 subLoss 16713.3 multi 1.00 import weight 0.00
Epoch 44 Iter 11 subLoss 17115.8 multi 1.00 import weight 0.00
Epoch 44 Acc: 79.30 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1711 train Loss: 16758.8 test Loss: 2223.5
Epoch 45 Iter 0 subLoss 16133.8 multi 3.99 import weight 0.00
Epoch 45 Iter 1 subLoss 15578.0 multi 1.00 import weight 0.00
Epoch 45 Iter 2 subLoss 15775.0 multi 1.00 import weight 0.00
Epoch 45 Iter 3 subLoss 15663.4 multi 1.00 import weight 0.00
Epoch 45 Iter 4 subLoss 14048.0 multi 1.00 import weight 0.00
Epoch 45 Iter 5 subLoss 14217.1 multi 1.00 import weight 0.00
Epoch 45 Iter 6 subLoss 14343.9 multi -1.99 import weight 0.00
Epoch 45 Iter 7 subLoss 14881.8 multi 1.00 import weight 0.00
Epoch 45 Iter 8 subLoss 13999.3 multi 1.00 import weight 0.00
Epoch 45 Iter 9 subLoss 15133.3 multi 1.00 import weight 0.00
Epoch 45 Iter 10 subLoss 14273.3 multi 1.00 import weight 0.00
Epoch 45 Iter 11 subLoss 14543.9 multi -1.99 import weight 0.00
Epoch 45 Acc: 85.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1454 train Loss: 14661.7 test Loss: 1913.0
Epoch 46 Iter 0 subLoss 14583.4 multi 1.00 import weight 0.00
Epoch 46 Iter 1 subLoss 14229.1 multi -4.97 import weight 0.00
Epoch 46 Iter 2 subLoss 15779.3 multi 3.99 import weight 0.00
Epoch 46 Iter 3 subLoss 27691.4 multi 1.00 import weight 0.00
Epoch 46 Iter 4 subLoss 18403.8 multi -1.99 import weight 0.00
Epoch 46 Iter 5 subLoss 68701.1 multi 1.00 import weight 0.00
Epoch 46 Iter 6 subLoss 40003.7 multi 1.00 import weight 0.00
Epoch 46 Iter 7 subLoss 26146.2 multi 1.00 import weight 0.00
Epoch 46 Iter 8 subLoss 22368.9 multi 3.99 import weight 0.00
Epoch 46 Iter 9 subLoss 17258.7 multi -1.99 import weight 0.00
Epoch 46 Iter 10 subLoss 28760.0 multi 1.00 import weight 0.00
Epoch 46 Iter 11 subLoss 19034.3 multi 1.00 import weight 0.00
Epoch 46 Acc: 80.97 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1903 train Loss: 16546.6 test Loss: 2160.1
Epoch 47 Iter 0 subLoss 15863.3 multi 1.00 import weight 0.00
Epoch 47 Iter 1 subLoss 16106.3 multi 1.00 import weight 0.00
Epoch 47 Iter 2 subLoss 15253.1 multi 1.00 import weight 0.00
Epoch 47 Iter 3 subLoss 15142.5 multi 1.00 import weight 0.00
Epoch 47 Iter 4 subLoss 14002.1 multi -1.99 import weight 0.00
Epoch 47 Iter 5 subLoss 14885.4 multi 3.99 import weight 0.00
Epoch 47 Iter 6 subLoss 13509.0 multi 1.00 import weight 0.00
Epoch 47 Iter 7 subLoss 13547.9 multi 3.99 import weight 0.00
Epoch 47 Iter 8 subLoss 12729.9 multi 3.99 import weight 0.00
Epoch 47 Iter 9 subLoss 13521.9 multi 1.00 import weight 0.00
Epoch 47 Iter 10 subLoss 12014.9 multi 1.00 import weight 0.00
Epoch 47 Iter 11 subLoss 11436.8 multi 1.00 import weight 0.00
Epoch 47 Acc: 91.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1143 train Loss: 11319.8 test Loss: 1381.5
Epoch 48 Iter 0 subLoss 10986.3 multi -1.99 import weight 0.00
Epoch 48 Iter 1 subLoss 12078.5 multi 1.00 import weight 0.00
Epoch 48 Iter 2 subLoss 11418.1 multi 3.99 import weight 0.00
Epoch 48 Iter 3 subLoss 11166.8 multi -1.99 import weight 0.00
Epoch 48 Iter 4 subLoss 11435.8 multi 3.99 import weight 0.00
Epoch 48 Iter 5 subLoss 13352.7 multi 1.00 import weight 0.00
Epoch 48 Iter 6 subLoss 10669.4 multi 1.00 import weight 0.00
Epoch 48 Iter 7 subLoss 10493.7 multi 1.00 import weight 0.00
Epoch 48 Iter 8 subLoss 10175.8 multi 1.00 import weight 0.00
Epoch 48 Iter 9 subLoss 10521.7 multi -1.99 import weight 0.00
Epoch 48 Iter 10 subLoss 9748.6 multi 1.00 import weight 0.00
Epoch 48 Iter 11 subLoss 10024.1 multi 1.00 import weight 0.00
Epoch 48 Acc: 93.87 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1002 train Loss: 10101.9 test Loss: 1169.9
Epoch 49 Iter 0 subLoss 9585.4 multi 1.00 import weight 0.00
Epoch 49 Iter 1 subLoss 9688.1 multi 1.00 import weight 0.00
Epoch 49 Iter 2 subLoss 9716.6 multi 1.00 import weight 0.00
Epoch 49 Iter 3 subLoss 9514.6 multi 1.00 import weight 0.00
Epoch 49 Iter 4 subLoss 9180.9 multi 1.00 import weight 0.00
Epoch 49 Iter 5 subLoss 9309.7 multi 1.00 import weight 0.00
Epoch 49 Iter 6 subLoss 9323.0 multi 1.00 import weight 0.00
Epoch 49 Iter 7 subLoss 9409.8 multi 1.00 import weight 0.00
Epoch 49 Iter 8 subLoss 9225.4 multi -4.97 import weight 0.00
Epoch 49 Iter 9 subLoss 9662.1 multi 1.00 import weight 0.00
Epoch 49 Iter 10 subLoss 9502.0 multi 1.00 import weight 0.00
Epoch 49 Iter 11 subLoss 9849.7 multi 1.00 import weight 0.00
Epoch 49 Acc: 94.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 984 train Loss: 9532.2 test Loss: 1091.4
Epoch 50 Iter 0 subLoss 9585.4 multi 3.99 import weight 0.00
Epoch 50 Iter 1 subLoss 8615.1 multi 1.00 import weight 0.00
Epoch 50 Iter 2 subLoss 8968.8 multi 1.00 import weight 0.00
Epoch 50 Iter 3 subLoss 8952.4 multi 1.00 import weight 0.00
Epoch 50 Iter 4 subLoss 8993.0 multi 1.00 import weight 0.00
Epoch 50 Iter 5 subLoss 8842.0 multi 1.00 import weight 0.00
Epoch 50 Iter 6 subLoss 8130.1 multi 1.00 import weight 0.00
Epoch 50 Iter 7 subLoss 8615.9 multi 3.99 import weight 0.00
Epoch 50 Iter 8 subLoss 8343.0 multi -1.99 import weight 0.00
Epoch 50 Iter 9 subLoss 12317.2 multi 1.00 import weight 0.00
Epoch 50 Iter 10 subLoss 8087.9 multi 1.00 import weight 0.00
Epoch 50 Iter 11 subLoss 7998.2 multi -1.99 import weight 0.00
Epoch 50 Acc: 92.04 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 799 train Loss: 10275.4 test Loss: 1219.8
Epoch 51 Iter 0 subLoss 10356.4 multi 1.00 import weight 0.00
Epoch 51 Iter 1 subLoss 8227.1 multi 1.00 import weight 0.00
Epoch 51 Iter 2 subLoss 8044.1 multi 1.00 import weight 0.00
Epoch 51 Iter 3 subLoss 8261.7 multi 1.00 import weight 0.00
Epoch 51 Iter 4 subLoss 8337.5 multi 3.99 import weight 0.00
Epoch 51 Iter 5 subLoss 8022.3 multi -1.99 import weight 0.00
Epoch 51 Iter 6 subLoss 9930.2 multi 3.99 import weight 0.00
Epoch 51 Iter 7 subLoss 66867.9 multi 1.00 import weight 0.00
Epoch 51 Iter 8 subLoss 41633.7 multi 1.00 import weight 0.00
Epoch 51 Iter 9 subLoss 17109.9 multi 1.00 import weight 0.00
Epoch 51 Iter 10 subLoss 15927.1 multi 1.00 import weight 0.00
Epoch 51 Iter 11 subLoss 13393.9 multi 1.00 import weight 0.00
Epoch 51 Acc: 93.60 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1339 train Loss: 11561.7 test Loss: 1423.5
Epoch 52 Iter 0 subLoss 11270.1 multi 1.00 import weight 0.00
Epoch 52 Iter 1 subLoss 10266.3 multi 1.00 import weight 0.00
Epoch 52 Iter 2 subLoss 9601.2 multi 1.00 import weight 0.00
Epoch 52 Iter 3 subLoss 9778.1 multi 1.00 import weight 0.00
Epoch 52 Iter 4 subLoss 9122.3 multi 1.00 import weight 0.00
Epoch 52 Iter 5 subLoss 8644.0 multi 1.00 import weight 0.00
Epoch 52 Iter 6 subLoss 8521.0 multi 1.00 import weight 0.00
Epoch 52 Iter 7 subLoss 8559.4 multi 1.00 import weight 0.00
Epoch 52 Iter 8 subLoss 8877.1 multi 3.99 import weight 0.00
Epoch 52 Iter 9 subLoss 8727.4 multi 1.00 import weight 0.00
Epoch 52 Iter 10 subLoss 8049.3 multi 3.99 import weight 0.00
Epoch 52 Iter 11 subLoss 7441.6 multi 1.00 import weight 0.00
Epoch 52 Acc: 95.58 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 744 train Loss: 7701.3 test Loss: 818.6
Epoch 53 Iter 0 subLoss 7560.0 multi -1.99 import weight 0.00
Epoch 53 Iter 1 subLoss 7461.5 multi 1.00 import weight 0.00
Epoch 53 Iter 2 subLoss 7858.8 multi -1.99 import weight 0.00
Epoch 53 Iter 3 subLoss 9128.8 multi 3.99 import weight 0.00
Epoch 53 Iter 4 subLoss 15827.1 multi 1.00 import weight 0.00
Epoch 53 Iter 5 subLoss 8245.6 multi 3.99 import weight 0.00
Epoch 53 Iter 6 subLoss 11358.0 multi 3.99 import weight 0.00
Epoch 53 Iter 7 subLoss 25161.2 multi 1.00 import weight 0.00
Epoch 53 Iter 8 subLoss 17137.8 multi 1.00 import weight 0.00
Epoch 53 Iter 9 subLoss 16641.0 multi 1.00 import weight 0.00
Epoch 53 Iter 10 subLoss 14824.5 multi 1.00 import weight 0.00
Epoch 53 Iter 11 subLoss 14527.3 multi 1.00 import weight 0.00
Epoch 53 Acc: 90.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1452 train Loss: 12812.0 test Loss: 1602.3
Epoch 54 Iter 0 subLoss 13069.2 multi 1.00 import weight 0.00
Epoch 54 Iter 1 subLoss 11009.2 multi 1.00 import weight 0.00
Epoch 54 Iter 2 subLoss 9710.6 multi 3.99 import weight 0.00
Epoch 54 Iter 3 subLoss 8091.7 multi -1.99 import weight 0.00
Epoch 54 Iter 4 subLoss 8591.2 multi 1.00 import weight 0.00
Epoch 54 Iter 5 subLoss 8758.2 multi 3.99 import weight 0.00
Epoch 54 Iter 6 subLoss 7358.2 multi 1.00 import weight 0.00
Epoch 54 Iter 7 subLoss 7029.0 multi 1.00 import weight 0.00
Epoch 54 Iter 8 subLoss 6753.0 multi 3.99 import weight 0.00
Epoch 54 Iter 9 subLoss 6886.8 multi 1.00 import weight 0.00
Epoch 54 Iter 10 subLoss 6812.4 multi 1.00 import weight 0.00
Epoch 54 Iter 11 subLoss 6929.8 multi 3.99 import weight 0.00
Epoch 54 Acc: 96.34 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 692 train Loss: 6724.4 test Loss: 692.0
Epoch 55 Iter 0 subLoss 6632.6 multi 1.00 import weight 0.00
Epoch 55 Iter 1 subLoss 6629.7 multi 1.00 import weight 0.00
Epoch 55 Iter 2 subLoss 6014.6 multi 1.00 import weight 0.00
Epoch 55 Iter 3 subLoss 6255.2 multi 3.99 import weight 0.00
Epoch 55 Iter 4 subLoss 6314.1 multi 1.00 import weight 0.00
Epoch 55 Iter 5 subLoss 5999.1 multi 1.00 import weight 0.00
Epoch 55 Iter 6 subLoss 5728.2 multi 1.00 import weight 0.00
Epoch 55 Iter 7 subLoss 5830.2 multi 1.00 import weight 0.00
Epoch 55 Iter 8 subLoss 5667.5 multi 1.00 import weight 0.00
Epoch 55 Iter 9 subLoss 6170.3 multi -4.97 import weight 0.00
Epoch 55 Iter 10 subLoss 6340.1 multi 3.99 import weight 0.00
Epoch 55 Iter 11 subLoss 8853.8 multi -1.99 import weight 0.00
Epoch 55 Acc: 36.35 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 885 train Loss: 108018.0 test Loss: 17794.0
Epoch 56 Iter 0 subLoss 107148.6 multi 1.00 import weight 0.00
Epoch 56 Iter 1 subLoss 56593.3 multi 1.00 import weight 0.00
Epoch 56 Iter 2 subLoss 18218.4 multi 1.00 import weight 0.00
Epoch 56 Iter 3 subLoss 14609.2 multi 1.00 import weight 0.00
Epoch 56 Iter 4 subLoss 13193.2 multi 1.00 import weight 0.00
Epoch 56 Iter 5 subLoss 12223.6 multi 1.00 import weight 0.00
Epoch 56 Iter 6 subLoss 11289.9 multi -1.99 import weight 0.00
Epoch 56 Iter 7 subLoss 12462.9 multi 3.99 import weight 0.00
Epoch 56 Iter 8 subLoss 10305.2 multi 1.00 import weight 0.00
Epoch 56 Iter 9 subLoss 9853.8 multi -1.99 import weight 0.00
Epoch 56 Iter 10 subLoss 11106.3 multi 1.00 import weight 0.00
Epoch 56 Iter 11 subLoss 10126.5 multi -1.99 import weight 0.00
Epoch 56 Acc: 92.35 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1012 train Loss: 11406.1 test Loss: 1371.3
Epoch 57 Iter 0 subLoss 11100.2 multi 3.99 import weight 0.00
Epoch 57 Iter 1 subLoss 9276.3 multi 1.00 import weight 0.00
Epoch 57 Iter 2 subLoss 9172.3 multi -1.99 import weight 0.00
Epoch 57 Iter 3 subLoss 10336.7 multi 1.00 import weight 0.00
Epoch 57 Iter 4 subLoss 9687.9 multi 3.99 import weight 0.00
Epoch 57 Iter 5 subLoss 8872.1 multi 6.97 import weight 1.00
Epoch 57 Iter 6 subLoss 7472.2 multi -1.99 import weight 0.00
Epoch 57 Iter 7 subLoss 9354.4 multi 1.00 import weight 0.00
Epoch 57 Iter 8 subLoss 7286.4 multi -1.99 import weight 0.00
Epoch 57 Iter 9 subLoss 8410.1 multi 1.00 import weight 0.00
Epoch 57 Iter 10 subLoss 7755.9 multi 1.00 import weight 0.00
Epoch 57 Iter 11 subLoss 7791.6 multi 1.00 import weight 0.00
Epoch 57 Acc: 96.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 779 train Loss: 7498.4 test Loss: 765.4
Epoch 58 Iter 0 subLoss 7470.0 multi 1.00 import weight 0.00
Epoch 58 Iter 1 subLoss 7182.9 multi 3.99 import weight 0.00
Epoch 58 Iter 2 subLoss 6969.1 multi 1.00 import weight 0.00
Epoch 58 Iter 3 subLoss 6720.3 multi 3.98 import weight 1.00
Epoch 58 Iter 4 subLoss 6578.8 multi 3.99 import weight 0.00
Epoch 58 Iter 5 subLoss 6264.6 multi -1.98 import weight 0.00
Epoch 58 Iter 6 subLoss 7159.3 multi 1.00 import weight 0.00
Epoch 58 Iter 7 subLoss 6659.9 multi 1.00 import weight 0.00
Epoch 58 Iter 8 subLoss 6241.7 multi 1.00 import weight 0.00
Epoch 58 Iter 9 subLoss 6152.4 multi 6.97 import weight 1.00
Epoch 58 Iter 10 subLoss 6317.5 multi 3.99 import weight 0.00
Epoch 58 Iter 11 subLoss 6083.9 multi 1.00 import weight 0.00
Epoch 58 Acc: 96.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 608 train Loss: 6029.3 test Loss: 576.7
Epoch 59 Iter 0 subLoss 5854.3 multi 1.00 import weight 0.00
Epoch 59 Iter 1 subLoss 6306.6 multi 1.00 import weight 0.00
Epoch 59 Iter 2 subLoss 5735.6 multi -1.99 import weight 0.00
Epoch 59 Iter 3 subLoss 6063.3 multi 1.00 import weight 0.00
Epoch 59 Iter 4 subLoss 5633.4 multi -1.99 import weight 0.00
Epoch 59 Iter 5 subLoss 6429.6 multi 1.00 import weight 0.00
Epoch 59 Iter 6 subLoss 5371.1 multi 1.00 import weight 0.00
Epoch 59 Iter 7 subLoss 5421.0 multi 1.00 import weight 0.00
Epoch 59 Iter 8 subLoss 5859.3 multi 3.98 import weight 1.00
Epoch 59 Iter 9 subLoss 5812.1 multi 1.00 import weight 0.00
Epoch 59 Iter 10 subLoss 5959.5 multi 1.00 import weight 0.00
Epoch 59 Iter 11 subLoss 5985.7 multi 1.00 import weight 0.00
Epoch 59 Acc: 96.89 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 598 train Loss: 5749.6 test Loss: 564.9
Epoch 60 Iter 0 subLoss 4803.2 multi 1.00 import weight 0.00
Epoch 60 Iter 1 subLoss 6052.7 multi 1.00 import weight 0.00
Epoch 60 Iter 2 subLoss 5930.5 multi 1.00 import weight 0.00
Epoch 60 Iter 3 subLoss 4925.2 multi 1.00 import weight 0.00
Epoch 60 Iter 4 subLoss 5512.6 multi 1.00 import weight 0.00
Epoch 60 Iter 5 subLoss 5375.3 multi 3.99 import weight 0.00
Epoch 60 Iter 6 subLoss 5815.2 multi 3.99 import weight 0.00
Epoch 60 Iter 7 subLoss 8650.9 multi -1.99 import weight 0.00
Epoch 60 Iter 8 subLoss 22657.9 multi 1.00 import weight 0.00
Epoch 60 Iter 9 subLoss 9871.1 multi 3.99 import weight 0.00
Epoch 60 Iter 10 subLoss 11272.0 multi 3.99 import weight 0.00
Epoch 60 Iter 11 subLoss 27788.0 multi 1.00 import weight 0.00
Epoch 60 Acc: 83.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2778 train Loss: 17032.6 test Loss: 2324.4
Epoch 61 Iter 0 subLoss 16497.1 multi 1.00 import weight 0.00
Epoch 61 Iter 1 subLoss 14426.8 multi 1.00 import weight 0.00
Epoch 61 Iter 2 subLoss 11873.2 multi 1.00 import weight 0.00
Epoch 61 Iter 3 subLoss 10876.7 multi 3.99 import weight 0.00
Epoch 61 Iter 4 subLoss 6976.3 multi -1.99 import weight 0.00
Epoch 61 Iter 5 subLoss 7404.8 multi 1.00 import weight 0.00
Epoch 61 Iter 6 subLoss 6679.8 multi 1.00 import weight 0.00
Epoch 61 Iter 7 subLoss 6538.6 multi 1.00 import weight 0.00
Epoch 61 Iter 8 subLoss 6485.7 multi 1.00 import weight 0.00
Epoch 61 Iter 9 subLoss 6229.3 multi 1.00 import weight 0.00
Epoch 61 Iter 10 subLoss 6622.7 multi 3.99 import weight 0.00
Epoch 61 Iter 11 subLoss 5950.1 multi 3.99 import weight 0.00
Epoch 61 Acc: 96.93 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 595 train Loss: 5652.0 test Loss: 554.1
Epoch 62 Iter 0 subLoss 5154.1 multi 1.00 import weight 0.00
Epoch 62 Iter 1 subLoss 5839.1 multi 3.99 import weight 0.00
Epoch 62 Iter 2 subLoss 5505.5 multi -1.99 import weight 0.00
Epoch 62 Iter 3 subLoss 5529.5 multi -1.99 import weight 0.00
Epoch 62 Iter 4 subLoss 5884.6 multi 1.00 import weight 0.00
Epoch 62 Iter 5 subLoss 5140.8 multi 1.00 import weight 0.00
Epoch 62 Iter 6 subLoss 5322.7 multi 3.99 import weight 0.00
Epoch 62 Iter 7 subLoss 5134.6 multi 1.00 import weight 0.00
Epoch 62 Iter 8 subLoss 5451.5 multi -1.99 import weight 0.00
Epoch 62 Iter 9 subLoss 5451.8 multi 1.00 import weight 0.00
Epoch 62 Iter 10 subLoss 5053.4 multi 1.00 import weight 0.00
Epoch 62 Iter 11 subLoss 4973.8 multi 1.00 import weight 0.00
Epoch 62 Acc: 96.85 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 497 train Loss: 5316.9 test Loss: 525.6
Epoch 63 Iter 0 subLoss 5134.7 multi 3.99 import weight 0.00
Epoch 63 Iter 1 subLoss 4564.7 multi 1.00 import weight 0.00
Epoch 63 Iter 2 subLoss 5279.2 multi 3.99 import weight 0.00
Epoch 63 Iter 3 subLoss 4944.1 multi 1.00 import weight 0.00
Epoch 63 Iter 4 subLoss 4535.3 multi 1.00 import weight 0.00
Epoch 63 Iter 5 subLoss 5042.9 multi 1.00 import weight 0.00
Epoch 63 Iter 6 subLoss 5403.6 multi 3.99 import weight 0.00
Epoch 63 Iter 7 subLoss 5180.2 multi -1.99 import weight 0.00
Epoch 63 Iter 8 subLoss 5063.1 multi -1.99 import weight 0.00
Epoch 63 Iter 9 subLoss 5385.6 multi -4.97 import weight 0.00
Epoch 63 Iter 10 subLoss 30938.4 multi 1.00 import weight 0.00
Epoch 63 Iter 11 subLoss 11659.2 multi 1.00 import weight 0.00
Epoch 63 Acc: 96.01 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1165 train Loss: 5889.1 test Loss: 666.7
Epoch 64 Iter 0 subLoss 5955.4 multi 6.97 import weight 1.00
Epoch 64 Iter 1 subLoss 10690.9 multi 1.00 import weight 0.00
Epoch 64 Iter 2 subLoss 5506.8 multi 1.00 import weight 0.00
Epoch 64 Iter 3 subLoss 5176.7 multi 3.99 import weight 0.00
Epoch 64 Iter 4 subLoss 5337.7 multi -4.97 import weight 0.00
Epoch 64 Iter 5 subLoss 5863.4 multi -7.96 import weight 0.00
Epoch 64 Iter 6 subLoss 68397.2 multi 1.00 import weight 0.00
Epoch 64 Iter 7 subLoss 11161.8 multi 1.00 import weight 0.00
Epoch 64 Iter 8 subLoss 8521.9 multi 3.99 import weight 0.00
Epoch 64 Iter 9 subLoss 6486.0 multi 3.99 import weight 0.00
Epoch 64 Iter 10 subLoss 5985.2 multi 3.99 import weight 0.00
Epoch 64 Iter 11 subLoss 5870.9 multi -1.99 import weight 0.00
Epoch 64 Acc: 95.64 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 587 train Loss: 6269.7 test Loss: 729.8
Epoch 65 Iter 0 subLoss 5692.1 multi 1.00 import weight 0.00
Epoch 65 Iter 1 subLoss 6233.9 multi -1.99 import weight 0.00
Epoch 65 Iter 2 subLoss 6563.2 multi 1.00 import weight 0.00
Epoch 65 Iter 3 subLoss 5468.3 multi 1.00 import weight 1.00
Epoch 65 Iter 4 subLoss 5309.4 multi 1.00 import weight 0.00
Epoch 65 Iter 5 subLoss 5517.8 multi -1.98 import weight 0.00
Epoch 65 Iter 6 subLoss 6166.8 multi -1.99 import weight 1.00
Epoch 65 Iter 7 subLoss 6734.9 multi -7.96 import weight 0.00
Epoch 65 Iter 8 subLoss 28994.3 multi 1.00 import weight 0.00
Epoch 65 Iter 9 subLoss 8149.9 multi -1.99 import weight 0.00
Epoch 65 Iter 10 subLoss 10650.9 multi 1.00 import weight 0.00
Epoch 65 Iter 11 subLoss 7556.2 multi 1.00 import weight 0.00
Epoch 65 Acc: 95.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 755 train Loss: 7508.4 test Loss: 878.5
Epoch 66 Iter 0 subLoss 7304.0 multi 3.99 import weight 0.00
Epoch 66 Iter 1 subLoss 6405.0 multi 1.00 import weight 0.00
Epoch 66 Iter 2 subLoss 5972.4 multi 1.00 import weight 0.00
Epoch 66 Iter 3 subLoss 6450.7 multi 1.00 import weight 0.00
Epoch 66 Iter 4 subLoss 6180.2 multi 3.98 import weight 1.00
Epoch 66 Iter 5 subLoss 5447.8 multi 3.99 import weight 0.00
Epoch 66 Iter 6 subLoss 5733.6 multi 1.00 import weight 0.00
Epoch 66 Iter 7 subLoss 5722.5 multi 3.99 import weight 0.00
Epoch 66 Iter 8 subLoss 5342.6 multi -1.99 import weight 0.00
Epoch 66 Iter 9 subLoss 5894.1 multi -1.99 import weight 0.00
Epoch 66 Iter 10 subLoss 6340.8 multi 6.97 import weight 1.00
Epoch 66 Iter 11 subLoss 9954.6 multi 1.00 import weight 0.00
Epoch 66 Acc: 96.32 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 995 train Loss: 6161.1 test Loss: 563.3
Epoch 67 Iter 0 subLoss 5871.8 multi 1.00 import weight 0.00
Epoch 67 Iter 1 subLoss 5039.6 multi 1.00 import weight 0.00
Epoch 67 Iter 2 subLoss 5314.4 multi -1.99 import weight 0.00
Epoch 67 Iter 3 subLoss 5126.2 multi 1.00 import weight 0.00
Epoch 67 Iter 4 subLoss 5937.1 multi 3.98 import weight 1.00
Epoch 67 Iter 5 subLoss 5163.8 multi -1.99 import weight 0.00
Epoch 67 Iter 6 subLoss 5424.5 multi 3.99 import weight 0.00
Epoch 67 Iter 7 subLoss 5443.2 multi 6.97 import weight 1.00
Epoch 67 Iter 8 subLoss 13694.3 multi 1.00 import weight 0.00
Epoch 67 Iter 9 subLoss 5748.3 multi -4.97 import weight 0.00
Epoch 67 Iter 10 subLoss 11779.0 multi 1.00 import weight 0.00
Epoch 67 Iter 11 subLoss 5675.7 multi -1.99 import weight 0.00
Epoch 67 Acc: 93.73 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 567 train Loss: 7297.5 test Loss: 886.4
Epoch 68 Iter 0 subLoss 6562.9 multi 3.99 import weight 0.00
Epoch 68 Iter 1 subLoss 7070.1 multi 3.99 import weight 0.00
Epoch 68 Iter 2 subLoss 7416.2 multi -1.99 import weight 0.00
Epoch 68 Iter 3 subLoss 20220.3 multi -1.99 import weight 0.00
Epoch 68 Iter 4 subLoss 150084.2 multi 1.00 import weight 0.00
Epoch 68 Iter 5 subLoss 11119.7 multi -4.97 import weight 0.00
Epoch 68 Iter 6 subLoss 79048.3 multi 1.00 import weight 0.00
Epoch 68 Iter 7 subLoss 18592.0 multi 1.00 import weight 0.00
Epoch 68 Iter 8 subLoss 13440.7 multi -1.99 import weight 0.00
Epoch 68 Iter 9 subLoss 20943.0 multi 1.00 import weight 0.00
Epoch 68 Iter 10 subLoss 15231.1 multi 1.00 import weight 0.00
Epoch 68 Iter 11 subLoss 12095.6 multi 3.99 import weight 0.00
Epoch 68 Acc: 93.73 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1209 train Loss: 8259.1 test Loss: 1034.5
Epoch 69 Iter 0 subLoss 8342.5 multi -1.98 import weight 0.00
Epoch 69 Iter 1 subLoss 9706.5 multi -1.99 import weight 0.00
Epoch 69 Iter 2 subLoss 14660.1 multi 1.00 import weight 0.00
Epoch 69 Iter 3 subLoss 9314.4 multi -1.99 import weight 0.00
Epoch 69 Iter 4 subLoss 12725.0 multi 6.97 import weight 1.00
Epoch 69 Iter 5 subLoss 18926.0 multi 1.00 import weight 0.00
Epoch 69 Iter 6 subLoss 11602.6 multi 1.00 import weight 0.00
Epoch 69 Iter 7 subLoss 9694.2 multi -1.98 import weight 0.00
Epoch 69 Iter 8 subLoss 12468.9 multi 6.97 import weight 1.00
Epoch 69 Iter 9 subLoss 13890.6 multi 3.99 import weight 0.00
Epoch 69 Iter 10 subLoss 13262.4 multi 1.00 import weight 0.00
Epoch 69 Iter 11 subLoss 7097.8 multi -1.99 import weight 0.00
Epoch 69 Acc: 94.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 709 train Loss: 9422.9 test Loss: 1005.9
Epoch 70 Iter 0 subLoss 9227.7 multi -1.98 import weight 0.00
Epoch 70 Iter 1 subLoss 16378.3 multi 1.00 import weight 0.00
Epoch 70 Iter 2 subLoss 8807.9 multi -1.99 import weight 0.00
Epoch 70 Iter 3 subLoss 12234.5 multi -1.99 import weight 0.00
Epoch 70 Iter 4 subLoss 22680.3 multi 1.00 import weight 0.00
Epoch 70 Iter 5 subLoss 16346.9 multi 1.00 import weight 0.00
Epoch 70 Iter 6 subLoss 11579.1 multi -1.99 import weight 0.00
Epoch 70 Iter 7 subLoss 17621.6 multi 1.00 import weight 0.00
Epoch 70 Iter 8 subLoss 13680.9 multi 1.00 import weight 0.00
Epoch 70 Iter 9 subLoss 10928.6 multi 1.00 import weight 0.00
Epoch 70 Iter 10 subLoss 8743.2 multi 1.00 import weight 0.00
Epoch 70 Iter 11 subLoss 7664.4 multi 3.99 import weight 0.00
Epoch 70 Acc: 96.75 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 766 train Loss: 6250.0 test Loss: 637.3
Epoch 71 Iter 0 subLoss 5951.0 multi 9.96 import weight 1.00
Epoch 71 Iter 1 subLoss 5275.7 multi 6.97 import weight 0.00
Epoch 71 Iter 2 subLoss 6000.9 multi -1.99 import weight 0.00
Epoch 71 Iter 3 subLoss 7422.3 multi -1.99 import weight 0.00
Epoch 71 Iter 4 subLoss 17933.5 multi 1.00 import weight 0.00
Epoch 71 Iter 5 subLoss 6824.3 multi 1.00 import weight 0.00
Epoch 71 Iter 6 subLoss 5871.2 multi 3.98 import weight 0.00
Epoch 71 Iter 7 subLoss 5239.1 multi 1.00 import weight 0.00
Epoch 71 Iter 8 subLoss 5013.5 multi 1.00 import weight 0.00
Epoch 71 Iter 9 subLoss 5253.9 multi 1.00 import weight 0.00
Epoch 71 Iter 10 subLoss 5175.9 multi 3.98 import weight 0.00
Epoch 71 Iter 11 subLoss 4582.0 multi 1.00 import weight 0.00
Epoch 71 Acc: 97.06 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 458 train Loss: 4964.9 test Loss: 491.4
Epoch 72 Iter 0 subLoss 5093.6 multi 1.00 import weight 0.00
Epoch 72 Iter 1 subLoss 4737.7 multi 1.00 import weight 0.00
Epoch 72 Iter 2 subLoss 4512.9 multi 1.00 import weight 0.00
Epoch 72 Iter 3 subLoss 5454.3 multi -1.99 import weight 0.00
Epoch 72 Iter 4 subLoss 5464.9 multi 1.00 import weight 1.00
Epoch 72 Iter 5 subLoss 4848.6 multi 1.00 import weight 0.00
Epoch 72 Iter 6 subLoss 4368.2 multi 1.00 import weight 0.00
Epoch 72 Iter 7 subLoss 4705.4 multi 1.00 import weight 0.00
Epoch 72 Iter 8 subLoss 5035.6 multi 3.99 import weight 0.00
Epoch 72 Iter 9 subLoss 4455.3 multi 1.00 import weight 0.00
Epoch 72 Iter 10 subLoss 4942.9 multi 3.99 import weight 0.00
Epoch 72 Iter 11 subLoss 4589.0 multi 3.99 import weight 0.00
Epoch 72 Acc: 97.22 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 458 train Loss: 4780.4 test Loss: 464.7
Epoch 73 Iter 0 subLoss 4681.3 multi 1.00 import weight 0.00
Epoch 73 Iter 1 subLoss 4055.8 multi 1.00 import weight 0.00
Epoch 73 Iter 2 subLoss 3757.2 multi 1.00 import weight 0.00
Epoch 73 Iter 3 subLoss 4823.9 multi 1.00 import weight 0.00
Epoch 73 Iter 4 subLoss 4408.6 multi 1.00 import weight 0.00
Epoch 73 Iter 5 subLoss 4578.1 multi -1.99 import weight 0.00
Epoch 73 Iter 6 subLoss 4599.1 multi -4.97 import weight 0.00
Epoch 73 Iter 7 subLoss 5312.4 multi 1.00 import weight 0.00
Epoch 73 Iter 8 subLoss 4817.5 multi -1.99 import weight 0.00
Epoch 73 Iter 9 subLoss 4613.6 multi 1.00 import weight 0.00
Epoch 73 Iter 10 subLoss 5351.9 multi -1.99 import weight 0.00
Epoch 73 Iter 11 subLoss 5048.8 multi -1.98 import weight 0.00
Epoch 73 Acc: 95.64 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 504 train Loss: 6997.4 test Loss: 737.5
Epoch 74 Iter 0 subLoss 6977.4 multi 1.00 import weight 0.00
Epoch 74 Iter 1 subLoss 5226.4 multi 1.00 import weight 0.00
Epoch 74 Iter 2 subLoss 4029.3 multi 1.00 import weight 0.00
Epoch 74 Iter 3 subLoss 5420.0 multi -4.97 import weight 0.00
Epoch 74 Iter 4 subLoss 5664.8 multi 3.99 import weight 0.00
Epoch 74 Iter 5 subLoss 5288.3 multi -7.96 import weight 0.00
Epoch 74 Iter 6 subLoss 31640.7 multi 1.00 import weight 0.00
Epoch 74 Iter 7 subLoss 7630.3 multi 3.99 import weight 0.00
Epoch 74 Iter 8 subLoss 5161.6 multi 1.00 import weight 0.00
Epoch 74 Iter 9 subLoss 4833.2 multi -1.99 import weight 0.00
Epoch 74 Iter 10 subLoss 5276.8 multi 9.96 import weight 1.00
Epoch 74 Iter 11 subLoss 8796.6 multi 3.99 import weight 0.00
Epoch 74 Acc: 94.90 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 879 train Loss: 6539.9 test Loss: 818.0
Epoch 75 Iter 0 subLoss 7023.7 multi 3.99 import weight 0.00
Epoch 75 Iter 1 subLoss 4788.8 multi 1.00 import weight 0.00
Epoch 75 Iter 2 subLoss 4781.7 multi 3.99 import weight 0.00
Epoch 75 Iter 3 subLoss 4819.8 multi 1.00 import weight 0.00
Epoch 75 Iter 4 subLoss 4460.3 multi -1.99 import weight 0.00
Epoch 75 Iter 5 subLoss 4333.5 multi 1.00 import weight 0.00
Epoch 75 Iter 6 subLoss 4572.5 multi 1.00 import weight 0.00
Epoch 75 Iter 7 subLoss 4513.7 multi 3.99 import weight 0.00
Epoch 75 Iter 8 subLoss 4437.0 multi 1.00 import weight 0.00
Epoch 75 Iter 9 subLoss 3917.0 multi 1.00 import weight 0.00
Epoch 75 Iter 10 subLoss 4307.9 multi 1.00 import weight 0.00
Epoch 75 Iter 11 subLoss 4032.1 multi -1.99 import weight 0.00
Epoch 75 Acc: 97.14 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 403 train Loss: 4545.2 test Loss: 454.0
Epoch 76 Iter 0 subLoss 4613.1 multi 3.99 import weight 0.00
Epoch 76 Iter 1 subLoss 4275.6 multi 1.00 import weight 0.00
Epoch 76 Iter 2 subLoss 3895.1 multi 1.00 import weight 0.00
Epoch 76 Iter 3 subLoss 3795.9 multi 1.00 import weight 0.00
Epoch 76 Iter 4 subLoss 4303.6 multi 3.99 import weight 0.00
Epoch 76 Iter 5 subLoss 4297.9 multi 1.00 import weight 0.00
Epoch 76 Iter 6 subLoss 4878.0 multi 1.00 import weight 0.00
Epoch 76 Iter 7 subLoss 3928.0 multi -1.99 import weight 0.00
Epoch 76 Iter 8 subLoss 4257.1 multi 1.00 import weight 0.00
Epoch 76 Iter 9 subLoss 4763.7 multi 1.00 import weight 0.00
Epoch 76 Iter 10 subLoss 4210.2 multi 1.00 import weight 0.00
Epoch 76 Iter 11 subLoss 3956.0 multi 1.00 import weight 0.00
Epoch 76 Acc: 97.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 395 train Loss: 4181.2 test Loss: 425.4
Epoch 77 Iter 0 subLoss 4412.5 multi -1.99 import weight 0.00
Epoch 77 Iter 1 subLoss 4247.7 multi 1.00 import weight 0.00
Epoch 77 Iter 2 subLoss 3744.6 multi 1.00 import weight 0.00
Epoch 77 Iter 3 subLoss 3963.2 multi -1.99 import weight 0.00
Epoch 77 Iter 4 subLoss 4174.3 multi 1.00 import weight 0.00
Epoch 77 Iter 5 subLoss 3644.3 multi 1.00 import weight 0.00
Epoch 77 Iter 6 subLoss 4306.2 multi 3.98 import weight 0.00
Epoch 77 Iter 7 subLoss 4048.7 multi -1.99 import weight 0.00
Epoch 77 Iter 8 subLoss 4374.3 multi -1.99 import weight 0.00
Epoch 77 Iter 9 subLoss 5264.3 multi -1.99 import weight 0.00
Epoch 77 Iter 10 subLoss 5369.4 multi -1.99 import weight 0.00
Epoch 77 Iter 11 subLoss 13749.9 multi 3.99 import weight 0.00
Epoch 77 Acc: 61.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1374 train Loss: 53793.8 test Loss: 8936.7
Epoch 78 Iter 0 subLoss 53677.7 multi 1.00 import weight 0.00
Epoch 78 Iter 1 subLoss 12161.2 multi 1.00 import weight 0.00
Epoch 78 Iter 2 subLoss 11251.5 multi -1.99 import weight 0.00
Epoch 78 Iter 3 subLoss 14867.8 multi 1.00 import weight 0.00
Epoch 78 Iter 4 subLoss 12572.4 multi 3.99 import weight 0.00
Epoch 78 Iter 5 subLoss 7197.2 multi -4.97 import weight 0.00
Epoch 78 Iter 6 subLoss 10879.5 multi 6.97 import weight 0.00
Epoch 78 Iter 7 subLoss 7293.7 multi -1.99 import weight 0.00
Epoch 78 Iter 8 subLoss 9454.4 multi 1.00 import weight 0.00
Epoch 78 Iter 9 subLoss 7341.2 multi -1.99 import weight 0.00
Epoch 78 Iter 10 subLoss 9481.4 multi 1.00 import weight 0.00
Epoch 78 Iter 11 subLoss 7413.4 multi 1.00 import weight 0.00
Epoch 78 Acc: 95.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 741 train Loss: 7151.0 test Loss: 797.2
Epoch 79 Iter 0 subLoss 6811.6 multi 3.99 import weight 0.00
Epoch 79 Iter 1 subLoss 5946.0 multi -7.96 import weight 0.00
Epoch 79 Iter 2 subLoss 7787.7 multi 1.00 import weight 0.00
Epoch 79 Iter 3 subLoss 7142.2 multi 1.00 import weight 0.00
Epoch 79 Iter 4 subLoss 6517.4 multi 1.00 import weight 0.00
Epoch 79 Iter 5 subLoss 6196.5 multi -7.96 import weight 0.00
Epoch 79 Iter 6 subLoss 9542.0 multi 1.00 import weight 0.00
Epoch 79 Iter 7 subLoss 8320.0 multi -1.99 import weight 0.00
Epoch 79 Iter 8 subLoss 10312.2 multi -1.99 import weight 0.00
Epoch 79 Iter 9 subLoss 13800.6 multi 1.00 import weight 0.00
Epoch 79 Iter 10 subLoss 10469.0 multi 1.00 import weight 0.00
Epoch 79 Iter 11 subLoss 8846.1 multi 3.99 import weight 0.00
Epoch 79 Acc: 96.21 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 884 train Loss: 7002.0 test Loss: 763.4
Epoch 80 Iter 0 subLoss 6876.9 multi 1.00 import weight 0.00
Epoch 80 Iter 1 subLoss 6304.9 multi 3.99 import weight 0.00
Epoch 80 Iter 2 subLoss 5788.1 multi 1.00 import weight 0.00
Epoch 80 Iter 3 subLoss 5474.1 multi -10.94 import weight 0.00
Epoch 80 Iter 4 subLoss 7860.3 multi -1.99 import weight 0.00
Epoch 80 Iter 5 subLoss 9315.9 multi 1.00 import weight 0.00
Epoch 80 Iter 6 subLoss 7965.1 multi 6.97 import weight 0.00
Epoch 80 Iter 7 subLoss 7022.7 multi 6.97 import weight 0.00
Epoch 80 Iter 8 subLoss 5660.8 multi 6.97 import weight 0.00
Epoch 80 Iter 9 subLoss 6102.7 multi 1.00 import weight 0.00
Epoch 80 Iter 10 subLoss 5509.1 multi 3.98 import weight 0.00
Epoch 80 Iter 11 subLoss 5431.2 multi -4.97 import weight 0.00
Epoch 80 Acc: 96.73 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 543 train Loss: 5147.6 test Loss: 524.4
Epoch 81 Iter 0 subLoss 4846.6 multi 1.00 import weight 0.00
Epoch 81 Iter 1 subLoss 5395.9 multi -1.99 import weight 0.00
Epoch 81 Iter 2 subLoss 4980.9 multi -1.99 import weight 0.00
Epoch 81 Iter 3 subLoss 5711.7 multi 1.00 import weight 0.00
Epoch 81 Iter 4 subLoss 5920.9 multi 3.99 import weight 0.00
Epoch 81 Iter 5 subLoss 4612.8 multi 6.97 import weight 0.00
Epoch 81 Iter 6 subLoss 4837.1 multi 1.00 import weight 0.00
Epoch 81 Iter 7 subLoss 4375.7 multi 1.00 import weight 0.00
Epoch 81 Iter 8 subLoss 4815.5 multi 3.98 import weight 0.00
Epoch 81 Iter 9 subLoss 5320.0 multi 3.98 import weight 0.00
Epoch 81 Iter 10 subLoss 3927.8 multi 1.00 import weight 0.00
Epoch 81 Iter 11 subLoss 4812.7 multi 6.97 import weight 1.00
Epoch 81 Acc: 97.35 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 6.97 Pidx 481 train Loss: 4452.6 test Loss: 417.7
Epoch 82 Iter 0 subLoss 4457.3 multi 3.99 import weight 0.00
Epoch 82 Iter 1 subLoss 4395.8 multi 1.00 import weight 0.00
Epoch 82 Iter 2 subLoss 4727.1 multi 1.00 import weight 0.00
Epoch 82 Iter 3 subLoss 3909.9 multi -1.99 import weight 0.00
Epoch 82 Iter 4 subLoss 4693.7 multi -1.99 import weight 0.00
Epoch 82 Iter 5 subLoss 4066.0 multi -1.99 import weight 0.00
Epoch 82 Iter 6 subLoss 4955.4 multi -4.97 import weight 0.00
Epoch 82 Iter 7 subLoss 9841.8 multi 3.99 import weight 0.00
Epoch 82 Iter 8 subLoss 8268.9 multi 3.99 import weight 0.00
Epoch 82 Iter 9 subLoss 5143.8 multi -1.98 import weight 0.00
Epoch 82 Iter 10 subLoss 5761.0 multi -1.99 import weight 0.00
Epoch 82 Iter 11 subLoss 7973.2 multi -7.96 import weight 0.00
Epoch 82 Acc: 21.64 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 797 train Loss: 140635.0 test Loss: 25191.5
Epoch 83 Iter 0 subLoss 141480.7 multi 1.00 import weight 0.00
Epoch 83 Iter 1 subLoss 20982.3 multi 1.00 import weight 0.00
Epoch 83 Iter 2 subLoss 15086.3 multi 1.00 import weight 0.00
Epoch 83 Iter 3 subLoss 13221.5 multi 1.00 import weight 0.00
Epoch 83 Iter 4 subLoss 11305.9 multi 1.00 import weight 0.00
Epoch 83 Iter 5 subLoss 10371.1 multi 1.00 import weight 0.00
Epoch 83 Iter 6 subLoss 9334.6 multi -1.99 import weight 0.00
Epoch 83 Iter 7 subLoss 10869.1 multi 1.00 import weight 0.00
Epoch 83 Iter 8 subLoss 10086.8 multi 1.00 import weight 0.00
Epoch 83 Iter 9 subLoss 8190.9 multi -1.99 import weight 0.00
Epoch 83 Iter 10 subLoss 10031.0 multi -1.99 import weight 0.00
Epoch 83 Iter 11 subLoss 12723.6 multi 9.96 import weight 1.00
Epoch 83 Acc: 92.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 9.96 Pidx 1272 train Loss: 9215.9 test Loss: 1205.6
Epoch 84 Iter 0 subLoss 8950.8 multi 3.99 import weight 0.00
Epoch 84 Iter 1 subLoss 6327.9 multi 1.00 import weight 0.00
Epoch 84 Iter 2 subLoss 5482.3 multi -1.99 import weight 0.00
Epoch 84 Iter 3 subLoss 5966.7 multi -10.94 import weight 0.00
Epoch 84 Iter 4 subLoss 16595.1 multi -1.99 import weight 0.00
Epoch 84 Iter 5 subLoss 76237.2 multi 1.00 import weight 0.00
Epoch 84 Iter 6 subLoss 25501.5 multi 1.00 import weight 0.00
Epoch 84 Iter 7 subLoss 17420.0 multi 1.00 import weight 0.00
Epoch 84 Iter 8 subLoss 12870.5 multi 1.00 import weight 0.00
Epoch 84 Iter 9 subLoss 11129.7 multi -1.99 import weight 0.00
Epoch 84 Iter 10 subLoss 14724.7 multi 1.00 import weight 0.00
Epoch 84 Iter 11 subLoss 12165.8 multi 3.99 import weight 0.00
Epoch 84 Acc: 95.14 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1216 train Loss: 7689.7 test Loss: 962.9
Epoch 85 Iter 0 subLoss 7536.4 multi 1.00 import weight 0.00
Epoch 85 Iter 1 subLoss 6974.8 multi 3.98 import weight 0.00
Epoch 85 Iter 2 subLoss 6305.2 multi 6.97 import weight 0.00
Epoch 85 Iter 3 subLoss 5506.1 multi 6.97 import weight 1.00
Epoch 85 Iter 4 subLoss 4890.3 multi 1.00 import weight 0.00
Epoch 85 Iter 5 subLoss 5062.9 multi 1.00 import weight 0.00
Epoch 85 Iter 6 subLoss 5035.1 multi 6.97 import weight 0.00
Epoch 85 Iter 7 subLoss 4820.9 multi -7.96 import weight 0.00
Epoch 85 Iter 8 subLoss 5333.8 multi -1.98 import weight 0.00
Epoch 85 Iter 9 subLoss 5416.7 multi -1.98 import weight 0.00
Epoch 85 Iter 10 subLoss 6051.5 multi 3.99 import weight 0.00
Epoch 85 Iter 11 subLoss 5306.8 multi 3.99 import weight 0.00
Epoch 85 Acc: 96.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 530 train Loss: 5045.0 test Loss: 530.4
Epoch 86 Iter 0 subLoss 4688.1 multi 3.99 import weight 0.00
Epoch 86 Iter 1 subLoss 4919.0 multi 1.00 import weight 0.00
Epoch 86 Iter 2 subLoss 4972.3 multi 3.99 import weight 0.00
Epoch 86 Iter 3 subLoss 4170.0 multi 1.00 import weight 0.00
Epoch 86 Iter 4 subLoss 4460.8 multi -1.98 import weight 0.00
Epoch 86 Iter 5 subLoss 4857.5 multi -4.97 import weight 0.00
Epoch 86 Iter 6 subLoss 5135.9 multi 3.98 import weight 0.00
Epoch 86 Iter 7 subLoss 4716.7 multi -1.99 import weight 0.00
Epoch 86 Iter 8 subLoss 5227.5 multi 3.99 import weight 0.00
Epoch 86 Iter 9 subLoss 4490.3 multi 1.00 import weight 0.00
Epoch 86 Iter 10 subLoss 5432.9 multi -1.98 import weight 0.00
Epoch 86 Iter 11 subLoss 5066.5 multi 3.98 import weight 0.00
Epoch 86 Acc: 97.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 506 train Loss: 4708.3 test Loss: 465.8
Epoch 87 Iter 0 subLoss 4458.8 multi 6.97 import weight 0.00
Epoch 87 Iter 1 subLoss 4600.3 multi -1.99 import weight 0.00
Epoch 87 Iter 2 subLoss 4866.0 multi -1.99 import weight 0.00
Epoch 87 Iter 3 subLoss 4810.5 multi 9.96 import weight 1.00
Epoch 87 Iter 4 subLoss 5627.0 multi 3.99 import weight 0.00
Epoch 87 Iter 5 subLoss 4595.5 multi -1.98 import weight 0.00
Epoch 87 Iter 6 subLoss 4481.9 multi 1.00 import weight 0.00
Epoch 87 Iter 7 subLoss 4197.4 multi 1.00 import weight 0.00
Epoch 87 Iter 8 subLoss 3949.3 multi 1.00 import weight 0.00
Epoch 87 Iter 9 subLoss 4520.6 multi -4.97 import weight 0.00
Epoch 87 Iter 10 subLoss 4660.3 multi 1.00 import weight 0.00
Epoch 87 Iter 11 subLoss 4275.4 multi 3.99 import weight 0.00
Epoch 87 Acc: 97.47 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 427 train Loss: 4388.0 test Loss: 438.9
Epoch 88 Iter 0 subLoss 5091.6 multi 3.99 import weight 0.00
Epoch 88 Iter 1 subLoss 4178.6 multi 1.00 import weight 0.00
Epoch 88 Iter 2 subLoss 3883.0 multi 1.00 import weight 0.00
Epoch 88 Iter 3 subLoss 3557.0 multi 1.00 import weight 0.00
Epoch 88 Iter 4 subLoss 4402.2 multi 1.00 import weight 0.00
Epoch 88 Iter 5 subLoss 4573.6 multi 3.98 import weight 0.00
Epoch 88 Iter 6 subLoss 4239.9 multi 1.00 import weight 0.00
Epoch 88 Iter 7 subLoss 3806.9 multi -1.99 import weight 0.00
Epoch 88 Iter 8 subLoss 4499.5 multi 1.00 import weight 0.00
Epoch 88 Iter 9 subLoss 4088.9 multi 1.00 import weight 0.00
Epoch 88 Iter 10 subLoss 4384.3 multi -4.97 import weight 0.00
Epoch 88 Iter 11 subLoss 4231.7 multi 3.99 import weight 0.00
Epoch 88 Acc: 97.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 423 train Loss: 4283.3 test Loss: 415.6
Epoch 89 Iter 0 subLoss 4061.7 multi 1.00 import weight 0.00
Epoch 89 Iter 1 subLoss 4460.0 multi -1.99 import weight 0.00
Epoch 89 Iter 2 subLoss 4895.7 multi 3.99 import weight 0.00
Epoch 89 Iter 3 subLoss 4127.6 multi 1.00 import weight 0.00
Epoch 89 Iter 4 subLoss 4283.3 multi -4.97 import weight 0.00
Epoch 89 Iter 5 subLoss 4417.8 multi -1.98 import weight 0.00
Epoch 89 Iter 6 subLoss 4529.4 multi -1.98 import weight 0.00
Epoch 89 Iter 7 subLoss 4900.2 multi -4.97 import weight 0.00
Epoch 89 Iter 8 subLoss 17643.1 multi 1.00 import weight 0.00
Epoch 89 Iter 9 subLoss 4223.1 multi -1.99 import weight 0.00
Epoch 89 Iter 10 subLoss 5181.3 multi -4.97 import weight 0.00
Epoch 89 Iter 11 subLoss 7553.5 multi 3.98 import weight 0.00
Epoch 89 Acc: 96.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 755 train Loss: 4973.5 test Loss: 533.0
Epoch 90 Iter 0 subLoss 4581.9 multi -1.99 import weight 0.00
Epoch 90 Iter 1 subLoss 5502.4 multi 9.96 import weight 1.00
Epoch 90 Iter 2 subLoss 5002.6 multi 1.00 import weight 0.00
Epoch 90 Iter 3 subLoss 4459.7 multi 9.96 import weight 0.00
Epoch 90 Iter 4 subLoss 5423.8 multi 1.00 import weight 0.00
Epoch 90 Iter 5 subLoss 4370.3 multi 3.98 import weight 0.00
Epoch 90 Iter 6 subLoss 4336.9 multi 3.99 import weight 0.00
Epoch 90 Iter 7 subLoss 4247.2 multi -1.98 import weight 0.00
Epoch 90 Iter 8 subLoss 4367.7 multi 3.99 import weight 0.00
Epoch 90 Iter 9 subLoss 4060.8 multi 3.98 import weight 0.00
Epoch 90 Iter 10 subLoss 3990.4 multi 1.00 import weight 0.00
Epoch 90 Iter 11 subLoss 4254.4 multi -1.98 import weight 0.00
Epoch 90 Acc: 97.65 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 425 train Loss: 4073.1 test Loss: 389.4
Epoch 91 Iter 0 subLoss 3891.9 multi 1.00 import weight 0.00
Epoch 91 Iter 1 subLoss 4086.4 multi 3.99 import weight 0.00
Epoch 91 Iter 2 subLoss 4010.5 multi 1.00 import weight 0.00
Epoch 91 Iter 3 subLoss 3866.3 multi 1.00 import weight 0.00
Epoch 91 Iter 4 subLoss 3959.1 multi 1.00 import weight 0.00
Epoch 91 Iter 5 subLoss 3643.7 multi 3.99 import weight 0.00
Epoch 91 Iter 6 subLoss 3237.5 multi 1.00 import weight 0.00
Epoch 91 Iter 7 subLoss 3758.1 multi 1.00 import weight 0.00
Epoch 91 Iter 8 subLoss 3611.1 multi 1.00 import weight 0.00
Epoch 91 Iter 9 subLoss 3519.6 multi 1.00 import weight 0.00
Epoch 91 Iter 10 subLoss 3757.3 multi 3.98 import weight 0.00
Epoch 91 Iter 11 subLoss 4484.3 multi 3.99 import weight 0.00
Epoch 91 Acc: 97.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 448 train Loss: 3907.4 test Loss: 366.3
Epoch 92 Iter 0 subLoss 3604.1 multi 1.00 import weight 0.00
Epoch 92 Iter 1 subLoss 3694.0 multi 1.00 import weight 0.00
Epoch 92 Iter 2 subLoss 4003.8 multi -1.99 import weight 0.00
Epoch 92 Iter 3 subLoss 3675.8 multi 1.00 import weight 0.00
Epoch 92 Iter 4 subLoss 3913.5 multi 1.00 import weight 0.00
Epoch 92 Iter 5 subLoss 3621.5 multi -1.99 import weight 0.00
Epoch 92 Iter 6 subLoss 3872.3 multi -1.99 import weight 0.00
Epoch 92 Iter 7 subLoss 3644.1 multi 6.97 import weight 0.00
Epoch 92 Iter 8 subLoss 3696.0 multi 3.99 import weight 0.00
Epoch 92 Iter 9 subLoss 3579.0 multi 1.00 import weight 0.00
Epoch 92 Iter 10 subLoss 3405.3 multi 1.00 import weight 0.00
Epoch 92 Iter 11 subLoss 3815.4 multi -1.99 import weight 0.00
Epoch 92 Acc: 97.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 381 train Loss: 3799.4 test Loss: 354.0
Epoch 93 Iter 0 subLoss 3636.0 multi -1.99 import weight 0.00
Epoch 93 Iter 1 subLoss 3451.4 multi 1.00 import weight 0.00
Epoch 93 Iter 2 subLoss 4411.1 multi 1.00 import weight 0.00
Epoch 93 Iter 3 subLoss 3380.5 multi 1.00 import weight 0.00
Epoch 93 Iter 4 subLoss 3963.7 multi -1.98 import weight 0.00
Epoch 93 Iter 5 subLoss 3798.9 multi 3.99 import weight 0.00
Epoch 93 Iter 6 subLoss 3859.1 multi 1.00 import weight 0.00
Epoch 93 Iter 7 subLoss 3178.1 multi 1.00 import weight 0.00
Epoch 93 Iter 8 subLoss 3757.6 multi 6.97 import weight 0.00
Epoch 93 Iter 9 subLoss 3008.9 multi 1.00 import weight 0.00
Epoch 93 Iter 10 subLoss 4237.9 multi 3.98 import weight 0.00
Epoch 93 Iter 11 subLoss 4199.2 multi 3.99 import weight 0.00
Epoch 93 Acc: 97.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 419 train Loss: 3713.5 test Loss: 377.0
Epoch 94 Iter 0 subLoss 3719.3 multi 1.00 import weight 0.00
Epoch 94 Iter 1 subLoss 3500.1 multi 1.00 import weight 0.00
Epoch 94 Iter 2 subLoss 3619.2 multi 1.00 import weight 0.00
Epoch 94 Iter 3 subLoss 3762.3 multi -10.94 import weight 0.00
Epoch 94 Iter 4 subLoss 3841.5 multi 1.00 import weight 0.00
Epoch 94 Iter 5 subLoss 4099.3 multi -4.97 import weight 0.00
Epoch 94 Iter 6 subLoss 5527.5 multi -1.98 import weight 0.00
Epoch 94 Iter 7 subLoss 12554.8 multi -1.99 import weight 0.00
Epoch 94 Iter 8 subLoss 72236.2 multi 1.00 import weight 0.00
Epoch 94 Iter 9 subLoss 10760.0 multi 3.99 import weight 0.00
Epoch 94 Iter 10 subLoss 26627.8 multi 1.00 import weight 0.00
Epoch 94 Iter 11 subLoss 6094.1 multi -1.99 import weight 0.00
Epoch 94 Acc: 95.50 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 609 train Loss: 8358.3 test Loss: 770.9
Epoch 95 Iter 0 subLoss 8654.6 multi 1.00 import weight 0.00
Epoch 95 Iter 1 subLoss 5862.5 multi -4.97 import weight 0.00
Epoch 95 Iter 2 subLoss 12092.4 multi 6.97 import weight 0.00
Epoch 95 Iter 3 subLoss 16053.1 multi 1.00 import weight 0.00
Epoch 95 Iter 4 subLoss 8376.8 multi 1.00 import weight 0.00
Epoch 95 Iter 5 subLoss 6931.1 multi -1.98 import weight 0.00
Epoch 95 Iter 6 subLoss 8530.7 multi -1.98 import weight 0.00
Epoch 95 Iter 7 subLoss 14957.7 multi 1.00 import weight 0.00
Epoch 95 Iter 8 subLoss 8449.0 multi 1.00 import weight 0.00
Epoch 95 Iter 9 subLoss 7732.4 multi -1.99 import weight 0.00
Epoch 95 Iter 10 subLoss 9921.9 multi -1.99 import weight 0.00
Epoch 95 Iter 11 subLoss 17089.8 multi 1.00 import weight 0.00
Epoch 95 Acc: 90.41 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1708 train Loss: 10461.8 test Loss: 1449.5
Epoch 96 Iter 0 subLoss 9124.3 multi 6.97 import weight 0.00
Epoch 96 Iter 1 subLoss 7118.4 multi 1.00 import weight 0.00
Epoch 96 Iter 2 subLoss 5984.7 multi 3.98 import weight 0.00
Epoch 96 Iter 3 subLoss 4304.7 multi 6.97 import weight 0.00
Epoch 96 Iter 4 subLoss 4507.4 multi -4.97 import weight 0.00
Epoch 96 Iter 5 subLoss 4647.0 multi 1.00 import weight 0.00
Epoch 96 Iter 6 subLoss 4476.5 multi -7.96 import weight 0.00
Epoch 96 Iter 7 subLoss 5865.6 multi -1.99 import weight 0.00
Epoch 96 Iter 8 subLoss 7478.2 multi 3.98 import weight 0.00
Epoch 96 Iter 9 subLoss 4926.9 multi 1.00 import weight 0.00
Epoch 96 Iter 10 subLoss 5025.8 multi -1.99 import weight 0.00
Epoch 96 Iter 11 subLoss 5272.8 multi 9.96 import weight 1.00
Epoch 96 Acc: 97.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 9.96 Pidx 527 train Loss: 5375.7 test Loss: 484.6
Epoch 97 Iter 0 subLoss 5325.2 multi -1.99 import weight 0.00
Epoch 97 Iter 1 subLoss 7696.9 multi 1.00 import weight 0.00
Epoch 97 Iter 2 subLoss 5194.1 multi -4.97 import weight 0.00
Epoch 97 Iter 3 subLoss 12572.6 multi 6.97 import weight 0.00
Epoch 97 Iter 4 subLoss 52245.5 multi 1.00 import weight 0.00
Epoch 97 Iter 5 subLoss 13336.9 multi 1.00 import weight 0.00
Epoch 97 Iter 6 subLoss 10705.5 multi -1.99 import weight 0.00
Epoch 97 Iter 7 subLoss 14346.9 multi 1.00 import weight 0.00
Epoch 97 Iter 8 subLoss 12211.1 multi 1.00 import weight 0.00
Epoch 97 Iter 9 subLoss 9672.8 multi -1.99 import weight 0.00
Epoch 97 Iter 10 subLoss 11479.7 multi 1.00 import weight 0.00
Epoch 97 Iter 11 subLoss 10314.7 multi 1.00 import weight 0.00
Epoch 97 Acc: 93.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1031 train Loss: 9512.0 test Loss: 1194.9
Epoch 98 Iter 0 subLoss 9680.2 multi 3.98 import weight 0.00
Epoch 98 Iter 1 subLoss 7489.6 multi -7.96 import weight 0.00
Epoch 98 Iter 2 subLoss 10135.9 multi -1.99 import weight 0.00
Epoch 98 Iter 3 subLoss 12489.7 multi 1.00 import weight 0.00
Epoch 98 Iter 4 subLoss 11064.9 multi 1.00 import weight 0.00
Epoch 98 Iter 5 subLoss 9147.1 multi 1.00 import weight 0.00
Epoch 98 Iter 6 subLoss 9064.8 multi -1.99 import weight 0.00
Epoch 98 Iter 7 subLoss 9822.4 multi 1.00 import weight 0.00
Epoch 98 Iter 8 subLoss 9329.3 multi -1.98 import weight 0.00
Epoch 98 Iter 9 subLoss 10680.0 multi -1.99 import weight 0.00
Epoch 98 Iter 10 subLoss 12392.5 multi 1.00 import weight 0.00
Epoch 98 Iter 11 subLoss 10981.6 multi 1.00 import weight 0.00
Epoch 98 Acc: 94.38 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1098 train Loss: 10732.3 test Loss: 1300.8
Epoch 99 Iter 0 subLoss 10343.8 multi -1.99 import weight 0.00
Epoch 99 Iter 1 subLoss 12111.4 multi 1.00 import weight 0.00
Epoch 99 Iter 2 subLoss 10972.0 multi 3.99 import weight 0.00
Epoch 99 Iter 3 subLoss 8359.8 multi -4.97 import weight 0.00
Epoch 99 Iter 4 subLoss 10791.2 multi 1.00 import weight 0.00
Epoch 99 Iter 5 subLoss 10487.6 multi 1.00 import weight 0.00
Epoch 99 Iter 6 subLoss 9691.7 multi -1.99 import weight 0.00
Epoch 99 Iter 7 subLoss 10262.8 multi 3.99 import weight 0.00
Epoch 99 Iter 8 subLoss 8902.8 multi 1.00 import weight 0.00
Epoch 99 Iter 9 subLoss 8489.7 multi 1.00 import weight 0.00
Epoch 99 Iter 10 subLoss 8470.2 multi 1.00 import weight 0.00
Epoch 99 Iter 11 subLoss 7776.7 multi 1.00 import weight 0.00
Epoch 99 Acc: 95.60 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 777 train Loss: 7937.0 test Loss: 894.9
Epoch 100 Iter 0 subLoss 7590.4 multi 1.00 import weight 0.00
Epoch 100 Iter 1 subLoss 7225.5 multi -1.99 import weight 0.00
Epoch 100 Iter 2 subLoss 8118.1 multi 1.00 import weight 0.00
Epoch 100 Iter 3 subLoss 7624.4 multi 1.00 import weight 0.00
Epoch 100 Iter 4 subLoss 7504.1 multi 1.00 import weight 0.00
Epoch 100 Iter 5 subLoss 7353.0 multi 1.00 import weight 0.00
Epoch 100 Iter 6 subLoss 6629.9 multi 6.97 import weight 0.00
Epoch 100 Iter 7 subLoss 5774.0 multi -1.99 import weight 0.00
Epoch 100 Iter 8 subLoss 6575.2 multi 1.00 import weight 0.00
Epoch 100 Iter 9 subLoss 5993.7 multi -4.97 import weight 0.00
Epoch 100 Iter 10 subLoss 6452.7 multi 3.99 import weight 0.00
Epoch 100 Iter 11 subLoss 6081.1 multi 3.99 import weight 0.00
Epoch 100 Acc: 97.16 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 608 train Loss: 5850.7 test Loss: 566.5
Epoch 101 Iter 0 subLoss 5419.1 multi 1.00 import weight 0.00
Epoch 101 Iter 1 subLoss 5690.5 multi 3.99 import weight 0.00
Epoch 101 Iter 2 subLoss 5656.2 multi 1.00 import weight 0.00
Epoch 101 Iter 3 subLoss 4701.6 multi 1.00 import weight 0.00
Epoch 101 Iter 4 subLoss 4786.2 multi 6.97 import weight 0.00
Epoch 101 Iter 5 subLoss 4993.1 multi -1.99 import weight 0.00
Epoch 101 Iter 6 subLoss 4591.7 multi -1.99 import weight 0.00
Epoch 101 Iter 7 subLoss 5276.0 multi 12.94 import weight 1.00
Epoch 101 Iter 8 subLoss 5888.5 multi -4.97 import weight 0.00
Epoch 101 Iter 9 subLoss 10548.6 multi 1.00 import weight 0.00
Epoch 101 Iter 10 subLoss 6203.0 multi -1.99 import weight 0.00
Epoch 101 Iter 11 subLoss 9790.2 multi 1.00 import weight 0.00
Epoch 101 Acc: 96.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 979 train Loss: 6394.5 test Loss: 716.0
Epoch 102 Iter 0 subLoss 6532.9 multi 3.99 import weight 0.00
Epoch 102 Iter 1 subLoss 4569.9 multi 3.99 import weight 0.00
Epoch 102 Iter 2 subLoss 4290.4 multi 1.00 import weight 0.00
Epoch 102 Iter 3 subLoss 4587.2 multi 1.00 import weight 0.00
Epoch 102 Iter 4 subLoss 4417.1 multi 3.99 import weight 0.00
Epoch 102 Iter 5 subLoss 4057.7 multi 1.00 import weight 0.00
Epoch 102 Iter 6 subLoss 4581.8 multi 3.99 import weight 0.00
Epoch 102 Iter 7 subLoss 4371.3 multi 3.99 import weight 0.00
Epoch 102 Iter 8 subLoss 4290.1 multi 3.98 import weight 0.00
Epoch 102 Iter 9 subLoss 3711.1 multi 3.99 import weight 0.00
Epoch 102 Iter 10 subLoss 3786.4 multi 1.00 import weight 0.00
Epoch 102 Iter 11 subLoss 3850.4 multi 1.00 import weight 0.00
Epoch 102 Acc: 98.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 385 train Loss: 3975.5 test Loss: 369.2
Epoch 103 Iter 0 subLoss 3702.1 multi -4.97 import weight 0.00
Epoch 103 Iter 1 subLoss 3548.1 multi 1.00 import weight 0.00
Epoch 103 Iter 2 subLoss 4045.8 multi 1.00 import weight 0.00
Epoch 103 Iter 3 subLoss 3965.3 multi 1.00 import weight 0.00
Epoch 103 Iter 4 subLoss 4064.9 multi 3.99 import weight 0.00
Epoch 103 Iter 5 subLoss 4474.6 multi -4.97 import weight 0.00
Epoch 103 Iter 6 subLoss 4186.6 multi -4.97 import weight 0.00
Epoch 103 Iter 7 subLoss 4381.0 multi -7.96 import weight 0.00
Epoch 103 Iter 8 subLoss 13408.6 multi 1.00 import weight 0.00
Epoch 103 Iter 9 subLoss 5208.6 multi -1.99 import weight 0.00
Epoch 103 Iter 10 subLoss 7651.8 multi 1.00 import weight 0.00
Epoch 103 Iter 11 subLoss 4973.6 multi 6.97 import weight 0.00
Epoch 103 Acc: 97.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 497 train Loss: 4762.0 test Loss: 447.5
Epoch 104 Iter 0 subLoss 4589.0 multi 6.97 import weight 1.00
Epoch 104 Iter 1 subLoss 4448.4 multi -1.99 import weight 0.00
Epoch 104 Iter 2 subLoss 4732.1 multi 1.00 import weight 0.00
Epoch 104 Iter 3 subLoss 3838.8 multi 1.00 import weight 0.00
Epoch 104 Iter 4 subLoss 4383.6 multi -4.97 import weight 0.00
Epoch 104 Iter 5 subLoss 4629.2 multi -7.96 import weight 0.00
Epoch 104 Iter 6 subLoss 11273.4 multi 6.97 import weight 0.00
Epoch 104 Iter 7 subLoss 39812.4 multi 1.00 import weight 0.00
Epoch 104 Iter 8 subLoss 10374.7 multi 3.99 import weight 0.00
Epoch 104 Iter 9 subLoss 5539.5 multi -4.97 import weight 0.00
Epoch 104 Iter 10 subLoss 9174.1 multi 1.00 import weight 0.00
Epoch 104 Iter 11 subLoss 7588.4 multi -4.97 import weight 0.00
Epoch 104 Acc: 92.64 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 758 train Loss: 13257.8 test Loss: 1597.2
Epoch 105 Iter 0 subLoss 13695.1 multi 1.00 import weight 0.00
Epoch 105 Iter 1 subLoss 8939.5 multi 1.00 import weight 0.00
Epoch 105 Iter 2 subLoss 7935.2 multi 3.99 import weight 0.00
Epoch 105 Iter 3 subLoss 6595.4 multi 1.00 import weight 0.00
Epoch 105 Iter 4 subLoss 5697.2 multi 6.97 import weight 0.00
Epoch 105 Iter 5 subLoss 5582.7 multi 1.00 import weight 0.00
Epoch 105 Iter 6 subLoss 5393.8 multi 1.00 import weight 0.00
Epoch 105 Iter 7 subLoss 4805.9 multi 3.99 import weight 0.00
Epoch 105 Iter 8 subLoss 4431.7 multi 3.99 import weight 0.00
Epoch 105 Iter 9 subLoss 4188.2 multi -1.98 import weight 0.00
Epoch 105 Iter 10 subLoss 4384.2 multi -1.99 import weight 0.00
Epoch 105 Iter 11 subLoss 4887.2 multi -1.99 import weight 0.00
Epoch 105 Acc: 97.39 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 488 train Loss: 4976.4 test Loss: 478.3
Epoch 106 Iter 0 subLoss 4609.5 multi -4.97 import weight 0.00
Epoch 106 Iter 1 subLoss 5740.9 multi -1.98 import weight 0.00
Epoch 106 Iter 2 subLoss 6850.3 multi -1.99 import weight 0.00
Epoch 106 Iter 3 subLoss 8130.3 multi 3.99 import weight 0.00
Epoch 106 Iter 4 subLoss 5472.5 multi -7.96 import weight 0.00
Epoch 106 Iter 5 subLoss 7686.6 multi 1.00 import weight 0.00
Epoch 106 Iter 6 subLoss 6440.2 multi 1.00 import weight 0.00
Epoch 106 Iter 7 subLoss 6209.2 multi 1.00 import weight 0.00
Epoch 106 Iter 8 subLoss 5688.5 multi -1.99 import weight 0.00
Epoch 106 Iter 9 subLoss 6209.8 multi 3.98 import weight 0.00
Epoch 106 Iter 10 subLoss 4719.3 multi -1.98 import weight 0.00
Epoch 106 Iter 11 subLoss 5278.8 multi 15.93 import weight 1.00
Epoch 106 Acc: 97.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 15.93 Pidx 527 train Loss: 4748.7 test Loss: 398.5
Epoch 107 Iter 0 subLoss 4890.9 multi 3.98 import weight 0.00
Epoch 107 Iter 1 subLoss 4518.8 multi 3.98 import weight 0.00
Epoch 107 Iter 2 subLoss 3790.1 multi 3.98 import weight 0.00
Epoch 107 Iter 3 subLoss 4026.2 multi 1.00 import weight 0.00
Epoch 107 Iter 4 subLoss 3926.7 multi 1.00 import weight 0.00
Epoch 107 Iter 5 subLoss 4309.4 multi 3.99 import weight 0.00
Epoch 107 Iter 6 subLoss 4146.8 multi 1.00 import weight 0.00
Epoch 107 Iter 7 subLoss 4881.7 multi 1.00 import weight 0.00
Epoch 107 Iter 8 subLoss 4041.8 multi 3.98 import weight 0.00
Epoch 107 Iter 9 subLoss 3826.8 multi -1.99 import weight 0.00
Epoch 107 Iter 10 subLoss 4172.6 multi 3.98 import weight 0.00
Epoch 107 Iter 11 subLoss 3696.5 multi 6.97 import weight 0.00
Epoch 107 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 369 train Loss: 3914.9 test Loss: 356.5
Epoch 108 Iter 0 subLoss 3757.8 multi 9.96 import weight 0.00
Epoch 108 Iter 1 subLoss 3667.1 multi 1.00 import weight 0.00
Epoch 108 Iter 2 subLoss 3414.3 multi -1.99 import weight 0.00
Epoch 108 Iter 3 subLoss 3585.2 multi -1.99 import weight 0.00
Epoch 108 Iter 4 subLoss 4370.7 multi 6.97 import weight 0.00
Epoch 108 Iter 5 subLoss 3568.8 multi -1.99 import weight 0.00
Epoch 108 Iter 6 subLoss 4524.1 multi -1.99 import weight 0.00
Epoch 108 Iter 7 subLoss 3937.5 multi -7.96 import weight 0.00
Epoch 108 Iter 8 subLoss 6190.1 multi -4.97 import weight 0.00
Epoch 108 Iter 9 subLoss 29920.1 multi 3.99 import weight 0.00
Epoch 108 Iter 10 subLoss 31256.2 multi 1.00 import weight 0.00
Epoch 108 Iter 11 subLoss 12894.8 multi -1.99 import weight 0.00
Epoch 108 Acc: 80.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1289 train Loss: 22420.0 test Loss: 3852.7
Epoch 109 Iter 0 subLoss 22154.0 multi 1.00 import weight 0.00
Epoch 109 Iter 1 subLoss 13445.5 multi 1.00 import weight 0.00
Epoch 109 Iter 2 subLoss 9982.3 multi 1.00 import weight 0.00
Epoch 109 Iter 3 subLoss 8953.9 multi 6.97 import weight 0.00
Epoch 109 Iter 4 subLoss 6902.2 multi 6.97 import weight 0.00
Epoch 109 Iter 5 subLoss 5482.8 multi -1.98 import weight 0.00
Epoch 109 Iter 6 subLoss 6124.9 multi 1.00 import weight 0.00
Epoch 109 Iter 7 subLoss 5696.4 multi 6.97 import weight 0.00
Epoch 109 Iter 8 subLoss 5112.3 multi 1.00 import weight 0.00
Epoch 109 Iter 9 subLoss 4981.9 multi -4.97 import weight 0.00
Epoch 109 Iter 10 subLoss 5441.7 multi 3.99 import weight 0.00
Epoch 109 Iter 11 subLoss 5054.4 multi -1.98 import weight 0.00
Epoch 109 Acc: 97.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 505 train Loss: 5219.5 test Loss: 555.2
Epoch 110 Iter 0 subLoss 4648.6 multi 3.99 import weight 0.00
Epoch 110 Iter 1 subLoss 4847.5 multi 1.00 import weight 0.00
Epoch 110 Iter 2 subLoss 5110.0 multi -4.97 import weight 0.00
Epoch 110 Iter 3 subLoss 5320.4 multi 1.00 import weight 0.00
Epoch 110 Iter 4 subLoss 5620.4 multi 6.97 import weight 0.00
Epoch 110 Iter 5 subLoss 5034.8 multi 6.97 import weight 0.00
Epoch 110 Iter 6 subLoss 4646.0 multi 6.97 import weight 0.00
Epoch 110 Iter 7 subLoss 3948.0 multi 1.00 import weight 0.00
Epoch 110 Iter 8 subLoss 4315.1 multi -13.93 import weight 0.00
Epoch 110 Iter 9 subLoss 4566.4 multi 6.97 import weight 0.00
Epoch 110 Iter 10 subLoss 4213.0 multi 3.99 import weight 0.00
Epoch 110 Iter 11 subLoss 3815.5 multi 1.00 import weight 0.00
Epoch 110 Acc: 97.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 381 train Loss: 4265.7 test Loss: 457.3
Epoch 111 Iter 0 subLoss 4249.5 multi -1.99 import weight 0.00
Epoch 111 Iter 1 subLoss 3935.3 multi -4.97 import weight 0.00
Epoch 111 Iter 2 subLoss 4393.5 multi -7.96 import weight 0.00
Epoch 111 Iter 3 subLoss 5784.6 multi 1.00 import weight 0.00
Epoch 111 Iter 4 subLoss 5121.7 multi 1.00 import weight 0.00
Epoch 111 Iter 5 subLoss 4961.0 multi -1.99 import weight 0.00
Epoch 111 Iter 6 subLoss 5239.1 multi -1.98 import weight 0.00
Epoch 111 Iter 7 subLoss 5061.2 multi 3.99 import weight 0.00
Epoch 111 Iter 8 subLoss 4388.9 multi -1.98 import weight 0.00
Epoch 111 Iter 9 subLoss 4595.9 multi -7.96 import weight 0.00
Epoch 111 Iter 10 subLoss 5077.8 multi -10.94 import weight 0.00
Epoch 111 Iter 11 subLoss 40762.6 multi 1.00 import weight 0.00
Epoch 111 Acc: 87.43 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4076 train Loss: 12431.1 test Loss: 1854.3
Epoch 112 Iter 0 subLoss 11472.6 multi 3.99 import weight 0.00
Epoch 112 Iter 1 subLoss 5672.2 multi -4.97 import weight 0.00
Epoch 112 Iter 2 subLoss 7983.8 multi 1.00 import weight 0.00
Epoch 112 Iter 3 subLoss 6602.2 multi -4.97 import weight 0.00
Epoch 112 Iter 4 subLoss 11394.3 multi 1.00 import weight 0.00
Epoch 112 Iter 5 subLoss 8861.8 multi -1.99 import weight 0.00
Epoch 112 Iter 6 subLoss 11029.8 multi 1.00 import weight 0.00
Epoch 112 Iter 7 subLoss 9589.8 multi 6.97 import weight 0.00
Epoch 112 Iter 8 subLoss 5810.7 multi 6.97 import weight 0.00
Epoch 112 Iter 9 subLoss 5231.7 multi 1.00 import weight 0.00
Epoch 112 Iter 10 subLoss 4929.5 multi 3.98 import weight 0.00
Epoch 112 Iter 11 subLoss 5422.8 multi 1.00 import weight 0.00
Epoch 112 Acc: 97.10 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 542 train Loss: 4860.7 test Loss: 525.6
Epoch 113 Iter 0 subLoss 4609.5 multi -4.97 import weight 0.00
Epoch 113 Iter 1 subLoss 4986.2 multi -1.99 import weight 0.00
Epoch 113 Iter 2 subLoss 5087.2 multi -1.99 import weight 0.00
Epoch 113 Iter 3 subLoss 5189.8 multi -1.99 import weight 0.00
Epoch 113 Iter 4 subLoss 5327.1 multi 3.99 import weight 0.00
Epoch 113 Iter 5 subLoss 5219.6 multi -1.99 import weight 0.00
Epoch 113 Iter 6 subLoss 4874.2 multi 1.00 import weight 0.00
Epoch 113 Iter 7 subLoss 4719.2 multi 1.00 import weight 0.00
Epoch 113 Iter 8 subLoss 5005.6 multi 1.00 import weight 0.00
Epoch 113 Iter 9 subLoss 5131.6 multi 3.99 import weight 0.00
Epoch 113 Iter 10 subLoss 4191.5 multi 1.00 import weight 0.00
Epoch 113 Iter 11 subLoss 4677.0 multi -1.99 import weight 0.00
Epoch 113 Acc: 97.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 467 train Loss: 5065.0 test Loss: 531.2
Epoch 114 Iter 0 subLoss 4903.1 multi -4.97 import weight 0.00
Epoch 114 Iter 1 subLoss 4569.5 multi 9.96 import weight 0.00
Epoch 114 Iter 2 subLoss 4925.9 multi 6.97 import weight 0.00
Epoch 114 Iter 3 subLoss 3868.2 multi -1.98 import weight 0.00
Epoch 114 Iter 4 subLoss 4840.6 multi 3.99 import weight 0.00
Epoch 114 Iter 5 subLoss 4302.0 multi 6.97 import weight 0.00
Epoch 114 Iter 6 subLoss 4090.1 multi -1.98 import weight 0.00
Epoch 114 Iter 7 subLoss 4242.8 multi 1.00 import weight 0.00
Epoch 114 Iter 8 subLoss 4583.3 multi 9.96 import weight 1.00
Epoch 114 Iter 9 subLoss 4258.4 multi -4.97 import weight 0.00
Epoch 114 Iter 10 subLoss 4464.6 multi -1.99 import weight 0.00
Epoch 114 Iter 11 subLoss 5064.5 multi 6.97 import weight 0.00
Epoch 114 Acc: 97.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 506 train Loss: 4303.0 test Loss: 464.5
Epoch 115 Iter 0 subLoss 4048.2 multi 6.97 import weight 0.00
Epoch 115 Iter 1 subLoss 3288.8 multi 1.00 import weight 0.00
Epoch 115 Iter 2 subLoss 4391.6 multi -7.96 import weight 0.00
Epoch 115 Iter 3 subLoss 4296.9 multi 6.97 import weight 0.00
Epoch 115 Iter 4 subLoss 3721.5 multi -4.97 import weight 0.00
Epoch 115 Iter 5 subLoss 4840.7 multi 6.97 import weight 0.00
Epoch 115 Iter 6 subLoss 3998.4 multi 3.99 import weight 0.00
Epoch 115 Iter 7 subLoss 3739.7 multi -1.99 import weight 0.00
Epoch 115 Iter 8 subLoss 4209.0 multi -7.96 import weight 0.00
Epoch 115 Iter 9 subLoss 4810.7 multi 9.96 import weight 0.00
Epoch 115 Iter 10 subLoss 5704.0 multi -10.94 import weight 0.00
Epoch 115 Iter 11 subLoss 29449.0 multi 1.00 import weight 0.00
Epoch 115 Acc: 92.20 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2944 train Loss: 9273.5 test Loss: 1224.6
Epoch 116 Iter 0 subLoss 9870.0 multi 6.97 import weight 0.00
Epoch 116 Iter 1 subLoss 5513.2 multi -7.96 import weight 0.00
Epoch 116 Iter 2 subLoss 12668.4 multi 1.00 import weight 0.00
Epoch 116 Iter 3 subLoss 9724.0 multi -4.97 import weight 0.00
Epoch 116 Iter 4 subLoss 25207.5 multi 1.00 import weight 0.00
Epoch 116 Iter 5 subLoss 16295.5 multi 1.00 import weight 0.00
Epoch 116 Iter 6 subLoss 11668.1 multi -1.99 import weight 0.00
Epoch 116 Iter 7 subLoss 19283.4 multi 1.00 import weight 0.00
Epoch 116 Iter 8 subLoss 14120.8 multi 1.00 import weight 0.00
Epoch 116 Iter 9 subLoss 10500.3 multi 1.00 import weight 0.00
Epoch 116 Iter 10 subLoss 9295.1 multi 1.00 import weight 0.00
Epoch 116 Iter 11 subLoss 8249.5 multi 6.97 import weight 0.00
Epoch 116 Acc: 97.61 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 824 train Loss: 4413.1 test Loss: 477.3
Epoch 117 Iter 0 subLoss 4655.3 multi -7.96 import weight 0.00
Epoch 117 Iter 1 subLoss 4396.0 multi -4.97 import weight 0.00
Epoch 117 Iter 2 subLoss 6175.8 multi -4.97 import weight 0.00
Epoch 117 Iter 3 subLoss 15517.4 multi 1.00 import weight 0.00
Epoch 117 Iter 4 subLoss 8208.8 multi -1.99 import weight 0.00
Epoch 117 Iter 5 subLoss 9926.1 multi 1.00 import weight 0.00
Epoch 117 Iter 6 subLoss 7975.6 multi -4.97 import weight 0.00
Epoch 117 Iter 7 subLoss 19196.8 multi 1.00 import weight 0.00
Epoch 117 Iter 8 subLoss 11361.4 multi 1.00 import weight 0.00
Epoch 117 Iter 9 subLoss 9133.7 multi -7.96 import weight 0.00
Epoch 117 Iter 10 subLoss 28579.7 multi 1.00 import weight 0.00
Epoch 117 Iter 11 subLoss 15140.7 multi 3.98 import weight 0.00
Epoch 117 Acc: 92.04 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 1514 train Loss: 10386.2 test Loss: 1463.2
Epoch 118 Iter 0 subLoss 10295.1 multi 1.00 import weight 0.00
Epoch 118 Iter 1 subLoss 9264.0 multi 1.00 import weight 0.00
Epoch 118 Iter 2 subLoss 8148.4 multi -1.98 import weight 0.00
Epoch 118 Iter 3 subLoss 9779.6 multi 3.99 import weight 0.00
Epoch 118 Iter 4 subLoss 7220.7 multi 1.00 import weight 0.00
Epoch 118 Iter 5 subLoss 7023.3 multi 9.96 import weight 0.00
Epoch 118 Iter 6 subLoss 5281.9 multi -16.91 import weight 0.00
Epoch 118 Iter 7 subLoss 16487.5 multi 1.00 import weight 0.00
Epoch 118 Iter 8 subLoss 8015.3 multi 1.00 import weight 0.00
Epoch 118 Iter 9 subLoss 7013.1 multi -1.99 import weight 0.00
Epoch 118 Iter 10 subLoss 8537.9 multi 1.00 import weight 0.00
Epoch 118 Iter 11 subLoss 7181.1 multi 6.97 import weight 0.00
Epoch 118 Acc: 95.95 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 718 train Loss: 6116.4 test Loss: 729.0
Epoch 119 Iter 0 subLoss 6016.2 multi 1.00 import weight 0.00
Epoch 119 Iter 1 subLoss 5699.3 multi 9.96 import weight 0.00
Epoch 119 Iter 2 subLoss 5960.1 multi -7.96 import weight 0.00
Epoch 119 Iter 3 subLoss 17953.6 multi 1.00 import weight 0.00
Epoch 119 Iter 4 subLoss 11685.5 multi 1.00 import weight 0.00
Epoch 119 Iter 5 subLoss 6509.6 multi 1.00 import weight 0.00
Epoch 119 Iter 6 subLoss 5792.6 multi -4.97 import weight 0.00
Epoch 119 Iter 7 subLoss 11659.7 multi 3.99 import weight 0.00
Epoch 119 Iter 8 subLoss 5514.5 multi -4.97 import weight 0.00
Epoch 119 Iter 9 subLoss 7433.5 multi -1.99 import weight 0.00
Epoch 119 Iter 10 subLoss 14107.5 multi 1.00 import weight 0.00
Epoch 119 Iter 11 subLoss 6810.3 multi 6.97 import weight 0.00
Epoch 119 Acc: 97.20 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 681 train Loss: 5668.8 test Loss: 549.8
Epoch 120 Iter 0 subLoss 5752.7 multi -1.98 import weight 0.00
Epoch 120 Iter 1 subLoss 6832.0 multi -4.97 import weight 0.00
Epoch 120 Iter 2 subLoss 23634.6 multi 1.00 import weight 0.00
Epoch 120 Iter 3 subLoss 7168.3 multi -1.99 import weight 0.00
Epoch 120 Iter 4 subLoss 9638.3 multi 1.00 import weight 0.00
Epoch 120 Iter 5 subLoss 7548.3 multi 1.00 import weight 0.00
Epoch 120 Iter 6 subLoss 6142.1 multi 1.00 import weight 0.00
Epoch 120 Iter 7 subLoss 5826.0 multi -7.96 import weight 0.00
Epoch 120 Iter 8 subLoss 10962.9 multi 1.00 import weight 0.00
Epoch 120 Iter 9 subLoss 8656.9 multi 3.98 import weight 0.00
Epoch 120 Iter 10 subLoss 5725.1 multi 3.98 import weight 0.00
Epoch 120 Iter 11 subLoss 4717.7 multi 3.99 import weight 0.00
Epoch 120 Acc: 97.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 471 train Loss: 4964.8 test Loss: 509.3
Epoch 121 Iter 0 subLoss 5191.0 multi -4.97 import weight 0.00
Epoch 121 Iter 1 subLoss 4888.6 multi 1.00 import weight 0.00
Epoch 121 Iter 2 subLoss 4867.7 multi 1.00 import weight 0.00
Epoch 121 Iter 3 subLoss 5091.7 multi 3.98 import weight 0.00
Epoch 121 Iter 4 subLoss 5334.9 multi -7.96 import weight 0.00
Epoch 121 Iter 5 subLoss 5050.6 multi 1.00 import weight 0.00
Epoch 121 Iter 6 subLoss 5261.0 multi 1.00 import weight 0.00
Epoch 121 Iter 7 subLoss 5061.2 multi 6.97 import weight 0.00
Epoch 121 Iter 8 subLoss 5016.2 multi -1.98 import weight 0.00
Epoch 121 Iter 9 subLoss 5057.6 multi 3.99 import weight 0.00
Epoch 121 Iter 10 subLoss 4204.5 multi -4.97 import weight 0.00
Epoch 121 Iter 11 subLoss 4473.3 multi -4.97 import weight 0.00
Epoch 121 Acc: 97.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 447 train Loss: 5125.1 test Loss: 533.3
Epoch 122 Iter 0 subLoss 5599.7 multi -1.99 import weight 0.00
Epoch 122 Iter 1 subLoss 5080.1 multi 1.00 import weight 0.00
Epoch 122 Iter 2 subLoss 5116.5 multi 1.00 import weight 0.00
Epoch 122 Iter 3 subLoss 5099.7 multi 3.99 import weight 0.00
Epoch 122 Iter 4 subLoss 4277.4 multi 6.97 import weight 0.00
Epoch 122 Iter 5 subLoss 4259.7 multi -1.99 import weight 0.00
Epoch 122 Iter 6 subLoss 4642.2 multi 9.96 import weight 0.00
Epoch 122 Iter 7 subLoss 4358.6 multi 1.00 import weight 0.00
Epoch 122 Iter 8 subLoss 4478.9 multi -1.99 import weight 0.00
Epoch 122 Iter 9 subLoss 4864.8 multi 3.98 import weight 0.00
Epoch 122 Iter 10 subLoss 4174.8 multi 6.97 import weight 0.00
Epoch 122 Iter 11 subLoss 4376.2 multi 9.96 import weight 0.00
Epoch 122 Acc: 97.49 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 437 train Loss: 4020.5 test Loss: 420.1
Epoch 123 Iter 0 subLoss 4099.0 multi 1.00 import weight 0.00
Epoch 123 Iter 1 subLoss 3865.3 multi 1.00 import weight 0.00
Epoch 123 Iter 2 subLoss 4155.8 multi -1.99 import weight 0.00
Epoch 123 Iter 3 subLoss 4178.6 multi 9.96 import weight 0.00
Epoch 123 Iter 4 subLoss 3760.7 multi -10.94 import weight 0.00
Epoch 123 Iter 5 subLoss 4466.3 multi 1.00 import weight 0.00
Epoch 123 Iter 6 subLoss 4219.3 multi 1.00 import weight 0.00
Epoch 123 Iter 7 subLoss 3999.2 multi 6.97 import weight 0.00
Epoch 123 Iter 8 subLoss 3961.5 multi 3.99 import weight 0.00
Epoch 123 Iter 9 subLoss 3774.1 multi -4.97 import weight 0.00
Epoch 123 Iter 10 subLoss 4067.1 multi 6.97 import weight 0.00
Epoch 123 Iter 11 subLoss 3689.7 multi -1.99 import weight 0.00
Epoch 123 Acc: 97.35 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 368 train Loss: 4108.0 test Loss: 442.1
Epoch 124 Iter 0 subLoss 3768.6 multi -7.96 import weight 0.00
Epoch 124 Iter 1 subLoss 5003.5 multi 3.98 import weight 0.00
Epoch 124 Iter 2 subLoss 4245.8 multi 3.99 import weight 0.00
Epoch 124 Iter 3 subLoss 3974.3 multi -10.94 import weight 0.00
Epoch 124 Iter 4 subLoss 4536.4 multi -4.97 import weight 0.00
Epoch 124 Iter 5 subLoss 4644.4 multi 12.94 import weight 0.00
Epoch 124 Iter 6 subLoss 4419.7 multi 6.97 import weight 0.00
Epoch 124 Iter 7 subLoss 3684.9 multi 1.00 import weight 0.00
Epoch 124 Iter 8 subLoss 3386.4 multi 3.99 import weight 0.00
Epoch 124 Iter 9 subLoss 3791.3 multi 6.97 import weight 0.00
Epoch 124 Iter 10 subLoss 3501.8 multi 3.99 import weight 0.00
Epoch 124 Iter 11 subLoss 3566.4 multi 1.00 import weight 0.00
Epoch 124 Acc: 97.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 356 train Loss: 3755.7 test Loss: 349.4
Epoch 125 Iter 0 subLoss 3760.9 multi -4.97 import weight 0.00
Epoch 125 Iter 1 subLoss 4063.7 multi 9.96 import weight 0.00
Epoch 125 Iter 2 subLoss 3765.8 multi -1.98 import weight 0.00
Epoch 125 Iter 3 subLoss 4582.8 multi 12.94 import weight 1.00
Epoch 125 Iter 4 subLoss 3632.2 multi 1.00 import weight 0.00
Epoch 125 Iter 5 subLoss 4235.7 multi 6.97 import weight 0.00
Epoch 125 Iter 6 subLoss 3597.8 multi -1.99 import weight 0.00
Epoch 125 Iter 7 subLoss 3483.6 multi 1.00 import weight 0.00
Epoch 125 Iter 8 subLoss 3422.2 multi -1.99 import weight 0.00
Epoch 125 Iter 9 subLoss 3769.1 multi 1.00 import weight 0.00
Epoch 125 Iter 10 subLoss 3754.9 multi 12.94 import weight 0.00
Epoch 125 Iter 11 subLoss 3296.0 multi -1.99 import weight 0.00
Epoch 125 Acc: 97.22 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 329 train Loss: 4090.5 test Loss: 455.4
Epoch 126 Iter 0 subLoss 4224.1 multi -4.97 import weight 0.00
Epoch 126 Iter 1 subLoss 6826.4 multi -1.99 import weight 0.00
Epoch 126 Iter 2 subLoss 19495.0 multi 1.00 import weight 0.00
Epoch 126 Iter 3 subLoss 4649.8 multi 15.93 import weight 0.00
Epoch 126 Iter 4 subLoss 16174.6 multi 1.00 import weight 0.00
Epoch 126 Iter 5 subLoss 5783.4 multi 3.98 import weight 0.00
Epoch 126 Iter 6 subLoss 3397.1 multi -4.97 import weight 0.00
Epoch 126 Iter 7 subLoss 3797.7 multi 9.96 import weight 0.00
Epoch 126 Iter 8 subLoss 3786.7 multi 1.00 import weight 0.00
Epoch 126 Iter 9 subLoss 3733.3 multi 1.00 import weight 0.00
Epoch 126 Iter 10 subLoss 3364.9 multi 1.00 import weight 0.00
Epoch 126 Iter 11 subLoss 3090.3 multi 1.00 import weight 0.00
Epoch 126 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 309 train Loss: 3531.8 test Loss: 341.4
Epoch 127 Iter 0 subLoss 3420.7 multi 1.00 import weight 0.00
Epoch 127 Iter 1 subLoss 3455.1 multi 3.99 import weight 0.00
Epoch 127 Iter 2 subLoss 2767.5 multi 1.00 import weight 0.00
Epoch 127 Iter 3 subLoss 3094.4 multi 3.99 import weight 0.00
Epoch 127 Iter 4 subLoss 3310.6 multi 1.00 import weight 0.00
Epoch 127 Iter 5 subLoss 3240.9 multi -1.99 import weight 0.00
Epoch 127 Iter 6 subLoss 3512.9 multi -1.98 import weight 0.00
Epoch 127 Iter 7 subLoss 3306.2 multi -1.99 import weight 0.00
Epoch 127 Iter 8 subLoss 3561.7 multi 3.98 import weight 0.00
Epoch 127 Iter 9 subLoss 3109.2 multi -4.97 import weight 0.00
Epoch 127 Iter 10 subLoss 3643.7 multi 3.99 import weight 0.00
Epoch 127 Iter 11 subLoss 3701.6 multi -4.97 import weight 0.00
Epoch 127 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 370 train Loss: 3401.5 test Loss: 327.3
Epoch 128 Iter 0 subLoss 3237.3 multi 3.99 import weight 0.00
Epoch 128 Iter 1 subLoss 3626.0 multi -1.98 import weight 0.00
Epoch 128 Iter 2 subLoss 3366.8 multi 3.99 import weight 0.00
Epoch 128 Iter 3 subLoss 2864.0 multi 1.00 import weight 0.00
Epoch 128 Iter 4 subLoss 3261.6 multi 1.00 import weight 0.00
Epoch 128 Iter 5 subLoss 3023.4 multi 1.00 import weight 0.00
Epoch 128 Iter 6 subLoss 3154.5 multi 1.00 import weight 0.00
Epoch 128 Iter 7 subLoss 3804.5 multi -10.94 import weight 0.00
Epoch 128 Iter 8 subLoss 3361.2 multi 6.97 import weight 0.00
Epoch 128 Iter 9 subLoss 3586.2 multi 1.00 import weight 0.00
Epoch 128 Iter 10 subLoss 3406.1 multi 1.00 import weight 0.00
Epoch 128 Iter 11 subLoss 2959.7 multi 1.00 import weight 0.00
Epoch 128 Acc: 98.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 295 train Loss: 3370.2 test Loss: 331.8
Epoch 129 Iter 0 subLoss 2914.5 multi 1.00 import weight 0.00
Epoch 129 Iter 1 subLoss 3693.2 multi 3.99 import weight 0.00
Epoch 129 Iter 2 subLoss 2817.7 multi 1.00 import weight 0.00
Epoch 129 Iter 3 subLoss 3571.8 multi -4.97 import weight 0.00
Epoch 129 Iter 4 subLoss 3368.0 multi 9.96 import weight 0.00
Epoch 129 Iter 5 subLoss 3905.3 multi -1.98 import weight 0.00
Epoch 129 Iter 6 subLoss 4155.3 multi 1.00 import weight 0.00
Epoch 129 Iter 7 subLoss 3911.1 multi 1.00 import weight 0.00
Epoch 129 Iter 8 subLoss 4042.3 multi 9.96 import weight 0.00
Epoch 129 Iter 9 subLoss 3913.1 multi 3.99 import weight 0.00
Epoch 129 Iter 10 subLoss 3078.7 multi 1.00 import weight 0.00
Epoch 129 Iter 11 subLoss 3245.9 multi -1.98 import weight 0.00
Epoch 129 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 324 train Loss: 3332.1 test Loss: 341.6
Epoch 130 Iter 0 subLoss 2796.5 multi 1.00 import weight 0.00
Epoch 130 Iter 1 subLoss 2920.1 multi -1.99 import weight 0.00
Epoch 130 Iter 2 subLoss 3042.8 multi 1.00 import weight 0.00
Epoch 130 Iter 3 subLoss 2975.4 multi 1.00 import weight 0.00
Epoch 130 Iter 4 subLoss 2908.7 multi 1.00 import weight 0.00
Epoch 130 Iter 5 subLoss 2933.4 multi -1.99 import weight 0.00
Epoch 130 Iter 6 subLoss 3401.2 multi 3.98 import weight 0.00
Epoch 130 Iter 7 subLoss 3187.1 multi -1.99 import weight 0.00
Epoch 130 Iter 8 subLoss 3369.1 multi 12.94 import weight 0.00
Epoch 130 Iter 9 subLoss 3423.8 multi 3.98 import weight 0.00
Epoch 130 Iter 10 subLoss 3400.0 multi 6.97 import weight 0.00
Epoch 130 Iter 11 subLoss 3420.8 multi 6.97 import weight 0.00
Epoch 130 Acc: 98.29 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 342 train Loss: 3160.3 test Loss: 308.9
Epoch 131 Iter 0 subLoss 3256.4 multi -4.97 import weight 0.00
Epoch 131 Iter 1 subLoss 3278.4 multi -1.99 import weight 0.00
Epoch 131 Iter 2 subLoss 3199.3 multi -1.99 import weight 0.00
Epoch 131 Iter 3 subLoss 4108.1 multi -7.96 import weight 0.00
Epoch 131 Iter 4 subLoss 33951.3 multi 1.00 import weight 0.00
Epoch 131 Iter 5 subLoss 4011.2 multi 1.00 import weight 0.00
Epoch 131 Iter 6 subLoss 4106.3 multi -4.97 import weight 0.00
Epoch 131 Iter 7 subLoss 5342.6 multi -4.97 import weight 0.00
Epoch 131 Iter 8 subLoss 18079.8 multi 3.99 import weight 0.00
Epoch 131 Iter 9 subLoss 12796.9 multi 1.00 import weight 0.00
Epoch 131 Iter 10 subLoss 6777.0 multi 1.00 import weight 0.00
Epoch 131 Iter 11 subLoss 5260.4 multi 3.98 import weight 0.00
Epoch 131 Acc: 98.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 526 train Loss: 3692.4 test Loss: 342.1
Epoch 132 Iter 0 subLoss 4121.2 multi 3.99 import weight 0.00
Epoch 132 Iter 1 subLoss 3210.9 multi 1.00 import weight 0.00
Epoch 132 Iter 2 subLoss 3734.1 multi 3.98 import weight 0.00
Epoch 132 Iter 3 subLoss 3069.3 multi 1.00 import weight 0.00
Epoch 132 Iter 4 subLoss 2846.2 multi 1.00 import weight 0.00
Epoch 132 Iter 5 subLoss 3379.3 multi -13.93 import weight 0.00
Epoch 132 Iter 6 subLoss 3398.6 multi -1.98 import weight 0.00
Epoch 132 Iter 7 subLoss 3577.9 multi -1.99 import weight 0.00
Epoch 132 Iter 8 subLoss 4322.9 multi -1.99 import weight 0.00
Epoch 132 Iter 9 subLoss 4824.2 multi -10.94 import weight 0.00
Epoch 132 Iter 10 subLoss 7659.2 multi 3.99 import weight 0.00
Epoch 132 Iter 11 subLoss 4711.9 multi 6.97 import weight 0.00
Epoch 132 Acc: 98.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 471 train Loss: 3732.5 test Loss: 342.3
Epoch 133 Iter 0 subLoss 3877.9 multi -4.97 import weight 0.00
Epoch 133 Iter 1 subLoss 3676.4 multi 1.00 import weight 0.00
Epoch 133 Iter 2 subLoss 3850.9 multi 3.98 import weight 0.00
Epoch 133 Iter 3 subLoss 3702.3 multi -4.97 import weight 0.00
Epoch 133 Iter 4 subLoss 3991.8 multi 9.96 import weight 0.00
Epoch 133 Iter 5 subLoss 4089.1 multi 6.97 import weight 0.00
Epoch 133 Iter 6 subLoss 3731.3 multi 6.97 import weight 0.00
Epoch 133 Iter 7 subLoss 3577.9 multi 1.00 import weight 0.00
Epoch 133 Iter 8 subLoss 3080.5 multi -1.99 import weight 0.00
Epoch 133 Iter 9 subLoss 3420.1 multi 9.96 import weight 0.00
Epoch 133 Iter 10 subLoss 3295.1 multi 1.00 import weight 0.00
Epoch 133 Iter 11 subLoss 3321.4 multi -1.99 import weight 0.00
Epoch 133 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 332 train Loss: 3377.0 test Loss: 330.2
Epoch 134 Iter 0 subLoss 3236.5 multi 6.97 import weight 0.00
Epoch 134 Iter 1 subLoss 2952.7 multi 3.99 import weight 0.00
Epoch 134 Iter 2 subLoss 3474.7 multi 1.00 import weight 0.00
Epoch 134 Iter 3 subLoss 2725.8 multi 1.00 import weight 0.00
Epoch 134 Iter 4 subLoss 3500.6 multi 6.97 import weight 0.00
Epoch 134 Iter 5 subLoss 3212.9 multi 3.99 import weight 0.00
Epoch 134 Iter 6 subLoss 3331.6 multi -1.99 import weight 0.00
Epoch 134 Iter 7 subLoss 2690.7 multi 1.00 import weight 0.00
Epoch 134 Iter 8 subLoss 3003.0 multi 3.99 import weight 0.00
Epoch 134 Iter 9 subLoss 3216.9 multi 6.97 import weight 0.00
Epoch 134 Iter 10 subLoss 2792.0 multi 3.99 import weight 0.00
Epoch 134 Iter 11 subLoss 2965.7 multi -4.97 import weight 0.00
Epoch 134 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 296 train Loss: 3149.6 test Loss: 333.0
Epoch 135 Iter 0 subLoss 3116.1 multi -1.99 import weight 0.00
Epoch 135 Iter 1 subLoss 3732.0 multi 9.96 import weight 0.00
Epoch 135 Iter 2 subLoss 3357.7 multi 1.00 import weight 0.00
Epoch 135 Iter 3 subLoss 2729.3 multi 3.99 import weight 0.00
Epoch 135 Iter 4 subLoss 3119.8 multi 1.00 import weight 0.00
Epoch 135 Iter 5 subLoss 2808.5 multi -4.97 import weight 0.00
Epoch 135 Iter 6 subLoss 2749.7 multi 1.00 import weight 0.00
Epoch 135 Iter 7 subLoss 3203.3 multi -1.99 import weight 0.00
Epoch 135 Iter 8 subLoss 2952.3 multi 6.97 import weight 0.00
Epoch 135 Iter 9 subLoss 3142.3 multi 1.00 import weight 0.00
Epoch 135 Iter 10 subLoss 2557.4 multi 1.00 import weight 0.00
Epoch 135 Iter 11 subLoss 3485.9 multi 1.00 import weight 0.00
Epoch 135 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 348 train Loss: 3021.5 test Loss: 306.9
Epoch 136 Iter 0 subLoss 3116.4 multi 3.98 import weight 0.00
Epoch 136 Iter 1 subLoss 3557.1 multi 1.00 import weight 0.00
Epoch 136 Iter 2 subLoss 2706.1 multi -1.99 import weight 0.00
Epoch 136 Iter 3 subLoss 2769.4 multi 3.99 import weight 0.00
Epoch 136 Iter 4 subLoss 2800.8 multi -1.98 import weight 0.00
Epoch 136 Iter 5 subLoss 3033.3 multi -1.99 import weight 0.00
Epoch 136 Iter 6 subLoss 2811.1 multi -1.98 import weight 0.00
Epoch 136 Iter 7 subLoss 2697.5 multi 3.99 import weight 0.00
Epoch 136 Iter 8 subLoss 3078.8 multi 1.00 import weight 0.00
Epoch 136 Iter 9 subLoss 2701.7 multi -1.98 import weight 0.00
Epoch 136 Iter 10 subLoss 2858.7 multi -1.99 import weight 0.00
Epoch 136 Iter 11 subLoss 3294.1 multi 3.98 import weight 0.00
Epoch 136 Acc: 98.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 329 train Loss: 2988.8 test Loss: 325.7
Epoch 137 Iter 0 subLoss 3248.7 multi -1.99 import weight 0.00
Epoch 137 Iter 1 subLoss 2798.7 multi 6.97 import weight 0.00
Epoch 137 Iter 2 subLoss 2850.8 multi 1.00 import weight 0.00
Epoch 137 Iter 3 subLoss 2819.7 multi 1.00 import weight 0.00
Epoch 137 Iter 4 subLoss 3288.5 multi 1.00 import weight 0.00
Epoch 137 Iter 5 subLoss 2628.5 multi 1.00 import weight 0.00
Epoch 137 Iter 6 subLoss 2837.9 multi 1.00 import weight 0.00
Epoch 137 Iter 7 subLoss 2939.9 multi 1.00 import weight 0.00
Epoch 137 Iter 8 subLoss 2843.8 multi 1.00 import weight 0.00
Epoch 137 Iter 9 subLoss 2632.8 multi -1.99 import weight 0.00
Epoch 137 Iter 10 subLoss 2899.0 multi 1.00 import weight 0.00
Epoch 137 Iter 11 subLoss 2875.7 multi -1.99 import weight 0.00
Epoch 137 Acc: 98.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 287 train Loss: 2949.8 test Loss: 318.7
Epoch 138 Iter 0 subLoss 2623.2 multi 3.99 import weight 0.00
Epoch 138 Iter 1 subLoss 2622.1 multi 6.97 import weight 0.00
Epoch 138 Iter 2 subLoss 2996.3 multi 1.00 import weight 0.00
Epoch 138 Iter 3 subLoss 2580.7 multi 1.00 import weight 0.00
Epoch 138 Iter 4 subLoss 3020.8 multi 3.99 import weight 0.00
Epoch 138 Iter 5 subLoss 2947.6 multi -4.97 import weight 0.00
Epoch 138 Iter 6 subLoss 3135.2 multi 1.00 import weight 0.00
Epoch 138 Iter 7 subLoss 2853.8 multi 1.00 import weight 0.00
Epoch 138 Iter 8 subLoss 2695.7 multi 6.97 import weight 0.00
Epoch 138 Iter 9 subLoss 3271.0 multi 1.00 import weight 0.00
Epoch 138 Iter 10 subLoss 2968.5 multi -4.97 import weight 0.00
Epoch 138 Iter 11 subLoss 3066.2 multi 3.99 import weight 0.00
Epoch 138 Acc: 98.21 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 306 train Loss: 2991.9 test Loss: 309.9
Epoch 139 Iter 0 subLoss 2243.0 multi 1.00 import weight 0.00
Epoch 139 Iter 1 subLoss 3295.9 multi 3.99 import weight 0.00
Epoch 139 Iter 2 subLoss 2605.7 multi 1.00 import weight 0.00
Epoch 139 Iter 3 subLoss 2744.4 multi 3.99 import weight 0.00
Epoch 139 Iter 4 subLoss 2535.9 multi 1.00 import weight 0.00
Epoch 139 Iter 5 subLoss 2982.2 multi -1.99 import weight 0.00
Epoch 139 Iter 6 subLoss 2675.8 multi 1.00 import weight 0.00
Epoch 139 Iter 7 subLoss 2756.6 multi -4.97 import weight 0.00
Epoch 139 Iter 8 subLoss 2564.2 multi -1.99 import weight 0.00
Epoch 139 Iter 9 subLoss 2876.6 multi 1.00 import weight 0.00
Epoch 139 Iter 10 subLoss 3122.0 multi -7.96 import weight 0.00
Epoch 139 Iter 11 subLoss 4635.1 multi -1.99 import weight 0.00
Epoch 139 Acc: 92.66 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 463 train Loss: 6801.6 test Loss: 1047.2
Epoch 140 Iter 0 subLoss 6011.3 multi 3.98 import weight 0.00
Epoch 140 Iter 1 subLoss 3522.4 multi -4.97 import weight 0.00
Epoch 140 Iter 2 subLoss 4361.8 multi 3.98 import weight 0.00
Epoch 140 Iter 3 subLoss 2969.5 multi -1.99 import weight 0.00
Epoch 140 Iter 4 subLoss 3218.0 multi 6.97 import weight 0.00
Epoch 140 Iter 5 subLoss 3253.6 multi -4.97 import weight 0.00
Epoch 140 Iter 6 subLoss 2701.2 multi -1.99 import weight 0.00
Epoch 140 Iter 7 subLoss 3737.4 multi 12.94 import weight 0.00
Epoch 140 Iter 8 subLoss 6585.8 multi -4.97 import weight 0.00
Epoch 140 Iter 9 subLoss 25479.4 multi 1.00 import weight 0.00
Epoch 140 Iter 10 subLoss 8264.7 multi 6.97 import weight 0.00
Epoch 140 Iter 11 subLoss 4354.0 multi 3.99 import weight 0.00
Epoch 140 Acc: 98.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 435 train Loss: 3193.0 test Loss: 330.2
Epoch 141 Iter 0 subLoss 3332.7 multi 1.00 import weight 0.00
Epoch 141 Iter 1 subLoss 2749.0 multi 6.97 import weight 0.00
Epoch 141 Iter 2 subLoss 2932.7 multi 3.98 import weight 0.00
Epoch 141 Iter 3 subLoss 2936.3 multi 6.97 import weight 0.00
Epoch 141 Iter 4 subLoss 2729.9 multi 6.97 import weight 0.00
Epoch 141 Iter 5 subLoss 2841.2 multi 3.98 import weight 0.00
Epoch 141 Iter 6 subLoss 2867.5 multi -4.97 import weight 0.00
Epoch 141 Iter 7 subLoss 2871.7 multi 1.00 import weight 0.00
Epoch 141 Iter 8 subLoss 3135.4 multi 1.00 import weight 0.00
Epoch 141 Iter 9 subLoss 2693.1 multi 9.96 import weight 0.00
Epoch 141 Iter 10 subLoss 2765.5 multi 3.98 import weight 0.00
Epoch 141 Iter 11 subLoss 3143.8 multi -1.98 import weight 0.00
Epoch 141 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 314 train Loss: 2792.2 test Loss: 302.7
Epoch 142 Iter 0 subLoss 2629.0 multi 9.96 import weight 0.00
Epoch 142 Iter 1 subLoss 2696.5 multi 12.94 import weight 0.00
Epoch 142 Iter 2 subLoss 2439.5 multi 1.00 import weight 0.00
Epoch 142 Iter 3 subLoss 2939.8 multi 9.96 import weight 0.00
Epoch 142 Iter 4 subLoss 3256.6 multi -1.99 import weight 0.00
Epoch 142 Iter 5 subLoss 2994.9 multi 1.00 import weight 0.00
Epoch 142 Iter 6 subLoss 2982.1 multi 1.00 import weight 0.00
Epoch 142 Iter 7 subLoss 2586.4 multi 3.99 import weight 0.00
Epoch 142 Iter 8 subLoss 3050.9 multi -1.99 import weight 0.00
Epoch 142 Iter 9 subLoss 2584.8 multi 6.97 import weight 0.00
Epoch 142 Iter 10 subLoss 2411.8 multi 1.00 import weight 0.00
Epoch 142 Iter 11 subLoss 2317.7 multi 1.00 import weight 0.00
Epoch 142 Acc: 98.35 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 231 train Loss: 2691.3 test Loss: 276.1
Epoch 143 Iter 0 subLoss 2655.0 multi 1.00 import weight 0.00
Epoch 143 Iter 1 subLoss 2643.4 multi -1.99 import weight 0.00
Epoch 143 Iter 2 subLoss 2415.6 multi 3.99 import weight 0.00
Epoch 143 Iter 3 subLoss 2812.1 multi 3.99 import weight 0.00
Epoch 143 Iter 4 subLoss 2888.9 multi -7.96 import weight 0.00
Epoch 143 Iter 5 subLoss 2495.5 multi 1.00 import weight 0.00
Epoch 143 Iter 6 subLoss 2586.9 multi 9.96 import weight 0.00
Epoch 143 Iter 7 subLoss 2795.7 multi 9.96 import weight 0.00
Epoch 143 Iter 8 subLoss 3509.3 multi 9.96 import weight 0.00
Epoch 143 Iter 9 subLoss 3482.5 multi 3.98 import weight 0.00
Epoch 143 Iter 10 subLoss 2967.1 multi 1.00 import weight 0.00
Epoch 143 Iter 11 subLoss 2446.8 multi -1.99 import weight 0.00
Epoch 143 Acc: 98.35 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 244 train Loss: 2564.5 test Loss: 268.7
Epoch 144 Iter 0 subLoss 2421.1 multi -4.97 import weight 0.00
Epoch 144 Iter 1 subLoss 2342.0 multi 1.00 import weight 0.00
Epoch 144 Iter 2 subLoss 2916.7 multi 1.00 import weight 0.00
Epoch 144 Iter 3 subLoss 2400.3 multi 1.00 import weight 0.00
Epoch 144 Iter 4 subLoss 2032.5 multi 1.00 import weight 0.00
Epoch 144 Iter 5 subLoss 2845.4 multi 6.97 import weight 0.00
Epoch 144 Iter 6 subLoss 2473.2 multi 1.00 import weight 0.00
Epoch 144 Iter 7 subLoss 3284.3 multi 1.00 import weight 0.00
Epoch 144 Iter 8 subLoss 2668.4 multi -1.99 import weight 0.00
Epoch 144 Iter 9 subLoss 2435.7 multi 1.00 import weight 0.00
Epoch 144 Iter 10 subLoss 2584.8 multi 12.94 import weight 0.00
Epoch 144 Iter 11 subLoss 2924.0 multi -1.98 import weight 0.00
Epoch 144 Acc: 98.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 292 train Loss: 2782.1 test Loss: 274.1
Epoch 145 Iter 0 subLoss 2306.3 multi 1.00 import weight 0.00
Epoch 145 Iter 1 subLoss 2763.1 multi 6.97 import weight 0.00
Epoch 145 Iter 2 subLoss 2918.5 multi 3.98 import weight 0.00
Epoch 145 Iter 3 subLoss 2875.8 multi 3.99 import weight 0.00
Epoch 145 Iter 4 subLoss 2371.3 multi 1.00 import weight 0.00
Epoch 145 Iter 5 subLoss 3123.4 multi -4.97 import weight 0.00
Epoch 145 Iter 6 subLoss 2657.3 multi 1.00 import weight 0.00
Epoch 145 Iter 7 subLoss 2301.2 multi 3.99 import weight 0.00
Epoch 145 Iter 8 subLoss 2308.2 multi 6.97 import weight 0.00
Epoch 145 Iter 9 subLoss 2678.1 multi 1.00 import weight 0.00
Epoch 145 Iter 10 subLoss 2254.9 multi -1.99 import weight 0.00
Epoch 145 Iter 11 subLoss 2334.4 multi 1.00 import weight 0.00
Epoch 145 Acc: 98.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 233 train Loss: 2532.5 test Loss: 256.9
Epoch 146 Iter 0 subLoss 2505.3 multi -1.99 import weight 0.00
Epoch 146 Iter 1 subLoss 2566.9 multi 1.00 import weight 0.00
Epoch 146 Iter 2 subLoss 2526.3 multi 1.00 import weight 0.00
Epoch 146 Iter 3 subLoss 2868.2 multi -1.99 import weight 0.00
Epoch 146 Iter 4 subLoss 2311.8 multi -4.97 import weight 0.00
Epoch 146 Iter 5 subLoss 2820.7 multi -10.94 import weight 0.00
Epoch 146 Iter 6 subLoss 5122.8 multi 1.00 import weight 0.00
Epoch 146 Iter 7 subLoss 3711.1 multi -1.99 import weight 0.00
Epoch 146 Iter 8 subLoss 5477.6 multi -4.97 import weight 0.00
Epoch 146 Iter 9 subLoss 19542.8 multi 1.00 import weight 0.00
Epoch 146 Iter 10 subLoss 7685.3 multi 3.99 import weight 0.00
Epoch 146 Iter 11 subLoss 2833.6 multi 1.00 import weight 0.00
Epoch 146 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 283 train Loss: 2848.4 test Loss: 324.3
Epoch 147 Iter 0 subLoss 2789.0 multi 1.00 import weight 0.00
Epoch 147 Iter 1 subLoss 3052.9 multi 1.00 import weight 0.00
Epoch 147 Iter 2 subLoss 2593.8 multi -13.93 import weight 0.00
Epoch 147 Iter 3 subLoss 5067.3 multi 6.97 import weight 0.00
Epoch 147 Iter 4 subLoss 2894.1 multi 1.00 import weight 0.00
Epoch 147 Iter 5 subLoss 2912.9 multi 6.97 import weight 0.00
Epoch 147 Iter 6 subLoss 2686.5 multi -4.97 import weight 0.00
Epoch 147 Iter 7 subLoss 2344.6 multi 1.00 import weight 0.00
Epoch 147 Iter 8 subLoss 2383.9 multi -1.99 import weight 0.00
Epoch 147 Iter 9 subLoss 2888.2 multi -7.96 import weight 0.00
Epoch 147 Iter 10 subLoss 4463.5 multi 3.98 import weight 0.00
Epoch 147 Iter 11 subLoss 2535.3 multi 1.00 import weight 0.00
Epoch 147 Acc: 98.52 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 253 train Loss: 2820.0 test Loss: 279.7
Epoch 148 Iter 0 subLoss 2915.8 multi 9.96 import weight 0.00
Epoch 148 Iter 1 subLoss 2719.9 multi -7.96 import weight 0.00
Epoch 148 Iter 2 subLoss 2829.5 multi -7.96 import weight 0.00
Epoch 148 Iter 3 subLoss 3782.5 multi 3.98 import weight 0.00
Epoch 148 Iter 4 subLoss 2417.0 multi 3.98 import weight 0.00
Epoch 148 Iter 5 subLoss 2798.3 multi 9.96 import weight 0.00
Epoch 148 Iter 6 subLoss 2432.5 multi 3.98 import weight 0.00
Epoch 148 Iter 7 subLoss 2250.9 multi 1.00 import weight 0.00
Epoch 148 Iter 8 subLoss 2768.7 multi 9.96 import weight 0.00
Epoch 148 Iter 9 subLoss 2323.5 multi -4.97 import weight 0.00
Epoch 148 Iter 10 subLoss 2410.6 multi 6.97 import weight 0.00
Epoch 148 Iter 11 subLoss 2920.9 multi -7.96 import weight 0.00
Epoch 148 Acc: 98.42 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 292 train Loss: 2905.5 test Loss: 279.2
Epoch 149 Iter 0 subLoss 2867.6 multi 1.00 import weight 0.00
Epoch 149 Iter 1 subLoss 2916.4 multi 12.94 import weight 0.00
Epoch 149 Iter 2 subLoss 2237.0 multi 1.00 import weight 0.00
Epoch 149 Iter 3 subLoss 2577.4 multi -4.97 import weight 0.00
Epoch 149 Iter 4 subLoss 3170.6 multi 3.99 import weight 0.00
Epoch 149 Iter 5 subLoss 2534.9 multi 3.98 import weight 0.00
Epoch 149 Iter 6 subLoss 2749.5 multi 9.96 import weight 0.00
Epoch 149 Iter 7 subLoss 2281.4 multi 1.00 import weight 0.00
Epoch 149 Iter 8 subLoss 2119.9 multi 1.00 import weight 0.00
Epoch 149 Iter 9 subLoss 2587.6 multi 12.94 import weight 0.00
Epoch 149 Iter 10 subLoss 2466.9 multi 1.00 import weight 0.00
Epoch 149 Iter 11 subLoss 2328.9 multi -1.98 import weight 0.00
Epoch 149 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 232 train Loss: 2791.0 test Loss: 312.7
Epoch 150 Iter 0 subLoss 2571.2 multi -1.98 import weight 0.00
Epoch 150 Iter 1 subLoss 2931.5 multi 6.97 import weight 0.00
Epoch 150 Iter 2 subLoss 2442.3 multi -4.97 import weight 0.00
Epoch 150 Iter 3 subLoss 2419.8 multi 9.96 import weight 0.00
Epoch 150 Iter 4 subLoss 3457.8 multi 6.97 import weight 0.00
Epoch 150 Iter 5 subLoss 2577.6 multi 1.00 import weight 0.00
Epoch 150 Iter 6 subLoss 2240.3 multi 1.00 import weight 0.00
Epoch 150 Iter 7 subLoss 2362.4 multi 1.00 import weight 0.00
Epoch 150 Iter 8 subLoss 2025.9 multi 1.00 import weight 0.00
Epoch 150 Iter 9 subLoss 2567.1 multi 3.98 import weight 0.00
Epoch 150 Iter 10 subLoss 2332.3 multi -1.98 import weight 0.00
Epoch 150 Iter 11 subLoss 2626.0 multi 12.94 import weight 0.00
Epoch 150 Acc: 98.60 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 262 train Loss: 2478.7 test Loss: 235.5
Epoch 151 Iter 0 subLoss 2528.1 multi 3.99 import weight 0.00
Epoch 151 Iter 1 subLoss 2225.7 multi 1.00 import weight 0.00
Epoch 151 Iter 2 subLoss 2358.4 multi -4.97 import weight 0.00
Epoch 151 Iter 3 subLoss 2543.6 multi -7.96 import weight 0.00
Epoch 151 Iter 4 subLoss 2880.9 multi -4.97 import weight 0.00
Epoch 151 Iter 5 subLoss 3290.5 multi 3.99 import weight 0.00
Epoch 151 Iter 6 subLoss 2110.4 multi 3.99 import weight 0.00
Epoch 151 Iter 7 subLoss 2129.7 multi -4.97 import weight 0.00
Epoch 151 Iter 8 subLoss 2479.2 multi 1.00 import weight 0.00
Epoch 151 Iter 9 subLoss 2237.0 multi 1.00 import weight 0.00
Epoch 151 Iter 10 subLoss 2770.5 multi -13.93 import weight 0.00
Epoch 151 Iter 11 subLoss 3417.6 multi -7.96 import weight 0.00
Epoch 151 Acc: 90.29 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 341 train Loss: 13503.5 test Loss: 1530.9
Epoch 152 Iter 0 subLoss 13900.3 multi -1.98 import weight 0.00
Epoch 152 Iter 1 subLoss 123941.4 multi 1.00 import weight 0.00
Epoch 152 Iter 2 subLoss 29303.6 multi 1.00 import weight 0.00
Epoch 152 Iter 3 subLoss 11550.4 multi 1.00 import weight 0.00
Epoch 152 Iter 4 subLoss 8692.5 multi -1.99 import weight 0.00
Epoch 152 Iter 5 subLoss 13114.4 multi 1.00 import weight 0.00
Epoch 152 Iter 6 subLoss 9609.4 multi 3.99 import weight 0.00
Epoch 152 Iter 7 subLoss 4617.6 multi 1.00 import weight 0.00
Epoch 152 Iter 8 subLoss 4667.2 multi 1.00 import weight 0.00
Epoch 152 Iter 9 subLoss 4005.9 multi -7.96 import weight 0.00
Epoch 152 Iter 10 subLoss 6995.4 multi 1.00 import weight 0.00
Epoch 152 Iter 11 subLoss 5833.8 multi 3.98 import weight 0.00
Epoch 152 Acc: 97.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 583 train Loss: 4187.2 test Loss: 441.3
Epoch 153 Iter 0 subLoss 4177.2 multi 12.94 import weight 0.00
Epoch 153 Iter 1 subLoss 3272.5 multi 3.98 import weight 0.00
Epoch 153 Iter 2 subLoss 2610.0 multi 1.00 import weight 0.00
Epoch 153 Iter 3 subLoss 3301.2 multi -10.94 import weight 0.00
Epoch 153 Iter 4 subLoss 3233.6 multi 9.96 import weight 0.00
Epoch 153 Iter 5 subLoss 3219.5 multi 9.96 import weight 0.00
Epoch 153 Iter 6 subLoss 2904.1 multi -1.98 import weight 0.00
Epoch 153 Iter 7 subLoss 3115.5 multi 6.97 import weight 0.00
Epoch 153 Iter 8 subLoss 2848.4 multi 6.97 import weight 0.00
Epoch 153 Iter 9 subLoss 2768.8 multi 12.94 import weight 0.00
Epoch 153 Iter 10 subLoss 2318.0 multi -1.99 import weight 0.00
Epoch 153 Iter 11 subLoss 2366.7 multi 1.00 import weight 0.00
Epoch 153 Acc: 98.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 236 train Loss: 2823.6 test Loss: 276.4
Epoch 154 Iter 0 subLoss 2355.3 multi -1.98 import weight 0.00
Epoch 154 Iter 1 subLoss 3106.4 multi -1.98 import weight 0.00
Epoch 154 Iter 2 subLoss 3635.7 multi 1.00 import weight 0.00
Epoch 154 Iter 3 subLoss 3345.9 multi -4.97 import weight 0.00
Epoch 154 Iter 4 subLoss 4725.4 multi -10.94 import weight 0.00
Epoch 154 Iter 5 subLoss 367980.4 multi 1.00 import weight 0.00
Epoch 154 Iter 6 subLoss 26320.3 multi 3.99 import weight 0.00
Epoch 154 Iter 7 subLoss 41469.1 multi -1.99 import weight 0.00
Epoch 154 Iter 8 subLoss 789677.2 multi 1.00 import weight 0.00
Epoch 154 Iter 9 subLoss 54398.8 multi 1.00 import weight 0.00
Epoch 154 Iter 10 subLoss 51773.9 multi 1.00 import weight 0.00
Epoch 154 Iter 11 subLoss 49385.4 multi 1.00 import weight 0.00
Epoch 154 Acc: 37.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4938 train Loss: 46889.7 test Loss: 7585.3
Epoch 155 Iter 0 subLoss 45519.5 multi 1.00 import weight 0.00
Epoch 155 Iter 1 subLoss 43921.8 multi 1.00 import weight 0.00
Epoch 155 Iter 2 subLoss 42568.7 multi 1.00 import weight 0.00
Epoch 155 Iter 3 subLoss 42537.6 multi 1.00 import weight 0.00
Epoch 155 Iter 4 subLoss 41049.9 multi 1.00 import weight 0.00
Epoch 155 Iter 5 subLoss 40018.2 multi -1.99 import weight 0.00
Epoch 155 Iter 6 subLoss 42267.2 multi 3.99 import weight 0.00
Epoch 155 Iter 7 subLoss 41174.6 multi 1.00 import weight 0.00
Epoch 155 Iter 8 subLoss 37484.6 multi 1.00 import weight 0.00
Epoch 155 Iter 9 subLoss 36098.3 multi -1.99 import weight 0.00
Epoch 155 Iter 10 subLoss 39448.8 multi 1.00 import weight 0.00
Epoch 155 Iter 11 subLoss 37253.1 multi 1.00 import weight 0.00
Epoch 155 Acc: 68.50 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3725 train Loss: 36693.7 test Loss: 5241.0
Epoch 156 Iter 0 subLoss 36225.4 multi 3.99 import weight 0.00
Epoch 156 Iter 1 subLoss 30792.6 multi 3.99 import weight 0.00
Epoch 156 Iter 2 subLoss 26873.5 multi 1.00 import weight 0.00
Epoch 156 Iter 3 subLoss 23936.4 multi 1.00 import weight 0.00
Epoch 156 Iter 4 subLoss 22273.5 multi 1.00 import weight 0.00
Epoch 156 Iter 5 subLoss 20768.9 multi 1.00 import weight 0.00
Epoch 156 Iter 6 subLoss 19346.5 multi 1.00 import weight 0.00
Epoch 156 Iter 7 subLoss 16973.7 multi 1.00 import weight 0.00
Epoch 156 Iter 8 subLoss 17321.9 multi 3.99 import weight 0.00
Epoch 156 Iter 9 subLoss 14601.0 multi 3.99 import weight 0.00
Epoch 156 Iter 10 subLoss 13968.0 multi 1.00 import weight 0.00
Epoch 156 Iter 11 subLoss 11402.8 multi -1.99 import weight 0.00
Epoch 156 Acc: 93.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1140 train Loss: 12722.4 test Loss: 1556.9
Epoch 157 Iter 0 subLoss 12166.9 multi 6.97 import weight 0.00
Epoch 157 Iter 1 subLoss 12462.1 multi 9.96 import weight 0.00
Epoch 157 Iter 2 subLoss 71924.2 multi 1.00 import weight 0.00
Epoch 157 Iter 3 subLoss 15131.9 multi 3.99 import weight 0.00
Epoch 157 Iter 4 subLoss 23855.0 multi 1.00 import weight 0.00
Epoch 157 Iter 5 subLoss 9303.7 multi 1.00 import weight 0.00
Epoch 157 Iter 6 subLoss 8225.8 multi 3.99 import weight 0.00
Epoch 157 Iter 7 subLoss 6284.9 multi 3.99 import weight 0.00
Epoch 157 Iter 8 subLoss 5863.7 multi 1.00 import weight 0.00
Epoch 157 Iter 9 subLoss 6219.8 multi -7.96 import weight 0.00
Epoch 157 Iter 10 subLoss 7638.2 multi 3.98 import weight 0.00
Epoch 157 Iter 11 subLoss 7229.4 multi 3.98 import weight 0.00
Epoch 157 Acc: 95.60 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 722 train Loss: 6376.5 test Loss: 781.8
Epoch 158 Iter 0 subLoss 6430.0 multi -1.99 import weight 0.00
Epoch 158 Iter 1 subLoss 6975.3 multi 6.97 import weight 0.00
Epoch 158 Iter 2 subLoss 5604.0 multi -1.99 import weight 0.00
Epoch 158 Iter 3 subLoss 6240.2 multi 1.00 import weight 0.00
Epoch 158 Iter 4 subLoss 5665.3 multi 6.97 import weight 0.00
Epoch 158 Iter 5 subLoss 5528.9 multi -4.97 import weight 0.00
Epoch 158 Iter 6 subLoss 6690.3 multi 1.00 import weight 0.00
Epoch 158 Iter 7 subLoss 6241.6 multi 3.98 import weight 0.00
Epoch 158 Iter 8 subLoss 5242.4 multi -7.96 import weight 0.00
Epoch 158 Iter 9 subLoss 8647.7 multi 3.99 import weight 0.00
Epoch 158 Iter 10 subLoss 6876.3 multi 3.99 import weight 0.00
Epoch 158 Iter 11 subLoss 5291.6 multi -4.97 import weight 0.00
Epoch 158 Acc: 94.61 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 529 train Loss: 6114.5 test Loss: 865.5
Epoch 159 Iter 0 subLoss 5910.5 multi 1.00 import weight 0.00
Epoch 159 Iter 1 subLoss 5983.3 multi 6.97 import weight 0.00
Epoch 159 Iter 2 subLoss 5176.1 multi 3.99 import weight 0.00
Epoch 159 Iter 3 subLoss 4693.7 multi -1.98 import weight 0.00
Epoch 159 Iter 4 subLoss 5434.8 multi -4.97 import weight 0.00
Epoch 159 Iter 5 subLoss 5592.3 multi 1.00 import weight 0.00
Epoch 159 Iter 6 subLoss 5479.0 multi -1.99 import weight 0.00
Epoch 159 Iter 7 subLoss 5365.4 multi 1.00 import weight 0.00
Epoch 159 Iter 8 subLoss 5338.6 multi -4.97 import weight 0.00
Epoch 159 Iter 9 subLoss 5995.2 multi -4.97 import weight 0.00
Epoch 159 Iter 10 subLoss 9358.2 multi 3.99 import weight 0.00
Epoch 159 Iter 11 subLoss 6284.4 multi 6.97 import weight 0.00
Epoch 159 Acc: 93.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 628 train Loss: 7691.6 test Loss: 1067.6
Epoch 160 Iter 0 subLoss 7360.5 multi -4.97 import weight 0.00
Epoch 160 Iter 1 subLoss 30793.5 multi 6.97 import weight 0.00
Epoch 160 Iter 2 subLoss 37039.3 multi 1.00 import weight 0.00
Epoch 160 Iter 3 subLoss 21003.0 multi 1.00 import weight 0.00
Epoch 160 Iter 4 subLoss 16520.3 multi 3.99 import weight 0.00
Epoch 160 Iter 5 subLoss 13131.4 multi -1.99 import weight 0.00
Epoch 160 Iter 6 subLoss 14604.3 multi 6.97 import weight 0.00
Epoch 160 Iter 7 subLoss 15842.8 multi 1.00 import weight 0.00
Epoch 160 Iter 8 subLoss 11821.5 multi 1.00 import weight 0.00
Epoch 160 Iter 9 subLoss 10964.2 multi 3.99 import weight 0.00
Epoch 160 Iter 10 subLoss 9879.3 multi 9.96 import weight 0.00
Epoch 160 Iter 11 subLoss 14973.3 multi 1.00 import weight 0.00
Epoch 160 Acc: 96.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1497 train Loss: 8888.0 test Loss: 729.2
Epoch 161 Iter 0 subLoss 8866.7 multi 1.00 import weight 0.00
Epoch 161 Iter 1 subLoss 8298.5 multi 1.00 import weight 0.00
Epoch 161 Iter 2 subLoss 7558.0 multi 3.99 import weight 0.00
Epoch 161 Iter 3 subLoss 7310.8 multi -1.98 import weight 0.00
Epoch 161 Iter 4 subLoss 7386.1 multi 1.00 import weight 0.00
Epoch 161 Iter 5 subLoss 6872.0 multi 6.97 import weight 0.00
Epoch 161 Iter 6 subLoss 6793.5 multi 1.00 import weight 0.00
Epoch 161 Iter 7 subLoss 6831.4 multi -4.97 import weight 0.00
Epoch 161 Iter 8 subLoss 7463.1 multi 3.99 import weight 0.00
Epoch 161 Iter 9 subLoss 7115.5 multi 3.99 import weight 0.00
Epoch 161 Iter 10 subLoss 6138.2 multi -1.99 import weight 0.00
Epoch 161 Iter 11 subLoss 6908.2 multi 9.96 import weight 0.00
Epoch 161 Acc: 89.69 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 690 train Loss: 13339.9 test Loss: 1725.2
Epoch 162 Iter 0 subLoss 12667.8 multi 3.99 import weight 0.00
Epoch 162 Iter 1 subLoss 17117.5 multi 1.00 import weight 0.00
Epoch 162 Iter 2 subLoss 7310.7 multi 1.00 import weight 0.00
Epoch 162 Iter 3 subLoss 6383.3 multi 3.99 import weight 0.00
Epoch 162 Iter 4 subLoss 5112.1 multi 3.98 import weight 0.00
Epoch 162 Iter 5 subLoss 4846.2 multi 9.96 import weight 0.00
Epoch 162 Iter 6 subLoss 4369.3 multi 3.99 import weight 0.00
Epoch 162 Iter 7 subLoss 4616.2 multi 3.99 import weight 0.00
Epoch 162 Iter 8 subLoss 4154.1 multi 3.98 import weight 0.00
Epoch 162 Iter 9 subLoss 4112.8 multi -4.97 import weight 0.00
Epoch 162 Iter 10 subLoss 5414.0 multi 3.99 import weight 0.00
Epoch 162 Iter 11 subLoss 4106.4 multi -1.99 import weight 0.00
Epoch 162 Acc: 98.35 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 410 train Loss: 4614.2 test Loss: 344.1
Epoch 163 Iter 0 subLoss 4663.9 multi 3.98 import weight 0.00
Epoch 163 Iter 1 subLoss 4321.3 multi 1.00 import weight 0.00
Epoch 163 Iter 2 subLoss 4171.3 multi 15.93 import weight 0.00
Epoch 163 Iter 3 subLoss 5113.3 multi 6.97 import weight 0.00
Epoch 163 Iter 4 subLoss 8758.8 multi 3.98 import weight 0.00
Epoch 163 Iter 5 subLoss 5408.1 multi 1.00 import weight 0.00
Epoch 163 Iter 6 subLoss 4003.2 multi -4.97 import weight 0.00
Epoch 163 Iter 7 subLoss 9118.5 multi -1.99 import weight 0.00
Epoch 163 Iter 8 subLoss 50441.6 multi 1.00 import weight 0.00
Epoch 163 Iter 9 subLoss 7479.9 multi 3.99 import weight 0.00
Epoch 163 Iter 10 subLoss 4768.2 multi 3.99 import weight 0.00
Epoch 163 Iter 11 subLoss 4316.9 multi -13.93 import weight 0.00
Epoch 163 Acc: 97.92 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 431 train Loss: 5317.4 test Loss: 408.6
Epoch 164 Iter 0 subLoss 5331.5 multi -1.98 import weight 0.00
Epoch 164 Iter 1 subLoss 5677.5 multi -4.97 import weight 0.00
Epoch 164 Iter 2 subLoss 16863.3 multi 1.00 import weight 0.00
Epoch 164 Iter 3 subLoss 6307.1 multi 9.96 import weight 0.00
Epoch 164 Iter 4 subLoss 8366.4 multi -1.99 import weight 0.00
Epoch 164 Iter 5 subLoss 42164.1 multi 3.99 import weight 0.00
Epoch 164 Iter 6 subLoss 59802.7 multi 1.00 import weight 0.00
Epoch 164 Iter 7 subLoss 38318.1 multi -1.99 import weight 0.00
Epoch 164 Iter 8 subLoss 125859.9 multi 1.00 import weight 0.00
Epoch 164 Iter 9 subLoss 47893.2 multi -1.99 import weight 0.00
Epoch 164 Iter 10 subLoss 67154.6 multi 1.00 import weight 0.00
Epoch 164 Iter 11 subLoss 53767.7 multi 1.00 import weight 0.00
Epoch 164 Acc: 32.59 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 5376 train Loss: 51066.6 test Loss: 8405.8
Epoch 165 Iter 0 subLoss 49835.4 multi 1.00 import weight 0.00
Epoch 165 Iter 1 subLoss 46971.1 multi 1.00 import weight 0.00
Epoch 165 Iter 2 subLoss 43649.4 multi 1.00 import weight 0.00
Epoch 165 Iter 3 subLoss 40080.3 multi 1.00 import weight 0.00
Epoch 165 Iter 4 subLoss 34729.2 multi 1.00 import weight 0.00
Epoch 165 Iter 5 subLoss 30671.4 multi 1.00 import weight 0.00
Epoch 165 Iter 6 subLoss 27875.7 multi -1.99 import weight 0.00
Epoch 165 Iter 7 subLoss 31684.9 multi 1.00 import weight 0.00
Epoch 165 Iter 8 subLoss 29573.9 multi 1.00 import weight 0.00
Epoch 165 Iter 9 subLoss 26735.3 multi 1.00 import weight 0.00
Epoch 165 Iter 10 subLoss 26385.0 multi 1.00 import weight 0.00
Epoch 165 Iter 11 subLoss 24764.2 multi 1.00 import weight 0.00
Epoch 165 Acc: 80.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2476 train Loss: 23792.5 test Loss: 2731.9
Epoch 166 Iter 0 subLoss 23017.2 multi 3.99 import weight 0.00
Epoch 166 Iter 1 subLoss 19206.9 multi -1.99 import weight 0.00
Epoch 166 Iter 2 subLoss 21435.5 multi 1.00 import weight 0.00
Epoch 166 Iter 3 subLoss 21202.2 multi 3.99 import weight 0.00
Epoch 166 Iter 4 subLoss 16151.6 multi 1.00 import weight 0.00
Epoch 166 Iter 5 subLoss 17297.5 multi 1.00 import weight 0.00
Epoch 166 Iter 6 subLoss 16365.1 multi 1.00 import weight 0.00
Epoch 166 Iter 7 subLoss 16036.4 multi 3.99 import weight 0.00
Epoch 166 Iter 8 subLoss 14066.5 multi 3.99 import weight 0.00
Epoch 166 Iter 9 subLoss 10966.1 multi 6.97 import weight 0.00
Epoch 166 Iter 10 subLoss 9483.8 multi 3.99 import weight 0.00
Epoch 166 Iter 11 subLoss 8955.7 multi 9.96 import weight 0.00
Epoch 166 Acc: 70.42 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 895 train Loss: 33091.1 test Loss: 5501.4
Epoch 167 Iter 0 subLoss 33735.2 multi 1.00 import weight 0.00
Epoch 167 Iter 1 subLoss 8962.2 multi -7.96 import weight 0.00
Epoch 167 Iter 2 subLoss 115816.6 multi 1.00 import weight 0.00
Epoch 167 Iter 3 subLoss 15459.8 multi 6.97 import weight 0.00
Epoch 167 Iter 4 subLoss 18392.9 multi 3.99 import weight 0.00
Epoch 167 Iter 5 subLoss 9739.6 multi -1.99 import weight 0.00
Epoch 167 Iter 6 subLoss 16480.5 multi 3.99 import weight 0.00
Epoch 167 Iter 7 subLoss 12898.2 multi 1.00 import weight 0.00
Epoch 167 Iter 8 subLoss 8931.3 multi 3.99 import weight 0.00
Epoch 167 Iter 9 subLoss 7229.1 multi 6.97 import weight 0.00
Epoch 167 Iter 10 subLoss 6578.1 multi 3.99 import weight 0.00
Epoch 167 Iter 11 subLoss 5707.0 multi -10.94 import weight 0.00
Epoch 167 Acc: 93.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 570 train Loss: 10128.7 test Loss: 1051.3
Epoch 168 Iter 0 subLoss 9765.9 multi 1.00 import weight 0.00
Epoch 168 Iter 1 subLoss 7762.2 multi -1.99 import weight 0.00
Epoch 168 Iter 2 subLoss 10368.9 multi -1.99 import weight 0.00
Epoch 168 Iter 3 subLoss 20798.4 multi 3.99 import weight 0.00
Epoch 168 Iter 4 subLoss 25215.1 multi -1.99 import weight 0.00
Epoch 168 Iter 5 subLoss 120281.2 multi 1.00 import weight 0.00
Epoch 168 Iter 6 subLoss 16406.5 multi 1.00 import weight 0.00
Epoch 168 Iter 7 subLoss 13358.3 multi 3.99 import weight 0.00
Epoch 168 Iter 8 subLoss 9721.1 multi -1.98 import weight 0.00
Epoch 168 Iter 9 subLoss 10573.1 multi 3.99 import weight 0.00
Epoch 168 Iter 10 subLoss 8112.2 multi 3.99 import weight 0.00
Epoch 168 Iter 11 subLoss 7264.9 multi 1.00 import weight 0.00
Epoch 168 Acc: 95.50 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 726 train Loss: 7293.2 test Loss: 787.6
Epoch 169 Iter 0 subLoss 7399.4 multi -1.99 import weight 0.00
Epoch 169 Iter 1 subLoss 7744.3 multi -1.99 import weight 0.00
Epoch 169 Iter 2 subLoss 7909.2 multi 1.00 import weight 0.00
Epoch 169 Iter 3 subLoss 7846.5 multi 3.99 import weight 0.00
Epoch 169 Iter 4 subLoss 6610.7 multi -1.99 import weight 0.00
Epoch 169 Iter 5 subLoss 7283.7 multi 1.00 import weight 0.00
Epoch 169 Iter 6 subLoss 7489.2 multi -7.96 import weight 0.00
Epoch 169 Iter 7 subLoss 8161.7 multi 1.00 import weight 0.00
Epoch 169 Iter 8 subLoss 8531.4 multi 3.99 import weight 0.00
Epoch 169 Iter 9 subLoss 7117.3 multi 6.97 import weight 0.00
Epoch 169 Iter 10 subLoss 6171.4 multi -1.99 import weight 0.00
Epoch 169 Iter 11 subLoss 7168.3 multi 1.00 import weight 0.00
Epoch 169 Acc: 96.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 716 train Loss: 6656.9 test Loss: 652.3
Epoch 170 Iter 0 subLoss 6671.7 multi 3.99 import weight 0.00
Epoch 170 Iter 1 subLoss 6744.6 multi -1.99 import weight 0.00
Epoch 170 Iter 2 subLoss 6999.5 multi 3.99 import weight 0.00
Epoch 170 Iter 3 subLoss 5925.3 multi 3.98 import weight 0.00
Epoch 170 Iter 4 subLoss 5403.3 multi 3.99 import weight 0.00
Epoch 170 Iter 5 subLoss 5386.9 multi -1.98 import weight 0.00
Epoch 170 Iter 6 subLoss 5591.4 multi 3.98 import weight 0.00
Epoch 170 Iter 7 subLoss 5071.4 multi -16.91 import weight 0.00
Epoch 170 Iter 8 subLoss 8491.3 multi -4.97 import weight 0.00
Epoch 170 Iter 9 subLoss 22240.7 multi 1.00 import weight 0.00
Epoch 170 Iter 10 subLoss 10424.5 multi 1.00 import weight 0.00
Epoch 170 Iter 11 subLoss 8408.9 multi 1.00 import weight 0.00
Epoch 170 Acc: 96.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 840 train Loss: 7889.1 test Loss: 704.6
Epoch 171 Iter 0 subLoss 7967.6 multi 9.96 import weight 0.00
Epoch 171 Iter 1 subLoss 7319.5 multi 3.99 import weight 0.00
Epoch 171 Iter 2 subLoss 5514.6 multi -1.98 import weight 0.00
Epoch 171 Iter 3 subLoss 6171.0 multi 1.00 import weight 0.00
Epoch 171 Iter 4 subLoss 6165.6 multi 1.00 import weight 0.00
Epoch 171 Iter 5 subLoss 6342.7 multi 9.96 import weight 0.00
Epoch 171 Iter 6 subLoss 5049.2 multi -4.97 import weight 0.00
Epoch 171 Iter 7 subLoss 6585.5 multi -4.97 import weight 0.00
Epoch 171 Iter 8 subLoss 12385.6 multi 3.99 import weight 0.00
Epoch 171 Iter 9 subLoss 8564.2 multi -1.99 import weight 0.00
Epoch 171 Iter 10 subLoss 12050.0 multi 1.00 import weight 0.00
Epoch 171 Iter 11 subLoss 8761.9 multi -7.96 import weight 0.00
Epoch 171 Acc: 67.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 876 train Loss: 34183.0 test Loss: 4469.0
Epoch 172 Iter 0 subLoss 34662.8 multi 1.00 import weight 0.00
Epoch 172 Iter 1 subLoss 17576.9 multi 1.00 import weight 0.00
Epoch 172 Iter 2 subLoss 15150.8 multi -7.96 import weight 0.00
Epoch 172 Iter 3 subLoss 31915.9 multi -1.99 import weight 0.00
Epoch 172 Iter 4 subLoss 75261.8 multi 1.00 import weight 0.00
Epoch 172 Iter 5 subLoss 28052.2 multi 3.99 import weight 0.00
Epoch 172 Iter 6 subLoss 21199.0 multi 1.00 import weight 0.00
Epoch 172 Iter 7 subLoss 20138.9 multi 1.00 import weight 0.00
Epoch 172 Iter 8 subLoss 19831.3 multi 1.00 import weight 0.00
Epoch 172 Iter 9 subLoss 17711.4 multi 1.00 import weight 0.00
Epoch 172 Iter 10 subLoss 17753.3 multi -1.99 import weight 0.00
Epoch 172 Iter 11 subLoss 19083.3 multi 1.00 import weight 0.00
Epoch 172 Acc: 81.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1908 train Loss: 18666.7 test Loss: 2467.5
Epoch 173 Iter 0 subLoss 17999.3 multi 1.00 import weight 0.00
Epoch 173 Iter 1 subLoss 17688.7 multi -1.99 import weight 0.00
Epoch 173 Iter 2 subLoss 18132.6 multi 1.00 import weight 0.00
Epoch 173 Iter 3 subLoss 19206.4 multi 1.00 import weight 0.00
Epoch 173 Iter 4 subLoss 18246.7 multi -1.99 import weight 0.00
Epoch 173 Iter 5 subLoss 19534.3 multi 1.00 import weight 0.00
Epoch 173 Iter 6 subLoss 17539.2 multi 3.99 import weight 0.00
Epoch 173 Iter 7 subLoss 15422.9 multi 1.00 import weight 0.00
Epoch 173 Iter 8 subLoss 15653.2 multi 1.00 import weight 0.00
Epoch 173 Iter 9 subLoss 15045.0 multi -1.99 import weight 0.00
Epoch 173 Iter 10 subLoss 15806.7 multi -1.99 import weight 0.00
Epoch 173 Iter 11 subLoss 17266.7 multi -1.99 import weight 0.00
Epoch 173 Acc: 82.16 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1726 train Loss: 18712.8 test Loss: 2423.1
Epoch 174 Iter 0 subLoss 18721.5 multi 1.00 import weight 0.00
Epoch 174 Iter 1 subLoss 17468.1 multi 1.00 import weight 0.00
Epoch 174 Iter 2 subLoss 16451.3 multi 1.00 import weight 0.00
Epoch 174 Iter 3 subLoss 16908.4 multi 1.00 import weight 0.00
Epoch 174 Iter 4 subLoss 15966.2 multi 1.00 import weight 0.00
Epoch 174 Iter 5 subLoss 15750.9 multi 1.00 import weight 0.00
Epoch 174 Iter 6 subLoss 14300.3 multi 1.00 import weight 0.00
Epoch 174 Iter 7 subLoss 13929.5 multi 1.00 import weight 0.00
Epoch 174 Iter 8 subLoss 13371.7 multi 3.99 import weight 0.00
Epoch 174 Iter 9 subLoss 12036.1 multi 1.00 import weight 0.00
Epoch 174 Iter 10 subLoss 12264.2 multi 3.99 import weight 0.00
Epoch 174 Iter 11 subLoss 10249.0 multi 1.00 import weight 0.00
Epoch 174 Acc: 95.64 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1024 train Loss: 9760.2 test Loss: 956.1
Epoch 175 Iter 0 subLoss 9391.0 multi 1.00 import weight 0.00
Epoch 175 Iter 1 subLoss 9402.6 multi 1.00 import weight 0.00
Epoch 175 Iter 2 subLoss 9130.3 multi -4.97 import weight 0.00
Epoch 175 Iter 3 subLoss 11062.4 multi 3.99 import weight 0.00
Epoch 175 Iter 4 subLoss 8766.1 multi -4.97 import weight 0.00
Epoch 175 Iter 5 subLoss 10633.4 multi 1.00 import weight 0.00
Epoch 175 Iter 6 subLoss 11117.5 multi -1.98 import weight 0.00
Epoch 175 Iter 7 subLoss 11721.0 multi 1.00 import weight 0.00
Epoch 175 Iter 8 subLoss 11897.3 multi 1.00 import weight 0.00
Epoch 175 Iter 9 subLoss 11302.5 multi 3.99 import weight 0.00
Epoch 175 Iter 10 subLoss 9316.8 multi 1.00 import weight 0.00
Epoch 175 Iter 11 subLoss 9604.2 multi 6.97 import weight 0.00
Epoch 175 Acc: 96.52 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 960 train Loss: 7260.2 test Loss: 626.8
Epoch 176 Iter 0 subLoss 6674.6 multi 6.97 import weight 0.00
Epoch 176 Iter 1 subLoss 5911.5 multi 3.99 import weight 0.00
Epoch 176 Iter 2 subLoss 4929.5 multi 9.96 import weight 0.00
Epoch 176 Iter 3 subLoss 5456.0 multi -1.99 import weight 0.00
Epoch 176 Iter 4 subLoss 6799.3 multi 3.99 import weight 0.00
Epoch 176 Iter 5 subLoss 5385.9 multi 1.00 import weight 0.00
Epoch 176 Iter 6 subLoss 5840.1 multi -4.97 import weight 0.00
Epoch 176 Iter 7 subLoss 7615.8 multi 1.00 import weight 0.00
Epoch 176 Iter 8 subLoss 6374.0 multi -1.99 import weight 0.00
Epoch 176 Iter 9 subLoss 8464.3 multi 3.99 import weight 0.00
Epoch 176 Iter 10 subLoss 5768.4 multi -1.98 import weight 0.00
Epoch 176 Iter 11 subLoss 6356.0 multi -10.94 import weight 0.00
Epoch 176 Acc: 57.99 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 635 train Loss: 60460.3 test Loss: 10529.0
Epoch 177 Iter 0 subLoss 57924.5 multi 1.00 import weight 0.00
Epoch 177 Iter 1 subLoss 9815.2 multi 1.00 import weight 0.00
Epoch 177 Iter 2 subLoss 7560.9 multi -7.96 import weight 0.00
Epoch 177 Iter 3 subLoss 26134.3 multi -1.99 import weight 0.00
Epoch 177 Iter 4 subLoss 65787.9 multi 1.00 import weight 0.00
Epoch 177 Iter 5 subLoss 31642.3 multi 3.99 import weight 0.00
Epoch 177 Iter 6 subLoss 13512.1 multi -1.99 import weight 0.00
Epoch 177 Iter 7 subLoss 20730.0 multi 1.00 import weight 0.00
Epoch 177 Iter 8 subLoss 15441.2 multi 1.00 import weight 0.00
Epoch 177 Iter 9 subLoss 12313.5 multi 3.99 import weight 0.00
Epoch 177 Iter 10 subLoss 6762.3 multi -4.97 import weight 0.00
Epoch 177 Iter 11 subLoss 8471.6 multi 1.00 import weight 0.00
Epoch 177 Acc: 95.89 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 847 train Loss: 8225.5 test Loss: 827.7
Epoch 178 Iter 0 subLoss 8020.0 multi -1.98 import weight 0.00
Epoch 178 Iter 1 subLoss 8788.8 multi 1.00 import weight 0.00
Epoch 178 Iter 2 subLoss 8465.4 multi 6.97 import weight 0.00
Epoch 178 Iter 3 subLoss 5993.5 multi -1.99 import weight 0.00
Epoch 178 Iter 4 subLoss 6303.0 multi 12.94 import weight 0.00
Epoch 178 Iter 5 subLoss 6871.1 multi 9.96 import weight 0.00
Epoch 178 Iter 6 subLoss 10906.2 multi 1.00 import weight 0.00
Epoch 178 Iter 7 subLoss 7352.5 multi 3.98 import weight 0.00
Epoch 178 Iter 8 subLoss 4682.6 multi 3.98 import weight 0.00
Epoch 178 Iter 9 subLoss 4039.3 multi -1.98 import weight 0.00
Epoch 178 Iter 10 subLoss 4904.7 multi -1.99 import weight 0.00
Epoch 178 Iter 11 subLoss 5599.5 multi 6.97 import weight 0.00
Epoch 178 Acc: 97.57 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 559 train Loss: 4858.7 test Loss: 451.9
Epoch 179 Iter 0 subLoss 4731.6 multi 1.00 import weight 0.00
Epoch 179 Iter 1 subLoss 4674.6 multi -4.97 import weight 0.00
Epoch 179 Iter 2 subLoss 5181.3 multi -1.99 import weight 0.00
Epoch 179 Iter 3 subLoss 5901.2 multi -1.99 import weight 0.00
Epoch 179 Iter 4 subLoss 7629.2 multi 1.00 import weight 0.00
Epoch 179 Iter 5 subLoss 6484.8 multi 6.97 import weight 0.00
Epoch 179 Iter 6 subLoss 6018.8 multi 6.97 import weight 0.00
Epoch 179 Iter 7 subLoss 5080.3 multi 1.00 import weight 0.00
Epoch 179 Iter 8 subLoss 5169.3 multi 3.98 import weight 0.00
Epoch 179 Iter 9 subLoss 4113.7 multi -4.97 import weight 0.00
Epoch 179 Iter 10 subLoss 4690.7 multi -1.99 import weight 0.00
Epoch 179 Iter 11 subLoss 4702.6 multi -1.99 import weight 0.00
Epoch 179 Acc: 97.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 470 train Loss: 4944.4 test Loss: 482.7
Epoch 180 Iter 0 subLoss 4611.3 multi 6.97 import weight 0.00
Epoch 180 Iter 1 subLoss 5481.3 multi -4.97 import weight 0.00
Epoch 180 Iter 2 subLoss 4509.5 multi -1.98 import weight 0.00
Epoch 180 Iter 3 subLoss 6091.3 multi -1.98 import weight 0.00
Epoch 180 Iter 4 subLoss 7058.6 multi 1.00 import weight 0.00
Epoch 180 Iter 5 subLoss 5984.5 multi 9.96 import weight 0.00
Epoch 180 Iter 6 subLoss 7373.0 multi -1.99 import weight 0.00
Epoch 180 Iter 7 subLoss 9117.4 multi 1.00 import weight 0.00
Epoch 180 Iter 8 subLoss 7015.0 multi 1.00 import weight 0.00
Epoch 180 Iter 9 subLoss 6370.9 multi 1.00 import weight 0.00
Epoch 180 Iter 10 subLoss 5364.7 multi 3.98 import weight 0.00
Epoch 180 Iter 11 subLoss 4592.6 multi -10.94 import weight 0.00
Epoch 180 Acc: 96.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 459 train Loss: 5863.3 test Loss: 622.9
Epoch 181 Iter 0 subLoss 6143.8 multi 1.00 import weight 0.00
Epoch 181 Iter 1 subLoss 5234.9 multi 3.99 import weight 0.00
Epoch 181 Iter 2 subLoss 4346.3 multi -4.97 import weight 0.00
Epoch 181 Iter 3 subLoss 4902.0 multi 1.00 import weight 0.00
Epoch 181 Iter 4 subLoss 4823.1 multi -7.96 import weight 0.00
Epoch 181 Iter 5 subLoss 6093.5 multi 1.00 import weight 0.00
Epoch 181 Iter 6 subLoss 5235.0 multi 6.97 import weight 0.00
Epoch 181 Iter 7 subLoss 4596.9 multi -7.96 import weight 0.00
Epoch 181 Iter 8 subLoss 5383.2 multi 3.99 import weight 0.00
Epoch 181 Iter 9 subLoss 4112.6 multi -1.99 import weight 0.00
Epoch 181 Iter 10 subLoss 4998.5 multi -4.97 import weight 0.00
Epoch 181 Iter 11 subLoss 5550.4 multi 1.00 import weight 0.00
Epoch 181 Acc: 96.77 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 555 train Loss: 5247.8 test Loss: 521.8
Epoch 182 Iter 0 subLoss 5268.2 multi 6.97 import weight 0.00
Epoch 182 Iter 1 subLoss 4461.9 multi 6.97 import weight 0.00
Epoch 182 Iter 2 subLoss 4382.6 multi -1.99 import weight 0.00
Epoch 182 Iter 3 subLoss 4700.1 multi 1.00 import weight 0.00
Epoch 182 Iter 4 subLoss 4348.7 multi -1.98 import weight 0.00
Epoch 182 Iter 5 subLoss 4104.6 multi 1.00 import weight 0.00
Epoch 182 Iter 6 subLoss 4842.3 multi 12.94 import weight 0.00
Epoch 182 Iter 7 subLoss 4375.0 multi 6.97 import weight 0.00
Epoch 182 Iter 8 subLoss 4651.3 multi -13.93 import weight 0.00
Epoch 182 Iter 9 subLoss 6398.7 multi -4.97 import weight 0.00
Epoch 182 Iter 10 subLoss 21320.3 multi 1.00 import weight 0.00
Epoch 182 Iter 11 subLoss 9284.0 multi -1.99 import weight 0.00
Epoch 182 Acc: 87.18 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 928 train Loss: 15927.0 test Loss: 2248.6
Epoch 183 Iter 0 subLoss 14967.6 multi -1.99 import weight 0.00
Epoch 183 Iter 1 subLoss 54165.0 multi 1.00 import weight 0.00
Epoch 183 Iter 2 subLoss 9950.3 multi 3.99 import weight 0.00
Epoch 183 Iter 3 subLoss 5505.0 multi 12.94 import weight 0.00
Epoch 183 Iter 4 subLoss 5289.3 multi -13.93 import weight 0.00
Epoch 183 Iter 5 subLoss 17896.4 multi 3.99 import weight 0.00
Epoch 183 Iter 6 subLoss 11223.1 multi 1.00 import weight 0.00
Epoch 183 Iter 7 subLoss 7717.1 multi 1.00 import weight 0.00
Epoch 183 Iter 8 subLoss 8036.9 multi -4.97 import weight 0.00
Epoch 183 Iter 9 subLoss 11315.0 multi -4.97 import weight 0.00
Epoch 183 Iter 10 subLoss 66061.4 multi 1.00 import weight 0.00
Epoch 183 Iter 11 subLoss 11448.8 multi -1.98 import weight 0.00
Epoch 183 Acc: 82.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 1144 train Loss: 16879.8 test Loss: 2837.0
Epoch 184 Iter 0 subLoss 16727.7 multi -1.99 import weight 0.00
Epoch 184 Iter 1 subLoss 33345.0 multi 1.00 import weight 0.00
Epoch 184 Iter 2 subLoss 15697.2 multi 1.00 import weight 0.00
Epoch 184 Iter 3 subLoss 12681.0 multi 1.00 import weight 0.00
Epoch 184 Iter 4 subLoss 11026.1 multi 3.99 import weight 0.00
Epoch 184 Iter 5 subLoss 9596.4 multi -7.96 import weight 0.00
Epoch 184 Iter 6 subLoss 12371.3 multi 1.00 import weight 0.00
Epoch 184 Iter 7 subLoss 12227.1 multi 1.00 import weight 0.00
Epoch 184 Iter 8 subLoss 10781.3 multi 1.00 import weight 0.00
Epoch 184 Iter 9 subLoss 10674.3 multi 1.00 import weight 0.00
Epoch 184 Iter 10 subLoss 11400.9 multi 1.00 import weight 0.00
Epoch 184 Iter 11 subLoss 9870.8 multi 12.94 import weight 0.00
Epoch 184 Acc: 95.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 987 train Loss: 6505.6 test Loss: 740.4
Epoch 185 Iter 0 subLoss 6187.6 multi -1.99 import weight 0.00
Epoch 185 Iter 1 subLoss 6903.8 multi 12.94 import weight 0.00
Epoch 185 Iter 2 subLoss 6031.3 multi 1.00 import weight 0.00
Epoch 185 Iter 3 subLoss 5999.2 multi -1.98 import weight 0.00
Epoch 185 Iter 4 subLoss 6868.3 multi -1.99 import weight 0.00
Epoch 185 Iter 5 subLoss 8066.1 multi -1.99 import weight 0.00
Epoch 185 Iter 6 subLoss 9142.0 multi -1.98 import weight 0.00
Epoch 185 Iter 7 subLoss 13703.5 multi -4.97 import weight 0.00
Epoch 185 Iter 8 subLoss 45371.5 multi 1.00 import weight 0.00
Epoch 185 Iter 9 subLoss 19984.5 multi 3.99 import weight 0.00
Epoch 185 Iter 10 subLoss 11923.6 multi 1.00 import weight 0.00
Epoch 185 Iter 11 subLoss 10521.0 multi 1.00 import weight 0.00
Epoch 185 Acc: 88.01 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1052 train Loss: 10313.6 test Loss: 1617.6
Epoch 186 Iter 0 subLoss 10038.3 multi 1.00 import weight 0.00
Epoch 186 Iter 1 subLoss 9229.5 multi 1.00 import weight 0.00
Epoch 186 Iter 2 subLoss 8641.7 multi 6.97 import weight 0.00
Epoch 186 Iter 3 subLoss 5147.5 multi -4.97 import weight 0.00
Epoch 186 Iter 4 subLoss 6025.5 multi -7.96 import weight 0.00
Epoch 186 Iter 5 subLoss 7524.0 multi 1.00 import weight 0.00
Epoch 186 Iter 6 subLoss 6975.9 multi 9.96 import weight 0.00
Epoch 186 Iter 7 subLoss 5107.0 multi -7.96 import weight 0.00
Epoch 186 Iter 8 subLoss 6908.5 multi 15.93 import weight 0.00
Epoch 186 Iter 9 subLoss 10728.5 multi -1.99 import weight 0.00
Epoch 186 Iter 10 subLoss 16538.1 multi -4.97 import weight 0.00
Epoch 186 Iter 11 subLoss 75325.2 multi 1.00 import weight 0.00
Epoch 186 Acc: 80.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 7532 train Loss: 19491.0 test Loss: 3020.1
Epoch 187 Iter 0 subLoss 19989.0 multi 6.97 import weight 0.00
Epoch 187 Iter 1 subLoss 10762.8 multi -4.97 import weight 0.00
Epoch 187 Iter 2 subLoss 17089.5 multi 3.99 import weight 0.00
Epoch 187 Iter 3 subLoss 10343.6 multi 1.00 import weight 0.00
Epoch 187 Iter 4 subLoss 9898.0 multi 1.00 import weight 0.00
Epoch 187 Iter 5 subLoss 10189.7 multi -1.99 import weight 0.00
Epoch 187 Iter 6 subLoss 10787.2 multi 3.99 import weight 0.00
Epoch 187 Iter 7 subLoss 8604.8 multi -1.99 import weight 0.00
Epoch 187 Iter 8 subLoss 9306.1 multi 3.98 import weight 0.00
Epoch 187 Iter 9 subLoss 7450.5 multi -1.99 import weight 0.00
Epoch 187 Iter 10 subLoss 7875.1 multi -1.99 import weight 0.00
Epoch 187 Iter 11 subLoss 8886.6 multi -7.96 import weight 0.00
Epoch 187 Acc: 85.91 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 888 train Loss: 13294.6 test Loss: 1888.3
Epoch 188 Iter 0 subLoss 13215.4 multi -1.99 import weight 0.00
Epoch 188 Iter 1 subLoss 14285.5 multi -1.99 import weight 0.00
Epoch 188 Iter 2 subLoss 16433.7 multi 1.00 import weight 0.00
Epoch 188 Iter 3 subLoss 14633.5 multi 1.00 import weight 0.00
Epoch 188 Iter 4 subLoss 14670.8 multi -1.99 import weight 0.00
Epoch 188 Iter 5 subLoss 15826.8 multi 3.99 import weight 0.00
Epoch 188 Iter 6 subLoss 12491.9 multi -1.99 import weight 0.00
Epoch 188 Iter 7 subLoss 13480.4 multi 1.00 import weight 0.00
Epoch 188 Iter 8 subLoss 14396.2 multi 1.00 import weight 0.00
Epoch 188 Iter 9 subLoss 13308.6 multi 1.00 import weight 0.00
Epoch 188 Iter 10 subLoss 12510.4 multi 1.00 import weight 0.00
Epoch 188 Iter 11 subLoss 11639.3 multi 1.00 import weight 0.00
Epoch 188 Acc: 89.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1163 train Loss: 11472.0 test Loss: 1549.7
Epoch 189 Iter 0 subLoss 11982.9 multi 1.00 import weight 0.00
Epoch 189 Iter 1 subLoss 11039.5 multi -4.97 import weight 0.00
Epoch 189 Iter 2 subLoss 13048.5 multi 3.99 import weight 0.00
Epoch 189 Iter 3 subLoss 11438.1 multi 6.97 import weight 0.00
Epoch 189 Iter 4 subLoss 8698.5 multi 1.00 import weight 0.00
Epoch 189 Iter 5 subLoss 7139.1 multi 1.00 import weight 0.00
Epoch 189 Iter 6 subLoss 7492.2 multi -4.97 import weight 0.00
Epoch 189 Iter 7 subLoss 7962.4 multi 12.94 import weight 0.00
Epoch 189 Iter 8 subLoss 5839.4 multi 6.97 import weight 0.00
Epoch 189 Iter 9 subLoss 5492.8 multi -4.97 import weight 0.00
Epoch 189 Iter 10 subLoss 5528.0 multi -4.97 import weight 0.00
Epoch 189 Iter 11 subLoss 5700.4 multi -7.96 import weight 0.00
Epoch 189 Acc: 95.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 570 train Loss: 7484.3 test Loss: 882.3
Epoch 190 Iter 0 subLoss 7682.5 multi 6.97 import weight 0.00
Epoch 190 Iter 1 subLoss 5575.0 multi 1.00 import weight 0.00
Epoch 190 Iter 2 subLoss 5958.0 multi 9.96 import weight 0.00
Epoch 190 Iter 3 subLoss 4895.9 multi 1.00 import weight 0.00
Epoch 190 Iter 4 subLoss 4821.2 multi -4.97 import weight 0.00
Epoch 190 Iter 5 subLoss 5115.1 multi 6.97 import weight 0.00
Epoch 190 Iter 6 subLoss 4847.2 multi 15.93 import weight 1.00
Epoch 190 Iter 7 subLoss 4456.6 multi 9.96 import weight 0.00
Epoch 190 Iter 8 subLoss 4498.0 multi 1.00 import weight 0.00
Epoch 190 Iter 9 subLoss 4475.7 multi -7.96 import weight 0.00
Epoch 190 Iter 10 subLoss 4629.7 multi -13.93 import weight 0.00
Epoch 190 Iter 11 subLoss 9909.4 multi -1.99 import weight 0.00
Epoch 190 Acc: 83.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 990 train Loss: 21507.0 test Loss: 2156.2
Epoch 191 Iter 0 subLoss 21083.4 multi 1.00 import weight 0.00
Epoch 191 Iter 1 subLoss 9991.7 multi 1.00 import weight 0.00
Epoch 191 Iter 2 subLoss 6676.8 multi 9.96 import weight 0.00
Epoch 191 Iter 3 subLoss 7223.4 multi 9.96 import weight 0.00
Epoch 191 Iter 4 subLoss 12312.2 multi 6.97 import weight 0.00
Epoch 191 Iter 5 subLoss 9113.5 multi 3.98 import weight 0.00
Epoch 191 Iter 6 subLoss 4078.4 multi -16.91 import weight 0.00
Epoch 191 Iter 7 subLoss 6049.3 multi -4.97 import weight 0.00
Epoch 191 Iter 8 subLoss 8963.7 multi -4.97 import weight 0.00
Epoch 191 Iter 9 subLoss 21767.6 multi 1.00 import weight 0.00
Epoch 191 Iter 10 subLoss 12479.8 multi -10.94 import weight 0.00
Epoch 191 Iter 11 subLoss 94830.9 multi 1.00 import weight 0.00
Epoch 191 Acc: 37.95 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 9483 train Loss: 38388.7 test Loss: 6645.6
Epoch 192 Iter 0 subLoss 38562.6 multi -1.99 import weight 0.00
Epoch 192 Iter 1 subLoss 45887.8 multi 1.00 import weight 0.00
Epoch 192 Iter 2 subLoss 42166.3 multi 6.97 import weight 0.00
Epoch 192 Iter 3 subLoss 26760.4 multi 1.00 import weight 0.00
Epoch 192 Iter 4 subLoss 24339.4 multi -1.99 import weight 0.00
Epoch 192 Iter 5 subLoss 27546.1 multi -1.99 import weight 0.00
Epoch 192 Iter 6 subLoss 33254.8 multi 1.00 import weight 0.00
Epoch 192 Iter 7 subLoss 27757.3 multi 1.00 import weight 0.00
Epoch 192 Iter 8 subLoss 27042.7 multi 1.00 import weight 0.00
Epoch 192 Iter 9 subLoss 25258.6 multi 1.00 import weight 0.00
Epoch 192 Iter 10 subLoss 24692.4 multi -1.99 import weight 0.00
Epoch 192 Iter 11 subLoss 26500.3 multi 1.00 import weight 0.00
Epoch 192 Acc: 68.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2650 train Loss: 25472.0 test Loss: 4228.7
Epoch 193 Iter 0 subLoss 25355.4 multi 1.00 import weight 0.00
Epoch 193 Iter 1 subLoss 23702.9 multi 1.00 import weight 0.00
Epoch 193 Iter 2 subLoss 22556.2 multi 1.00 import weight 0.00
Epoch 193 Iter 3 subLoss 21569.1 multi 1.00 import weight 0.00
Epoch 193 Iter 4 subLoss 20029.2 multi -1.99 import weight 0.00
Epoch 193 Iter 5 subLoss 22587.1 multi 1.00 import weight 0.00
Epoch 193 Iter 6 subLoss 21267.4 multi 1.00 import weight 0.00
Epoch 193 Iter 7 subLoss 19967.1 multi 1.00 import weight 0.00
Epoch 193 Iter 8 subLoss 18182.2 multi 1.00 import weight 0.00
Epoch 193 Iter 9 subLoss 16389.5 multi -1.99 import weight 0.00
Epoch 193 Iter 10 subLoss 20118.3 multi 1.00 import weight 0.00
Epoch 193 Iter 11 subLoss 19263.7 multi -1.99 import weight 0.00
Epoch 193 Acc: 76.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1926 train Loss: 22171.9 test Loss: 3431.3
Epoch 194 Iter 0 subLoss 21841.4 multi 1.00 import weight 0.00
Epoch 194 Iter 1 subLoss 20381.3 multi 1.00 import weight 0.00
Epoch 194 Iter 2 subLoss 20056.5 multi 1.00 import weight 0.00
Epoch 194 Iter 3 subLoss 17054.2 multi 1.00 import weight 0.00
Epoch 194 Iter 4 subLoss 16803.3 multi 1.00 import weight 0.00
Epoch 194 Iter 5 subLoss 14961.5 multi 1.00 import weight 0.00
Epoch 194 Iter 6 subLoss 12922.7 multi 3.99 import weight 0.00
Epoch 194 Iter 7 subLoss 9071.7 multi -1.99 import weight 0.00
Epoch 194 Iter 8 subLoss 9960.2 multi -4.97 import weight 0.00
Epoch 194 Iter 9 subLoss 14424.5 multi 3.99 import weight 0.00
Epoch 194 Iter 10 subLoss 10080.0 multi 3.99 import weight 0.00
Epoch 194 Iter 11 subLoss 7913.3 multi 1.00 import weight 0.00
Epoch 194 Acc: 94.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 791 train Loss: 7852.7 test Loss: 934.9
Epoch 195 Iter 0 subLoss 7815.6 multi 1.00 import weight 0.00
Epoch 195 Iter 1 subLoss 7421.1 multi -1.98 import weight 0.00
Epoch 195 Iter 2 subLoss 8613.3 multi 3.98 import weight 0.00
Epoch 195 Iter 3 subLoss 6716.5 multi 1.00 import weight 0.00
Epoch 195 Iter 4 subLoss 6361.2 multi 1.00 import weight 0.00
Epoch 195 Iter 5 subLoss 6203.9 multi 3.99 import weight 0.00
Epoch 195 Iter 6 subLoss 5971.1 multi -1.98 import weight 0.00
Epoch 195 Iter 7 subLoss 6751.6 multi 3.98 import weight 0.00
Epoch 195 Iter 8 subLoss 5317.3 multi 3.99 import weight 0.00
Epoch 195 Iter 9 subLoss 5110.8 multi 9.96 import weight 0.00
Epoch 195 Iter 10 subLoss 5207.1 multi -1.98 import weight 0.00
Epoch 195 Iter 11 subLoss 4333.2 multi 1.00 import weight 0.00
Epoch 195 Acc: 97.59 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 433 train Loss: 4891.0 test Loss: 435.5
Epoch 196 Iter 0 subLoss 4841.3 multi 18.91 import weight 1.00
Epoch 196 Iter 1 subLoss 4257.8 multi -1.98 import weight 0.00
Epoch 196 Iter 2 subLoss 5153.6 multi -4.97 import weight 0.00
Epoch 196 Iter 3 subLoss 7603.1 multi -1.99 import weight 0.00
Epoch 196 Iter 4 subLoss 14414.1 multi 1.00 import weight 0.00
Epoch 196 Iter 5 subLoss 6050.3 multi 3.98 import weight 0.00
Epoch 196 Iter 6 subLoss 3804.4 multi -7.96 import weight 0.00
Epoch 196 Iter 7 subLoss 5044.0 multi -1.99 import weight 0.00
Epoch 196 Iter 8 subLoss 5825.3 multi -4.97 import weight 0.00
Epoch 196 Iter 9 subLoss 8164.9 multi 3.99 import weight 0.00
Epoch 196 Iter 10 subLoss 5209.3 multi 1.00 import weight 0.00
Epoch 196 Iter 11 subLoss 5000.4 multi 3.99 import weight 0.00
Epoch 196 Acc: 97.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 500 train Loss: 4579.1 test Loss: 415.0
Epoch 197 Iter 0 subLoss 4573.4 multi -1.99 import weight 0.00
Epoch 197 Iter 1 subLoss 4782.9 multi 9.96 import weight 0.00
Epoch 197 Iter 2 subLoss 4623.2 multi -10.94 import weight 0.00
Epoch 197 Iter 3 subLoss 4562.1 multi 12.94 import weight 0.00
Epoch 197 Iter 4 subLoss 3994.8 multi 12.94 import weight 0.00
Epoch 197 Iter 5 subLoss 3906.2 multi 1.00 import weight 0.00
Epoch 197 Iter 6 subLoss 4800.8 multi 6.97 import weight 0.00
Epoch 197 Iter 7 subLoss 3817.6 multi -1.99 import weight 0.00
Epoch 197 Iter 8 subLoss 3990.2 multi 15.93 import weight 0.00
Epoch 197 Iter 9 subLoss 3629.5 multi 1.00 import weight 0.00
Epoch 197 Iter 10 subLoss 3820.1 multi -4.97 import weight 0.00
Epoch 197 Iter 11 subLoss 5185.9 multi 1.00 import weight 0.00
Epoch 197 Acc: 97.20 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 518 train Loss: 4469.9 test Loss: 459.6
Epoch 198 Iter 0 subLoss 4285.0 multi -4.97 import weight 0.00
Epoch 198 Iter 1 subLoss 5643.5 multi -1.99 import weight 0.00
Epoch 198 Iter 2 subLoss 6603.4 multi -1.98 import weight 0.00
Epoch 198 Iter 3 subLoss 8944.0 multi -4.97 import weight 0.00
Epoch 198 Iter 4 subLoss 47860.8 multi 1.00 import weight 0.00
Epoch 198 Iter 5 subLoss 7527.3 multi 3.99 import weight 0.00
Epoch 198 Iter 6 subLoss 4803.2 multi 9.96 import weight 0.00
Epoch 198 Iter 7 subLoss 4682.2 multi 3.99 import weight 0.00
Epoch 198 Iter 8 subLoss 3556.6 multi 3.98 import weight 0.00
Epoch 198 Iter 9 subLoss 3527.4 multi -1.98 import weight 0.00
Epoch 198 Iter 10 subLoss 3761.1 multi 1.00 import weight 0.00
Epoch 198 Iter 11 subLoss 3888.2 multi -1.98 import weight 0.00
Epoch 198 Acc: 97.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 388 train Loss: 3916.8 test Loss: 393.3
Epoch 199 Iter 0 subLoss 3880.0 multi 1.00 import weight 0.00
Epoch 199 Iter 1 subLoss 4746.9 multi -7.96 import weight 0.00
Epoch 199 Iter 2 subLoss 4446.3 multi -1.98 import weight 0.00
Epoch 199 Iter 3 subLoss 4446.6 multi 1.00 import weight 0.00
Epoch 199 Iter 4 subLoss 4186.4 multi -13.93 import weight 0.00
Epoch 199 Iter 5 subLoss 9992.4 multi 3.98 import weight 0.00
Epoch 199 Iter 6 subLoss 5169.8 multi 3.99 import weight 0.00
Epoch 199 Iter 7 subLoss 4501.2 multi -1.99 import weight 0.00
Epoch 199 Iter 8 subLoss 4056.4 multi -7.96 import weight 0.00
Epoch 199 Iter 9 subLoss 4927.8 multi 12.94 import weight 0.00
Epoch 199 Iter 10 subLoss 4542.0 multi -4.97 import weight 0.00
Epoch 199 Iter 11 subLoss 5287.8 multi -10.94 import weight 0.00
Epoch 199 Acc: 72.04 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 528 train Loss: 28094.7 test Loss: 4702.3
Epoch 200 Iter 0 subLoss 26805.3 multi 1.00 import weight 0.00
Epoch 200 Iter 1 subLoss 9176.1 multi 3.98 import weight 0.00
Epoch 200 Iter 2 subLoss 5159.7 multi -1.99 import weight 0.00
Epoch 200 Iter 3 subLoss 5599.3 multi 9.96 import weight 0.00
Epoch 200 Iter 4 subLoss 4705.7 multi 3.99 import weight 0.00
Epoch 200 Iter 5 subLoss 4346.1 multi -1.99 import weight 0.00
Epoch 200 Iter 6 subLoss 4667.9 multi 3.99 import weight 0.00
Epoch 200 Iter 7 subLoss 4647.9 multi 15.93 import weight 0.00
Epoch 200 Iter 8 subLoss 3855.9 multi 6.97 import weight 0.00
Epoch 200 Iter 9 subLoss 3151.2 multi -1.98 import weight 0.00
Epoch 200 Iter 10 subLoss 3824.4 multi -1.99 import weight 0.00
Epoch 200 Iter 11 subLoss 4248.9 multi 3.98 import weight 0.00
Epoch 200 Acc: 97.59 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 424 train Loss: 4046.3 test Loss: 408.0
Epoch 201 Iter 0 subLoss 3747.4 multi -13.93 import weight 0.00
Epoch 201 Iter 1 subLoss 4857.7 multi -22.88 import weight 0.00
Epoch 201 Iter 2 subLoss 39852.2 multi 1.00 import weight 0.00
Epoch 201 Iter 3 subLoss 7086.7 multi -1.98 import weight 0.00
Epoch 201 Iter 4 subLoss 10067.9 multi 3.99 import weight 0.00
Epoch 201 Iter 5 subLoss 5788.1 multi 6.97 import weight 0.00
Epoch 201 Iter 6 subLoss 4737.4 multi 3.99 import weight 0.00
Epoch 201 Iter 7 subLoss 4393.3 multi -4.97 import weight 0.00
Epoch 201 Iter 8 subLoss 5017.8 multi -4.97 import weight 0.00
Epoch 201 Iter 9 subLoss 5453.2 multi 1.00 import weight 0.00
Epoch 201 Iter 10 subLoss 4907.6 multi 1.00 import weight 0.00
Epoch 201 Iter 11 subLoss 4818.6 multi 6.97 import weight 0.00
Epoch 201 Acc: 97.30 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 481 train Loss: 4550.5 test Loss: 474.8
Epoch 202 Iter 0 subLoss 4345.0 multi 1.00 import weight 0.00
Epoch 202 Iter 1 subLoss 5010.6 multi -1.99 import weight 0.00
Epoch 202 Iter 2 subLoss 4451.7 multi 6.97 import weight 0.00
Epoch 202 Iter 3 subLoss 4070.4 multi -13.93 import weight 0.00
Epoch 202 Iter 4 subLoss 4727.1 multi -7.96 import weight 0.00
Epoch 202 Iter 5 subLoss 6047.9 multi -1.98 import weight 0.00
Epoch 202 Iter 6 subLoss 8010.5 multi 3.98 import weight 0.00
Epoch 202 Iter 7 subLoss 5144.4 multi -1.99 import weight 0.00
Epoch 202 Iter 8 subLoss 5219.1 multi -4.97 import weight 0.00
Epoch 202 Iter 9 subLoss 6387.6 multi 1.00 import weight 0.00
Epoch 202 Iter 10 subLoss 5892.1 multi -1.98 import weight 0.00
Epoch 202 Iter 11 subLoss 6339.4 multi -7.96 import weight 0.00
Epoch 202 Acc: 92.74 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 633 train Loss: 13579.8 test Loss: 1604.7
Epoch 203 Iter 0 subLoss 13523.3 multi 1.00 import weight 0.00
Epoch 203 Iter 1 subLoss 10158.9 multi 1.00 import weight 0.00
Epoch 203 Iter 2 subLoss 8524.9 multi 6.97 import weight 0.00
Epoch 203 Iter 3 subLoss 5476.6 multi 1.00 import weight 0.00
Epoch 203 Iter 4 subLoss 5155.7 multi -1.99 import weight 0.00
Epoch 203 Iter 5 subLoss 4961.3 multi 1.00 import weight 0.00
Epoch 203 Iter 6 subLoss 5169.2 multi 1.00 import weight 0.00
Epoch 203 Iter 7 subLoss 5038.5 multi 9.96 import weight 0.00
Epoch 203 Iter 8 subLoss 4678.5 multi -4.97 import weight 0.00
Epoch 203 Iter 9 subLoss 5359.3 multi -1.98 import weight 0.00
Epoch 203 Iter 10 subLoss 5226.1 multi 1.00 import weight 0.00
Epoch 203 Iter 11 subLoss 4665.2 multi 6.97 import weight 0.00
Epoch 203 Acc: 97.49 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 466 train Loss: 4374.4 test Loss: 448.1
Epoch 204 Iter 0 subLoss 4601.8 multi -7.96 import weight 0.00
Epoch 204 Iter 1 subLoss 4389.3 multi -1.99 import weight 0.00
Epoch 204 Iter 2 subLoss 5299.9 multi -7.96 import weight 0.00
Epoch 204 Iter 3 subLoss 5914.0 multi 3.98 import weight 0.00
Epoch 204 Iter 4 subLoss 4911.1 multi -10.94 import weight 0.00
Epoch 204 Iter 5 subLoss 6283.2 multi 9.96 import weight 0.00
Epoch 204 Iter 6 subLoss 4665.5 multi 9.96 import weight 0.00
Epoch 204 Iter 7 subLoss 4368.6 multi 6.97 import weight 0.00
Epoch 204 Iter 8 subLoss 4141.0 multi 3.99 import weight 0.00
Epoch 204 Iter 9 subLoss 4231.0 multi 6.97 import weight 0.00
Epoch 204 Iter 10 subLoss 4028.1 multi 1.00 import weight 0.00
Epoch 204 Iter 11 subLoss 3850.0 multi 9.96 import weight 0.00
Epoch 204 Acc: 97.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 385 train Loss: 4187.3 test Loss: 405.4
Epoch 205 Iter 0 subLoss 4477.7 multi -4.97 import weight 0.00
Epoch 205 Iter 1 subLoss 3785.8 multi 6.97 import weight 0.00
Epoch 205 Iter 2 subLoss 4312.3 multi -10.94 import weight 0.00
Epoch 205 Iter 3 subLoss 4510.1 multi 1.00 import weight 0.00
Epoch 205 Iter 4 subLoss 3562.5 multi 1.00 import weight 0.00
Epoch 205 Iter 5 subLoss 4769.5 multi 6.97 import weight 0.00
Epoch 205 Iter 6 subLoss 4173.6 multi 18.91 import weight 0.00
Epoch 205 Iter 7 subLoss 3900.8 multi 3.99 import weight 0.00
Epoch 205 Iter 8 subLoss 3504.8 multi 12.94 import weight 0.00
Epoch 205 Iter 9 subLoss 3026.6 multi 6.97 import weight 0.00
Epoch 205 Iter 10 subLoss 3835.2 multi -4.97 import weight 0.00
Epoch 205 Iter 11 subLoss 4166.0 multi -4.97 import weight 0.00
Epoch 205 Acc: 97.88 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 416 train Loss: 3982.0 test Loss: 348.0
Epoch 206 Iter 0 subLoss 3849.1 multi -1.98 import weight 0.00
Epoch 206 Iter 1 subLoss 4404.5 multi -7.96 import weight 0.00
Epoch 206 Iter 2 subLoss 7642.4 multi -7.96 import weight 0.00
Epoch 206 Iter 3 subLoss 141591.1 multi 1.00 import weight 0.00
Epoch 206 Iter 4 subLoss 8654.7 multi 1.00 import weight 0.00
Epoch 206 Iter 5 subLoss 7051.3 multi 3.99 import weight 0.00
Epoch 206 Iter 6 subLoss 5311.3 multi 6.97 import weight 0.00
Epoch 206 Iter 7 subLoss 4208.4 multi -1.99 import weight 0.00
Epoch 206 Iter 8 subLoss 4101.8 multi 3.99 import weight 0.00
Epoch 206 Iter 9 subLoss 4286.1 multi -1.99 import weight 0.00
Epoch 206 Iter 10 subLoss 3899.0 multi -1.99 import weight 0.00
Epoch 206 Iter 11 subLoss 3966.8 multi 6.97 import weight 0.00
Epoch 206 Acc: 97.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 396 train Loss: 4096.6 test Loss: 393.6
Epoch 207 Iter 0 subLoss 3755.3 multi 12.94 import weight 0.00
Epoch 207 Iter 1 subLoss 4262.0 multi -13.93 import weight 0.00
Epoch 207 Iter 2 subLoss 4598.9 multi -4.97 import weight 0.00
Epoch 207 Iter 3 subLoss 7059.6 multi 6.97 import weight 0.00
Epoch 207 Iter 4 subLoss 4575.0 multi -1.98 import weight 0.00
Epoch 207 Iter 5 subLoss 5096.0 multi 3.99 import weight 0.00
Epoch 207 Iter 6 subLoss 3534.1 multi -4.97 import weight 0.00
Epoch 207 Iter 7 subLoss 4136.4 multi -4.97 import weight 0.00
Epoch 207 Iter 8 subLoss 4609.7 multi -7.96 import weight 0.00
Epoch 207 Iter 9 subLoss 7828.8 multi -1.99 import weight 0.00
Epoch 207 Iter 10 subLoss 11720.0 multi 3.99 import weight 0.00
Epoch 207 Iter 11 subLoss 5406.9 multi 6.97 import weight 0.00
Epoch 207 Acc: 97.65 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 540 train Loss: 4216.7 test Loss: 403.5
Epoch 208 Iter 0 subLoss 3826.4 multi 1.00 import weight 0.00
Epoch 208 Iter 1 subLoss 4091.8 multi 1.00 import weight 0.00
Epoch 208 Iter 2 subLoss 4076.2 multi -10.94 import weight 0.00
Epoch 208 Iter 3 subLoss 4690.9 multi -1.99 import weight 0.00
Epoch 208 Iter 4 subLoss 4583.5 multi 9.96 import weight 1.00
Epoch 208 Iter 5 subLoss 4436.1 multi 6.97 import weight 0.00
Epoch 208 Iter 6 subLoss 4347.1 multi 3.99 import weight 0.00
Epoch 208 Iter 7 subLoss 4005.7 multi -7.96 import weight 0.00
Epoch 208 Iter 8 subLoss 4535.5 multi -1.99 import weight 0.00
Epoch 208 Iter 9 subLoss 4263.8 multi -10.94 import weight 0.00
Epoch 208 Iter 10 subLoss 4952.3 multi -1.98 import weight 0.00
Epoch 208 Iter 11 subLoss 6151.4 multi 3.99 import weight 0.00
Epoch 208 Acc: 96.87 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 615 train Loss: 4616.6 test Loss: 493.3
Epoch 209 Iter 0 subLoss 4282.1 multi 1.00 import weight 0.00
Epoch 209 Iter 1 subLoss 4366.7 multi 9.96 import weight 0.00
Epoch 209 Iter 2 subLoss 4117.6 multi -4.97 import weight 0.00
Epoch 209 Iter 3 subLoss 4655.3 multi -13.93 import weight 0.00
Epoch 209 Iter 4 subLoss 4479.6 multi -1.99 import weight 0.00
Epoch 209 Iter 5 subLoss 5713.3 multi -4.97 import weight 0.00
Epoch 209 Iter 6 subLoss 7734.3 multi 1.00 import weight 0.00
Epoch 209 Iter 7 subLoss 6626.3 multi 6.97 import weight 0.00
Epoch 209 Iter 8 subLoss 4601.2 multi -4.97 import weight 0.00
Epoch 209 Iter 9 subLoss 4974.2 multi 3.99 import weight 0.00
Epoch 209 Iter 10 subLoss 3837.1 multi -4.97 import weight 0.00
Epoch 209 Iter 11 subLoss 4295.1 multi 1.00 import weight 0.00
Epoch 209 Acc: 97.28 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 429 train Loss: 4651.8 test Loss: 476.2
Epoch 210 Iter 0 subLoss 4853.4 multi -19.90 import weight 0.00
Epoch 210 Iter 1 subLoss 9942.1 multi -4.97 import weight 0.00
Epoch 210 Iter 2 subLoss 64993.6 multi 1.00 import weight 0.00
Epoch 210 Iter 3 subLoss 7054.0 multi 9.96 import weight 0.00
Epoch 210 Iter 4 subLoss 5673.4 multi -1.99 import weight 0.00
Epoch 210 Iter 5 subLoss 5848.9 multi -4.97 import weight 0.00
Epoch 210 Iter 6 subLoss 8274.7 multi -7.96 import weight 0.00
Epoch 210 Iter 7 subLoss 22033.0 multi 1.00 import weight 0.00
Epoch 210 Iter 8 subLoss 17294.8 multi 3.99 import weight 0.00
Epoch 210 Iter 9 subLoss 7029.5 multi 6.97 import weight 0.00
Epoch 210 Iter 10 subLoss 4688.8 multi 3.99 import weight 0.00
Epoch 210 Iter 11 subLoss 4805.0 multi 12.94 import weight 0.00
Epoch 210 Acc: 97.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 480 train Loss: 4377.5 test Loss: 458.1
Epoch 211 Iter 0 subLoss 4477.7 multi 1.00 import weight 0.00
Epoch 211 Iter 1 subLoss 3763.0 multi 1.00 import weight 0.00
Epoch 211 Iter 2 subLoss 4343.4 multi 6.97 import weight 0.00
Epoch 211 Iter 3 subLoss 3918.7 multi 1.00 import weight 0.00
Epoch 211 Iter 4 subLoss 4132.4 multi -1.98 import weight 0.00
Epoch 211 Iter 5 subLoss 3845.1 multi -1.99 import weight 0.00
Epoch 211 Iter 6 subLoss 4276.5 multi 3.99 import weight 0.00
Epoch 211 Iter 7 subLoss 4657.9 multi -10.94 import weight 0.00
Epoch 211 Iter 8 subLoss 4366.5 multi 12.94 import weight 0.00
Epoch 211 Iter 9 subLoss 4604.3 multi -1.99 import weight 0.00
Epoch 211 Iter 10 subLoss 4346.9 multi 9.96 import weight 0.00
Epoch 211 Iter 11 subLoss 3950.2 multi 1.00 import weight 0.00
Epoch 211 Acc: 97.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 395 train Loss: 4215.6 test Loss: 453.4
Epoch 212 Iter 0 subLoss 4342.1 multi 12.94 import weight 0.00
Epoch 212 Iter 1 subLoss 3737.7 multi 15.93 import weight 0.00
Epoch 212 Iter 2 subLoss 4309.8 multi 3.99 import weight 0.00
Epoch 212 Iter 3 subLoss 3511.0 multi -7.96 import weight 0.00
Epoch 212 Iter 4 subLoss 4354.3 multi -16.91 import weight 0.00
Epoch 212 Iter 5 subLoss 7240.0 multi -13.93 import weight 0.00
Epoch 212 Iter 6 subLoss 144289.8 multi 1.00 import weight 0.00
Epoch 212 Iter 7 subLoss 9249.9 multi 1.00 import weight 0.00
Epoch 212 Iter 8 subLoss 8515.0 multi 1.00 import weight 0.00
Epoch 212 Iter 9 subLoss 8147.9 multi 1.00 import weight 0.00
Epoch 212 Iter 10 subLoss 7657.0 multi 3.98 import weight 0.00
Epoch 212 Iter 11 subLoss 5596.0 multi 12.94 import weight 0.00
Epoch 212 Acc: 97.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 559 train Loss: 4607.1 test Loss: 493.3
Epoch 213 Iter 0 subLoss 4847.2 multi 21.90 import weight 1.00
Epoch 213 Iter 1 subLoss 3880.9 multi 3.99 import weight 0.00
Epoch 213 Iter 2 subLoss 3873.7 multi -1.99 import weight 0.00
Epoch 213 Iter 3 subLoss 4217.8 multi 1.00 import weight 0.00
Epoch 213 Iter 4 subLoss 4030.3 multi -1.99 import weight 0.00
Epoch 213 Iter 5 subLoss 4738.9 multi 3.99 import weight 0.00
Epoch 213 Iter 6 subLoss 3999.2 multi 18.91 import weight 0.00
Epoch 213 Iter 7 subLoss 3428.2 multi 9.96 import weight 0.00
Epoch 213 Iter 8 subLoss 3288.0 multi 1.00 import weight 0.00
Epoch 213 Iter 9 subLoss 3983.0 multi -1.99 import weight 0.00
Epoch 213 Iter 10 subLoss 4859.9 multi -19.90 import weight 0.00
Epoch 213 Iter 11 subLoss 5675.0 multi 1.00 import weight 0.00
Epoch 213 Acc: 97.06 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 567 train Loss: 5060.9 test Loss: 525.4
Epoch 214 Iter 0 subLoss 4699.6 multi -1.98 import weight 0.00
Epoch 214 Iter 1 subLoss 5392.4 multi -4.97 import weight 0.00
Epoch 214 Iter 2 subLoss 13978.6 multi -1.99 import weight 0.00
Epoch 214 Iter 3 subLoss 74280.7 multi 1.00 import weight 0.00
Epoch 214 Iter 4 subLoss 8000.8 multi 1.00 import weight 0.00
Epoch 214 Iter 5 subLoss 6009.1 multi -10.94 import weight 0.00
Epoch 214 Iter 6 subLoss 29116.6 multi 1.00 import weight 0.00
Epoch 214 Iter 7 subLoss 11168.4 multi 3.98 import weight 0.00
Epoch 214 Iter 8 subLoss 5102.3 multi -7.96 import weight 0.00
Epoch 214 Iter 9 subLoss 6917.0 multi -16.91 import weight 0.00
Epoch 214 Iter 10 subLoss 46436.6 multi 1.00 import weight 0.00
Epoch 214 Iter 11 subLoss 10973.6 multi -1.99 import weight 0.00
Epoch 214 Acc: 85.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1097 train Loss: 18678.3 test Loss: 2621.2
Epoch 215 Iter 0 subLoss 19411.8 multi 3.99 import weight 0.00
Epoch 215 Iter 1 subLoss 7920.9 multi -4.97 import weight 0.00
Epoch 215 Iter 2 subLoss 11818.7 multi 1.00 import weight 0.00
Epoch 215 Iter 3 subLoss 9685.2 multi 6.97 import weight 0.00
Epoch 215 Iter 4 subLoss 5729.6 multi 3.99 import weight 0.00
Epoch 215 Iter 5 subLoss 5472.3 multi 3.98 import weight 0.00
Epoch 215 Iter 6 subLoss 5360.2 multi 3.99 import weight 0.00
Epoch 215 Iter 7 subLoss 4976.0 multi 6.97 import weight 0.00
Epoch 215 Iter 8 subLoss 4234.3 multi 9.96 import weight 0.00
Epoch 215 Iter 9 subLoss 4210.1 multi 3.99 import weight 0.00
Epoch 215 Iter 10 subLoss 4541.6 multi -4.97 import weight 0.00
Epoch 215 Iter 11 subLoss 4561.2 multi 15.93 import weight 0.00
Epoch 215 Acc: 97.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 456 train Loss: 4135.5 test Loss: 414.4
Epoch 216 Iter 0 subLoss 4627.0 multi -7.96 import weight 0.00
Epoch 216 Iter 1 subLoss 4670.8 multi -7.96 import weight 0.00
Epoch 216 Iter 2 subLoss 6049.4 multi 1.00 import weight 0.00
Epoch 216 Iter 3 subLoss 5642.0 multi 1.00 import weight 0.00
Epoch 216 Iter 4 subLoss 5974.6 multi 1.00 import weight 0.00
Epoch 216 Iter 5 subLoss 5173.2 multi -1.98 import weight 0.00
Epoch 216 Iter 6 subLoss 5303.3 multi 1.00 import weight 0.00
Epoch 216 Iter 7 subLoss 5352.3 multi 1.00 import weight 0.00
Epoch 216 Iter 8 subLoss 4948.1 multi 6.97 import weight 0.00
Epoch 216 Iter 9 subLoss 4423.4 multi -13.93 import weight 0.00
Epoch 216 Iter 10 subLoss 4591.3 multi -4.97 import weight 0.00
Epoch 216 Iter 11 subLoss 5543.0 multi -1.99 import weight 0.00
Epoch 216 Acc: 94.90 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 554 train Loss: 7107.8 test Loss: 892.7
Epoch 217 Iter 0 subLoss 6798.3 multi 6.97 import weight 0.00
Epoch 217 Iter 1 subLoss 4139.7 multi 1.00 import weight 0.00
Epoch 217 Iter 2 subLoss 4606.8 multi -1.98 import weight 0.00
Epoch 217 Iter 3 subLoss 5127.5 multi -7.96 import weight 0.00
Epoch 217 Iter 4 subLoss 5090.1 multi 6.97 import weight 0.00
Epoch 217 Iter 5 subLoss 4268.6 multi -7.96 import weight 0.00
Epoch 217 Iter 6 subLoss 4926.5 multi 12.94 import weight 0.00
Epoch 217 Iter 7 subLoss 3812.4 multi 1.00 import weight 0.00
Epoch 217 Iter 8 subLoss 4826.9 multi -4.97 import weight 0.00
Epoch 217 Iter 9 subLoss 3963.7 multi 6.97 import weight 0.00
Epoch 217 Iter 10 subLoss 4713.7 multi 1.00 import weight 0.00
Epoch 217 Iter 11 subLoss 4392.3 multi -4.97 import weight 0.00
Epoch 217 Acc: 97.39 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 439 train Loss: 4504.1 test Loss: 410.2
Epoch 218 Iter 0 subLoss 4298.8 multi 3.98 import weight 0.00
Epoch 218 Iter 1 subLoss 4288.5 multi 1.00 import weight 0.00
Epoch 218 Iter 2 subLoss 4041.0 multi 6.97 import weight 0.00
Epoch 218 Iter 3 subLoss 4172.8 multi 18.91 import weight 0.00
Epoch 218 Iter 4 subLoss 4515.1 multi 3.99 import weight 0.00
Epoch 218 Iter 5 subLoss 4319.7 multi -10.94 import weight 0.00
Epoch 218 Iter 6 subLoss 4401.6 multi -7.96 import weight 0.00
Epoch 218 Iter 7 subLoss 4960.7 multi 1.00 import weight 0.00
Epoch 218 Iter 8 subLoss 4724.9 multi -7.96 import weight 0.00
Epoch 218 Iter 9 subLoss 6002.2 multi -7.96 import weight 0.00
Epoch 218 Iter 10 subLoss 49534.0 multi 1.00 import weight 0.00
Epoch 218 Iter 11 subLoss 5874.8 multi -1.99 import weight 0.00
Epoch 218 Acc: 93.75 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 587 train Loss: 7143.6 test Loss: 927.2
Epoch 219 Iter 0 subLoss 7217.6 multi 3.99 import weight 0.00
Epoch 219 Iter 1 subLoss 5397.1 multi -1.99 import weight 0.00
Epoch 219 Iter 2 subLoss 4956.6 multi -1.99 import weight 0.00
Epoch 219 Iter 3 subLoss 5789.3 multi 9.96 import weight 0.00
Epoch 219 Iter 4 subLoss 4480.5 multi -16.91 import weight 0.00
Epoch 219 Iter 5 subLoss 4901.6 multi 3.98 import weight 0.00
Epoch 219 Iter 6 subLoss 4809.5 multi 15.93 import weight 0.00
Epoch 219 Iter 7 subLoss 4360.7 multi 12.94 import weight 0.00
Epoch 219 Iter 8 subLoss 4231.8 multi 12.94 import weight 0.00
Epoch 219 Iter 9 subLoss 3687.2 multi 1.00 import weight 0.00
Epoch 219 Iter 10 subLoss 4804.2 multi 18.91 import weight 0.00
Epoch 219 Iter 11 subLoss 4087.1 multi 1.00 import weight 0.00
Epoch 219 Acc: 97.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 408 train Loss: 3867.0 test Loss: 372.2
Epoch 220 Iter 0 subLoss 3829.1 multi 1.00 import weight 0.00
Epoch 220 Iter 1 subLoss 3696.1 multi 3.99 import weight 0.00
Epoch 220 Iter 2 subLoss 3607.8 multi 1.00 import weight 0.00
Epoch 220 Iter 3 subLoss 3868.8 multi -4.97 import weight 0.00
Epoch 220 Iter 4 subLoss 3549.1 multi 1.00 import weight 0.00
Epoch 220 Iter 5 subLoss 3819.1 multi 3.99 import weight 0.00
Epoch 220 Iter 6 subLoss 3712.6 multi 1.00 import weight 0.00
Epoch 220 Iter 7 subLoss 3684.7 multi 3.99 import weight 0.00
Epoch 220 Iter 8 subLoss 3814.7 multi 6.97 import weight 0.00
Epoch 220 Iter 9 subLoss 3848.6 multi 1.00 import weight 0.00
Epoch 220 Iter 10 subLoss 3471.1 multi 3.99 import weight 0.00
Epoch 220 Iter 11 subLoss 3268.7 multi -4.97 import weight 0.00
Epoch 220 Acc: 97.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 326 train Loss: 3728.5 test Loss: 349.8
Epoch 221 Iter 0 subLoss 3611.5 multi 1.00 import weight 0.00
Epoch 221 Iter 1 subLoss 3294.4 multi 3.98 import weight 0.00
Epoch 221 Iter 2 subLoss 3582.9 multi -4.97 import weight 0.00
Epoch 221 Iter 3 subLoss 2825.9 multi -4.97 import weight 0.00
Epoch 221 Iter 4 subLoss 3606.4 multi 3.98 import weight 0.00
Epoch 221 Iter 5 subLoss 4143.0 multi -1.99 import weight 0.00
Epoch 221 Iter 6 subLoss 3107.1 multi 1.00 import weight 0.00
Epoch 221 Iter 7 subLoss 3583.1 multi -1.99 import weight 0.00
Epoch 221 Iter 8 subLoss 3843.0 multi 3.99 import weight 0.00
Epoch 221 Iter 9 subLoss 3781.9 multi 9.96 import weight 0.00
Epoch 221 Iter 10 subLoss 2743.9 multi 12.94 import weight 0.00
Epoch 221 Iter 11 subLoss 4389.3 multi 1.00 import weight 0.00
Epoch 221 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 438 train Loss: 3503.1 test Loss: 324.9
Epoch 222 Iter 0 subLoss 3571.2 multi 1.00 import weight 0.00
Epoch 222 Iter 1 subLoss 3728.9 multi -7.96 import weight 0.00
Epoch 222 Iter 2 subLoss 3587.9 multi -1.98 import weight 0.00
Epoch 222 Iter 3 subLoss 4089.1 multi 3.99 import weight 0.00
Epoch 222 Iter 4 subLoss 3699.7 multi 3.98 import weight 0.00
Epoch 222 Iter 5 subLoss 3200.8 multi 1.00 import weight 0.00
Epoch 222 Iter 6 subLoss 3355.8 multi 1.00 import weight 0.00
Epoch 222 Iter 7 subLoss 3434.5 multi -16.91 import weight 0.00
Epoch 222 Iter 8 subLoss 4306.5 multi 3.99 import weight 0.00
Epoch 222 Iter 9 subLoss 3765.3 multi 3.98 import weight 0.00
Epoch 222 Iter 10 subLoss 4102.2 multi 3.98 import weight 0.00
Epoch 222 Iter 11 subLoss 3572.5 multi 3.98 import weight 0.00
Epoch 222 Acc: 97.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 357 train Loss: 3439.2 test Loss: 316.1
Epoch 223 Iter 0 subLoss 3229.8 multi -13.93 import weight 0.00
Epoch 223 Iter 1 subLoss 3876.8 multi -1.99 import weight 0.00
Epoch 223 Iter 2 subLoss 3994.0 multi 18.91 import weight 0.00
Epoch 223 Iter 3 subLoss 4047.7 multi 9.96 import weight 0.00
Epoch 223 Iter 4 subLoss 3535.5 multi -1.98 import weight 0.00
Epoch 223 Iter 5 subLoss 3001.7 multi 1.00 import weight 0.00
Epoch 223 Iter 6 subLoss 3312.8 multi -1.98 import weight 0.00
Epoch 223 Iter 7 subLoss 3300.1 multi -10.94 import weight 0.00
Epoch 223 Iter 8 subLoss 5655.8 multi -1.98 import weight 0.00
Epoch 223 Iter 9 subLoss 7702.8 multi -1.99 import weight 0.00
Epoch 223 Iter 10 subLoss 14074.2 multi -4.97 import weight 0.00
Epoch 223 Iter 11 subLoss 228104.1 multi 1.00 import weight 0.00
Epoch 223 Acc: 85.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 22810 train Loss: 14486.4 test Loss: 2776.5
Epoch 224 Iter 0 subLoss 13922.0 multi 3.99 import weight 0.00
Epoch 224 Iter 1 subLoss 6296.0 multi -10.94 import weight 0.00
Epoch 224 Iter 2 subLoss 12101.8 multi -7.96 import weight 0.00
Epoch 224 Iter 3 subLoss 49311.1 multi 1.00 import weight 0.00
Epoch 224 Iter 4 subLoss 43502.2 multi 1.00 import weight 0.00
Epoch 224 Iter 5 subLoss 39898.9 multi 1.00 import weight 0.00
Epoch 224 Iter 6 subLoss 32253.2 multi -1.99 import weight 0.00
Epoch 224 Iter 7 subLoss 48191.9 multi 1.00 import weight 0.00
Epoch 224 Iter 8 subLoss 44754.7 multi 1.00 import weight 0.00
Epoch 224 Iter 9 subLoss 39047.6 multi 1.00 import weight 0.00
Epoch 224 Iter 10 subLoss 31950.1 multi 1.00 import weight 0.00
Epoch 224 Iter 11 subLoss 25840.0 multi -1.99 import weight 0.00
Epoch 224 Acc: 51.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 2583 train Loss: 40748.5 test Loss: 6812.1
Epoch 225 Iter 0 subLoss 39744.8 multi 1.00 import weight 0.00
Epoch 225 Iter 1 subLoss 34792.2 multi 1.00 import weight 0.00
Epoch 225 Iter 2 subLoss 29011.7 multi 1.00 import weight 0.00
Epoch 225 Iter 3 subLoss 22851.9 multi 1.00 import weight 0.00
Epoch 225 Iter 4 subLoss 16697.2 multi 3.99 import weight 0.00
Epoch 225 Iter 5 subLoss 7896.6 multi 1.00 import weight 0.00
Epoch 225 Iter 6 subLoss 6891.7 multi -1.99 import weight 0.00
Epoch 225 Iter 7 subLoss 7747.9 multi -1.98 import weight 0.00
Epoch 225 Iter 8 subLoss 9446.6 multi 1.00 import weight 0.00
Epoch 225 Iter 9 subLoss 8924.2 multi 1.00 import weight 0.00
Epoch 225 Iter 10 subLoss 7961.2 multi 15.93 import weight 0.00
Epoch 225 Iter 11 subLoss 4893.6 multi 3.99 import weight 0.00
Epoch 225 Acc: 96.30 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 489 train Loss: 4617.0 test Loss: 532.1
Epoch 226 Iter 0 subLoss 3985.6 multi 1.00 import weight 0.00
Epoch 226 Iter 1 subLoss 4398.5 multi -4.97 import weight 0.00
Epoch 226 Iter 2 subLoss 4113.7 multi -4.97 import weight 0.00
Epoch 226 Iter 3 subLoss 5541.2 multi 1.00 import weight 0.00
Epoch 226 Iter 4 subLoss 4998.7 multi -1.99 import weight 0.00
Epoch 226 Iter 5 subLoss 4778.8 multi -7.96 import weight 0.00
Epoch 226 Iter 6 subLoss 7842.2 multi 6.97 import weight 0.00
Epoch 226 Iter 7 subLoss 5501.0 multi 12.94 import weight 0.00
Epoch 226 Iter 8 subLoss 4626.1 multi -4.97 import weight 0.00
Epoch 226 Iter 9 subLoss 4856.6 multi -16.91 import weight 0.00
Epoch 226 Iter 10 subLoss 14990.3 multi 1.00 import weight 0.00
Epoch 226 Iter 11 subLoss 10147.3 multi -1.99 import weight 0.00
Epoch 226 Acc: 85.66 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1014 train Loss: 16190.2 test Loss: 2591.2
Epoch 227 Iter 0 subLoss 15976.1 multi -1.99 import weight 0.00
Epoch 227 Iter 1 subLoss 42470.6 multi 1.00 import weight 0.00
Epoch 227 Iter 2 subLoss 12522.0 multi 1.00 import weight 0.00
Epoch 227 Iter 3 subLoss 9009.5 multi -4.97 import weight 0.00
Epoch 227 Iter 4 subLoss 22207.8 multi 1.00 import weight 0.00
Epoch 227 Iter 5 subLoss 14374.1 multi 1.00 import weight 0.00
Epoch 227 Iter 6 subLoss 9906.9 multi 1.00 import weight 0.00
Epoch 227 Iter 7 subLoss 8466.0 multi 9.96 import weight 0.00
Epoch 227 Iter 8 subLoss 6022.4 multi -4.97 import weight 0.00
Epoch 227 Iter 9 subLoss 7216.0 multi 6.97 import weight 0.00
Epoch 227 Iter 10 subLoss 4538.2 multi 1.00 import weight 0.00
Epoch 227 Iter 11 subLoss 4988.1 multi -4.97 import weight 0.00
Epoch 227 Acc: 95.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 498 train Loss: 5520.5 test Loss: 768.3
Epoch 228 Iter 0 subLoss 5471.2 multi 6.97 import weight 0.00
Epoch 228 Iter 1 subLoss 4431.8 multi 6.97 import weight 0.00
Epoch 228 Iter 2 subLoss 4997.0 multi -1.99 import weight 0.00
Epoch 228 Iter 3 subLoss 4208.2 multi 1.00 import weight 0.00
Epoch 228 Iter 4 subLoss 4136.6 multi 3.99 import weight 0.00
Epoch 228 Iter 5 subLoss 4052.3 multi -10.94 import weight 0.00
Epoch 228 Iter 6 subLoss 4699.9 multi 1.00 import weight 0.00
Epoch 228 Iter 7 subLoss 4992.8 multi 1.00 import weight 0.00
Epoch 228 Iter 8 subLoss 4441.3 multi -1.99 import weight 0.00
Epoch 228 Iter 9 subLoss 4343.9 multi 15.93 import weight 0.00
Epoch 228 Iter 10 subLoss 4554.7 multi -4.97 import weight 0.00
Epoch 228 Iter 11 subLoss 4526.3 multi -4.97 import weight 0.00
Epoch 228 Acc: 95.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 452 train Loss: 5293.2 test Loss: 668.2
Epoch 229 Iter 0 subLoss 5835.3 multi 6.97 import weight 0.00
Epoch 229 Iter 1 subLoss 4211.1 multi 3.98 import weight 0.00
Epoch 229 Iter 2 subLoss 4554.7 multi -1.98 import weight 0.00
Epoch 229 Iter 3 subLoss 4025.4 multi 3.99 import weight 0.00
Epoch 229 Iter 4 subLoss 3581.6 multi -1.99 import weight 0.00
Epoch 229 Iter 5 subLoss 4172.9 multi 21.90 import weight 1.00
Epoch 229 Iter 6 subLoss 3610.9 multi 1.00 import weight 0.00
Epoch 229 Iter 7 subLoss 4178.0 multi 24.88 import weight 1.00
Epoch 229 Iter 8 subLoss 4021.9 multi 6.97 import weight 0.00
Epoch 229 Iter 9 subLoss 3488.4 multi 3.99 import weight 0.00
Epoch 229 Iter 10 subLoss 3101.1 multi 3.99 import weight 0.00
Epoch 229 Iter 11 subLoss 3343.5 multi -1.98 import weight 0.00
Epoch 229 Acc: 97.90 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 334 train Loss: 3543.9 test Loss: 356.8
Epoch 230 Iter 0 subLoss 3389.4 multi 3.98 import weight 0.00
Epoch 230 Iter 1 subLoss 4193.5 multi 1.00 import weight 0.00
Epoch 230 Iter 2 subLoss 3428.6 multi 12.94 import weight 0.00
Epoch 230 Iter 3 subLoss 3507.1 multi 15.93 import weight 0.00
Epoch 230 Iter 4 subLoss 3288.8 multi 3.99 import weight 0.00
Epoch 230 Iter 5 subLoss 2896.4 multi -1.99 import weight 0.00
Epoch 230 Iter 6 subLoss 3570.1 multi 6.97 import weight 0.00
Epoch 230 Iter 7 subLoss 3432.2 multi -16.91 import weight 0.00
Epoch 230 Iter 8 subLoss 3365.9 multi 9.96 import weight 0.00
Epoch 230 Iter 9 subLoss 3252.4 multi 1.00 import weight 0.00
Epoch 230 Iter 10 subLoss 3126.3 multi -4.97 import weight 0.00
Epoch 230 Iter 11 subLoss 3411.0 multi -4.97 import weight 0.00
Epoch 230 Acc: 97.76 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 341 train Loss: 4110.7 test Loss: 384.5
Epoch 231 Iter 0 subLoss 4109.3 multi 6.97 import weight 0.00
Epoch 231 Iter 1 subLoss 3562.9 multi 3.99 import weight 0.00
Epoch 231 Iter 2 subLoss 3170.0 multi 6.97 import weight 0.00
Epoch 231 Iter 3 subLoss 3142.1 multi 1.00 import weight 0.00
Epoch 231 Iter 4 subLoss 3343.9 multi 1.00 import weight 0.00
Epoch 231 Iter 5 subLoss 2901.7 multi -1.99 import weight 0.00
Epoch 231 Iter 6 subLoss 2903.2 multi 1.00 import weight 0.00
Epoch 231 Iter 7 subLoss 3163.5 multi -4.97 import weight 0.00
Epoch 231 Iter 8 subLoss 3302.1 multi -7.96 import weight 0.00
Epoch 231 Iter 9 subLoss 3834.2 multi -4.97 import weight 0.00
Epoch 231 Iter 10 subLoss 3541.4 multi 1.00 import weight 0.00
Epoch 231 Iter 11 subLoss 3585.6 multi -1.99 import weight 0.00
Epoch 231 Acc: 98.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 358 train Loss: 3759.5 test Loss: 336.4
Epoch 232 Iter 0 subLoss 3827.6 multi -1.99 import weight 0.00
Epoch 232 Iter 1 subLoss 3354.7 multi -1.99 import weight 0.00
Epoch 232 Iter 2 subLoss 4467.8 multi 3.99 import weight 0.00
Epoch 232 Iter 3 subLoss 3514.6 multi -7.96 import weight 0.00
Epoch 232 Iter 4 subLoss 4324.5 multi -4.97 import weight 0.00
Epoch 232 Iter 5 subLoss 5696.4 multi 12.94 import weight 0.00
Epoch 232 Iter 6 subLoss 6224.6 multi 1.00 import weight 0.00
Epoch 232 Iter 7 subLoss 5187.8 multi 1.00 import weight 0.00
Epoch 232 Iter 8 subLoss 4301.0 multi 6.97 import weight 0.00
Epoch 232 Iter 9 subLoss 3757.7 multi 15.93 import weight 0.00
Epoch 232 Iter 10 subLoss 3135.5 multi -1.99 import weight 0.00
Epoch 232 Iter 11 subLoss 2816.5 multi 6.97 import weight 0.00
Epoch 232 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 281 train Loss: 3355.0 test Loss: 321.3
Epoch 233 Iter 0 subLoss 3037.0 multi -4.97 import weight 0.00
Epoch 233 Iter 1 subLoss 3341.9 multi 3.99 import weight 0.00
Epoch 233 Iter 2 subLoss 2953.0 multi 6.97 import weight 0.00
Epoch 233 Iter 3 subLoss 3386.5 multi 6.97 import weight 0.00
Epoch 233 Iter 4 subLoss 3243.5 multi -1.99 import weight 0.00
Epoch 233 Iter 5 subLoss 3269.2 multi -4.97 import weight 0.00
Epoch 233 Iter 6 subLoss 3606.0 multi 6.97 import weight 0.00
Epoch 233 Iter 7 subLoss 2847.0 multi 9.96 import weight 0.00
Epoch 233 Iter 8 subLoss 3562.4 multi 6.97 import weight 0.00
Epoch 233 Iter 9 subLoss 3356.6 multi -1.99 import weight 0.00
Epoch 233 Iter 10 subLoss 3140.5 multi 1.00 import weight 0.00
Epoch 233 Iter 11 subLoss 3586.3 multi 1.00 import weight 0.00
Epoch 233 Acc: 98.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 358 train Loss: 3208.5 test Loss: 306.3
Epoch 234 Iter 0 subLoss 2924.8 multi -7.96 import weight 0.00
Epoch 234 Iter 1 subLoss 3446.2 multi -4.97 import weight 0.00
Epoch 234 Iter 2 subLoss 3536.7 multi 1.00 import weight 0.00
Epoch 234 Iter 3 subLoss 3108.1 multi 6.97 import weight 0.00
Epoch 234 Iter 4 subLoss 2790.1 multi 12.94 import weight 0.00
Epoch 234 Iter 5 subLoss 3557.0 multi 1.00 import weight 0.00
Epoch 234 Iter 6 subLoss 2718.4 multi -4.97 import weight 0.00
Epoch 234 Iter 7 subLoss 3503.9 multi 18.91 import weight 0.00
Epoch 234 Iter 8 subLoss 3107.1 multi 9.96 import weight 0.00
Epoch 234 Iter 9 subLoss 3120.9 multi -1.99 import weight 0.00
Epoch 234 Iter 10 subLoss 2648.8 multi 1.00 import weight 0.00
Epoch 234 Iter 11 subLoss 3415.2 multi -1.99 import weight 0.00
Epoch 234 Acc: 98.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 341 train Loss: 3110.5 test Loss: 313.7
Epoch 235 Iter 0 subLoss 3290.5 multi 3.99 import weight 0.00
Epoch 235 Iter 1 subLoss 3191.4 multi 1.00 import weight 0.00
Epoch 235 Iter 2 subLoss 2878.9 multi 1.00 import weight 0.00
Epoch 235 Iter 3 subLoss 2838.0 multi -1.99 import weight 0.00
Epoch 235 Iter 4 subLoss 3231.0 multi 9.96 import weight 0.00
Epoch 235 Iter 5 subLoss 3293.9 multi 6.97 import weight 0.00
Epoch 235 Iter 6 subLoss 2527.3 multi 6.97 import weight 0.00
Epoch 235 Iter 7 subLoss 2786.7 multi 1.00 import weight 0.00
Epoch 235 Iter 8 subLoss 2900.8 multi 3.99 import weight 0.00
Epoch 235 Iter 9 subLoss 3146.4 multi 3.99 import weight 0.00
Epoch 235 Iter 10 subLoss 2907.6 multi 6.97 import weight 0.00
Epoch 235 Iter 11 subLoss 2925.8 multi -4.97 import weight 0.00
Epoch 235 Acc: 98.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 292 train Loss: 2968.1 test Loss: 269.7
Epoch 236 Iter 0 subLoss 2954.4 multi 9.96 import weight 0.00
Epoch 236 Iter 1 subLoss 2983.5 multi 3.98 import weight 0.00
Epoch 236 Iter 2 subLoss 2872.6 multi 3.98 import weight 0.00
Epoch 236 Iter 3 subLoss 2905.6 multi 9.96 import weight 0.00
Epoch 236 Iter 4 subLoss 2844.4 multi 9.96 import weight 0.00
Epoch 236 Iter 5 subLoss 2931.9 multi 3.99 import weight 0.00
Epoch 236 Iter 6 subLoss 2425.0 multi -10.94 import weight 0.00
Epoch 236 Iter 7 subLoss 2991.8 multi -1.99 import weight 0.00
Epoch 236 Iter 8 subLoss 2954.1 multi 12.94 import weight 0.00
Epoch 236 Iter 9 subLoss 2850.9 multi -10.94 import weight 0.00
Epoch 236 Iter 10 subLoss 2465.8 multi 3.99 import weight 0.00
Epoch 236 Iter 11 subLoss 2719.1 multi -1.99 import weight 0.00
Epoch 236 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 271 train Loss: 3024.9 test Loss: 323.8
Epoch 237 Iter 0 subLoss 3100.5 multi 12.94 import weight 0.00
Epoch 237 Iter 1 subLoss 2980.6 multi 6.97 import weight 0.00
Epoch 237 Iter 2 subLoss 2762.5 multi 15.93 import weight 0.00
Epoch 237 Iter 3 subLoss 2567.3 multi 6.97 import weight 0.00
Epoch 237 Iter 4 subLoss 2979.1 multi -7.96 import weight 0.00
Epoch 237 Iter 5 subLoss 2592.3 multi -13.93 import weight 0.00
Epoch 237 Iter 6 subLoss 3792.4 multi 1.00 import weight 0.00
Epoch 237 Iter 7 subLoss 3149.1 multi 6.97 import weight 0.00
Epoch 237 Iter 8 subLoss 3101.9 multi 15.93 import weight 0.00
Epoch 237 Iter 9 subLoss 2808.8 multi -10.94 import weight 0.00
Epoch 237 Iter 10 subLoss 3173.3 multi 6.97 import weight 0.00
Epoch 237 Iter 11 subLoss 2616.5 multi -4.97 import weight 0.00
Epoch 237 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 261 train Loss: 3025.9 test Loss: 297.2
Epoch 238 Iter 0 subLoss 3019.5 multi -7.96 import weight 0.00
Epoch 238 Iter 1 subLoss 4484.9 multi -13.93 import weight 0.00
Epoch 238 Iter 2 subLoss 74367.0 multi 1.00 import weight 0.00
Epoch 238 Iter 3 subLoss 4819.8 multi 1.00 import weight 0.00
Epoch 238 Iter 4 subLoss 4218.3 multi 6.97 import weight 0.00
Epoch 238 Iter 5 subLoss 3010.1 multi -4.97 import weight 0.00
Epoch 238 Iter 6 subLoss 3054.8 multi 3.98 import weight 0.00
Epoch 238 Iter 7 subLoss 3073.3 multi 1.00 import weight 0.00
Epoch 238 Iter 8 subLoss 3093.8 multi 3.98 import weight 0.00
Epoch 238 Iter 9 subLoss 2808.0 multi -7.96 import weight 0.00
Epoch 238 Iter 10 subLoss 3310.4 multi -4.97 import weight 0.00
Epoch 238 Iter 11 subLoss 3430.2 multi -13.93 import weight 0.00
Epoch 238 Acc: 96.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 343 train Loss: 4280.7 test Loss: 573.1
Epoch 239 Iter 0 subLoss 4058.6 multi -7.96 import weight 0.00
Epoch 239 Iter 1 subLoss 6846.5 multi -1.98 import weight 0.00
Epoch 239 Iter 2 subLoss 9833.9 multi -1.99 import weight 0.00
Epoch 239 Iter 3 subLoss 26422.4 multi 1.00 import weight 0.00
Epoch 239 Iter 4 subLoss 9244.7 multi 3.99 import weight 0.00
Epoch 239 Iter 5 subLoss 4401.6 multi -7.96 import weight 0.00
Epoch 239 Iter 6 subLoss 6654.7 multi 3.99 import weight 0.00
Epoch 239 Iter 7 subLoss 5708.9 multi -7.96 import weight 0.00
Epoch 239 Iter 8 subLoss 7815.3 multi 3.99 import weight 0.00
Epoch 239 Iter 9 subLoss 5810.2 multi 9.96 import weight 0.00
Epoch 239 Iter 10 subLoss 3711.7 multi 3.99 import weight 0.00
Epoch 239 Iter 11 subLoss 3283.1 multi 6.97 import weight 0.00
Epoch 239 Acc: 97.59 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 328 train Loss: 3263.8 test Loss: 388.4
Epoch 240 Iter 0 subLoss 2808.5 multi -4.97 import weight 0.00
Epoch 240 Iter 1 subLoss 3472.9 multi 6.97 import weight 0.00
Epoch 240 Iter 2 subLoss 3169.7 multi -1.98 import weight 0.00
Epoch 240 Iter 3 subLoss 3289.9 multi 9.96 import weight 0.00
Epoch 240 Iter 4 subLoss 3107.3 multi 15.93 import weight 0.00
Epoch 240 Iter 5 subLoss 2883.2 multi -7.96 import weight 0.00
Epoch 240 Iter 6 subLoss 3364.8 multi 6.97 import weight 0.00
Epoch 240 Iter 7 subLoss 2930.1 multi 6.97 import weight 0.00
Epoch 240 Iter 8 subLoss 2467.0 multi 6.97 import weight 0.00
Epoch 240 Iter 9 subLoss 2683.0 multi -1.98 import weight 0.00
Epoch 240 Iter 10 subLoss 2944.5 multi -19.90 import weight 0.00
Epoch 240 Iter 11 subLoss 2876.9 multi 6.97 import weight 0.00
Epoch 240 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 287 train Loss: 3147.4 test Loss: 310.4
Epoch 241 Iter 0 subLoss 2955.2 multi 12.94 import weight 0.00
Epoch 241 Iter 1 subLoss 2837.8 multi 1.00 import weight 0.00
Epoch 241 Iter 2 subLoss 2875.6 multi 9.96 import weight 0.00
Epoch 241 Iter 3 subLoss 2766.7 multi 18.91 import weight 0.00
Epoch 241 Iter 4 subLoss 3377.1 multi -16.91 import weight 0.00
Epoch 241 Iter 5 subLoss 6463.4 multi -4.97 import weight 0.00
Epoch 241 Iter 6 subLoss 28818.3 multi 3.99 import weight 0.00
Epoch 241 Iter 7 subLoss 21489.3 multi 1.00 import weight 0.00
Epoch 241 Iter 8 subLoss 11693.1 multi -1.99 import weight 0.00
Epoch 241 Iter 9 subLoss 18891.9 multi -1.99 import weight 0.00
Epoch 241 Iter 10 subLoss 57763.2 multi 1.00 import weight 0.00
Epoch 241 Iter 11 subLoss 13481.5 multi 3.99 import weight 0.00
Epoch 241 Acc: 94.65 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1348 train Loss: 7218.4 test Loss: 1163.9
Epoch 242 Iter 0 subLoss 7347.4 multi 1.00 import weight 0.00
Epoch 242 Iter 1 subLoss 6571.5 multi 6.97 import weight 0.00
Epoch 242 Iter 2 subLoss 4747.6 multi -10.94 import weight 0.00
Epoch 242 Iter 3 subLoss 7194.5 multi -4.97 import weight 0.00
Epoch 242 Iter 4 subLoss 11211.5 multi 1.00 import weight 0.00
Epoch 242 Iter 5 subLoss 9256.3 multi -4.97 import weight 0.00
Epoch 242 Iter 6 subLoss 18733.4 multi -1.99 import weight 0.00
Epoch 242 Iter 7 subLoss 49725.6 multi 1.00 import weight 0.00
Epoch 242 Iter 8 subLoss 13501.6 multi 3.99 import weight 0.00
Epoch 242 Iter 9 subLoss 7734.8 multi 3.98 import weight 0.00
Epoch 242 Iter 10 subLoss 5814.1 multi 12.94 import weight 0.00
Epoch 242 Iter 11 subLoss 4037.3 multi -4.97 import weight 0.00
Epoch 242 Acc: 96.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 403 train Loss: 4958.4 test Loss: 725.9
Epoch 243 Iter 0 subLoss 4952.1 multi 1.00 import weight 0.00
Epoch 243 Iter 1 subLoss 4429.3 multi -10.94 import weight 0.00
Epoch 243 Iter 2 subLoss 6126.7 multi 3.99 import weight 0.00
Epoch 243 Iter 3 subLoss 5187.8 multi 3.99 import weight 0.00
Epoch 243 Iter 4 subLoss 4585.0 multi 12.94 import weight 0.00
Epoch 243 Iter 5 subLoss 3417.2 multi 1.00 import weight 0.00
Epoch 243 Iter 6 subLoss 3974.0 multi -13.93 import weight 0.00
Epoch 243 Iter 7 subLoss 4227.5 multi -13.93 import weight 0.00
Epoch 243 Iter 8 subLoss 6631.4 multi -7.96 import weight 0.00
Epoch 243 Iter 9 subLoss 13269.0 multi 3.99 import weight 0.00
Epoch 243 Iter 10 subLoss 6231.2 multi -1.98 import weight 0.00
Epoch 243 Iter 11 subLoss 6652.8 multi 6.97 import weight 0.00
Epoch 243 Acc: 96.28 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 665 train Loss: 5038.5 test Loss: 763.4
Epoch 244 Iter 0 subLoss 5200.9 multi 3.99 import weight 0.00
Epoch 244 Iter 1 subLoss 4260.1 multi -4.97 import weight 0.00
Epoch 244 Iter 2 subLoss 5216.4 multi -4.97 import weight 0.00
Epoch 244 Iter 3 subLoss 5422.0 multi 1.00 import weight 0.00
Epoch 244 Iter 4 subLoss 5737.4 multi -4.97 import weight 0.00
Epoch 244 Iter 5 subLoss 6214.2 multi -7.96 import weight 0.00
Epoch 244 Iter 6 subLoss 9005.8 multi -1.98 import weight 0.00
Epoch 244 Iter 7 subLoss 11239.3 multi -1.99 import weight 0.00
Epoch 244 Iter 8 subLoss 15560.2 multi 1.00 import weight 0.00
Epoch 244 Iter 9 subLoss 11905.4 multi -1.99 import weight 0.00
Epoch 244 Iter 10 subLoss 16181.8 multi -1.99 import weight 0.00
Epoch 244 Iter 11 subLoss 32114.8 multi 1.00 import weight 0.00
Epoch 244 Acc: 83.97 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3211 train Loss: 15760.4 test Loss: 2868.8
Epoch 245 Iter 0 subLoss 15217.1 multi 1.00 import weight 0.00
Epoch 245 Iter 1 subLoss 14317.1 multi -1.99 import weight 0.00
Epoch 245 Iter 2 subLoss 17492.4 multi 1.00 import weight 0.00
Epoch 245 Iter 3 subLoss 13756.3 multi -4.97 import weight 0.00
Epoch 245 Iter 4 subLoss 33574.1 multi 1.00 import weight 0.00
Epoch 245 Iter 5 subLoss 20563.4 multi 3.99 import weight 0.00
Epoch 245 Iter 6 subLoss 12621.8 multi 1.00 import weight 0.00
Epoch 245 Iter 7 subLoss 11362.5 multi 3.99 import weight 0.00
Epoch 245 Iter 8 subLoss 8074.2 multi -1.99 import weight 0.00
Epoch 245 Iter 9 subLoss 9787.7 multi -4.97 import weight 0.00
Epoch 245 Iter 10 subLoss 12464.1 multi 12.94 import weight 0.00
Epoch 245 Iter 11 subLoss 6427.6 multi 3.99 import weight 0.00
Epoch 245 Acc: 95.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 642 train Loss: 5647.8 test Loss: 882.7
Epoch 246 Iter 0 subLoss 6109.5 multi -4.97 import weight 0.00
Epoch 246 Iter 1 subLoss 6752.5 multi 6.97 import weight 0.00
Epoch 246 Iter 2 subLoss 5190.8 multi -13.93 import weight 0.00
Epoch 246 Iter 3 subLoss 7100.3 multi -1.99 import weight 0.00
Epoch 246 Iter 4 subLoss 8137.3 multi 6.97 import weight 0.00
Epoch 246 Iter 5 subLoss 6000.8 multi -4.97 import weight 0.00
Epoch 246 Iter 6 subLoss 6994.0 multi 6.97 import weight 0.00
Epoch 246 Iter 7 subLoss 5832.5 multi 9.96 import weight 0.00
Epoch 246 Iter 8 subLoss 4430.8 multi 6.97 import weight 0.00
Epoch 246 Iter 9 subLoss 4194.8 multi 3.99 import weight 0.00
Epoch 246 Iter 10 subLoss 4002.1 multi -10.94 import weight 0.00
Epoch 246 Iter 11 subLoss 4689.1 multi 3.98 import weight 0.00
Epoch 246 Acc: 97.18 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 468 train Loss: 4335.2 test Loss: 612.7
Epoch 247 Iter 0 subLoss 4664.6 multi 6.97 import weight 0.00
Epoch 247 Iter 1 subLoss 3878.9 multi 1.00 import weight 0.00
Epoch 247 Iter 2 subLoss 4220.6 multi -10.94 import weight 0.00
Epoch 247 Iter 3 subLoss 4275.8 multi 1.00 import weight 0.00
Epoch 247 Iter 4 subLoss 4364.0 multi 15.93 import weight 0.00
Epoch 247 Iter 5 subLoss 4091.1 multi -1.98 import weight 0.00
Epoch 247 Iter 6 subLoss 3206.6 multi 1.00 import weight 0.00
Epoch 247 Iter 7 subLoss 3825.8 multi 1.00 import weight 0.00
Epoch 247 Iter 8 subLoss 3811.1 multi 9.96 import weight 0.00
Epoch 247 Iter 9 subLoss 3539.4 multi 3.99 import weight 0.00
Epoch 247 Iter 10 subLoss 3612.3 multi 1.00 import weight 0.00
Epoch 247 Iter 11 subLoss 3288.7 multi 12.94 import weight 0.00
Epoch 247 Acc: 97.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 328 train Loss: 3261.2 test Loss: 426.6
Epoch 248 Iter 0 subLoss 3571.2 multi 3.99 import weight 0.00
Epoch 248 Iter 1 subLoss 3152.9 multi -10.94 import weight 0.00
Epoch 248 Iter 2 subLoss 3675.2 multi 3.98 import weight 0.00
Epoch 248 Iter 3 subLoss 3317.5 multi -1.99 import weight 0.00
Epoch 248 Iter 4 subLoss 3472.5 multi 9.96 import weight 0.00
Epoch 248 Iter 5 subLoss 3309.8 multi -10.94 import weight 0.00
Epoch 248 Iter 6 subLoss 3708.6 multi -7.96 import weight 0.00
Epoch 248 Iter 7 subLoss 4029.1 multi 9.96 import weight 0.00
Epoch 248 Iter 8 subLoss 3048.4 multi -1.98 import weight 0.00
Epoch 248 Iter 9 subLoss 3297.6 multi 1.00 import weight 0.00
Epoch 248 Iter 10 subLoss 3806.0 multi -7.96 import weight 0.00
Epoch 248 Iter 11 subLoss 3968.2 multi 9.96 import weight 0.00
Epoch 248 Acc: 97.61 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 396 train Loss: 3402.2 test Loss: 425.9
Epoch 249 Iter 0 subLoss 2994.9 multi -1.99 import weight 0.00
Epoch 249 Iter 1 subLoss 3339.4 multi 3.98 import weight 0.00
Epoch 249 Iter 2 subLoss 3857.1 multi 1.00 import weight 0.00
Epoch 249 Iter 3 subLoss 2796.8 multi 12.94 import weight 0.00
Epoch 249 Iter 4 subLoss 3634.2 multi 1.00 import weight 0.00
Epoch 249 Iter 5 subLoss 3462.9 multi -7.96 import weight 0.00
Epoch 249 Iter 6 subLoss 3790.1 multi 3.99 import weight 0.00
Epoch 249 Iter 7 subLoss 2935.7 multi 9.96 import weight 0.00
Epoch 249 Iter 8 subLoss 3269.7 multi -1.99 import weight 0.00
Epoch 249 Iter 9 subLoss 3438.7 multi -10.94 import weight 0.00
Epoch 249 Iter 10 subLoss 3455.9 multi 6.97 import weight 0.00
Epoch 249 Iter 11 subLoss 3149.1 multi 9.96 import weight 0.00
Epoch 249 Acc: 97.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 314 train Loss: 3195.9 test Loss: 388.0
Epoch 250 Iter 0 subLoss 3230.9 multi 12.94 import weight 0.00
Epoch 250 Iter 1 subLoss 3222.4 multi -10.94 import weight 0.00
Epoch 250 Iter 2 subLoss 3515.6 multi -7.96 import weight 0.00
Epoch 250 Iter 3 subLoss 3536.4 multi 6.97 import weight 0.00
Epoch 250 Iter 4 subLoss 2990.5 multi 1.00 import weight 0.00
Epoch 250 Iter 5 subLoss 3053.4 multi 3.99 import weight 0.00
Epoch 250 Iter 6 subLoss 3169.0 multi -1.99 import weight 0.00
Epoch 250 Iter 7 subLoss 3096.3 multi 6.97 import weight 0.00
Epoch 250 Iter 8 subLoss 2571.3 multi -1.99 import weight 0.00
Epoch 250 Iter 9 subLoss 2575.0 multi 1.00 import weight 0.00
Epoch 250 Iter 10 subLoss 3329.3 multi -7.96 import weight 0.00
Epoch 250 Iter 11 subLoss 3258.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0072 / 0.13955 / 10.91
Entropy seen (from low to high)
[3658, 481, 237, 130, 73, 63, 47, 43, 34, 30, 32, 20, 24, 15, 21, 16, 13, 15, 15, 21, 24, 22, 18, 7, 9, 11, 8, 9, 3, 7, 2, 6, 2, 6, 5, 3, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 4, 11, 8, 12, 28, 34, 49, 53, 45, 57, 59, 68, 154, 126, 122, 108, 111, 106, 131, 134, 114, 131, 137, 142, 144, 109, 112, 117, 92, 101, 92, 112, 105, 111, 125, 133, 130, 130, 172, 169, 195, 237, 307, 224]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.0, 32.8, 37.3, 41.4, 44.0, 47.8, 51.1, 54.2, 57.8, 60.9, 64.9, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 99.9, 66.6, 24.9, 40.9, 57.1, 49.9, 55.5, 71.4, 89.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 2, 3, 4, 22, 14, 18, 18, 21, 19]
Epoch 250 Acc: 97.68 BMA: 97.68 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 325 train Loss: 3295.0 test Loss: 388.5
Epoch 251 Iter 0 subLoss 3794.1 multi 6.97 import weight 0.00
Epoch 251 Iter 1 subLoss 2985.2 multi 6.97 import weight 0.00
Epoch 251 Iter 2 subLoss 2893.7 multi -1.99 import weight 0.00
Epoch 251 Iter 3 subLoss 2584.7 multi 3.99 import weight 0.00
Epoch 251 Iter 4 subLoss 3430.3 multi -7.96 import weight 0.00
Epoch 251 Iter 5 subLoss 3208.3 multi 3.99 import weight 0.00
Epoch 251 Iter 6 subLoss 3793.4 multi 9.96 import weight 0.00
Epoch 251 Iter 7 subLoss 2862.8 multi 1.00 import weight 0.00
Epoch 251 Iter 8 subLoss 3033.3 multi -1.99 import weight 0.00
Epoch 251 Iter 9 subLoss 2917.7 multi -1.99 import weight 0.00
Epoch 251 Iter 10 subLoss 2984.6 multi 9.96 import weight 0.00
Epoch 251 Iter 11 subLoss 2857.7 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0070 / 0.13680 / 9.01
Entropy seen (from low to high)
[3649, 457, 236, 138, 93, 54, 61, 46, 32, 32, 17, 25, 26, 17, 15, 24, 13, 18, 15, 29, 23, 15, 9, 11, 13, 13, 9, 1, 5, 4, 6, 11, 3, 5, 3, 2, 0, 2, 3, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 4, 3, 4, 17, 22, 33, 31, 52, 49, 53, 73, 46, 114, 118, 137, 122, 106, 122, 104, 114, 138, 132, 133, 171, 154, 130, 142, 107, 101, 99, 116, 87, 122, 118, 103, 128, 125, 148, 146, 158, 181, 181, 221, 250, 146]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 37.0, 41.5, 43.6, 47.8, 51.1, 54.3, 57.9, 61.2, 65.0, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 99.9, 33.3, 79.9, 57.1, 38.4, 66.6, 56.2, 56.2, 71.9, 66.6]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 5, 7, 13, 21, 16, 16, 25, 27]
Epoch 251 Acc: 97.86 BMA: 97.86 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 285 train Loss: 3249.2 test Loss: 391.7
Epoch 252 Iter 0 subLoss 2778.9 multi -19.90 import weight 0.00
Epoch 252 Iter 1 subLoss 4951.3 multi 3.99 import weight 0.00
Epoch 252 Iter 2 subLoss 2931.3 multi 12.94 import weight 0.00
Epoch 252 Iter 3 subLoss 2686.9 multi 1.00 import weight 0.00
Epoch 252 Iter 4 subLoss 3892.3 multi -1.99 import weight 0.00
Epoch 252 Iter 5 subLoss 2935.9 multi 15.93 import weight 1.00
Epoch 252 Iter 6 subLoss 3610.2 multi 3.98 import weight 0.00
Epoch 252 Iter 7 subLoss 3228.8 multi -7.96 import weight 0.00
Epoch 252 Iter 8 subLoss 3126.3 multi 1.00 import weight 0.00
Epoch 252 Iter 9 subLoss 2626.6 multi 12.94 import weight 0.00
Epoch 252 Iter 10 subLoss 2963.9 multi -7.96 import weight 0.00
Epoch 252 Iter 11 subLoss 3763.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0068 / 0.13708 / 12.25
Entropy seen (from low to high)
[3652, 468, 248, 124, 78, 67, 53, 42, 34, 31, 24, 21, 24, 17, 22, 19, 14, 21, 19, 19, 22, 13, 10, 15, 13, 10, 8, 4, 3, 6, 6, 7, 6, 6, 2, 2, 2, 1, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 5, 1, 2, 13, 25, 27, 44, 44, 45, 63, 59, 64, 91, 136, 125, 121, 132, 108, 92, 122, 132, 125, 143, 149, 164, 145, 136, 111, 116, 104, 99, 97, 99, 121, 111, 140, 113, 142, 146, 133, 183, 198, 219, 256, 160]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 33.9, 37.8, 40.1, 43.6, 47.2, 50.8, 54.5, 57.6, 61.3, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 77.7, 46.6, 55.5, 49.9, 78.2, 73.3, 61.5]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 3, 9, 15, 18, 18, 23, 15, 26]
Epoch 252 Acc: 97.72 BMA: 97.90 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 376 train Loss: 3121.8 test Loss: 370.1
Epoch 253 Iter 0 subLoss 2799.4 multi 15.93 import weight 0.00
Epoch 253 Iter 1 subLoss 2637.1 multi -13.93 import weight 0.00
Epoch 253 Iter 2 subLoss 3536.5 multi 9.96 import weight 0.00
Epoch 253 Iter 3 subLoss 3245.7 multi -4.97 import weight 0.00
Epoch 253 Iter 4 subLoss 3200.4 multi 6.97 import weight 0.00
Epoch 253 Iter 5 subLoss 3206.7 multi 9.96 import weight 0.00
Epoch 253 Iter 6 subLoss 2683.1 multi 3.99 import weight 0.00
Epoch 253 Iter 7 subLoss 3148.7 multi 12.94 import weight 0.00
Epoch 253 Iter 8 subLoss 3016.7 multi -1.99 import weight 0.00
Epoch 253 Iter 9 subLoss 2342.1 multi 1.00 import weight 0.00
Epoch 253 Iter 10 subLoss 2800.3 multi -7.96 import weight 0.00
Epoch 253 Iter 11 subLoss 2700.3 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0065 / 0.13642 / 10.12
Entropy seen (from low to high)
[3617, 491, 243, 137, 78, 64, 59, 37, 41, 29, 19, 24, 24, 17, 19, 25, 18, 16, 11, 25, 26, 12, 7, 16, 14, 10, 6, 7, 7, 4, 4, 6, 6, 7, 2, 0, 3, 1, 2, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 1, 3, 2, 1, 13, 26, 33, 37, 43, 44, 62, 69, 69, 89, 127, 132, 127, 125, 114, 113, 112, 139, 136, 143, 155, 152, 132, 127, 106, 127, 115, 112, 99, 92, 113, 123, 132, 129, 127, 133, 149, 178, 179, 228, 241, 152]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 33.7, 37.4, 40.4, 43.6, 48.1, 50.6, 54.4, 58.1, 60.9, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 24.9, 49.9, 66.6, 63.1, 45.4, 56.2, 63.1, 76.4, 79.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 4, 2, 6, 19, 22, 16, 19, 17, 20]
Epoch 253 Acc: 98.07 BMA: 97.94 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 270 train Loss: 2939.7 test Loss: 308.0
Epoch 254 Iter 0 subLoss 2913.5 multi 1.00 import weight 0.00
Epoch 254 Iter 1 subLoss 2895.1 multi 1.00 import weight 0.00
Epoch 254 Iter 2 subLoss 2620.0 multi -1.98 import weight 0.00
Epoch 254 Iter 3 subLoss 3196.3 multi 3.98 import weight 0.00
Epoch 254 Iter 4 subLoss 2813.0 multi -1.99 import weight 0.00
Epoch 254 Iter 5 subLoss 3002.3 multi -4.97 import weight 0.00
Epoch 254 Iter 6 subLoss 3197.3 multi 6.97 import weight 0.00
Epoch 254 Iter 7 subLoss 3131.4 multi -4.97 import weight 0.00
Epoch 254 Iter 8 subLoss 3093.8 multi 9.96 import weight 0.00
Epoch 254 Iter 9 subLoss 2586.1 multi 6.97 import weight 0.00
Epoch 254 Iter 10 subLoss 2386.4 multi 1.00 import weight 0.00
Epoch 254 Iter 11 subLoss 2769.2 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.13696 / 15.82
Entropy seen (from low to high)
[3647, 496, 233, 134, 80, 57, 44, 46, 32, 23, 23, 29, 27, 16, 17, 25, 16, 15, 19, 16, 26, 13, 10, 12, 10, 11, 12, 7, 9, 2, 4, 5, 7, 5, 0, 1, 2, 2, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 1, 1, 4, 6, 9, 24, 32, 32, 44, 48, 57, 64, 77, 75, 134, 124, 124, 120, 121, 112, 137, 133, 123, 129, 163, 130, 142, 116, 109, 148, 106, 115, 112, 96, 98, 115, 132, 124, 132, 134, 157, 173, 177, 233, 251, 167]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.6, 40.0, 43.7, 47.4, 50.4, 53.7, 57.7, 60.9, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 0.0, 22.2, 70.5, 57.1, 46.1, 56.2, 94.4, 73.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 2, 9, 17, 14, 26, 16, 18, 15]
Epoch 254 Acc: 97.96 BMA: 97.94 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 276 train Loss: 2883.6 test Loss: 342.1
Epoch 255 Iter 0 subLoss 3038.6 multi 1.00 import weight 0.00
Epoch 255 Iter 1 subLoss 3303.8 multi -10.94 import weight 0.00
Epoch 255 Iter 2 subLoss 3370.8 multi -13.93 import weight 0.00
Epoch 255 Iter 3 subLoss 9405.4 multi 3.98 import weight 0.00
Epoch 255 Iter 4 subLoss 2711.0 multi -1.99 import weight 0.00
Epoch 255 Iter 5 subLoss 3571.9 multi 6.97 import weight 0.00
Epoch 255 Iter 6 subLoss 2767.2 multi 24.88 import weight 0.00
Epoch 255 Iter 7 subLoss 2891.1 multi 3.98 import weight 0.00
Epoch 255 Iter 8 subLoss 2534.4 multi 1.00 import weight 0.00
Epoch 255 Iter 9 subLoss 2375.0 multi -1.98 import weight 0.00
Epoch 255 Iter 10 subLoss 2794.7 multi 18.91 import weight 0.00
Epoch 255 Iter 11 subLoss 2248.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.13686 / 11.14
Entropy seen (from low to high)
[3668, 500, 219, 122, 89, 62, 43, 44, 24, 21, 30, 27, 22, 17, 22, 21, 16, 18, 17, 21, 16, 16, 12, 10, 13, 11, 6, 9, 9, 3, 5, 5, 7, 1, 3, 1, 2, 1, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 1, 1, 5, 7, 6, 30, 29, 37, 37, 45, 52, 64, 65, 87, 136, 125, 123, 144, 116, 121, 125, 147, 118, 127, 130, 157, 122, 127, 135, 116, 116, 114, 105, 118, 100, 105, 117, 121, 134, 130, 156, 163, 174, 225, 268, 180]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.8, 0.0, 37.6, 40.1, 43.8, 48.0, 50.8, 54.3, 57.5, 61.1, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 49.9, 83.3, 37.4, 71.4, 49.9, 59.9, 58.3, 88.2, 64.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 2, 6, 8, 14, 18, 15, 24, 17, 17]
Epoch 255 Acc: 98.07 BMA: 98.07 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 224 train Loss: 2856.1 test Loss: 306.7
Epoch 256 Iter 0 subLoss 2688.5 multi 6.97 import weight 0.00
Epoch 256 Iter 1 subLoss 2653.6 multi 1.00 import weight 0.00
Epoch 256 Iter 2 subLoss 2365.2 multi 1.00 import weight 0.00
Epoch 256 Iter 3 subLoss 2316.9 multi 1.00 import weight 0.00
Epoch 256 Iter 4 subLoss 2793.6 multi 21.90 import weight 0.00
Epoch 256 Iter 5 subLoss 3466.0 multi -7.96 import weight 0.00
Epoch 256 Iter 6 subLoss 2975.8 multi -7.96 import weight 0.00
Epoch 256 Iter 7 subLoss 3041.5 multi -4.97 import weight 0.00
Epoch 256 Iter 8 subLoss 3261.1 multi -1.98 import weight 0.00
Epoch 256 Iter 9 subLoss 4269.7 multi -1.98 import weight 0.00
Epoch 256 Iter 10 subLoss 5710.2 multi -4.97 import weight 0.00
Epoch 256 Iter 11 subLoss 21247.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0109 / 0.10853 / 11.02
Entropy seen (from low to high)
[1673, 94, 59, 37, 26, 29, 19, 23, 31, 36, 61, 1257, 656, 344, 181, 127, 63, 54, 39, 21, 33, 36, 23, 26, 19, 19, 13, 20, 24, 16, 13, 13, 10, 8, 8, 4, 9, 4, 2, 2, 2, 1, 4, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 12, 12, 32, 34, 56, 78, 115, 110, 112, 107, 118, 120, 152, 187, 183, 189, 182, 194, 174, 157, 180, 156, 148, 144, 133, 160, 173, 177, 191, 210, 224, 287, 228, 31, 13, 6, 7, 7, 8, 6, 8, 10, 7, 10, 13]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 33.6, 37.3, 40.8, 43.6, 47.0, 50.7, 54.2, 57.8, 61.2, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 99.9, 44.4, 54.5, 52.6, 55.5, 76.1, 84.6, 61.2, 70.9, 79.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 4, 9, 11, 19, 18, 21, 13, 31, 31, 29]
Epoch 256 Acc: 39.25 BMA: 95.78 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 2124 train Loss: 270972.7 test Loss: 49394.2
Epoch 257 Iter 0 subLoss 253646.4 multi 1.00 import weight 0.00
Epoch 257 Iter 1 subLoss 27359.6 multi -4.97 import weight 0.00
Epoch 257 Iter 2 subLoss 393783.8 multi 1.00 import weight 0.00
Epoch 257 Iter 3 subLoss 60430.4 multi 1.00 import weight 0.00
Epoch 257 Iter 4 subLoss 30476.1 multi 1.00 import weight 0.00
Epoch 257 Iter 5 subLoss 26114.6 multi 1.00 import weight 0.00
Epoch 257 Iter 6 subLoss 23263.1 multi 1.00 import weight 0.00
Epoch 257 Iter 7 subLoss 22054.1 multi 1.00 import weight 0.00
Epoch 257 Iter 8 subLoss 19569.3 multi -1.99 import weight 0.00
Epoch 257 Iter 9 subLoss 22388.8 multi 1.00 import weight 0.00
Epoch 257 Iter 10 subLoss 21152.2 multi 1.00 import weight 0.00
Epoch 257 Iter 11 subLoss 19665.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0112 / 0.10349 / 12.82
Entropy seen (from low to high)
[230, 156, 105, 79, 65, 35, 52, 49, 56, 42, 396, 610, 728, 945, 510, 220, 155, 104, 94, 67, 59, 55, 37, 39, 25, 22, 30, 29, 18, 26, 12, 16, 16, 10, 14, 10, 6, 4, 3, 3, 2, 1, 4, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 14, 29, 31, 55, 80, 88, 129, 141, 137, 138, 145, 141, 171, 207, 214, 228, 223, 210, 205, 205, 167, 178, 144, 162, 146, 129, 159, 135, 182, 161, 146, 165, 128, 36, 7, 3, 4, 0, 6, 2, 3, 0, 4, 1, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.9, 0.0, 33.4, 36.9, 40.6, 44.1, 47.2, 50.8, 54.3, 57.8, 61.6, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 99.9, 44.4, 58.8, 49.9, 57.1, 70.8, 67.6, 59.9, 76.4, 87.1]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 6, 9, 17, 14, 21, 24, 34, 20, 34, 39]
Epoch 257 Acc: 72.76 BMA: 96.28 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1966 train Loss: 19276.5 test Loss: 3397.2
Epoch 258 Iter 0 subLoss 19380.0 multi 1.00 import weight 0.00
Epoch 258 Iter 1 subLoss 18079.7 multi 6.97 import weight 0.00
Epoch 258 Iter 2 subLoss 14530.9 multi 1.00 import weight 0.00
Epoch 258 Iter 3 subLoss 13304.0 multi 3.99 import weight 0.00
Epoch 258 Iter 4 subLoss 11054.8 multi -1.99 import weight 0.00
Epoch 258 Iter 5 subLoss 12684.7 multi 3.99 import weight 0.00
Epoch 258 Iter 6 subLoss 9566.2 multi -1.99 import weight 0.00
Epoch 258 Iter 7 subLoss 11382.2 multi -1.99 import weight 0.00
Epoch 258 Iter 8 subLoss 12188.3 multi -1.99 import weight 0.00
Epoch 258 Iter 9 subLoss 13286.6 multi 1.00 import weight 0.00
Epoch 258 Iter 10 subLoss 13045.7 multi 6.97 import weight 0.00
Epoch 258 Iter 11 subLoss 9698.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0120 / 0.10233 / 13.82
Entropy seen (from low to high)
[244, 165, 103, 76, 61, 36, 60, 50, 49, 130, 752, 448, 370, 306, 208, 164, 171, 233, 300, 493, 268, 82, 45, 41, 36, 24, 26, 34, 23, 21, 18, 21, 14, 14, 12, 10, 12, 4, 4, 3, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 22, 25, 34, 60, 71, 87, 139, 160, 178, 155, 141, 164, 204, 241, 226, 242, 218, 202, 181, 179, 148, 152, 132, 144, 143, 126, 136, 134, 140, 146, 142, 120, 140, 84, 18, 4, 4, 1, 2, 4, 2, 2, 4, 1, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 36.9, 40.5, 44.0, 47.4, 51.0, 54.2, 57.9, 61.3, 65.0, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 59.9, 90.9, 38.4, 52.9, 77.7, 54.8, 77.2, 67.5, 74.0, 85.9]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 11, 13, 17, 27, 31, 22, 37, 27, 50]
Epoch 258 Acc: 81.94 BMA: 96.61 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 969 train Loss: 12740.9 test Loss: 2258.6
Epoch 259 Iter 0 subLoss 12874.9 multi 3.99 import weight 0.00
Epoch 259 Iter 1 subLoss 9267.7 multi 1.00 import weight 0.00
Epoch 259 Iter 2 subLoss 8366.4 multi 1.00 import weight 0.00
Epoch 259 Iter 3 subLoss 8153.0 multi -7.96 import weight 0.00
Epoch 259 Iter 4 subLoss 12660.1 multi 6.97 import weight 0.00
Epoch 259 Iter 5 subLoss 9238.4 multi -7.96 import weight 0.00
Epoch 259 Iter 6 subLoss 52428.0 multi 1.00 import weight 0.00
Epoch 259 Iter 7 subLoss 12432.9 multi 1.00 import weight 0.00
Epoch 259 Iter 8 subLoss 10699.4 multi 3.99 import weight 0.00
Epoch 259 Iter 9 subLoss 8833.5 multi 1.00 import weight 0.00
Epoch 259 Iter 10 subLoss 7925.4 multi -1.98 import weight 0.00
Epoch 259 Iter 11 subLoss 8684.9 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0120 / 0.10233 / 14.27
Entropy seen (from low to high)
[265, 158, 105, 78, 49, 57, 48, 52, 71, 669, 460, 401, 311, 228, 183, 137, 136, 150, 164, 199, 241, 298, 251, 106, 47, 37, 30, 30, 34, 21, 17, 19, 18, 9, 14, 16, 8, 6, 4, 5, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 21, 25, 43, 58, 68, 92, 136, 175, 173, 180, 151, 155, 233, 232, 239, 214, 198, 215, 178, 170, 168, 138, 115, 137, 123, 133, 121, 116, 128, 148, 136, 113, 134, 108, 52, 9, 3, 2, 2, 4, 3, 2, 3, 1, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.4, 37.2, 40.7, 44.0, 47.3, 50.8, 54.4, 57.8, 61.0, 64.7, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.9, 85.7, 71.4, 49.9, 72.2, 65.5, 68.9, 59.9, 65.6, 81.2, 86.2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 7, 16, 18, 29, 29, 30, 32, 32, 51]
Epoch 259 Acc: 95.93 BMA: 96.83 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 868 train Loss: 7441.7 test Loss: 1107.0
Epoch 260 Iter 0 subLoss 7769.5 multi 1.00 import weight 0.00
Epoch 260 Iter 1 subLoss 7224.6 multi 6.97 import weight 0.00
Epoch 260 Iter 2 subLoss 5289.6 multi -7.96 import weight 0.00
Epoch 260 Iter 3 subLoss 6295.7 multi -7.96 import weight 0.00
Epoch 260 Iter 4 subLoss 9016.9 multi -4.97 import weight 0.00
Epoch 260 Iter 5 subLoss 12375.9 multi 3.99 import weight 0.00
Epoch 260 Iter 6 subLoss 8561.1 multi 1.00 import weight 0.00
Epoch 260 Iter 7 subLoss 8309.2 multi -1.99 import weight 0.00
Epoch 260 Iter 8 subLoss 8921.0 multi 3.99 import weight 0.00
Epoch 260 Iter 9 subLoss 7610.2 multi 1.00 import weight 0.00
Epoch 260 Iter 10 subLoss 7410.4 multi 3.98 import weight 0.00
Epoch 260 Iter 11 subLoss 6417.8 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0121 / 0.10277 / 15.81
Entropy seen (from low to high)
[283, 159, 103, 77, 44, 61, 61, 58, 424, 542, 442, 323, 273, 194, 156, 132, 114, 140, 135, 163, 159, 201, 183, 197, 162, 76, 53, 38, 41, 14, 25, 13, 15, 18, 11, 17, 6, 8, 6, 5, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 24, 28, 40, 57, 71, 96, 144, 158, 177, 187, 168, 188, 205, 224, 236, 211, 184, 210, 174, 156, 169, 135, 133, 123, 129, 109, 117, 105, 136, 125, 134, 124, 116, 126, 90, 24, 6, 2, 2, 4, 3, 2, 3, 1, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.7, 34.0, 35.8, 40.8, 44.0, 47.4, 50.8, 54.4, 57.9, 61.6, 64.9, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 66.6, 66.6, 47.6, 68.4, 73.0, 70.9, 55.2, 79.9, 81.0, 89.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 3, 9, 21, 19, 26, 31, 38, 30, 37, 49]
Epoch 260 Acc: 95.93 BMA: 96.79 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 641 train Loss: 6913.7 test Loss: 1020.8
Epoch 261 Iter 0 subLoss 6416.7 multi 1.00 import weight 0.00
Epoch 261 Iter 1 subLoss 6425.6 multi 1.00 import weight 0.00
Epoch 261 Iter 2 subLoss 6860.5 multi 1.00 import weight 0.00
Epoch 261 Iter 3 subLoss 6052.5 multi 1.00 import weight 0.00
Epoch 261 Iter 4 subLoss 5944.5 multi -4.97 import weight 0.00
Epoch 261 Iter 5 subLoss 6802.7 multi -7.96 import weight 0.00
Epoch 261 Iter 6 subLoss 8555.2 multi 3.99 import weight 0.00
Epoch 261 Iter 7 subLoss 7963.6 multi 18.91 import weight 0.00
Epoch 261 Iter 8 subLoss 5594.7 multi 15.93 import weight 0.00
Epoch 261 Iter 9 subLoss 6608.1 multi 1.00 import weight 0.00
Epoch 261 Iter 10 subLoss 5793.1 multi -10.94 import weight 0.00
Epoch 261 Iter 11 subLoss 21376.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0122 / 0.10051 / 15.92
Entropy seen (from low to high)
[286, 164, 104, 72, 51, 58, 55, 62, 488, 367, 360, 325, 255, 202, 203, 163, 181, 158, 190, 163, 228, 221, 213, 154, 81, 48, 47, 40, 33, 28, 21, 18, 16, 16, 20, 11, 8, 8, 8, 7, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 21, 35, 48, 53, 71, 119, 119, 163, 189, 195, 187, 217, 221, 253, 219, 249, 216, 192, 167, 150, 170, 140, 139, 127, 131, 108, 103, 101, 109, 107, 115, 97, 101, 97, 66, 33, 6, 4, 3, 1, 6, 2, 2, 2, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.1, 33.0, 36.3, 40.4, 43.7, 46.8, 50.5, 54.3, 57.8, 61.2, 65.1, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 66.6, 72.7, 49.9, 66.6, 67.7, 63.8, 62.4, 75.6, 89.3, 83.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 3, 11, 16, 24, 31, 36, 32, 41, 47, 60]
Epoch 261 Acc: 87.88 BMA: 96.73 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2137 train Loss: 12936.4 test Loss: 1944.8
Epoch 262 Iter 0 subLoss 12495.9 multi 1.00 import weight 0.00
Epoch 262 Iter 1 subLoss 9981.7 multi 3.99 import weight 0.00
Epoch 262 Iter 2 subLoss 5231.4 multi 6.97 import weight 0.00
Epoch 262 Iter 3 subLoss 4016.7 multi -7.96 import weight 0.00
Epoch 262 Iter 4 subLoss 4536.5 multi 1.00 import weight 0.00
Epoch 262 Iter 5 subLoss 4550.6 multi 1.00 import weight 0.00
Epoch 262 Iter 6 subLoss 4395.1 multi -1.98 import weight 0.00
Epoch 262 Iter 7 subLoss 4588.4 multi 15.93 import weight 1.00
Epoch 262 Iter 8 subLoss 3114.6 multi -13.93 import weight 0.00
Epoch 262 Iter 9 subLoss 4386.4 multi 3.98 import weight 0.00
Epoch 262 Iter 10 subLoss 4193.6 multi 6.97 import weight 0.00
Epoch 262 Iter 11 subLoss 3518.0 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0114 / 0.10184 / 14.77
Entropy seen (from low to high)
[300, 161, 107, 72, 56, 60, 53, 268, 447, 390, 357, 283, 230, 219, 178, 184, 170, 198, 167, 239, 222, 211, 137, 85, 42, 45, 36, 43, 27, 21, 21, 20, 14, 12, 16, 14, 9, 4, 9, 5, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 20, 36, 40, 54, 75, 108, 123, 158, 174, 201, 183, 204, 222, 228, 236, 217, 216, 200, 167, 161, 144, 165, 132, 128, 118, 131, 94, 107, 106, 105, 102, 110, 104, 103, 97, 49, 15, 5, 3, 2, 5, 2, 1, 3, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 33.3, 37.1, 40.7, 43.8, 47.6, 50.9, 54.2, 57.9, 61.3, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 79.9, 33.3, 59.9, 57.1, 71.4, 60.7, 66.6, 79.9, 80.9, 83.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 9, 20, 21, 28, 28, 36, 35, 42, 56]
Epoch 262 Acc: 97.90 BMA: 96.91 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 351 train Loss: 3749.6 test Loss: 350.3
Epoch 263 Iter 0 subLoss 3363.6 multi 9.96 import weight 0.00
Epoch 263 Iter 1 subLoss 3343.7 multi 3.99 import weight 0.00
Epoch 263 Iter 2 subLoss 3291.2 multi 3.99 import weight 0.00
Epoch 263 Iter 3 subLoss 3120.6 multi 1.00 import weight 0.00
Epoch 263 Iter 4 subLoss 3372.3 multi -13.93 import weight 0.00
Epoch 263 Iter 5 subLoss 3749.5 multi -13.93 import weight 0.00
Epoch 263 Iter 6 subLoss 5582.5 multi 1.00 import weight 0.00
Epoch 263 Iter 7 subLoss 5595.4 multi 15.93 import weight 0.00
Epoch 263 Iter 8 subLoss 8548.1 multi -10.94 import weight 0.00
Epoch 263 Iter 9 subLoss 155134.8 multi 1.00 import weight 0.00
Epoch 263 Iter 10 subLoss 20751.7 multi 1.00 import weight 0.00
Epoch 263 Iter 11 subLoss 15354.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0118 / 0.09904 / 17.52
Entropy seen (from low to high)
[302, 157, 103, 59, 62, 57, 59, 403, 295, 319, 245, 231, 193, 205, 219, 241, 254, 241, 276, 269, 220, 156, 99, 76, 52, 55, 35, 43, 32, 33, 26, 25, 13, 15, 16, 16, 9, 6, 9, 3, 5, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 9, 22, 38, 41, 75, 93, 122, 144, 170, 194, 193, 205, 242, 221, 245, 252, 230, 201, 186, 180, 161, 133, 141, 127, 131, 114, 86, 88, 98, 93, 108, 93, 97, 85, 93, 68, 48, 18, 3, 2, 4, 4, 0, 1, 2, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.5, 33.3, 37.0, 40.6, 44.0, 47.5, 51.0, 54.5, 57.7, 61.2, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 49.9, 69.9, 58.3, 62.9, 69.5, 58.0, 74.3, 84.8, 75.4, 95.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 6, 10, 24, 27, 23, 31, 39, 33, 61, 69]
Epoch 263 Acc: 84.16 BMA: 96.96 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1535 train Loss: 12671.7 test Loss: 2073.8
Epoch 264 Iter 0 subLoss 12540.1 multi 3.99 import weight 0.00
Epoch 264 Iter 1 subLoss 8724.0 multi 3.99 import weight 0.00
Epoch 264 Iter 2 subLoss 6487.2 multi 9.96 import weight 0.00
Epoch 264 Iter 3 subLoss 3690.0 multi 6.97 import weight 0.00
Epoch 264 Iter 4 subLoss 3635.5 multi 3.99 import weight 0.00
Epoch 264 Iter 5 subLoss 4139.8 multi 6.97 import weight 0.00
Epoch 264 Iter 6 subLoss 3476.8 multi 6.97 import weight 0.00
Epoch 264 Iter 7 subLoss 3623.1 multi -7.96 import weight 0.00
Epoch 264 Iter 8 subLoss 3347.6 multi 6.97 import weight 0.00
Epoch 264 Iter 9 subLoss 3335.0 multi 3.99 import weight 0.00
Epoch 264 Iter 10 subLoss 3248.8 multi -1.99 import weight 0.00
Epoch 264 Iter 11 subLoss 3634.1 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0111 / 0.10053 / 17.72
Entropy seen (from low to high)
[310, 164, 96, 64, 59, 64, 72, 498, 320, 289, 252, 218, 208, 219, 253, 248, 258, 292, 266, 242, 160, 92, 79, 50, 51, 44, 42, 33, 32, 30, 20, 24, 9, 16, 15, 14, 10, 7, 6, 3, 5, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 9, 23, 36, 40, 72, 84, 115, 147, 165, 167, 197, 179, 242, 227, 221, 234, 260, 200, 198, 176, 142, 160, 127, 141, 132, 110, 106, 92, 87, 91, 100, 108, 91, 92, 101, 85, 53, 30, 7, 2, 5, 4, 0, 0, 3, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.3, 33.2, 36.5, 40.8, 43.5, 46.9, 50.8, 54.2, 57.4, 61.1, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 83.3, 59.9, 57.1, 57.1, 76.9, 61.9, 72.2, 56.2, 80.5, 79.1, 91.2]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 5, 7, 21, 26, 21, 36, 32, 36, 48, 57]
Epoch 264 Acc: 97.80 BMA: 97.06 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 363 train Loss: 3297.2 test Loss: 364.9
Epoch 265 Iter 0 subLoss 3157.8 multi -13.93 import weight 0.00
Epoch 265 Iter 1 subLoss 4052.9 multi -4.97 import weight 0.00
Epoch 265 Iter 2 subLoss 4100.1 multi 6.97 import weight 0.00
Epoch 265 Iter 3 subLoss 3257.2 multi -1.99 import weight 0.00
Epoch 265 Iter 4 subLoss 3539.8 multi 12.94 import weight 0.00
Epoch 265 Iter 5 subLoss 3278.3 multi -4.97 import weight 0.00
Epoch 265 Iter 6 subLoss 4173.8 multi 27.87 import weight 1.00
Epoch 265 Iter 7 subLoss 16889.1 multi 1.00 import weight 0.00
Epoch 265 Iter 8 subLoss 8321.9 multi 1.00 import weight 0.00
Epoch 265 Iter 9 subLoss 6193.3 multi -4.97 import weight 0.00
Epoch 265 Iter 10 subLoss 16648.4 multi 3.99 import weight 0.00
Epoch 265 Iter 11 subLoss 3774.5 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0118 / 0.09866 / 19.75
Entropy seen (from low to high)
[91, 49, 55, 83, 92, 104, 305, 306, 237, 272, 238, 254, 262, 233, 265, 244, 317, 339, 288, 245, 172, 92, 84, 62, 57, 64, 57, 31, 43, 33, 27, 31, 17, 17, 12, 20, 8, 6, 11, 4, 7, 2, 3, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 9, 23, 44, 38, 79, 97, 128, 163, 177, 183, 207, 218, 245, 258, 251, 247, 216, 192, 187, 169, 158, 125, 121, 121, 114, 104, 93, 93, 85, 89, 93, 106, 90, 83, 84, 80, 46, 27, 10, 2, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.9, 33.5, 36.5, 40.9, 44.2, 46.9, 50.8, 54.3, 57.5, 61.5, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 49.9, 74.9, 59.9, 68.4, 67.8, 73.0, 56.0, 75.7, 82.0, 84.4, 93.5]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 15, 19, 28, 26, 41, 33, 39, 58, 78]
Epoch 265 Acc: 79.28 BMA: 97.10 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 377 train Loss: 14873.2 test Loss: 2854.2
Epoch 266 Iter 0 subLoss 15400.0 multi 3.99 import weight 0.00
Epoch 266 Iter 1 subLoss 5528.0 multi -1.98 import weight 0.00
Epoch 266 Iter 2 subLoss 6269.8 multi 1.00 import weight 0.00
Epoch 266 Iter 3 subLoss 5692.5 multi 15.93 import weight 0.00
Epoch 266 Iter 4 subLoss 3923.6 multi -4.97 import weight 0.00
Epoch 266 Iter 5 subLoss 4567.9 multi 9.96 import weight 0.00
Epoch 266 Iter 6 subLoss 3504.5 multi 21.90 import weight 0.00
Epoch 266 Iter 7 subLoss 3940.9 multi 1.00 import weight 0.00
Epoch 266 Iter 8 subLoss 3316.5 multi -4.97 import weight 0.00
Epoch 266 Iter 9 subLoss 4813.0 multi 3.98 import weight 0.00
Epoch 266 Iter 10 subLoss 2567.4 multi 9.96 import weight 0.00
Epoch 266 Iter 11 subLoss 2630.0 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0111 / 0.10032 / 17.91
Entropy seen (from low to high)
[93, 48, 61, 94, 99, 120, 412, 226, 284, 277, 257, 266, 233, 277, 268, 303, 346, 308, 257, 189, 104, 84, 70, 62, 61, 53, 36, 41, 29, 34, 25, 27, 12, 16, 16, 13, 11, 4, 9, 3, 7, 2, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 9, 24, 36, 39, 77, 87, 119, 146, 172, 175, 189, 203, 240, 246, 241, 255, 227, 198, 179, 177, 161, 143, 122, 122, 124, 111, 102, 88, 91, 81, 93, 100, 103, 84, 84, 88, 61, 44, 12, 1, 3, 4, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 33.0, 36.6, 40.7, 43.7, 47.2, 50.9, 54.3, 57.8, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 49.9, 49.9, 57.1, 76.9, 59.0, 65.8, 83.3, 67.6, 87.7, 88.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 4, 10, 21, 26, 22, 41, 30, 34, 57, 62]
Epoch 266 Acc: 98.13 BMA: 97.24 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 262 train Loss: 2994.8 test Loss: 325.4
Epoch 267 Iter 0 subLoss 3113.7 multi -10.94 import weight 0.00
Epoch 267 Iter 1 subLoss 3312.9 multi -1.99 import weight 0.00
Epoch 267 Iter 2 subLoss 3510.4 multi -4.97 import weight 0.00
Epoch 267 Iter 3 subLoss 5194.8 multi -10.94 import weight 0.00
Epoch 267 Iter 4 subLoss 27086.3 multi -1.99 import weight 0.00
Epoch 267 Iter 5 subLoss 232814.2 multi 1.00 import weight 0.00
Epoch 267 Iter 6 subLoss 24381.2 multi -1.99 import weight 0.00
Epoch 267 Iter 7 subLoss 133029.1 multi 1.00 import weight 0.00
Epoch 267 Iter 8 subLoss 9438.9 multi -1.99 import weight 0.00
Epoch 267 Iter 9 subLoss 12187.2 multi 1.00 import weight 0.00
Epoch 267 Iter 10 subLoss 9339.4 multi -1.98 import weight 0.00
Epoch 267 Iter 11 subLoss 12572.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0108 / 0.09918 / 17.71
Entropy seen (from low to high)
[98, 43, 62, 74, 100, 112, 445, 198, 244, 246, 249, 274, 257, 289, 335, 361, 353, 299, 232, 146, 97, 92, 65, 63, 63, 47, 42, 36, 30, 34, 27, 23, 16, 16, 14, 14, 12, 6, 9, 5, 5, 4, 1, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 12, 23, 41, 34, 84, 91, 129, 159, 176, 159, 208, 238, 232, 258, 246, 263, 215, 189, 197, 174, 145, 125, 121, 122, 118, 111, 88, 97, 84, 94, 96, 103, 84, 89, 78, 71, 60, 27, 13, 0, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.9, 33.6, 36.0, 40.4, 43.7, 47.0, 50.9, 54.2, 57.7, 61.2, 65.0, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 66.6, 53.8, 49.9, 72.4, 48.3, 75.8, 72.9, 68.4, 90.9, 90.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 3, 13, 14, 29, 31, 29, 37, 38, 55, 55]
Epoch 267 Acc: 96.15 BMA: 97.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 1257 train Loss: 10118.7 test Loss: 893.9
Epoch 268 Iter 0 subLoss 9632.0 multi 3.99 import weight 0.00
Epoch 268 Iter 1 subLoss 4697.0 multi 1.00 import weight 0.00
Epoch 268 Iter 2 subLoss 4604.7 multi 1.00 import weight 0.00
Epoch 268 Iter 3 subLoss 4500.0 multi -1.99 import weight 0.00
Epoch 268 Iter 4 subLoss 5345.3 multi -7.96 import weight 0.00
Epoch 268 Iter 5 subLoss 7580.3 multi -1.98 import weight 0.00
Epoch 268 Iter 6 subLoss 8506.4 multi -1.99 import weight 0.00
Epoch 268 Iter 7 subLoss 12869.1 multi 1.00 import weight 0.00
Epoch 268 Iter 8 subLoss 9389.4 multi 1.00 import weight 0.00
Epoch 268 Iter 9 subLoss 8281.2 multi -1.99 import weight 0.00
Epoch 268 Iter 10 subLoss 10305.6 multi 1.00 import weight 0.00
Epoch 268 Iter 11 subLoss 8774.4 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0109 / 0.09668 / 16.98
Entropy seen (from low to high)
[99, 44, 70, 83, 93, 201, 397, 161, 201, 243, 243, 245, 274, 314, 349, 361, 321, 272, 226, 150, 126, 92, 77, 74, 49, 57, 48, 34, 36, 32, 28, 22, 22, 17, 18, 15, 11, 8, 6, 7, 9, 2, 0, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 11, 23, 38, 49, 85, 96, 152, 172, 191, 188, 212, 247, 255, 243, 284, 257, 208, 197, 191, 158, 127, 139, 153, 107, 114, 98, 99, 110, 105, 75, 88, 77, 88, 62, 52, 49, 28, 13, 12, 1, 3, 4, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.6, 33.2, 36.2, 39.9, 44.0, 47.3, 50.7, 54.4, 57.7, 61.6, 64.9, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 74.9, 79.9, 53.8, 49.9, 48.1, 74.9, 63.1, 75.6, 76.3, 87.7, 89.5]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 5, 13, 20, 27, 28, 38, 37, 38, 49, 67]
Epoch 268 Acc: 91.57 BMA: 97.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 877 train Loss: 17253.3 test Loss: 1382.4
Epoch 269 Iter 0 subLoss 16490.3 multi -1.98 import weight 0.00
Epoch 269 Iter 1 subLoss 48137.2 multi 1.00 import weight 0.00
Epoch 269 Iter 2 subLoss 12771.7 multi 1.00 import weight 0.00
Epoch 269 Iter 3 subLoss 10784.9 multi 6.97 import weight 0.00
Epoch 269 Iter 4 subLoss 6009.3 multi -1.98 import weight 0.00
Epoch 269 Iter 5 subLoss 7187.2 multi 9.96 import weight 0.00
Epoch 269 Iter 6 subLoss 3960.1 multi 12.94 import weight 0.00
Epoch 269 Iter 7 subLoss 3249.8 multi 1.00 import weight 0.00
Epoch 269 Iter 8 subLoss 3010.8 multi -1.99 import weight 0.00
Epoch 269 Iter 9 subLoss 3708.6 multi -7.96 import weight 0.00
Epoch 269 Iter 10 subLoss 3540.6 multi -10.94 import weight 0.00
Epoch 269 Iter 11 subLoss 4493.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0104 / 0.09752 / 17.18
Entropy seen (from low to high)
[102, 44, 79, 82, 103, 342, 282, 173, 215, 250, 272, 234, 317, 335, 386, 357, 290, 253, 169, 138, 99, 79, 68, 60, 53, 54, 41, 30, 40, 29, 29, 20, 20, 23, 13, 14, 12, 7, 5, 7, 9, 2, 0, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 12, 23, 39, 54, 82, 101, 141, 171, 181, 190, 218, 229, 273, 241, 262, 235, 227, 180, 184, 179, 128, 122, 138, 122, 116, 92, 107, 107, 100, 86, 96, 89, 84, 64, 56, 49, 45, 15, 14, 2, 2, 5, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.2, 33.7, 36.6, 39.7, 43.8, 47.3, 50.7, 54.1, 57.6, 61.2, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 33.3, 88.8, 66.6, 62.4, 49.9, 64.5, 67.7, 76.3, 74.9, 88.4, 84.7]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 9, 9, 16, 30, 31, 31, 38, 36, 52, 59]
Epoch 269 Acc: 97.55 BMA: 97.59 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 449 train Loss: 4318.7 test Loss: 408.5
Epoch 270 Iter 0 subLoss 3961.3 multi 15.93 import weight 0.00
Epoch 270 Iter 1 subLoss 3737.0 multi 15.93 import weight 0.00
Epoch 270 Iter 2 subLoss 3355.0 multi -4.97 import weight 0.00
Epoch 270 Iter 3 subLoss 4424.2 multi -7.96 import weight 0.00
Epoch 270 Iter 4 subLoss 13796.7 multi 1.00 import weight 0.00
Epoch 270 Iter 5 subLoss 5706.8 multi -7.96 import weight 0.00
Epoch 270 Iter 6 subLoss 45005.6 multi -1.99 import weight 0.00
Epoch 270 Iter 7 subLoss 891002.0 multi 1.00 import weight 0.00
Epoch 270 Iter 8 subLoss 63944.2 multi 1.00 import weight 0.00
Epoch 270 Iter 9 subLoss 36483.6 multi 1.00 import weight 0.00
Epoch 270 Iter 10 subLoss 25459.3 multi 1.00 import weight 0.00
Epoch 270 Iter 11 subLoss 20590.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0109 / 0.09464 / 19.68
Entropy seen (from low to high)
[96, 48, 80, 87, 95, 398, 224, 127, 106, 119, 177, 306, 335, 360, 409, 379, 322, 234, 205, 176, 119, 98, 86, 73, 67, 50, 60, 41, 35, 37, 35, 25, 27, 22, 14, 20, 9, 11, 5, 7, 10, 2, 1, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 13, 24, 38, 69, 88, 116, 159, 187, 177, 226, 221, 268, 268, 261, 254, 246, 206, 195, 181, 156, 132, 139, 116, 129, 101, 137, 109, 111, 81, 94, 100, 84, 65, 48, 28, 11, 6, 7, 2, 1, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.9, 30.0, 33.8, 36.5, 40.4, 44.0, 47.2, 50.8, 54.0, 57.8, 61.3, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 24.9, 99.9, 72.7, 36.8, 67.7, 58.0, 58.3, 84.6, 76.9, 89.8, 90.1]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 9, 11, 19, 31, 31, 36, 39, 39, 59, 81]
Epoch 270 Acc: 88.56 BMA: 97.53 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2059 train Loss: 19168.3 test Loss: 2233.6
Epoch 271 Iter 0 subLoss 19078.3 multi -4.97 import weight 0.00
Epoch 271 Iter 1 subLoss 27780.5 multi 3.99 import weight 0.00
Epoch 271 Iter 2 subLoss 22140.0 multi 1.00 import weight 0.00
Epoch 271 Iter 3 subLoss 17821.1 multi 1.00 import weight 0.00
Epoch 271 Iter 4 subLoss 17067.3 multi -1.99 import weight 0.00
Epoch 271 Iter 5 subLoss 18432.5 multi 1.00 import weight 0.00
Epoch 271 Iter 6 subLoss 17720.3 multi -1.99 import weight 0.00
Epoch 271 Iter 7 subLoss 18309.7 multi 1.00 import weight 0.00
Epoch 271 Iter 8 subLoss 18645.5 multi 1.00 import weight 0.00
Epoch 271 Iter 9 subLoss 16913.7 multi -1.99 import weight 0.00
Epoch 271 Iter 10 subLoss 19239.6 multi 1.00 import weight 0.00
Epoch 271 Iter 11 subLoss 18080.1 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0117 / 0.09199 / 20.64
Entropy seen (from low to high)
[85, 46, 82, 91, 110, 406, 194, 116, 113, 103, 119, 146, 211, 297, 493, 417, 321, 266, 228, 210, 174, 120, 114, 92, 92, 61, 64, 56, 44, 42, 36, 39, 30, 31, 19, 17, 13, 10, 8, 4, 13, 3, 1, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 12, 30, 42, 65, 110, 135, 165, 194, 207, 239, 238, 280, 279, 270, 248, 252, 204, 200, 144, 167, 149, 118, 142, 128, 128, 116, 105, 110, 104, 91, 72, 38, 21, 13, 15, 6, 9, 4, 3, 1, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 29.5, 33.4, 36.3, 40.3, 43.8, 47.3, 51.2, 54.4, 57.7, 61.2, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 49.9, 48.1, 64.5, 60.5, 74.1, 79.4, 83.3, 88.8, 93.9]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 6, 7, 12, 27, 31, 38, 31, 39, 54, 72, 100]
Epoch 271 Acc: 79.80 BMA: 97.53 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 1808 train Loss: 25286.8 test Loss: 3023.2
Epoch 272 Iter 0 subLoss 23323.0 multi 1.00 import weight 0.00
Epoch 272 Iter 1 subLoss 23012.7 multi 6.97 import weight 0.00
Epoch 272 Iter 2 subLoss 18651.4 multi -1.99 import weight 0.00
Epoch 272 Iter 3 subLoss 20112.0 multi 3.99 import weight 0.00
Epoch 272 Iter 4 subLoss 16611.3 multi 1.00 import weight 0.00
Epoch 272 Iter 5 subLoss 16451.5 multi 3.99 import weight 0.00
Epoch 272 Iter 6 subLoss 14107.0 multi 3.99 import weight 0.00
Epoch 272 Iter 7 subLoss 11756.5 multi 1.00 import weight 0.00
Epoch 272 Iter 8 subLoss 12172.3 multi -4.97 import weight 0.00
Epoch 272 Iter 9 subLoss 13712.4 multi -1.99 import weight 0.00
Epoch 272 Iter 10 subLoss 14877.8 multi -1.99 import weight 0.00
Epoch 272 Iter 11 subLoss 16784.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0120 / 0.09033 / 21.45
Entropy seen (from low to high)
[85, 49, 86, 97, 116, 438, 163, 120, 106, 119, 112, 158, 199, 246, 329, 361, 360, 318, 274, 226, 183, 128, 108, 113, 101, 71, 69, 66, 43, 45, 45, 41, 34, 29, 27, 17, 14, 9, 8, 6, 11, 6, 1, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 13, 29, 42, 71, 116, 136, 170, 213, 231, 231, 232, 304, 274, 274, 244, 259, 210, 190, 183, 139, 155, 157, 149, 121, 119, 123, 134, 102, 67, 42, 32, 21, 18, 15, 12, 10, 8, 5, 1, 2, 4, 2, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 29.2, 33.2, 36.9, 40.7, 43.8, 46.9, 50.5, 54.6, 58.0, 61.5, 65.0, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.8, 71.4, 56.2, 39.1, 72.4, 58.9, 75.6, 75.6, 90.1, 88.3, 94.0]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 7, 7, 16, 23, 29, 39, 41, 41, 61, 77, 101]
Epoch 272 Acc: 89.22 BMA: 97.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1678 train Loss: 15735.4 test Loss: 1780.3
Epoch 273 Iter 0 subLoss 15393.9 multi 1.00 import weight 0.00
Epoch 273 Iter 1 subLoss 14135.6 multi -1.99 import weight 0.00
Epoch 273 Iter 2 subLoss 16466.6 multi -4.97 import weight 0.00
Epoch 273 Iter 3 subLoss 23729.5 multi 1.00 import weight 0.00
Epoch 273 Iter 4 subLoss 17592.0 multi 1.00 import weight 0.00
Epoch 273 Iter 5 subLoss 17246.7 multi 3.99 import weight 0.00
Epoch 273 Iter 6 subLoss 15622.9 multi 1.00 import weight 0.00
Epoch 273 Iter 7 subLoss 14596.5 multi -1.99 import weight 0.00
Epoch 273 Iter 8 subLoss 15421.7 multi 3.99 import weight 0.00
Epoch 273 Iter 9 subLoss 14181.8 multi 1.00 import weight 0.00
Epoch 273 Iter 10 subLoss 12985.2 multi 1.00 import weight 0.00
Epoch 273 Iter 11 subLoss 12421.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0122 / 0.08921 / 22.08
Entropy seen (from low to high)
[87, 54, 86, 100, 159, 423, 149, 122, 106, 124, 118, 172, 212, 288, 302, 251, 210, 292, 338, 282, 206, 132, 122, 110, 105, 88, 70, 66, 52, 39, 53, 47, 33, 35, 29, 17, 16, 9, 9, 6, 10, 7, 1, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 12, 30, 41, 74, 124, 152, 162, 212, 261, 223, 230, 311, 271, 269, 272, 250, 204, 204, 177, 152, 178, 163, 124, 140, 127, 128, 107, 75, 36, 21, 30, 19, 16, 20, 10, 13, 6, 7, 1, 3, 3, 2, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.5, 29.4, 33.0, 37.0, 40.9, 43.6, 47.3, 50.6, 54.5, 57.9, 61.5, 64.9, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 49.9, 64.7, 40.9, 64.8, 68.7, 75.6, 73.3, 90.4, 91.1, 93.7]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 6, 10, 17, 22, 37, 32, 41, 45, 63, 90, 96]
Epoch 273 Acc: 92.22 BMA: 97.41 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1242 train Loss: 12816.9 test Loss: 1491.4
Epoch 274 Iter 0 subLoss 12097.3 multi 9.96 import weight 0.00
Epoch 274 Iter 1 subLoss 9364.0 multi -4.97 import weight 0.00
Epoch 274 Iter 2 subLoss 10769.1 multi -1.98 import weight 0.00
Epoch 274 Iter 3 subLoss 10753.9 multi 6.97 import weight 0.00
Epoch 274 Iter 4 subLoss 9266.6 multi 3.98 import weight 0.00
Epoch 274 Iter 5 subLoss 8231.0 multi -4.97 import weight 0.00
Epoch 274 Iter 6 subLoss 9264.7 multi 6.97 import weight 0.00
Epoch 274 Iter 7 subLoss 7186.0 multi 12.94 import weight 0.00
Epoch 274 Iter 8 subLoss 6251.2 multi -1.99 import weight 0.00
Epoch 274 Iter 9 subLoss 5967.3 multi -7.96 import weight 0.00
Epoch 274 Iter 10 subLoss 7363.9 multi -4.97 import weight 0.00
Epoch 274 Iter 11 subLoss 9915.6 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0125 / 0.08805 / 22.93
Entropy seen (from low to high)
[90, 53, 97, 103, 268, 329, 149, 114, 102, 129, 134, 167, 201, 250, 259, 252, 206, 218, 281, 329, 246, 194, 133, 134, 99, 80, 83, 83, 36, 47, 47, 49, 35, 35, 29, 21, 11, 10, 9, 6, 12, 5, 2, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 31, 45, 73, 122, 162, 183, 217, 240, 238, 232, 278, 308, 262, 282, 251, 233, 204, 177, 188, 163, 149, 146, 146, 128, 107, 62, 36, 30, 23, 28, 17, 18, 19, 12, 13, 4, 10, 1, 3, 3, 2, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.1, 30.8, 33.5, 36.5, 40.3, 43.8, 47.2, 50.4, 54.2, 57.6, 61.3, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 49.9, 81.8, 68.4, 52.1, 52.9, 75.7, 73.6, 76.7, 85.2, 89.1, 96.8]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 4, 11, 19, 23, 34, 33, 38, 43, 61, 101, 94]
Epoch 274 Acc: 84.12 BMA: 97.41 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 991 train Loss: 14930.8 test Loss: 2172.0
Epoch 275 Iter 0 subLoss 14508.6 multi 1.00 import weight 0.00
Epoch 275 Iter 1 subLoss 9986.3 multi 6.97 import weight 0.00
Epoch 275 Iter 2 subLoss 6808.1 multi -4.97 import weight 0.00
Epoch 275 Iter 3 subLoss 8109.0 multi -1.99 import weight 0.00
Epoch 275 Iter 4 subLoss 8832.1 multi 3.99 import weight 0.00
Epoch 275 Iter 5 subLoss 6736.9 multi -4.97 import weight 0.00
Epoch 275 Iter 6 subLoss 8875.7 multi 3.99 import weight 0.00
Epoch 275 Iter 7 subLoss 7159.9 multi 1.00 import weight 0.00
Epoch 275 Iter 8 subLoss 7381.3 multi 1.00 import weight 0.00
Epoch 275 Iter 9 subLoss 6935.4 multi 1.00 import weight 0.00
Epoch 275 Iter 10 subLoss 7230.7 multi -13.93 import weight 0.00
Epoch 275 Iter 11 subLoss 9707.2 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0129 / 0.08687 / 23.41
Entropy seen (from low to high)
[92, 57, 95, 104, 324, 283, 132, 113, 114, 119, 143, 165, 186, 262, 259, 253, 191, 171, 182, 300, 284, 230, 161, 128, 129, 98, 82, 85, 60, 51, 34, 54, 44, 36, 27, 31, 12, 11, 5, 9, 12, 5, 4, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 15, 31, 50, 75, 131, 187, 190, 224, 268, 253, 246, 279, 279, 287, 268, 253, 224, 187, 210, 181, 148, 139, 164, 131, 110, 70, 38, 31, 38, 21, 19, 25, 19, 16, 17, 9, 9, 7, 3, 3, 3, 1, 2, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.6, 30.2, 33.4, 37.3, 40.3, 44.0, 47.4, 50.8, 54.5, 57.6, 61.3, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.1, 59.9, 66.6, 51.9, 57.4, 82.7, 73.6, 76.0, 88.6, 91.2, 95.2]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 7, 10, 21, 25, 40, 29, 38, 46, 79, 103, 106]
Epoch 275 Acc: 89.18 BMA: 97.37 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 970 train Loss: 15705.2 test Loss: 1909.1
Epoch 276 Iter 0 subLoss 15745.5 multi 1.00 import weight 0.00
Epoch 276 Iter 1 subLoss 13569.8 multi 1.00 import weight 0.00
Epoch 276 Iter 2 subLoss 12337.7 multi 3.99 import weight 0.00
Epoch 276 Iter 3 subLoss 9893.5 multi 3.99 import weight 0.00
Epoch 276 Iter 4 subLoss 8233.5 multi -1.98 import weight 0.00
Epoch 276 Iter 5 subLoss 9145.1 multi 1.00 import weight 0.00
Epoch 276 Iter 6 subLoss 8730.4 multi -4.97 import weight 0.00
Epoch 276 Iter 7 subLoss 9560.7 multi 1.00 import weight 0.00
Epoch 276 Iter 8 subLoss 9157.0 multi -7.96 import weight 0.00
Epoch 276 Iter 9 subLoss 11733.2 multi -4.97 import weight 0.00
Epoch 276 Iter 10 subLoss 16807.6 multi 3.99 import weight 0.00
Epoch 276 Iter 11 subLoss 11756.8 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0131 / 0.08598 / 24.52
Entropy seen (from low to high)
[93, 61, 97, 106, 381, 238, 121, 113, 123, 124, 159, 160, 210, 261, 269, 237, 168, 165, 137, 196, 293, 247, 212, 134, 140, 102, 92, 73, 75, 49, 42, 50, 54, 32, 31, 29, 17, 11, 5, 9, 12, 5, 4, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 35, 50, 86, 141, 197, 195, 245, 260, 251, 248, 275, 295, 280, 276, 245, 227, 204, 194, 177, 149, 149, 166, 119, 86, 46, 33, 27, 34, 26, 18, 24, 20, 21, 11, 10, 8, 6, 5, 1, 4, 0, 3, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 30.4, 33.6, 37.2, 40.2, 44.1, 47.3, 50.7, 54.3, 57.8, 61.3, 64.6, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 33.3, 63.6, 68.1, 62.9, 56.0, 84.6, 78.9, 78.3, 87.0, 93.3, 96.4]
[0, 0, 0, 0, 0, 0, 0, 1, 4, 6, 11, 22, 27, 41, 26, 38, 60, 77, 105, 113]
Epoch 276 Acc: 94.24 BMA: 97.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1175 train Loss: 10624.7 test Loss: 1410.0
Epoch 277 Iter 0 subLoss 10871.1 multi 6.97 import weight 0.00
Epoch 277 Iter 1 subLoss 8672.8 multi -1.99 import weight 0.00
Epoch 277 Iter 2 subLoss 8751.3 multi 6.97 import weight 0.00
Epoch 277 Iter 3 subLoss 7735.4 multi 6.97 import weight 0.00
Epoch 277 Iter 4 subLoss 7042.4 multi 1.00 import weight 0.00
Epoch 277 Iter 5 subLoss 6238.0 multi 1.00 import weight 0.00
Epoch 277 Iter 6 subLoss 6582.0 multi -4.97 import weight 0.00
Epoch 277 Iter 7 subLoss 7091.8 multi -1.98 import weight 0.00
Epoch 277 Iter 8 subLoss 7184.2 multi 15.93 import weight 0.00
Epoch 277 Iter 9 subLoss 5102.0 multi -7.96 import weight 0.00
Epoch 277 Iter 10 subLoss 6037.2 multi -1.99 import weight 0.00
Epoch 277 Iter 11 subLoss 7012.4 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0129 / 0.08609 / 24.52
Entropy seen (from low to high)
[93, 65, 100, 112, 430, 196, 117, 124, 122, 136, 159, 188, 235, 272, 266, 212, 170, 133, 163, 157, 285, 255, 204, 137, 137, 93, 92, 78, 65, 52, 32, 59, 43, 36, 27, 30, 18, 8, 6, 8, 14, 4, 4, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 33, 56, 87, 144, 196, 200, 245, 243, 249, 244, 273, 292, 272, 281, 250, 225, 199, 206, 179, 158, 138, 170, 122, 80, 45, 34, 28, 33, 30, 12, 27, 19, 22, 16, 7, 11, 7, 6, 1, 4, 0, 3, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 33.2, 37.2, 40.2, 43.9, 47.4, 50.4, 54.2, 58.0, 61.3, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.9, 19.9, 66.6, 68.1, 57.6, 64.1, 80.7, 74.9, 79.3, 89.1, 91.9, 95.2]
[0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 12, 22, 26, 39, 26, 40, 58, 74, 100, 105]
Epoch 277 Acc: 96.94 BMA: 97.53 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 701 train Loss: 6130.0 test Loss: 698.0
Epoch 278 Iter 0 subLoss 5655.1 multi 1.00 import weight 0.00
Epoch 278 Iter 1 subLoss 5939.6 multi 1.00 import weight 0.00
Epoch 278 Iter 2 subLoss 5713.4 multi -4.97 import weight 0.00
Epoch 278 Iter 3 subLoss 6174.4 multi 1.00 import weight 0.00
Epoch 278 Iter 4 subLoss 6005.4 multi 1.00 import weight 0.00
Epoch 278 Iter 5 subLoss 6022.9 multi -1.99 import weight 0.00
Epoch 278 Iter 6 subLoss 6272.9 multi -7.96 import weight 0.00
Epoch 278 Iter 7 subLoss 8498.2 multi -1.98 import weight 0.00
Epoch 278 Iter 8 subLoss 8497.6 multi 1.00 import weight 0.00
Epoch 278 Iter 9 subLoss 8011.0 multi 3.99 import weight 0.00
Epoch 278 Iter 10 subLoss 6548.1 multi -1.98 import weight 0.00
Epoch 278 Iter 11 subLoss 7416.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0127 / 0.08619 / 24.39
Entropy seen (from low to high)
[93, 68, 107, 117, 449, 185, 121, 117, 134, 150, 162, 204, 259, 282, 249, 187, 162, 150, 140, 145, 278, 253, 200, 141, 139, 96, 75, 77, 61, 53, 38, 55, 37, 36, 26, 32, 14, 9, 5, 10, 13, 3, 5, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 15, 27, 58, 94, 153, 190, 209, 225, 237, 258, 244, 267, 294, 262, 286, 247, 228, 202, 199, 176, 167, 142, 161, 131, 75, 43, 32, 35, 29, 30, 18, 25, 22, 23, 14, 9, 12, 7, 7, 1, 4, 0, 3, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.8, 32.9, 37.2, 40.2, 43.8, 47.4, 50.6, 54.3, 58.0, 61.4, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.9, 19.9, 66.6, 76.1, 57.1, 61.7, 72.7, 82.8, 75.8, 88.4, 92.7, 95.2]
[0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 12, 21, 28, 34, 33, 35, 58, 69, 97, 106]
Epoch 278 Acc: 97.10 BMA: 97.55 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 741 train Loss: 6127.8 test Loss: 679.5
Epoch 279 Iter 0 subLoss 6160.6 multi 1.00 import weight 0.00
Epoch 279 Iter 1 subLoss 6024.0 multi 1.00 import weight 0.00
Epoch 279 Iter 2 subLoss 6007.1 multi 3.99 import weight 0.00
Epoch 279 Iter 3 subLoss 5907.0 multi -1.98 import weight 0.00
Epoch 279 Iter 4 subLoss 5816.9 multi 15.93 import weight 0.00
Epoch 279 Iter 5 subLoss 5387.8 multi 6.97 import weight 0.00
Epoch 279 Iter 6 subLoss 4084.3 multi 6.97 import weight 0.00
Epoch 279 Iter 7 subLoss 4517.3 multi 6.97 import weight 0.00
Epoch 279 Iter 8 subLoss 4203.9 multi -4.97 import weight 0.00
Epoch 279 Iter 9 subLoss 4572.7 multi -4.97 import weight 0.00
Epoch 279 Iter 10 subLoss 5011.1 multi 1.00 import weight 0.00
Epoch 279 Iter 11 subLoss 4298.3 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0124 / 0.08685 / 23.83
Entropy seen (from low to high)
[96, 77, 106, 127, 460, 175, 116, 121, 142, 167, 167, 223, 280, 275, 240, 187, 143, 149, 133, 184, 310, 223, 167, 139, 114, 100, 71, 77, 56, 39, 45, 52, 34, 25, 36, 26, 11, 9, 6, 10, 11, 3, 5, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 24, 67, 89, 155, 176, 213, 220, 218, 271, 220, 265, 287, 269, 283, 240, 217, 219, 188, 177, 173, 157, 151, 138, 95, 50, 40, 31, 24, 36, 24, 17, 28, 18, 21, 11, 10, 8, 9, 0, 5, 0, 3, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 33.1, 36.8, 40.2, 43.9, 47.3, 50.5, 54.4, 57.9, 61.3, 64.7, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 59.9, 74.9, 61.2, 66.6, 73.6, 73.5, 75.5, 87.8, 91.6, 94.1]
[0, 0, 0, 0, 0, 0, 0, 0, 6, 4, 10, 20, 31, 27, 38, 34, 49, 66, 84, 120]
Epoch 279 Acc: 97.63 BMA: 97.59 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 429 train Loss: 4601.6 test Loss: 449.9
Epoch 280 Iter 0 subLoss 4384.0 multi 6.97 import weight 0.00
Epoch 280 Iter 1 subLoss 4380.2 multi 9.96 import weight 0.00
Epoch 280 Iter 2 subLoss 3837.9 multi -7.96 import weight 0.00
Epoch 280 Iter 3 subLoss 4671.0 multi -7.96 import weight 0.00
Epoch 280 Iter 4 subLoss 5059.6 multi 1.00 import weight 0.00
Epoch 280 Iter 5 subLoss 5244.2 multi -13.93 import weight 0.00
Epoch 280 Iter 6 subLoss 15874.7 multi -1.99 import weight 0.00
Epoch 280 Iter 7 subLoss 57658.8 multi 1.00 import weight 0.00
Epoch 280 Iter 8 subLoss 15312.6 multi 3.99 import weight 0.00
Epoch 280 Iter 9 subLoss 6378.1 multi 1.00 import weight 0.00
Epoch 280 Iter 10 subLoss 5977.9 multi 1.00 import weight 0.00
Epoch 280 Iter 11 subLoss 5373.4 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0123 / 0.08705 / 23.68
Entropy seen (from low to high)
[99, 76, 113, 129, 481, 169, 118, 124, 145, 173, 185, 246, 284, 279, 214, 173, 142, 154, 112, 192, 297, 227, 166, 127, 112, 92, 71, 73, 53, 43, 45, 47, 34, 26, 36, 25, 10, 10, 6, 14, 6, 4, 5, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 15, 24, 68, 85, 158, 187, 202, 212, 230, 255, 219, 254, 293, 255, 281, 244, 233, 213, 181, 181, 186, 155, 152, 140, 92, 53, 35, 38, 28, 35, 25, 15, 27, 19, 21, 14, 11, 7, 10, 0, 5, 0, 3, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 33.3, 37.0, 40.5, 43.8, 47.4, 50.8, 54.5, 57.7, 61.3, 64.7, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 14.2, 49.9, 70.8, 59.9, 67.7, 71.0, 76.6, 74.9, 90.3, 89.4, 95.5]
[0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 8, 24, 25, 31, 38, 30, 52, 62, 85, 113]
Epoch 280 Acc: 95.84 BMA: 97.49 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 537 train Loss: 6267.5 test Loss: 826.2
Epoch 281 Iter 0 subLoss 6893.2 multi 1.00 import weight 0.00
Epoch 281 Iter 1 subLoss 5977.0 multi 3.99 import weight 0.00
Epoch 281 Iter 2 subLoss 5209.1 multi 1.00 import weight 0.00
Epoch 281 Iter 3 subLoss 5421.4 multi 3.98 import weight 0.00
Epoch 281 Iter 4 subLoss 4776.5 multi -4.97 import weight 0.00
Epoch 281 Iter 5 subLoss 5063.2 multi 6.97 import weight 0.00
Epoch 281 Iter 6 subLoss 5005.0 multi -1.98 import weight 0.00
Epoch 281 Iter 7 subLoss 5371.9 multi -1.99 import weight 0.00
Epoch 281 Iter 8 subLoss 5058.7 multi 3.98 import weight 0.00
Epoch 281 Iter 9 subLoss 4236.9 multi 9.96 import weight 0.00
Epoch 281 Iter 10 subLoss 4670.8 multi -4.97 import weight 0.00
Epoch 281 Iter 11 subLoss 4871.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0120 / 0.08764 / 22.35
Entropy seen (from low to high)
[100, 78, 118, 149, 483, 153, 125, 134, 147, 174, 204, 276, 294, 265, 195, 166, 150, 128, 125, 241, 264, 212, 154, 129, 107, 80, 72, 67, 50, 40, 44, 40, 41, 18, 37, 26, 6, 9, 8, 13, 7, 3, 5, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 15, 27, 61, 89, 150, 181, 208, 207, 222, 236, 230, 249, 285, 261, 265, 253, 232, 205, 198, 175, 167, 179, 140, 139, 112, 71, 35, 38, 28, 29, 32, 21, 27, 16, 24, 15, 10, 10, 8, 3, 2, 3, 3, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 33.3, 37.0, 40.8, 43.7, 47.2, 50.6, 54.3, 57.6, 61.3, 64.7, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 28.5, 55.5, 59.0, 47.6, 69.6, 72.2, 73.5, 74.4, 89.8, 87.3, 96.5]
[0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 9, 22, 21, 33, 36, 34, 47, 59, 79, 117]
Epoch 281 Acc: 97.16 BMA: 97.53 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 487 train Loss: 4928.1 test Loss: 540.6
Epoch 282 Iter 0 subLoss 5217.0 multi -4.97 import weight 0.00
Epoch 282 Iter 1 subLoss 5322.5 multi 1.00 import weight 0.00
Epoch 282 Iter 2 subLoss 5108.9 multi -4.97 import weight 0.00
Epoch 282 Iter 3 subLoss 5637.6 multi -4.97 import weight 0.00
Epoch 282 Iter 4 subLoss 7877.1 multi 1.00 import weight 0.00
Epoch 282 Iter 5 subLoss 7010.4 multi 6.97 import weight 0.00
Epoch 282 Iter 6 subLoss 4124.4 multi -7.96 import weight 0.00
Epoch 282 Iter 7 subLoss 4879.8 multi 1.00 import weight 0.00
Epoch 282 Iter 8 subLoss 4872.5 multi 3.99 import weight 0.00
Epoch 282 Iter 9 subLoss 4912.9 multi -10.94 import weight 0.00
Epoch 282 Iter 10 subLoss 5438.2 multi -7.96 import weight 0.00
Epoch 282 Iter 11 subLoss 6908.1 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0118 / 0.08800 / 22.56
Entropy seen (from low to high)
[101, 84, 119, 241, 404, 150, 126, 138, 156, 182, 233, 279, 299, 250, 199, 143, 150, 120, 138, 259, 252, 208, 135, 118, 100, 81, 63, 72, 52, 47, 31, 38, 35, 28, 37, 19, 8, 7, 7, 13, 6, 4, 5, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 18, 24, 61, 96, 146, 179, 204, 213, 218, 222, 221, 252, 280, 259, 260, 237, 243, 210, 204, 166, 183, 171, 152, 135, 118, 76, 40, 35, 32, 20, 38, 22, 30, 18, 21, 16, 9, 12, 7, 5, 2, 3, 3, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.2, 33.2, 36.5, 40.4, 43.3, 47.2, 50.8, 54.4, 57.8, 61.4, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 14.2, 62.4, 70.5, 58.3, 64.1, 75.8, 69.9, 78.2, 85.7, 89.8, 93.9]
[0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 8, 17, 24, 39, 29, 40, 46, 49, 79, 116]
Epoch 282 Acc: 96.71 BMA: 97.65 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 690 train Loss: 5362.5 test Loss: 577.0
Epoch 283 Iter 0 subLoss 5057.9 multi 6.97 import weight 0.00
Epoch 283 Iter 1 subLoss 4784.1 multi 6.97 import weight 0.00
Epoch 283 Iter 2 subLoss 4194.2 multi 9.96 import weight 0.00
Epoch 283 Iter 3 subLoss 4287.3 multi 1.00 import weight 0.00
Epoch 283 Iter 4 subLoss 3783.6 multi 9.96 import weight 0.00
Epoch 283 Iter 5 subLoss 4013.6 multi -4.97 import weight 0.00
Epoch 283 Iter 6 subLoss 4566.6 multi 12.94 import weight 0.00
Epoch 283 Iter 7 subLoss 3880.8 multi -1.98 import weight 0.00
Epoch 283 Iter 8 subLoss 4309.0 multi 6.97 import weight 0.00
Epoch 283 Iter 9 subLoss 4281.6 multi 3.99 import weight 0.00
Epoch 283 Iter 10 subLoss 3879.4 multi 3.98 import weight 0.00
Epoch 283 Iter 11 subLoss 4362.9 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0115 / 0.08836 / 22.17
Entropy seen (from low to high)
[102, 88, 123, 300, 355, 140, 134, 147, 170, 181, 258, 300, 294, 229, 181, 153, 149, 109, 174, 292, 237, 166, 137, 116, 77, 76, 62, 64, 51, 39, 37, 40, 25, 33, 33, 17, 6, 10, 5, 12, 7, 4, 4, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 19, 26, 51, 106, 148, 172, 190, 220, 222, 200, 238, 244, 262, 276, 250, 237, 241, 196, 210, 176, 177, 174, 154, 142, 127, 80, 51, 39, 27, 28, 30, 28, 18, 26, 19, 15, 10, 11, 7, 6, 2, 3, 2, 1, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.2, 29.5, 33.7, 36.7, 40.6, 43.7, 46.9, 50.4, 54.2, 57.5, 61.3, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 49.9, 71.4, 63.1, 60.8, 62.4, 75.8, 66.6, 83.7, 83.3, 87.6, 94.1]
[0, 0, 0, 0, 0, 0, 0, 1, 5, 6, 7, 19, 23, 32, 29, 42, 43, 48, 73, 119]
Epoch 283 Acc: 97.37 BMA: 97.66 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 436 train Loss: 4341.2 test Loss: 472.7
Epoch 284 Iter 0 subLoss 4392.0 multi -7.96 import weight 0.00
Epoch 284 Iter 1 subLoss 5941.8 multi -4.97 import weight 0.00
Epoch 284 Iter 2 subLoss 14783.7 multi 1.00 import weight 0.00
Epoch 284 Iter 3 subLoss 8545.6 multi -7.96 import weight 0.00
Epoch 284 Iter 4 subLoss 60092.8 multi 1.00 import weight 0.00
Epoch 284 Iter 5 subLoss 20083.9 multi 1.00 import weight 0.00
Epoch 284 Iter 6 subLoss 15861.3 multi 3.99 import weight 0.00
Epoch 284 Iter 7 subLoss 6053.8 multi 3.99 import weight 0.00
Epoch 284 Iter 8 subLoss 4987.3 multi -1.98 import weight 0.00
Epoch 284 Iter 9 subLoss 5008.3 multi 1.00 import weight 0.00
Epoch 284 Iter 10 subLoss 4778.2 multi -1.99 import weight 0.00
Epoch 284 Iter 11 subLoss 5129.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0114 / 0.08808 / 22.52
Entropy seen (from low to high)
[102, 89, 129, 339, 311, 148, 131, 153, 189, 180, 277, 310, 291, 208, 178, 150, 136, 120, 214, 288, 215, 159, 135, 101, 74, 68, 64, 64, 52, 34, 36, 37, 21, 37, 29, 20, 6, 9, 6, 14, 5, 4, 4, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 19, 28, 58, 105, 145, 177, 192, 223, 216, 203, 237, 233, 271, 284, 235, 244, 208, 217, 213, 183, 183, 161, 162, 129, 132, 100, 54, 42, 26, 29, 22, 24, 24, 18, 16, 13, 10, 6, 9, 4, 1, 4, 0, 1, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.4, 29.4, 32.8, 36.1, 40.4, 43.6, 47.0, 50.6, 54.3, 57.6, 61.2, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 0.0, 55.5, 68.4, 62.4, 64.5, 74.9, 69.9, 78.5, 81.2, 90.2, 94.5]
[0, 0, 0, 0, 0, 0, 0, 1, 5, 3, 9, 19, 24, 31, 32, 40, 42, 48, 72, 110]
Epoch 284 Acc: 96.44 BMA: 97.68 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 512 train Loss: 6675.9 test Loss: 671.2
Epoch 285 Iter 0 subLoss 6403.1 multi 1.00 import weight 0.00
Epoch 285 Iter 1 subLoss 6151.5 multi 6.97 import weight 0.00
Epoch 285 Iter 2 subLoss 4240.8 multi -4.97 import weight 0.00
Epoch 285 Iter 3 subLoss 4921.9 multi 12.94 import weight 0.00
Epoch 285 Iter 4 subLoss 4028.7 multi 6.97 import weight 0.00
Epoch 285 Iter 5 subLoss 3993.9 multi 18.91 import weight 0.00
Epoch 285 Iter 6 subLoss 3953.8 multi 1.00 import weight 0.00
Epoch 285 Iter 7 subLoss 3966.6 multi 15.93 import weight 0.00
Epoch 285 Iter 8 subLoss 3736.7 multi 18.91 import weight 0.00
Epoch 285 Iter 9 subLoss 4275.2 multi 1.00 import weight 0.00
Epoch 285 Iter 10 subLoss 4156.9 multi 1.00 import weight 0.00
Epoch 285 Iter 11 subLoss 3801.3 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0113 / 0.08765 / 23.13
Entropy seen (from low to high)
[102, 63, 108, 341, 259, 155, 137, 176, 208, 219, 306, 320, 293, 223, 152, 156, 119, 134, 262, 248, 203, 139, 134, 94, 88, 59, 72, 55, 56, 30, 36, 31, 36, 24, 36, 13, 10, 7, 8, 12, 5, 3, 5, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 17, 33, 52, 110, 145, 181, 179, 230, 208, 218, 232, 250, 275, 271, 254, 242, 220, 213, 214, 197, 162, 146, 163, 144, 114, 95, 60, 32, 25, 28, 23, 29, 19, 19, 11, 16, 9, 7, 7, 5, 1, 3, 1, 1, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.0, 29.2, 34.0, 37.1, 40.3, 43.9, 46.8, 50.5, 54.3, 57.7, 61.2, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 39.9, 49.9, 71.4, 66.6, 72.4, 68.7, 70.3, 67.4, 84.0, 81.8, 93.0, 90.9]
[0, 0, 0, 0, 0, 0, 0, 1, 5, 6, 7, 15, 29, 32, 27, 43, 44, 44, 72, 99]
Epoch 285 Acc: 88.38 BMA: 97.70 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 380 train Loss: 11949.2 test Loss: 1599.5
Epoch 286 Iter 0 subLoss 12404.6 multi -4.97 import weight 0.00
Epoch 286 Iter 1 subLoss 531739.4 multi 1.00 import weight 0.00
Epoch 286 Iter 2 subLoss 19457.2 multi 1.00 import weight 0.00
Epoch 286 Iter 3 subLoss 14555.3 multi -1.99 import weight 0.00
Epoch 286 Iter 4 subLoss 20007.7 multi -1.99 import weight 0.00
Epoch 286 Iter 5 subLoss 35474.7 multi 1.00 import weight 0.00
Epoch 286 Iter 6 subLoss 19990.8 multi -4.97 import weight 0.00
Epoch 286 Iter 7 subLoss 43079.3 multi 1.00 import weight 0.00
Epoch 286 Iter 8 subLoss 29929.3 multi 6.97 import weight 0.00
Epoch 286 Iter 9 subLoss 15431.9 multi -4.97 import weight 0.00
Epoch 286 Iter 10 subLoss 66203.0 multi 1.00 import weight 0.00
Epoch 286 Iter 11 subLoss 21339.8 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0119 / 0.08595 / 23.83
Entropy seen (from low to high)
[67, 50, 42, 243, 222, 134, 129, 133, 184, 247, 297, 341, 317, 264, 220, 176, 129, 132, 154, 277, 261, 208, 123, 133, 94, 87, 64, 63, 63, 36, 40, 33, 33, 33, 28, 26, 11, 9, 6, 13, 6, 5, 4, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 17, 39, 60, 122, 155, 183, 205, 230, 221, 230, 239, 260, 271, 280, 268, 239, 233, 215, 217, 184, 151, 164, 147, 123, 103, 69, 39, 27, 23, 28, 26, 17, 21, 14, 12, 8, 7, 6, 3, 2, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 29.9, 33.6, 36.3, 40.1, 44.0, 47.0, 50.6, 54.3, 57.7, 61.4, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 33.3, 49.9, 59.9, 61.9, 75.7, 70.3, 70.9, 69.5, 85.7, 83.9, 88.3, 94.3]
[0, 0, 0, 0, 0, 0, 0, 3, 3, 6, 5, 21, 33, 27, 31, 46, 42, 50, 77, 124]
Epoch 286 Acc: 65.03 BMA: 97.70 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 2133 train Loss: 29239.6 test Loss: 4902.2
Epoch 287 Iter 0 subLoss 28911.6 multi 1.00 import weight 0.00
Epoch 287 Iter 1 subLoss 23750.9 multi 1.00 import weight 0.00
Epoch 287 Iter 2 subLoss 19701.5 multi 1.00 import weight 0.00
Epoch 287 Iter 3 subLoss 17369.5 multi 1.00 import weight 0.00
Epoch 287 Iter 4 subLoss 15632.0 multi -1.99 import weight 0.00
Epoch 287 Iter 5 subLoss 18334.1 multi 1.00 import weight 0.00
Epoch 287 Iter 6 subLoss 16766.4 multi 1.00 import weight 0.00
Epoch 287 Iter 7 subLoss 15605.7 multi 1.00 import weight 0.00
Epoch 287 Iter 8 subLoss 14667.5 multi 3.99 import weight 0.00
Epoch 287 Iter 9 subLoss 11404.3 multi 3.98 import weight 0.00
Epoch 287 Iter 10 subLoss 9761.0 multi 3.99 import weight 0.00
Epoch 287 Iter 11 subLoss 9232.5 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0121 / 0.08533 / 24.12
Entropy seen (from low to high)
[70, 51, 39, 274, 193, 138, 133, 138, 185, 253, 305, 357, 295, 254, 203, 170, 112, 129, 154, 220, 279, 187, 164, 141, 99, 83, 72, 73, 71, 40, 38, 38, 31, 34, 31, 24, 15, 10, 5, 14, 5, 5, 5, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 18, 38, 62, 133, 160, 182, 221, 218, 229, 233, 242, 267, 275, 279, 267, 250, 231, 228, 195, 190, 147, 157, 137, 122, 85, 66, 31, 25, 27, 27, 25, 18, 21, 11, 15, 8, 7, 5, 4, 2, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 29.6, 33.5, 37.2, 40.4, 43.7, 47.1, 50.7, 54.2, 57.9, 61.3, 65.0, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 24.9, 42.8, 87.4, 64.7, 67.7, 68.5, 70.3, 73.4, 79.9, 84.7, 93.5, 92.8]
[0, 0, 0, 0, 0, 0, 0, 2, 4, 7, 8, 17, 31, 35, 27, 49, 40, 59, 93, 112]
Epoch 287 Acc: 89.84 BMA: 97.72 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 923 train Loss: 10823.1 test Loss: 1675.7
Epoch 288 Iter 0 subLoss 10205.7 multi 1.00 import weight 0.00
Epoch 288 Iter 1 subLoss 10325.2 multi -4.97 import weight 0.00
Epoch 288 Iter 2 subLoss 12883.8 multi -1.98 import weight 0.00
Epoch 288 Iter 3 subLoss 13898.2 multi 6.97 import weight 0.00
Epoch 288 Iter 4 subLoss 9706.5 multi -4.97 import weight 0.00
Epoch 288 Iter 5 subLoss 12470.4 multi -10.94 import weight 0.00
Epoch 288 Iter 6 subLoss 24590.7 multi 1.00 import weight 0.00
Epoch 288 Iter 7 subLoss 19845.0 multi -1.99 import weight 0.00
Epoch 288 Iter 8 subLoss 26447.4 multi 1.00 import weight 0.00
Epoch 288 Iter 9 subLoss 22118.7 multi 1.00 import weight 0.00
Epoch 288 Iter 10 subLoss 19385.3 multi 3.99 import weight 0.00
Epoch 288 Iter 11 subLoss 15301.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0125 / 0.08440 / 24.39
Entropy seen (from low to high)
[72, 51, 38, 296, 181, 132, 126, 133, 203, 253, 318, 347, 299, 215, 178, 183, 114, 96, 129, 167, 241, 225, 202, 161, 121, 98, 77, 79, 83, 54, 35, 41, 34, 31, 35, 27, 14, 11, 9, 11, 7, 4, 5, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 18, 42, 71, 137, 158, 194, 223, 218, 244, 250, 247, 258, 281, 303, 242, 276, 225, 220, 220, 164, 159, 130, 146, 100, 66, 51, 23, 28, 27, 26, 25, 15, 20, 15, 11, 9, 5, 5, 4, 2, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 29.3, 32.8, 36.9, 40.4, 43.7, 47.1, 50.7, 53.9, 57.8, 61.1, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 33.3, 28.5, 79.9, 70.5, 64.7, 66.6, 71.8, 73.8, 80.8, 89.0, 90.8, 94.1]
[0, 0, 0, 0, 0, 0, 0, 2, 3, 7, 10, 17, 34, 33, 32, 42, 47, 64, 109, 120]
Epoch 288 Acc: 84.08 BMA: 97.72 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1530 train Loss: 14105.0 test Loss: 2461.6
Epoch 289 Iter 0 subLoss 14023.7 multi 1.00 import weight 0.00
Epoch 289 Iter 1 subLoss 13281.3 multi 3.99 import weight 0.00
Epoch 289 Iter 2 subLoss 12578.7 multi 12.94 import weight 0.00
Epoch 289 Iter 3 subLoss 9025.5 multi -1.99 import weight 0.00
Epoch 289 Iter 4 subLoss 9366.8 multi -1.98 import weight 0.00
Epoch 289 Iter 5 subLoss 11062.7 multi 3.98 import weight 0.00
Epoch 289 Iter 6 subLoss 7910.6 multi 3.98 import weight 0.00
Epoch 289 Iter 7 subLoss 7150.8 multi 3.98 import weight 0.00
Epoch 289 Iter 8 subLoss 6708.6 multi 1.00 import weight 0.00
Epoch 289 Iter 9 subLoss 6359.3 multi -7.96 import weight 0.00
Epoch 289 Iter 10 subLoss 7559.6 multi 6.97 import weight 0.00
Epoch 289 Iter 11 subLoss 6333.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0125 / 0.08449 / 24.84
Entropy seen (from low to high)
[74, 52, 38, 321, 171, 128, 125, 143, 226, 266, 322, 339, 287, 219, 171, 163, 112, 95, 130, 180, 242, 226, 184, 148, 119, 100, 79, 76, 82, 56, 32, 42, 39, 29, 31, 26, 17, 8, 13, 10, 6, 4, 5, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 18, 40, 79, 133, 154, 199, 216, 213, 252, 259, 239, 251, 289, 304, 228, 291, 211, 217, 219, 176, 143, 136, 128, 117, 63, 62, 27, 29, 27, 25, 25, 15, 21, 13, 12, 10, 5, 5, 5, 2, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.3, 29.0, 32.7, 37.1, 40.7, 43.8, 47.1, 50.9, 54.1, 57.9, 61.2, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 28.5, 72.7, 78.9, 64.2, 65.7, 70.5, 78.0, 78.2, 88.0, 92.4, 94.1]
[0, 0, 0, 0, 0, 0, 0, 1, 4, 7, 11, 19, 28, 38, 34, 41, 46, 67, 106, 120]
Epoch 289 Acc: 94.71 BMA: 97.72 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 633 train Loss: 7312.3 test Loss: 895.7
Epoch 290 Iter 0 subLoss 6795.5 multi 9.96 import weight 0.00
Epoch 290 Iter 1 subLoss 5934.3 multi 3.99 import weight 0.00
Epoch 290 Iter 2 subLoss 5577.1 multi 3.99 import weight 0.00
Epoch 290 Iter 3 subLoss 5462.2 multi -1.98 import weight 0.00
Epoch 290 Iter 4 subLoss 5160.4 multi 3.98 import weight 0.00
Epoch 290 Iter 5 subLoss 5179.2 multi -1.99 import weight 0.00
Epoch 290 Iter 6 subLoss 5419.9 multi -1.98 import weight 0.00
Epoch 290 Iter 7 subLoss 5960.0 multi 6.97 import weight 0.00
Epoch 290 Iter 8 subLoss 4737.5 multi 3.98 import weight 0.00
Epoch 290 Iter 9 subLoss 4416.9 multi 1.00 import weight 0.00
Epoch 290 Iter 10 subLoss 4490.6 multi 3.98 import weight 0.00
Epoch 290 Iter 11 subLoss 4471.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0123 / 0.08503 / 23.49
Entropy seen (from low to high)
[74, 54, 39, 337, 165, 131, 130, 156, 224, 298, 337, 323, 274, 209, 170, 149, 109, 105, 144, 198, 247, 215, 175, 138, 107, 99, 70, 72, 84, 50, 35, 37, 34, 31, 32, 22, 17, 10, 11, 10, 5, 4, 6, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 17, 41, 77, 128, 151, 191, 220, 212, 247, 250, 240, 252, 284, 288, 243, 271, 228, 200, 223, 186, 142, 137, 130, 126, 75, 70, 27, 27, 32, 26, 24, 18, 19, 15, 12, 12, 4, 6, 5, 2, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.4, 33.2, 36.9, 40.5, 44.0, 47.1, 50.7, 54.4, 58.0, 61.4, 65.0, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 16.6, 77.7, 64.9, 60.7, 61.7, 74.3, 76.1, 77.4, 86.9, 92.5, 93.3]
[0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 9, 20, 28, 34, 39, 42, 40, 69, 94, 121]
Epoch 290 Acc: 97.12 BMA: 97.72 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 447 train Loss: 4661.9 test Loss: 507.5
Epoch 291 Iter 0 subLoss 4051.6 multi -1.99 import weight 0.00
Epoch 291 Iter 1 subLoss 4895.8 multi 6.97 import weight 0.00
Epoch 291 Iter 2 subLoss 4531.9 multi 3.98 import weight 0.00
Epoch 291 Iter 3 subLoss 4225.4 multi -7.96 import weight 0.00
Epoch 291 Iter 4 subLoss 4157.2 multi 3.99 import weight 0.00
Epoch 291 Iter 5 subLoss 4361.2 multi 21.90 import weight 0.00
Epoch 291 Iter 6 subLoss 4469.7 multi 6.97 import weight 0.00
Epoch 291 Iter 7 subLoss 3964.7 multi 18.91 import weight 0.00
Epoch 291 Iter 8 subLoss 3641.0 multi -4.97 import weight 0.00
Epoch 291 Iter 9 subLoss 4543.2 multi -10.94 import weight 0.00
Epoch 291 Iter 10 subLoss 10365.2 multi 1.00 import weight 0.00
Epoch 291 Iter 11 subLoss 7906.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0121 / 0.08545 / 23.87
Entropy seen (from low to high)
[75, 53, 43, 357, 154, 135, 130, 160, 239, 306, 360, 319, 251, 195, 174, 135, 113, 110, 157, 223, 243, 206, 164, 130, 92, 87, 80, 66, 82, 43, 44, 32, 36, 34, 28, 20, 16, 10, 10, 9, 6, 5, 5, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 16, 39, 79, 123, 152, 181, 225, 212, 230, 255, 233, 267, 263, 298, 244, 254, 237, 191, 227, 184, 145, 147, 126, 133, 87, 73, 37, 24, 28, 30, 24, 16, 17, 17, 14, 12, 5, 5, 6, 2, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.6, 29.4, 33.8, 36.9, 40.2, 43.6, 47.1, 50.7, 54.1, 57.8, 61.5, 64.9, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 19.9, 16.6, 62.4, 76.4, 70.3, 56.7, 75.6, 72.9, 78.5, 85.7, 91.4, 94.7]
[0, 0, 0, 0, 0, 0, 0, 1, 5, 6, 8, 17, 27, 37, 41, 37, 42, 70, 94, 114]
Epoch 291 Acc: 95.21 BMA: 97.74 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 790 train Loss: 6421.0 test Loss: 695.8
Epoch 292 Iter 0 subLoss 6418.9 multi 1.00 import weight 0.00
Epoch 292 Iter 1 subLoss 6108.7 multi -1.99 import weight 0.00
Epoch 292 Iter 2 subLoss 6479.8 multi -1.99 import weight 0.00
Epoch 292 Iter 3 subLoss 8916.5 multi -1.99 import weight 0.00
Epoch 292 Iter 4 subLoss 15144.2 multi 3.99 import weight 0.00
Epoch 292 Iter 5 subLoss 5069.1 multi 3.98 import weight 0.00
Epoch 292 Iter 6 subLoss 4491.6 multi 6.97 import weight 0.00
Epoch 292 Iter 7 subLoss 4288.4 multi 3.99 import weight 0.00
Epoch 292 Iter 8 subLoss 3158.2 multi -10.94 import weight 0.00
Epoch 292 Iter 9 subLoss 4019.1 multi -1.98 import weight 0.00
Epoch 292 Iter 10 subLoss 4208.7 multi -4.97 import weight 0.00
Epoch 292 Iter 11 subLoss 4214.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0119 / 0.08595 / 23.46
Entropy seen (from low to high)
[75, 55, 44, 364, 148, 143, 136, 173, 249, 325, 364, 311, 233, 199, 179, 113, 110, 121, 184, 238, 233, 190, 144, 126, 91, 79, 80, 67, 72, 44, 40, 38, 26, 37, 28, 20, 16, 11, 7, 7, 7, 4, 5, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 15, 39, 73, 123, 152, 176, 215, 215, 220, 249, 239, 272, 247, 301, 244, 247, 238, 198, 205, 203, 154, 138, 137, 133, 101, 65, 52, 26, 26, 33, 25, 15, 16, 22, 12, 13, 4, 5, 8, 1, 4, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 29.7, 33.4, 36.7, 40.4, 43.7, 47.1, 50.7, 54.1, 57.7, 61.4, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 39.9, 24.9, 54.5, 79.9, 67.8, 66.6, 71.7, 74.1, 71.1, 86.4, 90.2, 95.0]
[0, 0, 0, 0, 0, 0, 0, 1, 5, 4, 11, 15, 28, 33, 46, 31, 45, 59, 92, 122]
Epoch 292 Acc: 97.22 BMA: 97.74 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 421 train Loss: 4255.5 test Loss: 459.3
Epoch 293 Iter 0 subLoss 4118.1 multi -7.96 import weight 0.00
Epoch 293 Iter 1 subLoss 4472.1 multi 1.00 import weight 0.00
Epoch 293 Iter 2 subLoss 4651.7 multi -7.96 import weight 0.00
Epoch 293 Iter 3 subLoss 5715.2 multi -1.98 import weight 0.00
Epoch 293 Iter 4 subLoss 5860.4 multi 3.99 import weight 0.00
Epoch 293 Iter 5 subLoss 4764.9 multi 9.96 import weight 0.00
Epoch 293 Iter 6 subLoss 4413.7 multi 3.99 import weight 0.00
Epoch 293 Iter 7 subLoss 3988.3 multi 1.00 import weight 0.00
Epoch 293 Iter 8 subLoss 3876.0 multi 6.97 import weight 0.00
Epoch 293 Iter 9 subLoss 3974.9 multi -25.87 import weight 0.00
Epoch 293 Iter 10 subLoss 4923.5 multi 15.93 import weight 0.00
Epoch 293 Iter 11 subLoss 4374.5 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0117 / 0.08629 / 23.35
Entropy seen (from low to high)
[76, 55, 44, 383, 147, 138, 139, 174, 261, 336, 369, 301, 236, 180, 175, 115, 107, 145, 191, 257, 223, 177, 121, 119, 102, 67, 80, 64, 68, 49, 34, 37, 29, 35, 27, 20, 15, 10, 8, 6, 7, 4, 5, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 43, 72, 126, 142, 172, 227, 199, 229, 243, 242, 264, 248, 288, 252, 233, 232, 211, 204, 196, 165, 133, 134, 134, 112, 73, 55, 28, 28, 31, 28, 17, 14, 25, 14, 10, 5, 4, 7, 3, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 30.2, 32.7, 36.3, 39.8, 43.7, 47.4, 50.9, 54.0, 57.5, 61.2, 64.6, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 24.9, 44.4, 81.2, 61.2, 55.8, 80.9, 74.9, 75.6, 80.7, 91.0, 95.8]
[0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 9, 16, 31, 34, 42, 32, 41, 57, 89, 121]
Epoch 293 Acc: 96.67 BMA: 97.84 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 437 train Loss: 6287.2 test Loss: 582.4
Epoch 294 Iter 0 subLoss 6358.4 multi -4.97 import weight 0.00
Epoch 294 Iter 1 subLoss 12015.1 multi 3.99 import weight 0.00
Epoch 294 Iter 2 subLoss 4859.3 multi -13.93 import weight 0.00
Epoch 294 Iter 3 subLoss 5499.5 multi -1.99 import weight 0.00
Epoch 294 Iter 4 subLoss 6080.8 multi 6.97 import weight 0.00
Epoch 294 Iter 5 subLoss 4559.3 multi 1.00 import weight 0.00
Epoch 294 Iter 6 subLoss 4407.3 multi -10.94 import weight 0.00
Epoch 294 Iter 7 subLoss 5426.0 multi 3.99 import weight 0.00
Epoch 294 Iter 8 subLoss 4807.9 multi 21.90 import weight 0.00
Epoch 294 Iter 9 subLoss 4473.5 multi 3.99 import weight 0.00
Epoch 294 Iter 10 subLoss 3845.7 multi 1.00 import weight 0.00
Epoch 294 Iter 11 subLoss 4089.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0115 / 0.08693 / 22.68
Entropy seen (from low to high)
[76, 56, 47, 396, 142, 139, 146, 187, 284, 340, 363, 300, 221, 180, 156, 124, 118, 142, 223, 256, 212, 161, 133, 96, 92, 60, 81, 67, 58, 50, 35, 32, 34, 30, 28, 18, 13, 10, 9, 6, 6, 4, 5, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 15, 42, 70, 120, 137, 173, 215, 194, 232, 232, 248, 246, 259, 281, 262, 218, 223, 219, 194, 215, 151, 158, 137, 122, 123, 86, 64, 31, 26, 32, 28, 19, 13, 25, 18, 8, 7, 4, 7, 3, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 30.9, 33.9, 36.8, 40.4, 43.9, 47.3, 50.6, 54.0, 57.8, 61.4, 64.6, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 24.9, 49.9, 39.9, 93.3, 53.5, 61.2, 77.4, 72.4, 78.0, 79.9, 91.2, 94.0]
[0, 0, 0, 0, 0, 0, 0, 2, 4, 4, 10, 15, 28, 31, 40, 40, 41, 50, 80, 117]
Epoch 294 Acc: 97.30 BMA: 97.82 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 408 train Loss: 3947.2 test Loss: 421.8
Epoch 295 Iter 0 subLoss 3777.3 multi -22.88 import weight 0.00
Epoch 295 Iter 1 subLoss 4112.4 multi -4.97 import weight 0.00
Epoch 295 Iter 2 subLoss 5170.3 multi 1.00 import weight 0.00
Epoch 295 Iter 3 subLoss 5132.2 multi -1.98 import weight 0.00
Epoch 295 Iter 4 subLoss 4560.0 multi 12.94 import weight 0.00
Epoch 295 Iter 5 subLoss 5522.3 multi 1.00 import weight 0.00
Epoch 295 Iter 6 subLoss 4217.8 multi 6.97 import weight 0.00
Epoch 295 Iter 7 subLoss 3620.4 multi -4.97 import weight 0.00
Epoch 295 Iter 8 subLoss 4458.4 multi 6.97 import weight 0.00
Epoch 295 Iter 9 subLoss 3991.2 multi 18.91 import weight 0.00
Epoch 295 Iter 10 subLoss 3764.7 multi 6.97 import weight 0.00
Epoch 295 Iter 11 subLoss 3294.9 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0113 / 0.08755 / 22.03
Entropy seen (from low to high)
[77, 57, 50, 402, 145, 135, 147, 216, 301, 344, 364, 286, 218, 175, 145, 116, 126, 169, 238, 251, 198, 154, 125, 86, 82, 72, 74, 63, 53, 43, 38, 31, 37, 25, 27, 15, 14, 9, 9, 6, 5, 5, 4, 2, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 15, 41, 66, 114, 130, 175, 209, 193, 227, 227, 254, 247, 246, 272, 267, 224, 218, 211, 207, 195, 184, 148, 137, 122, 134, 94, 64, 42, 28, 28, 30, 19, 16, 23, 17, 12, 7, 4, 5, 4, 3, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 30.0, 33.5, 36.4, 40.3, 43.9, 47.2, 50.6, 54.2, 57.8, 61.4, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 33.3, 42.8, 77.7, 58.3, 61.7, 72.2, 73.1, 81.3, 76.7, 89.0, 95.6]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 6, 7, 18, 24, 34, 36, 41, 43, 43, 82, 114]
Epoch 295 Acc: 97.78 BMA: 97.86 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 329 train Loss: 3738.0 test Loss: 372.3
Epoch 296 Iter 0 subLoss 3765.7 multi 9.96 import weight 1.00
Epoch 296 Iter 1 subLoss 3974.8 multi -22.88 import weight 0.00
Epoch 296 Iter 2 subLoss 5322.8 multi 3.99 import weight 0.00
Epoch 296 Iter 3 subLoss 3508.8 multi 24.88 import weight 0.00
Epoch 296 Iter 4 subLoss 4221.2 multi -10.94 import weight 0.00
Epoch 296 Iter 5 subLoss 6534.2 multi 6.97 import weight 0.00
Epoch 296 Iter 6 subLoss 3719.8 multi 1.00 import weight 0.00
Epoch 296 Iter 7 subLoss 4210.6 multi 9.96 import weight 0.00
Epoch 296 Iter 8 subLoss 3265.2 multi -1.99 import weight 0.00
Epoch 296 Iter 9 subLoss 3436.1 multi -4.97 import weight 0.00
Epoch 296 Iter 10 subLoss 3777.6 multi -25.87 import weight 0.00
Epoch 296 Iter 11 subLoss 14647.4 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0114 / 0.08681 / 22.45
Entropy seen (from low to high)
[77, 58, 78, 372, 93, 63, 97, 203, 305, 347, 375, 326, 235, 236, 143, 131, 152, 194, 240, 238, 186, 144, 116, 110, 84, 71, 68, 73, 59, 37, 39, 28, 36, 29, 27, 12, 15, 12, 7, 7, 5, 4, 4, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 38, 71, 110, 138, 180, 200, 202, 239, 224, 260, 251, 237, 289, 255, 237, 227, 240, 194, 200, 166, 140, 145, 126, 113, 91, 48, 28, 29, 27, 33, 20, 15, 24, 12, 12, 10, 4, 5, 2, 3, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 30.4, 33.6, 37.0, 40.7, 44.1, 47.4, 51.0, 54.2, 57.6, 61.1, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 33.3, 33.3, 81.2, 62.9, 61.1, 70.9, 71.4, 79.4, 79.9, 91.2, 95.4]
[0, 0, 0, 0, 0, 0, 0, 1, 4, 3, 12, 16, 27, 36, 31, 42, 39, 45, 91, 110]
Epoch 296 Acc: 71.78 BMA: 97.84 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1464 train Loss: 50515.1 test Loss: 9303.7
Epoch 297 Iter 0 subLoss 48531.8 multi -1.99 import weight 0.00
Epoch 297 Iter 1 subLoss 268847.2 multi 1.00 import weight 0.00
Epoch 297 Iter 2 subLoss 22418.3 multi 1.00 import weight 0.00
Epoch 297 Iter 3 subLoss 18900.7 multi -1.99 import weight 0.00
Epoch 297 Iter 4 subLoss 28634.7 multi 1.00 import weight 0.00
Epoch 297 Iter 5 subLoss 20634.7 multi 1.00 import weight 0.00
Epoch 297 Iter 6 subLoss 18444.4 multi -1.99 import weight 0.00
Epoch 297 Iter 7 subLoss 23638.9 multi 3.99 import weight 0.00
Epoch 297 Iter 8 subLoss 14062.2 multi 6.97 import weight 0.00
Epoch 297 Iter 9 subLoss 4560.7 multi 15.93 import weight 0.00
Epoch 297 Iter 10 subLoss 4184.3 multi -25.87 import weight 0.00
Epoch 297 Iter 11 subLoss 9668.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0113 / 0.08711 / 21.78
Entropy seen (from low to high)
[80, 56, 114, 346, 86, 62, 119, 202, 315, 381, 360, 314, 226, 229, 143, 132, 161, 207, 252, 227, 178, 123, 116, 107, 79, 61, 79, 69, 53, 35, 42, 27, 35, 31, 24, 14, 14, 11, 6, 7, 5, 4, 4, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 36, 69, 112, 139, 177, 197, 196, 249, 215, 258, 255, 246, 270, 252, 247, 214, 226, 210, 195, 163, 152, 134, 143, 105, 99, 54, 33, 29, 30, 34, 19, 14, 22, 14, 13, 10, 3, 6, 2, 3, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 29.7, 32.7, 36.8, 40.5, 44.1, 47.4, 50.8, 54.4, 57.7, 61.2, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 39.9, 39.9, 70.5, 65.5, 56.2, 72.7, 73.3, 77.7, 80.4, 89.4, 95.4]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 5, 10, 17, 29, 32, 33, 45, 36, 46, 85, 109]
Epoch 297 Acc: 97.22 BMA: 97.86 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 966 train Loss: 4632.1 test Loss: 537.3
Epoch 298 Iter 0 subLoss 4563.8 multi 18.91 import weight 0.00
Epoch 298 Iter 1 subLoss 3803.2 multi -10.94 import weight 0.00
Epoch 298 Iter 2 subLoss 5419.9 multi 1.00 import weight 0.00
Epoch 298 Iter 3 subLoss 4600.0 multi -7.96 import weight 0.00
Epoch 298 Iter 4 subLoss 7256.6 multi 1.00 import weight 0.00
Epoch 298 Iter 5 subLoss 5634.3 multi -1.99 import weight 0.00
Epoch 298 Iter 6 subLoss 7720.2 multi 1.00 import weight 0.00
Epoch 298 Iter 7 subLoss 6577.6 multi 9.96 import weight 0.00
Epoch 298 Iter 8 subLoss 4887.5 multi -4.97 import weight 0.00
Epoch 298 Iter 9 subLoss 6662.7 multi -7.96 import weight 0.00
Epoch 298 Iter 10 subLoss 20955.4 multi -1.99 import weight 0.00
Epoch 298 Iter 11 subLoss 39323.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0115 / 0.08662 / 22.49
Entropy seen (from low to high)
[48, 48, 67, 286, 142, 79, 76, 116, 248, 363, 381, 332, 277, 282, 200, 148, 185, 236, 266, 232, 174, 120, 130, 98, 82, 65, 72, 66, 54, 38, 40, 31, 32, 29, 29, 11, 16, 11, 5, 9, 4, 5, 3, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 36, 72, 113, 137, 182, 191, 212, 239, 223, 257, 253, 268, 249, 256, 242, 237, 221, 203, 199, 168, 143, 141, 126, 117, 89, 51, 47, 38, 25, 26, 19, 20, 12, 11, 7, 5, 4, 4, 2, 1, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 33.1, 36.5, 40.4, 44.0, 47.1, 50.8, 54.2, 57.7, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 0.0, 44.4, 72.2, 64.2, 54.8, 72.2, 70.4, 86.1, 77.5, 89.8, 96.1]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 9, 18, 28, 31, 36, 44, 36, 49, 89, 104]
Epoch 298 Acc: 72.15 BMA: 97.90 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3932 train Loss: 28499.9 test Loss: 4668.8
Epoch 299 Iter 0 subLoss 28203.9 multi 1.00 import weight 0.00
Epoch 299 Iter 1 subLoss 20914.0 multi 1.00 import weight 0.00
Epoch 299 Iter 2 subLoss 15020.7 multi -1.99 import weight 0.00
Epoch 299 Iter 3 subLoss 24158.5 multi 1.00 import weight 0.00
Epoch 299 Iter 4 subLoss 18590.5 multi 3.98 import weight 0.00
Epoch 299 Iter 5 subLoss 6263.6 multi 1.00 import weight 0.00
Epoch 299 Iter 6 subLoss 6272.1 multi -7.96 import weight 0.00
Epoch 299 Iter 7 subLoss 10150.8 multi 1.00 import weight 0.00
Epoch 299 Iter 8 subLoss 8421.5 multi -1.99 import weight 0.00
Epoch 299 Iter 9 subLoss 11356.1 multi 6.97 import weight 0.00
Epoch 299 Iter 10 subLoss 4857.7 multi -10.94 import weight 0.00
Epoch 299 Iter 11 subLoss 6302.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0113 / 0.08704 / 22.05
Entropy seen (from low to high)
[48, 50, 78, 282, 143, 71, 82, 126, 277, 376, 377, 324, 278, 275, 182, 155, 197, 246, 266, 213, 169, 118, 118, 93, 80, 71, 67, 61, 60, 34, 39, 30, 31, 27, 28, 13, 14, 11, 6, 8, 4, 5, 3, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 36, 71, 106, 136, 180, 186, 221, 226, 231, 245, 256, 270, 249, 252, 238, 217, 234, 218, 181, 180, 142, 141, 133, 124, 88, 56, 49, 35, 31, 25, 20, 22, 11, 12, 7, 5, 5, 4, 1, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.4, 33.2, 37.1, 40.5, 43.8, 47.1, 50.7, 54.2, 57.8, 61.5, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 16.6, 49.9, 69.2, 64.5, 57.1, 71.0, 72.0, 82.0, 80.4, 89.2, 95.1]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 6, 10, 13, 31, 28, 38, 43, 39, 46, 84, 104]
Epoch 299 Acc: 97.43 BMA: 97.88 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 630 train Loss: 4972.5 test Loss: 498.5
Epoch 300 Iter 0 subLoss 5035.1 multi 12.94 import weight 0.00
Epoch 300 Iter 1 subLoss 4142.3 multi -4.97 import weight 0.00
Epoch 300 Iter 2 subLoss 4684.0 multi 1.00 import weight 0.00
Epoch 300 Iter 3 subLoss 4283.3 multi 6.97 import weight 0.00
Epoch 300 Iter 4 subLoss 3754.7 multi 15.93 import weight 0.00
Epoch 300 Iter 5 subLoss 3487.4 multi -1.98 import weight 0.00
Epoch 300 Iter 6 subLoss 3976.8 multi -19.90 import weight 0.00
Epoch 300 Iter 7 subLoss 4080.5 multi 12.94 import weight 0.00
Epoch 300 Iter 8 subLoss 3026.9 multi -1.99 import weight 0.00
Epoch 300 Iter 9 subLoss 4160.8 multi -7.96 import weight 0.00
Epoch 300 Iter 10 subLoss 4699.5 multi 1.00 import weight 0.00
Epoch 300 Iter 11 subLoss 4217.0 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0111 / 0.08759 / 21.74
Entropy seen (from low to high)
[48, 50, 97, 278, 137, 66, 89, 147, 285, 403, 359, 328, 284, 252, 179, 159, 210, 255, 262, 206, 148, 126, 108, 87, 81, 59, 68, 58, 61, 35, 36, 33, 26, 27, 29, 9, 15, 11, 5, 7, 5, 6, 2, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 14, 33, 70, 101, 135, 175, 182, 218, 217, 229, 248, 257, 267, 243, 256, 226, 222, 240, 210, 172, 192, 152, 135, 140, 123, 96, 69, 50, 38, 35, 24, 17, 22, 15, 12, 8, 5, 4, 5, 0, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.2, 33.2, 37.1, 40.5, 43.8, 46.8, 50.7, 54.2, 57.6, 61.4, 64.9, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 16.6, 55.5, 57.1, 68.9, 53.8, 77.4, 65.8, 81.5, 84.7, 86.8, 94.1]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 6, 9, 14, 29, 26, 40, 41, 38, 46, 76, 102]
Epoch 300 Acc: 97.63 BMA: 97.88 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 421 train Loss: 3941.5 test Loss: 390.0
Epoch 301 Iter 0 subLoss 4064.6 multi -1.99 import weight 0.00
Epoch 301 Iter 1 subLoss 4157.4 multi 3.98 import weight 0.00
Epoch 301 Iter 2 subLoss 3760.6 multi 9.96 import weight 1.00
Epoch 301 Iter 3 subLoss 3481.4 multi 1.00 import weight 0.00
Epoch 301 Iter 4 subLoss 3584.5 multi -1.99 import weight 0.00
Epoch 301 Iter 5 subLoss 3353.1 multi -1.99 import weight 0.00
Epoch 301 Iter 6 subLoss 3526.7 multi -13.93 import weight 0.00
Epoch 301 Iter 7 subLoss 3987.1 multi -4.97 import weight 0.00
Epoch 301 Iter 8 subLoss 4664.1 multi 6.97 import weight 0.00
Epoch 301 Iter 9 subLoss 4602.9 multi 1.00 import weight 0.00
Epoch 301 Iter 10 subLoss 3880.2 multi -4.97 import weight 0.00
Epoch 301 Iter 11 subLoss 4426.0 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0110 / 0.08798 / 21.60
Entropy seen (from low to high)
[48, 51, 115, 268, 133, 68, 88, 167, 311, 417, 341, 318, 305, 227, 171, 164, 223, 278, 253, 182, 142, 117, 107, 84, 82, 51, 70, 56, 53, 41, 37, 29, 28, 26, 25, 12, 13, 11, 4, 8, 4, 7, 1, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 13, 33, 68, 95, 133, 170, 188, 223, 211, 212, 257, 262, 256, 244, 254, 219, 235, 232, 207, 186, 178, 156, 141, 134, 133, 95, 78, 50, 44, 37, 16, 26, 18, 17, 13, 8, 6, 4, 4, 1, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.3, 33.4, 37.3, 40.6, 43.7, 46.9, 50.8, 54.2, 57.5, 61.2, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 16.6, 66.6, 42.8, 74.0, 58.6, 76.3, 69.2, 78.3, 86.3, 83.5, 94.0]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 6, 9, 14, 27, 29, 38, 39, 37, 44, 73, 101]
Epoch 301 Acc: 96.94 BMA: 97.86 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 442 train Loss: 5146.0 test Loss: 504.3
Epoch 302 Iter 0 subLoss 5217.3 multi -1.98 import weight 0.00
Epoch 302 Iter 1 subLoss 5543.9 multi 3.98 import weight 0.00
Epoch 302 Iter 2 subLoss 4280.9 multi 9.96 import weight 0.00
Epoch 302 Iter 3 subLoss 4356.5 multi -16.91 import weight 0.00
Epoch 302 Iter 4 subLoss 5109.6 multi -1.99 import weight 0.00
Epoch 302 Iter 5 subLoss 5515.7 multi -4.97 import weight 0.00
Epoch 302 Iter 6 subLoss 7911.3 multi 3.99 import weight 0.00
Epoch 302 Iter 7 subLoss 4516.2 multi 9.96 import weight 0.00
Epoch 302 Iter 8 subLoss 3599.0 multi -22.88 import weight 0.00
Epoch 302 Iter 9 subLoss 5210.2 multi 1.00 import weight 0.00
Epoch 302 Iter 10 subLoss 5218.6 multi 3.99 import weight 0.00
Epoch 302 Iter 11 subLoss 4463.9 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0109 / 0.08842 / 21.56
Entropy seen (from low to high)
[48, 52, 133, 256, 131, 70, 97, 182, 325, 427, 335, 311, 313, 213, 165, 174, 236, 277, 238, 177, 128, 120, 104, 80, 79, 57, 66, 55, 44, 42, 37, 29, 26, 29, 21, 13, 11, 12, 3, 8, 5, 6, 1, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 13, 33, 67, 90, 128, 171, 185, 208, 225, 205, 252, 268, 245, 251, 250, 220, 223, 250, 190, 195, 180, 155, 138, 139, 128, 108, 80, 55, 48, 38, 18, 25, 19, 21, 11, 10, 6, 4, 3, 2, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.2, 33.2, 36.7, 39.9, 43.6, 47.1, 50.9, 54.1, 57.5, 61.3, 64.9, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 19.9, 49.9, 33.3, 80.6, 62.0, 74.9, 68.4, 78.3, 86.6, 82.8, 93.5]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 5, 8, 12, 31, 29, 36, 38, 37, 45, 70, 93]
Epoch 302 Acc: 97.70 BMA: 97.88 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 446 train Loss: 4290.8 test Loss: 402.1
Epoch 303 Iter 0 subLoss 3889.2 multi -1.99 import weight 0.00
Epoch 303 Iter 1 subLoss 4182.2 multi -22.88 import weight 0.00
Epoch 303 Iter 2 subLoss 5367.0 multi 3.99 import weight 0.00
Epoch 303 Iter 3 subLoss 4810.3 multi 3.99 import weight 0.00
Epoch 303 Iter 4 subLoss 4955.3 multi 6.97 import weight 0.00
Epoch 303 Iter 5 subLoss 4173.2 multi 27.87 import weight 1.00
Epoch 303 Iter 6 subLoss 4187.3 multi -22.88 import weight 0.00
Epoch 303 Iter 7 subLoss 6123.4 multi 6.97 import weight 0.00
Epoch 303 Iter 8 subLoss 3458.0 multi 9.96 import weight 0.00
Epoch 303 Iter 9 subLoss 3594.3 multi -19.90 import weight 0.00
Epoch 303 Iter 10 subLoss 4299.2 multi -7.96 import weight 0.00
Epoch 303 Iter 11 subLoss 4953.8 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0107 / 0.08889 / 21.25
Entropy seen (from low to high)
[49, 56, 141, 253, 128, 76, 98, 201, 352, 421, 340, 299, 316, 201, 156, 199, 242, 271, 227, 171, 114, 124, 91, 80, 78, 55, 62, 60, 40, 37, 35, 30, 26, 29, 19, 13, 14, 10, 3, 7, 6, 6, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 13, 33, 66, 84, 132, 160, 189, 195, 228, 199, 258, 262, 246, 258, 230, 218, 237, 229, 200, 195, 184, 158, 147, 137, 132, 105, 95, 56, 51, 37, 20, 25, 20, 20, 12, 11, 6, 3, 4, 2, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.5, 33.0, 37.0, 40.4, 43.7, 47.2, 51.0, 53.9, 57.5, 61.4, 65.0, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 24.9, 45.4, 49.9, 79.9, 67.7, 70.9, 69.9, 75.6, 85.7, 79.6, 94.9]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 11, 10, 30, 31, 31, 40, 37, 49, 59, 99]
Epoch 303 Acc: 97.26 BMA: 97.88 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 495 train Loss: 4333.7 test Loss: 469.0
Epoch 304 Iter 0 subLoss 4313.8 multi -16.91 import weight 0.00
Epoch 304 Iter 1 subLoss 6413.9 multi 3.99 import weight 0.00
Epoch 304 Iter 2 subLoss 4434.9 multi 3.98 import weight 0.00
Epoch 304 Iter 3 subLoss 4101.0 multi 9.96 import weight 0.00
Epoch 304 Iter 4 subLoss 4184.8 multi -19.90 import weight 0.00
Epoch 304 Iter 5 subLoss 4665.0 multi 9.96 import weight 0.00
Epoch 304 Iter 6 subLoss 4501.0 multi -10.94 import weight 0.00
Epoch 304 Iter 7 subLoss 4745.2 multi -10.94 import weight 0.00
Epoch 304 Iter 8 subLoss 6166.5 multi 1.00 import weight 0.00
Epoch 304 Iter 9 subLoss 5516.8 multi -1.99 import weight 0.00
Epoch 304 Iter 10 subLoss 5996.8 multi 1.00 import weight 0.00
Epoch 304 Iter 11 subLoss 5507.7 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0106 / 0.08937 / 21.13
Entropy seen (from low to high)
[50, 57, 165, 236, 126, 76, 108, 217, 369, 419, 353, 298, 290, 201, 157, 198, 260, 273, 216, 150, 122, 114, 89, 76, 69, 63, 58, 54, 41, 42, 33, 29, 23, 26, 22, 15, 9, 10, 3, 8, 6, 5, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 12, 36, 60, 78, 140, 153, 187, 190, 230, 194, 249, 262, 249, 256, 217, 227, 245, 222, 204, 194, 182, 158, 152, 132, 135, 111, 95, 69, 48, 38, 27, 25, 19, 22, 12, 10, 8, 3, 4, 2, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 33.3, 37.2, 40.3, 43.9, 47.2, 51.0, 54.1, 57.9, 61.4, 64.9, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 0.0, 49.9, 54.5, 82.7, 65.6, 65.5, 68.1, 82.3, 86.6, 76.7, 94.1]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 10, 11, 29, 32, 29, 44, 34, 45, 56, 102]
Epoch 304 Acc: 97.31 BMA: 97.90 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 550 train Loss: 4651.4 test Loss: 454.1
Epoch 305 Iter 0 subLoss 4915.6 multi -7.96 import weight 0.00
Epoch 305 Iter 1 subLoss 5803.1 multi -4.97 import weight 0.00
Epoch 305 Iter 2 subLoss 8058.9 multi -1.98 import weight 0.00
Epoch 305 Iter 3 subLoss 25105.4 multi 1.00 import weight 0.00
Epoch 305 Iter 4 subLoss 7185.4 multi 18.91 import weight 0.00
Epoch 305 Iter 5 subLoss 10726.3 multi 1.00 import weight 0.00
Epoch 305 Iter 6 subLoss 7929.7 multi -4.97 import weight 0.00
Epoch 305 Iter 7 subLoss 18425.3 multi 1.00 import weight 0.00
Epoch 305 Iter 8 subLoss 11482.6 multi -4.97 import weight 0.00
Epoch 305 Iter 9 subLoss 42887.2 multi 1.00 import weight 0.00
Epoch 305 Iter 10 subLoss 21680.5 multi 1.00 import weight 0.00
Epoch 305 Iter 11 subLoss 13599.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0106 / 0.08910 / 21.59
Entropy seen (from low to high)
[50, 57, 170, 238, 115, 81, 113, 237, 374, 415, 314, 304, 269, 194, 177, 204, 266, 283, 209, 156, 116, 113, 92, 67, 77, 60, 53, 57, 47, 39, 31, 29, 20, 31, 21, 14, 13, 6, 5, 8, 6, 5, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 12, 35, 63, 79, 141, 150, 188, 193, 219, 218, 228, 268, 258, 256, 220, 227, 237, 224, 207, 187, 179, 168, 147, 137, 126, 115, 98, 72, 52, 43, 18, 23, 22, 13, 13, 6, 6, 3, 5, 1, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 32.9, 37.0, 40.1, 43.9, 46.9, 50.9, 53.9, 57.7, 61.2, 65.0, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 0.0, 55.5, 41.6, 88.4, 67.6, 61.2, 78.0, 74.9, 86.6, 74.1, 97.0]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 9, 12, 26, 34, 31, 41, 36, 45, 62, 103]
Epoch 305 Acc: 90.25 BMA: 97.98 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1359 train Loss: 11121.3 test Loss: 1348.9
Epoch 306 Iter 0 subLoss 11501.0 multi 1.00 import weight 0.00
Epoch 306 Iter 1 subLoss 9688.5 multi 9.96 import weight 0.00
Epoch 306 Iter 2 subLoss 4515.6 multi 9.96 import weight 0.00
Epoch 306 Iter 3 subLoss 3596.6 multi -16.91 import weight 0.00
Epoch 306 Iter 4 subLoss 4936.1 multi -25.87 import weight 0.00
Epoch 306 Iter 5 subLoss 8632.4 multi 1.00 import weight 0.00
Epoch 306 Iter 6 subLoss 7412.1 multi 9.96 import weight 0.00
Epoch 306 Iter 7 subLoss 4860.9 multi -10.94 import weight 0.00
Epoch 306 Iter 8 subLoss 6731.8 multi -1.99 import weight 0.00
Epoch 306 Iter 9 subLoss 6785.8 multi -1.99 import weight 0.00
Epoch 306 Iter 10 subLoss 7277.6 multi 1.00 import weight 0.00
Epoch 306 Iter 11 subLoss 6509.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0106 / 0.08923 / 20.82
Entropy seen (from low to high)
[51, 58, 180, 235, 112, 78, 121, 265, 393, 393, 319, 297, 269, 186, 173, 201, 271, 278, 203, 148, 113, 120, 83, 70, 74, 61, 54, 59, 46, 36, 32, 29, 23, 29, 19, 15, 11, 8, 6, 6, 6, 5, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 9, 38, 63, 78, 143, 152, 177, 202, 230, 196, 238, 259, 262, 253, 213, 233, 232, 223, 205, 196, 175, 173, 142, 139, 132, 113, 99, 73, 50, 49, 18, 20, 23, 15, 11, 7, 7, 3, 5, 1, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.8, 33.4, 36.9, 40.1, 43.8, 46.9, 51.0, 54.1, 57.8, 61.2, 64.7, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.9, 0.0, 49.9, 46.1, 88.4, 58.8, 63.6, 76.1, 79.4, 84.0, 71.1, 96.3]
[0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 8, 13, 26, 34, 33, 42, 34, 44, 52, 110]
Epoch 306 Acc: 95.89 BMA: 97.98 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 650 train Loss: 5882.1 test Loss: 740.8
Epoch 307 Iter 0 subLoss 6009.0 multi 3.99 import weight 0.00
Epoch 307 Iter 1 subLoss 5679.2 multi 3.98 import weight 0.00
Epoch 307 Iter 2 subLoss 5419.8 multi 3.99 import weight 0.00
Epoch 307 Iter 3 subLoss 4600.0 multi 3.99 import weight 0.00
Epoch 307 Iter 4 subLoss 5153.1 multi 1.00 import weight 0.00
Epoch 307 Iter 5 subLoss 4376.6 multi -7.96 import weight 0.00
Epoch 307 Iter 6 subLoss 4941.5 multi 6.97 import weight 0.00
Epoch 307 Iter 7 subLoss 4702.5 multi -7.96 import weight 0.00
Epoch 307 Iter 8 subLoss 5241.9 multi -10.94 import weight 0.00
Epoch 307 Iter 9 subLoss 6149.7 multi 3.98 import weight 0.00
Epoch 307 Iter 10 subLoss 5468.0 multi 1.00 import weight 0.00
Epoch 307 Iter 11 subLoss 5085.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0105 / 0.08951 / 20.63
Entropy seen (from low to high)
[53, 57, 189, 235, 106, 81, 128, 291, 406, 380, 327, 302, 246, 181, 169, 204, 290, 264, 192, 141, 118, 113, 82, 67, 74, 60, 53, 57, 48, 38, 28, 28, 25, 27, 19, 16, 11, 6, 6, 7, 6, 5, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 44, 61, 79, 140, 148, 173, 202, 233, 189, 240, 258, 268, 241, 217, 240, 217, 219, 204, 200, 184, 156, 152, 143, 128, 123, 94, 78, 53, 48, 23, 17, 26, 17, 12, 5, 8, 3, 5, 2, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.7, 32.6, 37.0, 40.5, 43.9, 46.9, 50.8, 54.0, 57.6, 61.0, 64.7, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 49.9, 46.6, 86.9, 61.7, 62.4, 74.4, 81.8, 80.9, 74.9, 95.5]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 8, 15, 23, 34, 32, 43, 33, 42, 56, 113]
Epoch 307 Acc: 96.71 BMA: 97.96 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 508 train Loss: 5216.6 test Loss: 598.5
Epoch 308 Iter 0 subLoss 5281.2 multi -4.97 import weight 0.00
Epoch 308 Iter 1 subLoss 5231.8 multi 9.96 import weight 0.00
Epoch 308 Iter 2 subLoss 4843.1 multi 24.88 import weight 0.00
Epoch 308 Iter 3 subLoss 4427.3 multi -7.96 import weight 0.00
Epoch 308 Iter 4 subLoss 4845.1 multi 27.87 import weight 0.00
Epoch 308 Iter 5 subLoss 5350.5 multi 1.00 import weight 0.00
Epoch 308 Iter 6 subLoss 5006.3 multi 3.99 import weight 0.00
Epoch 308 Iter 7 subLoss 4150.3 multi 6.97 import weight 0.00
Epoch 308 Iter 8 subLoss 3589.9 multi 1.00 import weight 0.00
Epoch 308 Iter 9 subLoss 3727.2 multi -10.94 import weight 0.00
Epoch 308 Iter 10 subLoss 4146.9 multi -1.98 import weight 0.00
Epoch 308 Iter 11 subLoss 3933.8 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0105 / 0.08977 / 20.58
Entropy seen (from low to high)
[53, 58, 206, 225, 105, 79, 143, 300, 407, 382, 334, 290, 243, 171, 177, 223, 310, 235, 182, 137, 118, 111, 76, 68, 73, 58, 50, 51, 59, 27, 29, 33, 22, 26, 18, 16, 13, 4, 6, 8, 5, 5, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 43, 57, 78, 137, 153, 171, 194, 239, 189, 230, 254, 270, 234, 239, 226, 214, 222, 201, 197, 188, 151, 157, 149, 129, 118, 102, 89, 47, 53, 25, 17, 24, 18, 11, 6, 7, 2, 7, 2, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 32.4, 37.0, 40.4, 44.1, 47.0, 50.9, 54.2, 57.8, 61.1, 65.0, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.9, 57.1, 49.9, 86.9, 61.7, 65.6, 71.4, 79.9, 83.3, 74.9, 95.3]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 7, 16, 23, 34, 32, 42, 35, 42, 56, 107]
Epoch 308 Acc: 96.54 BMA: 97.92 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 393 train Loss: 4889.2 test Loss: 587.8
Epoch 309 Iter 0 subLoss 4842.1 multi 30.85 import weight 1.00
Epoch 309 Iter 1 subLoss 9287.3 multi 1.00 import weight 0.00
Epoch 309 Iter 2 subLoss 5686.0 multi -13.93 import weight 0.00
Epoch 309 Iter 3 subLoss 143257.6 multi 1.00 import weight 0.00
Epoch 309 Iter 4 subLoss 8379.5 multi -1.98 import weight 0.00
Epoch 309 Iter 5 subLoss 10280.2 multi 1.00 import weight 0.00
Epoch 309 Iter 6 subLoss 8870.0 multi 3.98 import weight 0.00
Epoch 309 Iter 7 subLoss 5964.2 multi -7.96 import weight 0.00
Epoch 309 Iter 8 subLoss 7598.1 multi -1.98 import weight 0.00
Epoch 309 Iter 9 subLoss 10015.7 multi 1.00 import weight 0.00
Epoch 309 Iter 10 subLoss 8334.7 multi 1.00 import weight 0.00
Epoch 309 Iter 11 subLoss 7350.0 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0104 / 0.08988 / 20.24
Entropy seen (from low to high)
[55, 57, 218, 221, 98, 84, 147, 310, 420, 382, 321, 295, 234, 176, 187, 242, 296, 227, 181, 125, 117, 101, 80, 71, 65, 58, 49, 52, 54, 32, 23, 36, 21, 27, 18, 17, 12, 3, 5, 9, 5, 5, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 44, 58, 75, 139, 155, 167, 189, 237, 193, 231, 254, 268, 233, 232, 229, 217, 231, 194, 207, 176, 150, 163, 147, 121, 118, 102, 94, 50, 55, 27, 14, 27, 18, 12, 5, 6, 5, 5, 2, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.9, 33.3, 37.2, 40.4, 44.1, 47.1, 51.0, 54.3, 57.9, 61.1, 65.1, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 0.0, 57.1, 43.7, 83.3, 64.7, 67.7, 69.0, 80.5, 82.4, 81.9, 92.0]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 7, 16, 24, 34, 31, 42, 36, 40, 61, 101]
Epoch 309 Acc: 96.24 BMA: 97.92 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 734 train Loss: 5515.7 test Loss: 677.4
Epoch 310 Iter 0 subLoss 5996.1 multi 3.99 import weight 0.00
Epoch 310 Iter 1 subLoss 4329.2 multi -4.97 import weight 0.00
Epoch 310 Iter 2 subLoss 5286.2 multi -1.99 import weight 0.00
Epoch 310 Iter 3 subLoss 6330.1 multi -1.99 import weight 0.00
Epoch 310 Iter 4 subLoss 7015.2 multi 9.96 import weight 0.00
Epoch 310 Iter 5 subLoss 4240.5 multi -1.98 import weight 0.00
Epoch 310 Iter 6 subLoss 4345.9 multi 18.91 import weight 0.00
Epoch 310 Iter 7 subLoss 4187.4 multi -16.91 import weight 0.00
Epoch 310 Iter 8 subLoss 5569.8 multi -1.99 import weight 0.00
Epoch 310 Iter 9 subLoss 7702.2 multi 1.00 import weight 0.00
Epoch 310 Iter 10 subLoss 5991.5 multi 6.97 import weight 0.00
Epoch 310 Iter 11 subLoss 3726.5 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0103 / 0.09016 / 20.85
Entropy seen (from low to high)
[55, 61, 224, 212, 99, 89, 161, 321, 439, 370, 325, 293, 219, 178, 193, 253, 297, 219, 168, 120, 111, 101, 81, 66, 62, 56, 48, 52, 56, 28, 26, 32, 21, 27, 18, 18, 12, 1, 7, 7, 5, 5, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 43, 56, 73, 137, 155, 161, 183, 242, 195, 229, 253, 270, 222, 239, 220, 227, 221, 202, 198, 183, 151, 164, 150, 114, 122, 109, 95, 59, 53, 27, 19, 24, 19, 10, 6, 6, 4, 6, 2, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.2, 32.8, 37.2, 40.5, 43.9, 47.0, 51.0, 54.2, 57.5, 61.1, 65.0, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.9, 71.4, 21.4, 90.9, 63.8, 68.7, 65.7, 79.9, 81.3, 79.9, 91.6]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 7, 14, 22, 36, 32, 35, 40, 43, 55, 96]
Epoch 310 Acc: 97.51 BMA: 97.98 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 372 train Loss: 4584.7 test Loss: 441.2
Epoch 311 Iter 0 subLoss 4296.6 multi -4.97 import weight 0.00
Epoch 311 Iter 1 subLoss 5433.2 multi -7.96 import weight 0.00
Epoch 311 Iter 2 subLoss 9547.6 multi 3.99 import weight 0.00
Epoch 311 Iter 3 subLoss 4650.1 multi -4.97 import weight 0.00
Epoch 311 Iter 4 subLoss 5920.3 multi 1.00 import weight 0.00
Epoch 311 Iter 5 subLoss 5317.1 multi 6.97 import weight 0.00
Epoch 311 Iter 6 subLoss 4302.2 multi 3.99 import weight 0.00
Epoch 311 Iter 7 subLoss 4030.7 multi -7.96 import weight 0.00
Epoch 311 Iter 8 subLoss 4133.3 multi 6.97 import weight 0.00
Epoch 311 Iter 9 subLoss 3776.3 multi -25.87 import weight 0.00
Epoch 311 Iter 10 subLoss 6103.3 multi 1.00 import weight 0.00
Epoch 311 Iter 11 subLoss 5355.1 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0102 / 0.09039 / 20.73
Entropy seen (from low to high)
[56, 61, 235, 207, 98, 92, 177, 344, 445, 355, 318, 296, 210, 178, 196, 263, 299, 216, 152, 117, 108, 100, 72, 71, 58, 54, 50, 53, 50, 32, 24, 27, 24, 28, 14, 21, 10, 2, 5, 9, 5, 4, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 42, 55, 76, 134, 153, 157, 189, 242, 188, 220, 258, 263, 233, 232, 218, 222, 229, 199, 197, 188, 153, 158, 151, 118, 125, 112, 92, 66, 49, 33, 20, 24, 17, 11, 7, 5, 5, 5, 3, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.5, 33.3, 37.4, 40.5, 43.8, 46.8, 50.8, 54.3, 57.7, 61.2, 65.0, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 74.9, 18.1, 85.7, 63.8, 71.8, 62.1, 82.4, 83.3, 81.4, 89.5]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 8, 11, 21, 36, 32, 37, 40, 42, 54, 96]
Epoch 311 Acc: 97.24 BMA: 97.96 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 535 train Loss: 5040.1 test Loss: 461.8
Epoch 312 Iter 0 subLoss 4498.9 multi 9.96 import weight 0.00
Epoch 312 Iter 1 subLoss 4741.7 multi -7.96 import weight 0.00
Epoch 312 Iter 2 subLoss 4583.9 multi 15.93 import weight 0.00
Epoch 312 Iter 3 subLoss 4327.7 multi -1.98 import weight 0.00
Epoch 312 Iter 4 subLoss 4051.3 multi 1.00 import weight 0.00
Epoch 312 Iter 5 subLoss 3727.3 multi -4.97 import weight 0.00
Epoch 312 Iter 6 subLoss 4080.1 multi 15.93 import weight 0.00
Epoch 312 Iter 7 subLoss 4455.7 multi 9.96 import weight 0.00
Epoch 312 Iter 8 subLoss 3760.2 multi 12.94 import weight 1.00
Epoch 312 Iter 9 subLoss 3876.4 multi 9.96 import weight 0.00
Epoch 312 Iter 10 subLoss 3942.1 multi 1.00 import weight 0.00
Epoch 312 Iter 11 subLoss 3359.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0101 / 0.09087 / 20.05
Entropy seen (from low to high)
[56, 62, 250, 198, 96, 94, 194, 366, 440, 355, 319, 286, 207, 178, 203, 280, 282, 208, 143, 123, 111, 87, 65, 73, 60, 51, 47, 62, 42, 27, 25, 30, 26, 22, 15, 18, 10, 3, 5, 9, 4, 4, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 41, 53, 80, 119, 159, 154, 182, 238, 199, 209, 255, 256, 234, 236, 218, 223, 228, 194, 194, 186, 167, 155, 152, 121, 118, 123, 90, 78, 50, 35, 23, 21, 19, 13, 8, 5, 5, 5, 3, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.4, 33.6, 37.2, 40.5, 44.0, 47.0, 51.0, 54.5, 57.8, 61.2, 65.0, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 71.4, 24.9, 81.8, 67.6, 68.7, 61.1, 82.0, 83.3, 80.7, 89.5]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 7, 12, 22, 34, 32, 36, 39, 42, 52, 86]
Epoch 312 Acc: 97.65 BMA: 97.96 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 335 train Loss: 3728.5 test Loss: 385.5
Epoch 313 Iter 0 subLoss 3920.3 multi -1.98 import weight 0.00
Epoch 313 Iter 1 subLoss 3778.1 multi -25.87 import weight 0.00
Epoch 313 Iter 2 subLoss 4102.9 multi 12.94 import weight 0.00
Epoch 313 Iter 3 subLoss 4445.8 multi -4.97 import weight 0.00
Epoch 313 Iter 4 subLoss 3889.7 multi -1.98 import weight 0.00
Epoch 313 Iter 5 subLoss 3754.5 multi 18.91 import weight 0.00
Epoch 313 Iter 6 subLoss 4013.8 multi 1.00 import weight 0.00
Epoch 313 Iter 7 subLoss 3355.1 multi 3.99 import weight 0.00
Epoch 313 Iter 8 subLoss 3351.6 multi 6.97 import weight 0.00
Epoch 313 Iter 9 subLoss 3874.2 multi 12.94 import weight 0.00
Epoch 313 Iter 10 subLoss 3861.9 multi -4.97 import weight 0.00
Epoch 313 Iter 11 subLoss 3633.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0099 / 0.09134 / 19.20
Entropy seen (from low to high)
[57, 64, 261, 188, 97, 97, 215, 371, 448, 351, 326, 273, 205, 182, 204, 305, 258, 194, 143, 125, 104, 84, 70, 64, 62, 53, 43, 62, 38, 24, 25, 27, 28, 23, 16, 14, 10, 3, 6, 8, 4, 4, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 40, 55, 79, 115, 156, 150, 176, 242, 202, 195, 258, 244, 240, 233, 216, 225, 219, 200, 189, 190, 174, 145, 162, 126, 120, 127, 93, 78, 54, 41, 21, 23, 19, 11, 12, 6, 5, 5, 2, 4, 0, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.4, 33.3, 37.0, 40.5, 44.0, 47.2, 51.1, 54.5, 57.8, 61.1, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 62.4, 29.9, 79.1, 64.7, 69.9, 62.8, 76.3, 82.4, 81.6, 89.6]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 8, 10, 24, 34, 30, 35, 38, 40, 49, 87]
Epoch 313 Acc: 97.84 BMA: 97.96 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 363 train Loss: 3609.4 test Loss: 365.5
Epoch 314 Iter 0 subLoss 3440.6 multi -13.93 import weight 0.00
Epoch 314 Iter 1 subLoss 3986.6 multi -1.98 import weight 0.00
Epoch 314 Iter 2 subLoss 3662.2 multi 3.99 import weight 0.00
Epoch 314 Iter 3 subLoss 3491.3 multi -16.91 import weight 0.00
Epoch 314 Iter 4 subLoss 4365.0 multi 21.90 import weight 0.00
Epoch 314 Iter 5 subLoss 3724.8 multi -1.99 import weight 0.00
Epoch 314 Iter 6 subLoss 4048.7 multi 6.97 import weight 0.00
Epoch 314 Iter 7 subLoss 3486.7 multi 3.99 import weight 0.00
Epoch 314 Iter 8 subLoss 3791.0 multi 9.96 import weight 0.00
Epoch 314 Iter 9 subLoss 3531.9 multi 12.94 import weight 0.00
Epoch 314 Iter 10 subLoss 3314.5 multi 1.00 import weight 0.00
Epoch 314 Iter 11 subLoss 3424.3 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0098 / 0.09183 / 18.92
Entropy seen (from low to high)
[57, 66, 271, 183, 93, 104, 228, 393, 448, 340, 337, 261, 191, 190, 228, 317, 240, 184, 133, 114, 106, 86, 62, 66, 59, 48, 44, 60, 40, 24, 24, 29, 26, 22, 16, 14, 8, 3, 5, 8, 6, 2, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 41, 53, 79, 111, 156, 146, 171, 241, 197, 199, 241, 244, 254, 217, 223, 220, 213, 214, 183, 194, 173, 151, 156, 130, 130, 112, 111, 85, 49, 44, 27, 22, 20, 11, 13, 5, 5, 5, 3, 4, 0, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.2, 33.4, 36.9, 40.5, 44.2, 47.4, 51.0, 54.3, 57.8, 60.9, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 71.4, 27.2, 79.1, 66.6, 61.5, 66.6, 74.9, 85.7, 84.3, 85.7]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 7, 11, 24, 33, 26, 36, 40, 35, 51, 84]
Epoch 314 Acc: 97.98 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 342 train Loss: 3319.7 test Loss: 345.8
Epoch 315 Iter 0 subLoss 3261.2 multi 1.00 import weight 0.00
Epoch 315 Iter 1 subLoss 3260.7 multi 3.99 import weight 0.00
Epoch 315 Iter 2 subLoss 2680.6 multi 9.96 import weight 0.00
Epoch 315 Iter 3 subLoss 3205.2 multi 6.97 import weight 0.00
Epoch 315 Iter 4 subLoss 2720.0 multi -1.99 import weight 0.00
Epoch 315 Iter 5 subLoss 3070.5 multi 3.99 import weight 0.00
Epoch 315 Iter 6 subLoss 2903.5 multi 3.99 import weight 0.00
Epoch 315 Iter 7 subLoss 3109.5 multi 12.94 import weight 0.00
Epoch 315 Iter 8 subLoss 3309.7 multi -13.93 import weight 0.00
Epoch 315 Iter 9 subLoss 3704.2 multi -4.97 import weight 0.00
Epoch 315 Iter 10 subLoss 3828.5 multi 1.00 import weight 0.00
Epoch 315 Iter 11 subLoss 3615.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0097 / 0.09229 / 17.86
Entropy seen (from low to high)
[58, 65, 282, 178, 93, 117, 238, 409, 439, 349, 315, 273, 182, 201, 244, 310, 233, 174, 122, 113, 108, 78, 64, 61, 59, 48, 44, 60, 34, 26, 22, 30, 26, 21, 15, 14, 7, 3, 6, 7, 7, 1, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 41, 51, 72, 107, 158, 137, 172, 237, 197, 199, 235, 240, 257, 214, 225, 216, 219, 220, 177, 196, 171, 150, 161, 135, 122, 122, 108, 93, 53, 51, 28, 20, 21, 12, 13, 6, 5, 5, 3, 4, 0, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 33.4, 37.3, 41.0, 44.1, 47.1, 50.5, 54.4, 57.9, 61.0, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 55.5, 37.4, 76.1, 65.5, 62.4, 69.4, 69.9, 88.5, 85.1, 84.6]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 9, 8, 21, 29, 32, 36, 40, 35, 47, 78]
Epoch 315 Acc: 97.94 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 361 train Loss: 3264.6 test Loss: 351.4
Epoch 316 Iter 0 subLoss 3112.7 multi -10.94 import weight 0.00
Epoch 316 Iter 1 subLoss 3919.9 multi 3.98 import weight 0.00
Epoch 316 Iter 2 subLoss 3633.6 multi 6.97 import weight 0.00
Epoch 316 Iter 3 subLoss 3446.9 multi -10.94 import weight 0.00
Epoch 316 Iter 4 subLoss 3750.1 multi 21.90 import weight 0.00
Epoch 316 Iter 5 subLoss 3260.3 multi 6.97 import weight 0.00
Epoch 316 Iter 6 subLoss 2896.9 multi 6.97 import weight 0.00
Epoch 316 Iter 7 subLoss 2944.8 multi -25.87 import weight 0.00
Epoch 316 Iter 8 subLoss 3014.0 multi 1.00 import weight 0.00
Epoch 316 Iter 9 subLoss 3693.8 multi 9.96 import weight 0.00
Epoch 316 Iter 10 subLoss 3198.6 multi 9.96 import weight 0.00
Epoch 316 Iter 11 subLoss 3297.2 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0096 / 0.09277 / 18.02
Entropy seen (from low to high)
[59, 64, 292, 174, 91, 123, 257, 420, 440, 357, 305, 264, 182, 198, 266, 302, 223, 170, 122, 109, 104, 74, 63, 61, 49, 51, 52, 48, 37, 24, 22, 30, 23, 21, 14, 15, 7, 3, 5, 7, 7, 2, 1, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 40, 49, 75, 99, 155, 138, 172, 218, 205, 198, 222, 245, 260, 215, 215, 215, 224, 228, 171, 193, 172, 160, 150, 141, 120, 130, 111, 99, 63, 48, 29, 22, 20, 14, 14, 4, 6, 6, 3, 2, 2, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 33.5, 37.3, 40.7, 43.8, 47.1, 50.6, 54.4, 57.8, 61.0, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 74.9, 24.9, 61.1, 71.8, 62.4, 66.6, 71.7, 86.4, 84.7, 84.6]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 8, 8, 18, 32, 32, 33, 39, 37, 46, 78]
Epoch 316 Acc: 98.15 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 329 train Loss: 3123.5 test Loss: 324.5
Epoch 317 Iter 0 subLoss 2734.9 multi -10.94 import weight 0.00
Epoch 317 Iter 1 subLoss 3550.0 multi -10.94 import weight 0.00
Epoch 317 Iter 2 subLoss 3979.4 multi -16.91 import weight 0.00
Epoch 317 Iter 3 subLoss 6467.9 multi -1.98 import weight 0.00
Epoch 317 Iter 4 subLoss 8990.8 multi 3.98 import weight 0.00
Epoch 317 Iter 5 subLoss 3829.5 multi 3.98 import weight 0.00
Epoch 317 Iter 6 subLoss 3709.2 multi -4.97 import weight 0.00
Epoch 317 Iter 7 subLoss 3754.7 multi 24.88 import weight 0.00
Epoch 317 Iter 8 subLoss 4534.9 multi 6.97 import weight 0.00
Epoch 317 Iter 9 subLoss 3141.1 multi 12.94 import weight 0.00
Epoch 317 Iter 10 subLoss 2673.2 multi 3.98 import weight 0.00
Epoch 317 Iter 11 subLoss 2977.9 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0095 / 0.09324 / 17.63
Entropy seen (from low to high)
[62, 63, 296, 174, 92, 129, 272, 449, 429, 345, 317, 240, 187, 206, 270, 306, 222, 150, 126, 108, 88, 71, 70, 58, 50, 47, 56, 44, 35, 20, 24, 30, 20, 23, 14, 15, 6, 3, 4, 7, 7, 2, 1, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 39, 46, 75, 94, 151, 140, 161, 223, 205, 196, 219, 247, 252, 221, 219, 203, 220, 218, 196, 174, 179, 162, 146, 157, 124, 120, 121, 101, 67, 48, 33, 20, 25, 14, 12, 4, 8, 4, 5, 2, 2, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 30.3, 33.5, 37.4, 40.6, 44.0, 47.4, 50.8, 54.5, 57.9, 61.2, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 71.4, 33.3, 57.8, 70.9, 64.5, 65.6, 75.6, 79.9, 88.0, 85.1]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 7, 9, 19, 31, 31, 32, 41, 35, 42, 74]
Epoch 317 Acc: 98.03 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 297 train Loss: 3097.5 test Loss: 330.8
Epoch 318 Iter 0 subLoss 3047.0 multi -1.99 import weight 0.00
Epoch 318 Iter 1 subLoss 3008.0 multi -1.98 import weight 0.00
Epoch 318 Iter 2 subLoss 2950.6 multi 12.94 import weight 0.00
Epoch 318 Iter 3 subLoss 2786.3 multi 1.00 import weight 0.00
Epoch 318 Iter 4 subLoss 2979.3 multi -1.98 import weight 0.00
Epoch 318 Iter 5 subLoss 3041.9 multi 1.00 import weight 0.00
Epoch 318 Iter 6 subLoss 3206.8 multi 6.97 import weight 0.00
Epoch 318 Iter 7 subLoss 2923.5 multi -7.96 import weight 0.00
Epoch 318 Iter 8 subLoss 3160.3 multi -4.97 import weight 0.00
Epoch 318 Iter 9 subLoss 2717.5 multi 1.00 import weight 0.00
Epoch 318 Iter 10 subLoss 3084.9 multi -7.96 import weight 0.00
Epoch 318 Iter 11 subLoss 4104.6 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0094 / 0.09362 / 16.48
Entropy seen (from low to high)
[62, 65, 304, 174, 90, 127, 301, 454, 418, 355, 304, 237, 190, 204, 297, 285, 209, 152, 126, 105, 87, 73, 64, 54, 50, 46, 58, 41, 34, 22, 20, 29, 23, 20, 14, 15, 5, 3, 5, 7, 6, 2, 1, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 37, 46, 76, 89, 145, 148, 157, 209, 217, 185, 214, 256, 245, 225, 218, 201, 222, 203, 203, 178, 178, 160, 152, 160, 128, 120, 122, 97, 78, 49, 38, 21, 25, 15, 12, 4, 8, 5, 4, 2, 2, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 30.3, 33.4, 37.5, 40.2, 43.7, 47.4, 50.8, 54.3, 57.8, 61.2, 64.8, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 66.6, 37.4, 54.9, 66.6, 64.5, 66.6, 74.4, 79.9, 88.8, 83.0]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 6, 8, 20, 30, 31, 30, 43, 35, 45, 71]
Epoch 318 Acc: 97.80 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 410 train Loss: 3306.8 test Loss: 373.7
Epoch 319 Iter 0 subLoss 3596.2 multi -16.91 import weight 0.00
Epoch 319 Iter 1 subLoss 5479.5 multi 3.99 import weight 0.00
Epoch 319 Iter 2 subLoss 3476.6 multi 9.96 import weight 0.00
Epoch 319 Iter 3 subLoss 2729.5 multi -1.98 import weight 0.00
Epoch 319 Iter 4 subLoss 3316.3 multi 1.00 import weight 0.00
Epoch 319 Iter 5 subLoss 3544.7 multi -7.96 import weight 0.00
Epoch 319 Iter 6 subLoss 3380.9 multi 1.00 import weight 0.00
Epoch 319 Iter 7 subLoss 3493.1 multi -16.91 import weight 0.00
Epoch 319 Iter 8 subLoss 3521.4 multi -10.94 import weight 0.00
Epoch 319 Iter 9 subLoss 7668.0 multi -1.99 import weight 0.00
Epoch 319 Iter 10 subLoss 13966.1 multi 3.99 import weight 0.00
Epoch 319 Iter 11 subLoss 3276.9 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0093 / 0.09368 / 17.13
Entropy seen (from low to high)
[63, 67, 308, 169, 91, 134, 327, 449, 425, 344, 311, 223, 191, 216, 304, 278, 191, 144, 123, 107, 87, 74, 60, 53, 52, 46, 54, 40, 36, 21, 22, 30, 21, 19, 15, 13, 6, 3, 5, 8, 5, 2, 1, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 39, 43, 75, 86, 147, 149, 149, 205, 229, 184, 210, 252, 246, 222, 218, 209, 223, 204, 216, 172, 179, 157, 157, 158, 117, 128, 114, 100, 73, 52, 41, 22, 24, 16, 12, 5, 7, 7, 2, 2, 3, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 30.0, 33.2, 37.3, 40.6, 43.7, 47.5, 50.8, 54.1, 57.4, 60.9, 64.7, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 79.9, 41.6, 59.9, 74.2, 62.4, 57.5, 75.6, 80.4, 89.1, 83.8]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 5, 12, 15, 35, 24, 33, 37, 41, 46, 68]
Epoch 319 Acc: 95.95 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 327 train Loss: 5246.9 test Loss: 750.9
Epoch 320 Iter 0 subLoss 5747.4 multi -1.99 import weight 0.00
Epoch 320 Iter 1 subLoss 6871.6 multi 6.97 import weight 0.00
Epoch 320 Iter 2 subLoss 3515.1 multi -4.97 import weight 0.00
Epoch 320 Iter 3 subLoss 3097.8 multi 9.96 import weight 0.00
Epoch 320 Iter 4 subLoss 3466.2 multi -7.96 import weight 0.00
Epoch 320 Iter 5 subLoss 3512.1 multi -1.99 import weight 0.00
Epoch 320 Iter 6 subLoss 4162.7 multi -10.94 import weight 0.00
Epoch 320 Iter 7 subLoss 4869.3 multi -7.96 import weight 0.00
Epoch 320 Iter 8 subLoss 8137.4 multi 9.96 import weight 0.00
Epoch 320 Iter 9 subLoss 3584.6 multi 3.99 import weight 0.00
Epoch 320 Iter 10 subLoss 4353.1 multi -16.91 import weight 0.00
Epoch 320 Iter 11 subLoss 4245.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0093 / 0.09391 / 16.90
Entropy seen (from low to high)
[64, 68, 312, 167, 91, 142, 337, 471, 416, 338, 307, 225, 183, 224, 331, 256, 186, 134, 119, 109, 85, 70, 54, 55, 50, 50, 49, 40, 35, 22, 22, 29, 22, 17, 16, 12, 6, 3, 5, 8, 5, 2, 1, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 36, 46, 74, 89, 143, 142, 157, 198, 228, 181, 209, 261, 243, 215, 218, 207, 227, 204, 211, 173, 181, 148, 163, 159, 117, 123, 123, 102, 79, 49, 47, 20, 21, 20, 12, 5, 7, 6, 3, 2, 3, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.6, 30.0, 33.1, 37.1, 40.4, 43.6, 47.6, 50.7, 54.2, 57.5, 61.1, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 79.9, 45.4, 56.2, 70.5, 67.9, 57.1, 77.1, 82.2, 85.3, 84.3]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 5, 11, 16, 34, 25, 35, 35, 45, 41, 64]
Epoch 320 Acc: 97.24 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 424 train Loss: 4500.4 test Loss: 497.9
Epoch 321 Iter 0 subLoss 4700.0 multi -4.97 import weight 0.00
Epoch 321 Iter 1 subLoss 4912.5 multi -4.97 import weight 0.00
Epoch 321 Iter 2 subLoss 5208.0 multi 3.98 import weight 0.00
Epoch 321 Iter 3 subLoss 4435.4 multi 3.99 import weight 0.00
Epoch 321 Iter 4 subLoss 4499.2 multi 12.94 import weight 0.00
Epoch 321 Iter 5 subLoss 3555.4 multi -4.97 import weight 0.00
Epoch 321 Iter 6 subLoss 4059.8 multi 1.00 import weight 0.00
Epoch 321 Iter 7 subLoss 3767.6 multi 6.97 import weight 1.00
Epoch 321 Iter 8 subLoss 3737.2 multi 9.96 import weight 0.00
Epoch 321 Iter 9 subLoss 3949.4 multi 3.99 import weight 0.00
Epoch 321 Iter 10 subLoss 3783.1 multi 1.00 import weight 0.00
Epoch 321 Iter 11 subLoss 3610.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0092 / 0.09433 / 16.43
Entropy seen (from low to high)
[66, 66, 317, 164, 94, 151, 357, 491, 394, 347, 299, 213, 192, 245, 322, 243, 185, 128, 115, 107, 79, 63, 64, 49, 50, 50, 48, 36, 35, 22, 25, 26, 23, 17, 16, 9, 6, 4, 5, 7, 5, 2, 1, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 34, 48, 72, 85, 138, 144, 154, 189, 235, 177, 207, 249, 253, 205, 219, 212, 227, 212, 199, 173, 180, 159, 157, 167, 115, 123, 120, 108, 83, 53, 51, 22, 19, 21, 15, 6, 5, 8, 3, 2, 3, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.3, 29.9, 33.2, 37.2, 40.3, 43.3, 47.5, 50.6, 54.2, 57.4, 61.0, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 79.9, 49.9, 53.3, 65.6, 65.5, 54.5, 82.3, 81.8, 83.3, 84.9]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 5, 10, 15, 32, 29, 33, 34, 44, 42, 60]
Epoch 321 Acc: 98.05 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 361 train Loss: 3363.7 test Loss: 335.2
Epoch 322 Iter 0 subLoss 3171.3 multi 1.00 import weight 0.00
Epoch 322 Iter 1 subLoss 3937.8 multi -4.97 import weight 0.00
Epoch 322 Iter 2 subLoss 3574.9 multi 9.96 import weight 0.00
Epoch 322 Iter 3 subLoss 3688.7 multi 3.99 import weight 0.00
Epoch 322 Iter 4 subLoss 2964.7 multi -7.96 import weight 0.00
Epoch 322 Iter 5 subLoss 3346.2 multi 6.97 import weight 0.00
Epoch 322 Iter 6 subLoss 2917.9 multi 1.00 import weight 0.00
Epoch 322 Iter 7 subLoss 3329.1 multi -16.91 import weight 0.00
Epoch 322 Iter 8 subLoss 3432.8 multi -4.97 import weight 0.00
Epoch 322 Iter 9 subLoss 3067.7 multi -4.97 import weight 0.00
Epoch 322 Iter 10 subLoss 3732.8 multi 12.94 import weight 0.00
Epoch 322 Iter 11 subLoss 3289.9 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0091 / 0.09472 / 16.68
Entropy seen (from low to high)
[67, 65, 323, 163, 96, 160, 375, 497, 390, 358, 278, 205, 210, 254, 323, 235, 167, 132, 117, 96, 71, 68, 62, 46, 49, 52, 48, 33, 32, 22, 28, 22, 24, 17, 15, 9, 5, 5, 5, 6, 5, 2, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 8, 32, 49, 70, 85, 136, 144, 150, 188, 228, 178, 202, 246, 250, 205, 218, 216, 224, 213, 201, 169, 179, 159, 157, 162, 127, 125, 126, 106, 88, 57, 57, 21, 20, 22, 12, 8, 6, 8, 3, 2, 3, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.3, 30.3, 34.2, 36.3, 40.2, 43.6, 47.5, 50.5, 54.3, 57.5, 61.2, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 99.9, 41.6, 52.9, 70.8, 62.8, 57.5, 81.8, 82.2, 83.7, 86.4]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 2, 3, 12, 17, 24, 35, 33, 33, 45, 37, 59]
Epoch 322 Acc: 97.98 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 328 train Loss: 3385.0 test Loss: 362.1
Epoch 323 Iter 0 subLoss 3369.1 multi -1.99 import weight 0.00
Epoch 323 Iter 1 subLoss 3221.1 multi -4.97 import weight 0.00
Epoch 323 Iter 2 subLoss 3911.9 multi 6.97 import weight 0.00
Epoch 323 Iter 3 subLoss 3450.0 multi 6.97 import weight 0.00
Epoch 323 Iter 4 subLoss 3484.6 multi 3.99 import weight 0.00
Epoch 323 Iter 5 subLoss 3677.1 multi 3.99 import weight 0.00
Epoch 323 Iter 6 subLoss 2533.3 multi 3.99 import weight 0.00
Epoch 323 Iter 7 subLoss 3549.8 multi -4.97 import weight 0.00
Epoch 323 Iter 8 subLoss 3380.8 multi 3.98 import weight 0.00
Epoch 323 Iter 9 subLoss 2911.1 multi 3.99 import weight 0.00
Epoch 323 Iter 10 subLoss 2715.0 multi 3.98 import weight 0.00
Epoch 323 Iter 11 subLoss 3201.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0090 / 0.09516 / 16.24
Entropy seen (from low to high)
[68, 65, 333, 155, 99, 178, 396, 483, 390, 353, 282, 201, 209, 270, 317, 229, 166, 130, 108, 91, 75, 63, 59, 45, 48, 56, 46, 28, 33, 20, 30, 20, 25, 14, 15, 10, 4, 5, 5, 6, 5, 2, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 32, 47, 74, 81, 129, 144, 146, 178, 230, 182, 199, 236, 265, 198, 218, 216, 216, 211, 208, 159, 186, 166, 155, 165, 127, 123, 123, 118, 87, 68, 53, 27, 20, 23, 11, 10, 6, 7, 4, 2, 3, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 29.6, 33.3, 36.4, 40.2, 43.6, 47.7, 50.5, 54.1, 57.5, 61.2, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 99.9, 45.4, 44.4, 69.5, 59.3, 61.1, 77.4, 84.4, 83.7, 87.9]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 11, 18, 23, 32, 36, 31, 45, 37, 58]
Epoch 323 Acc: 98.13 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 320 train Loss: 3093.1 test Loss: 323.2
Epoch 324 Iter 0 subLoss 3421.9 multi 9.96 import weight 0.00
Epoch 324 Iter 1 subLoss 2731.9 multi -10.94 import weight 0.00
Epoch 324 Iter 2 subLoss 3398.4 multi -10.94 import weight 0.00
Epoch 324 Iter 3 subLoss 3627.1 multi -7.96 import weight 0.00
Epoch 324 Iter 4 subLoss 4558.8 multi 3.99 import weight 0.00
Epoch 324 Iter 5 subLoss 3043.2 multi 3.98 import weight 0.00
Epoch 324 Iter 6 subLoss 2720.7 multi -1.99 import weight 0.00
Epoch 324 Iter 7 subLoss 3379.2 multi -13.93 import weight 0.00
Epoch 324 Iter 8 subLoss 4613.5 multi -13.93 import weight 0.00
Epoch 324 Iter 9 subLoss 14190.1 multi 1.00 import weight 0.00
Epoch 324 Iter 10 subLoss 8765.6 multi -4.97 import weight 0.00
Epoch 324 Iter 11 subLoss 35254.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0090 / 0.09531 / 15.75
Entropy seen (from low to high)
[68, 66, 339, 151, 102, 189, 392, 487, 380, 323, 298, 186, 210, 280, 316, 228, 162, 139, 102, 99, 76, 63, 55, 54, 42, 58, 46, 31, 32, 22, 33, 15, 26, 16, 14, 11, 3, 4, 6, 6, 5, 2, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 30, 46, 72, 81, 134, 148, 139, 181, 227, 182, 203, 233, 255, 208, 218, 224, 199, 209, 211, 163, 184, 155, 158, 160, 143, 113, 131, 117, 85, 74, 53, 31, 20, 21, 14, 9, 6, 7, 4, 2, 3, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 29.3, 33.6, 36.1, 40.0, 43.7, 47.8, 50.6, 54.2, 57.8, 61.2, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 66.6, 49.9, 42.1, 61.5, 69.9, 57.8, 75.7, 87.4, 84.9, 84.9]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 10, 19, 26, 30, 38, 33, 40, 40, 53]
Epoch 324 Acc: 86.53 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3525 train Loss: 13412.0 test Loss: 2176.0
Epoch 325 Iter 0 subLoss 13888.5 multi 1.00 import weight 0.00
Epoch 325 Iter 1 subLoss 9349.9 multi -4.97 import weight 0.00
Epoch 325 Iter 2 subLoss 26579.4 multi 1.00 import weight 0.00
Epoch 325 Iter 3 subLoss 16025.1 multi 1.00 import weight 0.00
Epoch 325 Iter 4 subLoss 10898.0 multi 1.00 import weight 0.00
Epoch 325 Iter 5 subLoss 8478.0 multi -1.99 import weight 0.00
Epoch 325 Iter 6 subLoss 13645.7 multi 1.00 import weight 0.00
Epoch 325 Iter 7 subLoss 10509.9 multi 3.98 import weight 0.00
Epoch 325 Iter 8 subLoss 5492.7 multi 1.00 import weight 0.00
Epoch 325 Iter 9 subLoss 4563.6 multi 18.91 import weight 0.00
Epoch 325 Iter 10 subLoss 3810.5 multi 3.99 import weight 0.00
Epoch 325 Iter 11 subLoss 3404.8 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0089 / 0.09562 / 16.00
Entropy seen (from low to high)
[68, 66, 344, 154, 100, 201, 405, 494, 385, 315, 283, 190, 222, 280, 307, 220, 163, 133, 105, 92, 68, 73, 51, 53, 43, 56, 45, 30, 31, 21, 31, 17, 24, 18, 15, 8, 3, 5, 7, 6, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 7, 28, 49, 70, 83, 131, 144, 136, 178, 220, 193, 197, 228, 256, 206, 218, 220, 204, 215, 201, 175, 173, 161, 155, 163, 140, 122, 126, 118, 90, 78, 55, 34, 19, 21, 14, 10, 6, 6, 5, 2, 3, 1, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 29.2, 33.4, 36.4, 40.1, 43.6, 47.6, 50.6, 54.2, 57.7, 61.2, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 99.9, 39.9, 41.1, 61.5, 66.6, 57.1, 76.4, 89.1, 84.9, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 10, 17, 26, 33, 35, 34, 37, 40, 56]
Epoch 325 Acc: 97.66 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 340 train Loss: 3506.4 test Loss: 385.1
Epoch 326 Iter 0 subLoss 3525.8 multi -13.93 import weight 0.00
Epoch 326 Iter 1 subLoss 3948.2 multi 3.98 import weight 0.00
Epoch 326 Iter 2 subLoss 3654.2 multi -13.93 import weight 0.00
Epoch 326 Iter 3 subLoss 4902.4 multi 1.00 import weight 0.00
Epoch 326 Iter 4 subLoss 4209.2 multi -1.99 import weight 0.00
Epoch 326 Iter 5 subLoss 3927.8 multi -4.97 import weight 0.00
Epoch 326 Iter 6 subLoss 5247.0 multi -10.94 import weight 0.00
Epoch 326 Iter 7 subLoss 15415.2 multi -4.97 import weight 0.00
Epoch 326 Iter 8 subLoss 112979.4 multi 1.00 import weight 0.00
Epoch 326 Iter 9 subLoss 16398.6 multi -1.99 import weight 0.00
Epoch 326 Iter 10 subLoss 29871.3 multi 1.00 import weight 0.00
Epoch 326 Iter 11 subLoss 15527.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0091 / 0.09472 / 17.02
Entropy seen (from low to high)
[27, 31, 212, 194, 178, 244, 445, 489, 405, 331, 268, 183, 178, 241, 296, 250, 174, 148, 104, 114, 74, 62, 63, 47, 46, 57, 48, 33, 31, 24, 25, 24, 22, 17, 17, 9, 3, 5, 6, 7, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 33, 54, 67, 81, 140, 140, 150, 182, 219, 191, 190, 243, 247, 211, 213, 234, 205, 217, 212, 169, 186, 165, 169, 156, 130, 139, 126, 111, 74, 53, 46, 22, 19, 21, 13, 7, 9, 6, 1, 4, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 33.7, 36.6, 40.0, 43.7, 47.6, 50.7, 54.4, 57.5, 61.1, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 49.9, 36.8, 71.9, 57.1, 61.7, 76.6, 81.8, 86.4, 89.8]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 2, 10, 19, 25, 35, 34, 30, 44, 37, 59]
Epoch 326 Acc: 74.64 BMA: 98.07 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1552 train Loss: 28338.4 test Loss: 4957.2
Epoch 327 Iter 0 subLoss 27287.7 multi 3.99 import weight 0.00
Epoch 327 Iter 1 subLoss 7753.4 multi -1.98 import weight 0.00
Epoch 327 Iter 2 subLoss 9438.7 multi 1.00 import weight 0.00
Epoch 327 Iter 3 subLoss 8061.9 multi -1.98 import weight 0.00
Epoch 327 Iter 4 subLoss 9964.2 multi -1.98 import weight 0.00
Epoch 327 Iter 5 subLoss 12447.5 multi -1.99 import weight 0.00
Epoch 327 Iter 6 subLoss 22311.2 multi 1.00 import weight 0.00
Epoch 327 Iter 7 subLoss 13524.2 multi 3.98 import weight 0.00
Epoch 327 Iter 8 subLoss 7996.2 multi -1.98 import weight 0.00
Epoch 327 Iter 9 subLoss 8864.1 multi 6.97 import weight 0.00
Epoch 327 Iter 10 subLoss 5949.6 multi -4.97 import weight 0.00
Epoch 327 Iter 11 subLoss 7462.5 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0091 / 0.09455 / 17.32
Entropy seen (from low to high)
[27, 31, 220, 196, 174, 262, 462, 486, 395, 329, 254, 184, 176, 236, 293, 246, 169, 151, 105, 111, 76, 63, 63, 44, 50, 57, 45, 34, 29, 27, 24, 29, 20, 17, 15, 10, 5, 4, 4, 9, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 34, 55, 72, 78, 142, 134, 151, 186, 223, 188, 192, 242, 255, 195, 223, 238, 198, 232, 207, 177, 173, 168, 168, 150, 129, 146, 118, 105, 77, 52, 44, 24, 18, 21, 13, 7, 9, 7, 1, 4, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.3, 34.4, 37.1, 40.2, 43.5, 47.6, 50.8, 54.3, 57.4, 61.0, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 33.3, 99.9, 44.4, 33.3, 69.9, 54.8, 67.6, 74.1, 81.8, 89.4, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 9, 18, 30, 31, 34, 31, 44, 38, 63]
Epoch 327 Acc: 95.86 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 746 train Loss: 5993.6 test Loss: 812.1
Epoch 328 Iter 0 subLoss 5774.7 multi -1.98 import weight 0.00
Epoch 328 Iter 1 subLoss 6427.5 multi -1.99 import weight 0.00
Epoch 328 Iter 2 subLoss 6564.3 multi 6.97 import weight 0.00
Epoch 328 Iter 3 subLoss 5326.6 multi 3.99 import weight 0.00
Epoch 328 Iter 4 subLoss 4841.2 multi 33.84 import weight 0.00
Epoch 328 Iter 5 subLoss 3734.2 multi 15.93 import weight 0.00
Epoch 328 Iter 6 subLoss 3261.9 multi 9.96 import weight 0.00
Epoch 328 Iter 7 subLoss 3369.2 multi 1.00 import weight 0.00
Epoch 328 Iter 8 subLoss 3481.3 multi 6.97 import weight 0.00
Epoch 328 Iter 9 subLoss 3120.5 multi -1.99 import weight 0.00
Epoch 328 Iter 10 subLoss 3222.7 multi -1.98 import weight 0.00
Epoch 328 Iter 11 subLoss 2835.9 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0090 / 0.09493 / 17.79
Entropy seen (from low to high)
[27, 31, 222, 205, 169, 279, 484, 468, 402, 319, 255, 180, 184, 241, 286, 248, 162, 148, 102, 106, 70, 65, 64, 39, 53, 56, 46, 32, 32, 23, 25, 27, 19, 17, 16, 9, 4, 4, 6, 7, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 32, 55, 70, 74, 142, 135, 148, 183, 218, 187, 194, 238, 241, 216, 218, 227, 199, 226, 218, 171, 176, 174, 167, 160, 124, 148, 116, 116, 75, 58, 43, 26, 18, 20, 14, 8, 9, 7, 0, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.9, 29.8, 34.2, 36.6, 40.3, 43.4, 47.5, 50.7, 54.3, 57.5, 61.2, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 99.9, 44.4, 31.2, 72.4, 53.1, 65.7, 78.1, 78.5, 89.7, 89.6]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 2, 4, 9, 16, 29, 32, 35, 32, 42, 39, 58]
Epoch 328 Acc: 97.82 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 283 train Loss: 3281.2 test Loss: 355.2
Epoch 329 Iter 0 subLoss 3198.9 multi 12.94 import weight 0.00
Epoch 329 Iter 1 subLoss 3068.8 multi -1.99 import weight 0.00
Epoch 329 Iter 2 subLoss 3156.8 multi -10.94 import weight 0.00
Epoch 329 Iter 3 subLoss 3060.0 multi -4.97 import weight 0.00
Epoch 329 Iter 4 subLoss 3326.9 multi -13.93 import weight 0.00
Epoch 329 Iter 5 subLoss 3515.5 multi 1.00 import weight 0.00
Epoch 329 Iter 6 subLoss 3868.2 multi -1.99 import weight 0.00
Epoch 329 Iter 7 subLoss 4491.2 multi 15.93 import weight 0.00
Epoch 329 Iter 8 subLoss 3396.0 multi -7.96 import weight 0.00
Epoch 329 Iter 9 subLoss 4180.8 multi -13.93 import weight 0.00
Epoch 329 Iter 10 subLoss 5678.4 multi 6.97 import weight 0.00
Epoch 329 Iter 11 subLoss 3281.4 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0089 / 0.09529 / 16.55
Entropy seen (from low to high)
[27, 33, 233, 202, 170, 293, 502, 459, 390, 325, 248, 173, 190, 250, 289, 241, 162, 136, 104, 96, 75, 65, 60, 40, 54, 50, 48, 29, 35, 21, 25, 27, 18, 17, 16, 8, 4, 4, 6, 7, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 32, 54, 68, 77, 136, 135, 147, 178, 220, 185, 197, 230, 235, 226, 205, 219, 213, 217, 224, 177, 178, 168, 165, 158, 135, 139, 131, 103, 85, 63, 41, 32, 19, 19, 14, 10, 8, 7, 1, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.4, 34.0, 36.5, 40.4, 43.7, 47.5, 50.7, 54.1, 57.3, 61.2, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 0.0, 99.9, 44.4, 41.1, 70.8, 54.0, 72.4, 71.4, 79.0, 91.1, 87.3]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 4, 9, 17, 24, 37, 29, 35, 43, 34, 63]
Epoch 329 Acc: 97.78 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 328 train Loss: 3409.5 test Loss: 366.0
Epoch 330 Iter 0 subLoss 3451.3 multi 9.96 import weight 0.00
Epoch 330 Iter 1 subLoss 3244.4 multi 3.99 import weight 0.00
Epoch 330 Iter 2 subLoss 2841.9 multi 6.97 import weight 0.00
Epoch 330 Iter 3 subLoss 3246.8 multi 6.97 import weight 0.00
Epoch 330 Iter 4 subLoss 3171.4 multi 3.98 import weight 0.00
Epoch 330 Iter 5 subLoss 3000.8 multi 1.00 import weight 0.00
Epoch 330 Iter 6 subLoss 2741.7 multi 9.96 import weight 0.00
Epoch 330 Iter 7 subLoss 3095.0 multi 12.94 import weight 0.00
Epoch 330 Iter 8 subLoss 2937.1 multi 15.93 import weight 0.00
Epoch 330 Iter 9 subLoss 3463.6 multi -10.94 import weight 0.00
Epoch 330 Iter 10 subLoss 2888.5 multi -10.94 import weight 0.00
Epoch 330 Iter 11 subLoss 4921.7 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0089 / 0.09539 / 16.25
Entropy seen (from low to high)
[27, 33, 216, 192, 179, 316, 516, 459, 383, 331, 242, 179, 197, 257, 293, 226, 162, 130, 104, 90, 74, 60, 66, 37, 56, 47, 49, 26, 35, 20, 24, 28, 17, 18, 15, 7, 2, 6, 6, 7, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 31, 52, 68, 77, 128, 138, 148, 170, 214, 197, 193, 233, 228, 230, 207, 225, 206, 226, 208, 177, 182, 170, 165, 158, 138, 135, 142, 104, 87, 61, 46, 32, 16, 17, 17, 10, 7, 6, 1, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.5, 34.2, 36.7, 40.4, 43.5, 47.4, 50.6, 54.0, 57.5, 61.3, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 33.3, 99.9, 33.3, 42.8, 70.3, 51.5, 72.7, 71.4, 79.9, 88.5, 88.3]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 9, 14, 27, 33, 33, 35, 40, 35, 60]
Epoch 330 Acc: 95.17 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 492 train Loss: 7843.8 test Loss: 849.4
Epoch 331 Iter 0 subLoss 8470.1 multi 1.00 import weight 0.00
Epoch 331 Iter 1 subLoss 5816.4 multi 15.93 import weight 0.00
Epoch 331 Iter 2 subLoss 5935.2 multi 3.98 import weight 0.00
Epoch 331 Iter 3 subLoss 3742.4 multi -25.87 import weight 0.00
Epoch 331 Iter 4 subLoss 9864.4 multi -1.99 import weight 0.00
Epoch 331 Iter 5 subLoss 18278.6 multi 1.00 import weight 0.00
Epoch 331 Iter 6 subLoss 9764.4 multi 6.97 import weight 0.00
Epoch 331 Iter 7 subLoss 3679.8 multi 6.97 import weight 0.00
Epoch 331 Iter 8 subLoss 3115.6 multi -7.96 import weight 0.00
Epoch 331 Iter 9 subLoss 3467.6 multi -7.96 import weight 0.00
Epoch 331 Iter 10 subLoss 4423.2 multi -4.97 import weight 0.00
Epoch 331 Iter 11 subLoss 6289.5 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0088 / 0.09564 / 15.73
Entropy seen (from low to high)
[27, 35, 223, 190, 186, 329, 521, 452, 387, 326, 231, 180, 210, 264, 287, 217, 164, 122, 103, 88, 71, 58, 66, 42, 48, 51, 42, 30, 33, 22, 22, 28, 16, 21, 14, 5, 2, 6, 7, 6, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 31, 50, 70, 79, 122, 135, 152, 162, 216, 199, 190, 230, 229, 238, 195, 232, 205, 218, 209, 182, 180, 168, 161, 163, 136, 138, 139, 109, 92, 62, 49, 33, 17, 18, 16, 10, 7, 7, 1, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 30.0, 34.2, 37.2, 40.5, 43.9, 47.5, 50.7, 54.1, 57.5, 61.2, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 49.9, 99.9, 39.9, 49.9, 59.9, 59.9, 68.7, 69.6, 80.4, 88.5, 88.5]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 4, 2, 10, 14, 25, 35, 32, 33, 41, 35, 61]
Epoch 331 Acc: 97.45 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 628 train Loss: 3524.9 test Loss: 433.5
Epoch 332 Iter 0 subLoss 3527.2 multi -13.93 import weight 0.00
Epoch 332 Iter 1 subLoss 4209.9 multi 1.00 import weight 0.00
Epoch 332 Iter 2 subLoss 3780.5 multi 3.99 import weight 0.00
Epoch 332 Iter 3 subLoss 3367.8 multi 3.99 import weight 0.00
Epoch 332 Iter 4 subLoss 4272.6 multi 3.99 import weight 0.00
Epoch 332 Iter 5 subLoss 3486.1 multi 9.96 import weight 0.00
Epoch 332 Iter 6 subLoss 2899.8 multi 6.97 import weight 0.00
Epoch 332 Iter 7 subLoss 3179.4 multi 6.97 import weight 0.00
Epoch 332 Iter 8 subLoss 3066.7 multi -1.98 import weight 0.00
Epoch 332 Iter 9 subLoss 3104.0 multi 9.96 import weight 0.00
Epoch 332 Iter 10 subLoss 3279.4 multi -13.93 import weight 0.00
Epoch 332 Iter 11 subLoss 3482.3 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0088 / 0.09600 / 16.17
Entropy seen (from low to high)
[28, 36, 226, 189, 195, 341, 526, 462, 382, 319, 222, 185, 210, 277, 284, 199, 166, 114, 114, 73, 77, 60, 54, 42, 52, 50, 42, 32, 28, 21, 25, 25, 18, 20, 12, 5, 3, 6, 6, 6, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 32, 49, 69, 79, 116, 137, 146, 163, 210, 202, 195, 214, 234, 236, 195, 226, 206, 222, 213, 184, 178, 169, 154, 168, 139, 135, 140, 113, 93, 73, 47, 34, 18, 20, 13, 13, 7, 7, 1, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 30.1, 34.1, 37.5, 40.3, 43.6, 47.2, 50.5, 54.0, 57.7, 61.2, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 49.9, 66.6, 28.5, 57.1, 58.3, 58.8, 66.6, 72.7, 81.0, 86.1, 89.6]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 4, 3, 7, 14, 24, 34, 36, 33, 37, 36, 58]
Epoch 332 Acc: 97.90 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 348 train Loss: 3212.5 test Loss: 357.9
Epoch 333 Iter 0 subLoss 2902.5 multi 1.00 import weight 0.00
Epoch 333 Iter 1 subLoss 3104.3 multi 12.94 import weight 0.00
Epoch 333 Iter 2 subLoss 2633.6 multi -13.93 import weight 0.00
Epoch 333 Iter 3 subLoss 3275.1 multi -10.94 import weight 0.00
Epoch 333 Iter 4 subLoss 4715.9 multi -1.99 import weight 0.00
Epoch 333 Iter 5 subLoss 4884.5 multi -1.98 import weight 0.00
Epoch 333 Iter 6 subLoss 3670.4 multi 9.96 import weight 0.00
Epoch 333 Iter 7 subLoss 3362.4 multi 6.97 import weight 0.00
Epoch 333 Iter 8 subLoss 3392.9 multi -4.97 import weight 0.00
Epoch 333 Iter 9 subLoss 3122.7 multi -1.98 import weight 0.00
Epoch 333 Iter 10 subLoss 3564.5 multi 3.99 import weight 0.00
Epoch 333 Iter 11 subLoss 2615.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0087 / 0.09632 / 15.97
Entropy seen (from low to high)
[28, 39, 231, 187, 198, 362, 528, 458, 391, 303, 219, 185, 224, 286, 272, 200, 153, 110, 109, 75, 73, 61, 53, 42, 57, 41, 42, 33, 25, 21, 29, 24, 16, 19, 12, 6, 2, 6, 6, 7, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 30, 51, 67, 76, 113, 141, 140, 163, 206, 201, 205, 201, 243, 230, 189, 227, 213, 215, 218, 188, 167, 174, 154, 162, 148, 124, 151, 107, 104, 78, 46, 35, 21, 21, 11, 14, 6, 8, 2, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 34.0, 36.8, 40.1, 43.9, 47.5, 50.7, 54.1, 57.9, 61.2, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 49.9, 99.9, 37.4, 59.9, 55.9, 58.0, 70.2, 72.7, 82.3, 84.2, 88.8]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 2, 8, 15, 25, 31, 37, 33, 34, 38, 54]
Epoch 333 Acc: 97.78 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 261 train Loss: 3226.5 test Loss: 353.2
Epoch 334 Iter 0 subLoss 2940.9 multi -25.87 import weight 0.00
Epoch 334 Iter 1 subLoss 4193.9 multi -4.97 import weight 0.00
Epoch 334 Iter 2 subLoss 3933.6 multi -4.97 import weight 0.00
Epoch 334 Iter 3 subLoss 5322.6 multi 6.97 import weight 0.00
Epoch 334 Iter 4 subLoss 3474.8 multi 3.99 import weight 0.00
Epoch 334 Iter 5 subLoss 3357.8 multi 6.97 import weight 0.00
Epoch 334 Iter 6 subLoss 3160.9 multi -4.97 import weight 0.00
Epoch 334 Iter 7 subLoss 3731.1 multi 18.91 import weight 0.00
Epoch 334 Iter 8 subLoss 3183.6 multi -16.91 import weight 0.00
Epoch 334 Iter 9 subLoss 3689.3 multi -1.99 import weight 0.00
Epoch 334 Iter 10 subLoss 3734.8 multi 21.90 import weight 0.00
Epoch 334 Iter 11 subLoss 3444.4 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0086 / 0.09659 / 15.59
Entropy seen (from low to high)
[28, 47, 229, 189, 195, 389, 530, 464, 372, 311, 204, 191, 233, 293, 263, 182, 158, 105, 107, 74, 70, 68, 45, 51, 47, 45, 41, 30, 26, 21, 30, 23, 15, 19, 12, 5, 2, 6, 7, 5, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 31, 52, 62, 75, 108, 144, 141, 159, 207, 199, 202, 196, 239, 239, 179, 239, 203, 218, 218, 183, 177, 166, 155, 175, 144, 117, 158, 107, 105, 81, 48, 38, 22, 21, 12, 13, 7, 8, 2, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 30.1, 33.6, 36.4, 40.4, 44.2, 47.3, 50.8, 54.0, 57.9, 61.3, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 66.6, 66.6, 44.4, 57.1, 45.8, 61.7, 74.9, 64.8, 84.8, 86.1, 88.6]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 9, 14, 24, 34, 32, 37, 33, 36, 53]
Epoch 334 Acc: 97.41 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 344 train Loss: 4471.0 test Loss: 445.4
Epoch 335 Iter 0 subLoss 4361.7 multi 21.90 import weight 0.00
Epoch 335 Iter 1 subLoss 3998.6 multi 15.93 import weight 0.00
Epoch 335 Iter 2 subLoss 3024.5 multi -1.98 import weight 0.00
Epoch 335 Iter 3 subLoss 2807.4 multi -10.94 import weight 0.00
Epoch 335 Iter 4 subLoss 3515.4 multi 3.99 import weight 0.00
Epoch 335 Iter 5 subLoss 3410.8 multi 1.00 import weight 0.00
Epoch 335 Iter 6 subLoss 3251.5 multi -7.96 import weight 0.00
Epoch 335 Iter 7 subLoss 3379.9 multi -19.90 import weight 0.00
Epoch 335 Iter 8 subLoss 6377.6 multi 3.99 import weight 0.00
Epoch 335 Iter 9 subLoss 4024.0 multi 3.99 import weight 0.00
Epoch 335 Iter 10 subLoss 3547.6 multi -1.98 import weight 0.00
Epoch 335 Iter 11 subLoss 3423.4 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0086 / 0.09688 / 15.96
Entropy seen (from low to high)
[28, 50, 230, 193, 194, 401, 551, 448, 370, 311, 201, 190, 238, 303, 262, 174, 152, 101, 103, 75, 69, 64, 47, 47, 47, 46, 39, 28, 29, 19, 31, 22, 14, 19, 11, 5, 2, 6, 7, 6, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 30, 49, 64, 76, 99, 151, 131, 166, 196, 205, 198, 205, 230, 239, 179, 235, 211, 206, 219, 192, 169, 166, 152, 171, 161, 115, 154, 110, 110, 80, 53, 40, 22, 21, 13, 13, 8, 8, 2, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.5, 33.4, 36.3, 40.4, 44.3, 47.4, 50.8, 54.1, 58.0, 61.1, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 66.6, 66.6, 44.4, 57.1, 45.8, 65.6, 74.2, 67.6, 83.8, 81.5, 90.3]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 9, 14, 24, 32, 35, 34, 31, 38, 52]
Epoch 335 Acc: 97.45 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 342 train Loss: 3294.9 test Loss: 431.0
Epoch 336 Iter 0 subLoss 3250.9 multi -4.97 import weight 0.00
Epoch 336 Iter 1 subLoss 2823.0 multi -7.96 import weight 0.00
Epoch 336 Iter 2 subLoss 3251.9 multi -1.99 import weight 0.00
Epoch 336 Iter 3 subLoss 3574.5 multi 9.96 import weight 0.00
Epoch 336 Iter 4 subLoss 3636.7 multi 6.97 import weight 0.00
Epoch 336 Iter 5 subLoss 3632.8 multi 9.96 import weight 0.00
Epoch 336 Iter 6 subLoss 2993.9 multi -1.99 import weight 0.00
Epoch 336 Iter 7 subLoss 3271.7 multi -7.96 import weight 0.00
Epoch 336 Iter 8 subLoss 3529.6 multi -13.93 import weight 0.00
Epoch 336 Iter 9 subLoss 4173.9 multi 27.87 import weight 0.00
Epoch 336 Iter 10 subLoss 9427.7 multi 3.99 import weight 0.00
Epoch 336 Iter 11 subLoss 3239.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0085 / 0.09720 / 15.16
Entropy seen (from low to high)
[28, 54, 229, 198, 198, 417, 557, 440, 376, 299, 196, 189, 261, 291, 258, 174, 145, 102, 95, 74, 73, 61, 41, 54, 42, 47, 36, 32, 25, 20, 30, 20, 15, 20, 10, 5, 2, 7, 7, 5, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 30, 47, 65, 72, 98, 146, 138, 157, 188, 208, 198, 204, 230, 235, 179, 238, 210, 208, 218, 185, 179, 160, 160, 171, 158, 123, 146, 117, 110, 82, 59, 42, 21, 20, 15, 11, 11, 7, 3, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 29.9, 33.3, 36.4, 40.6, 44.2, 47.1, 50.7, 53.9, 58.0, 61.2, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 33.3, 66.6, 44.4, 49.9, 51.9, 64.5, 71.4, 70.2, 86.2, 79.9, 88.6]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 9, 12, 25, 31, 35, 37, 29, 35, 53]
Epoch 336 Acc: 97.43 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 323 train Loss: 3545.9 test Loss: 457.0
Epoch 337 Iter 0 subLoss 3138.5 multi -10.94 import weight 0.00
Epoch 337 Iter 1 subLoss 4529.6 multi -10.94 import weight 0.00
Epoch 337 Iter 2 subLoss 10225.4 multi 1.00 import weight 0.00
Epoch 337 Iter 3 subLoss 7231.3 multi -10.94 import weight 0.00
Epoch 337 Iter 4 subLoss 36892.8 multi -1.99 import weight 0.00
Epoch 337 Iter 5 subLoss 192479.1 multi 1.00 import weight 0.00
Epoch 337 Iter 6 subLoss 15063.7 multi 1.00 import weight 0.00
Epoch 337 Iter 7 subLoss 11139.0 multi -1.99 import weight 0.00
Epoch 337 Iter 8 subLoss 17738.6 multi -1.99 import weight 0.00
Epoch 337 Iter 9 subLoss 37617.2 multi 1.00 import weight 0.00
Epoch 337 Iter 10 subLoss 16613.2 multi 3.99 import weight 0.00
Epoch 337 Iter 11 subLoss 6935.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0085 / 0.09711 / 16.06
Entropy seen (from low to high)
[28, 58, 234, 198, 201, 429, 560, 447, 369, 290, 196, 187, 253, 297, 247, 168, 142, 106, 96, 70, 68, 72, 41, 47, 48, 41, 40, 32, 27, 17, 31, 22, 15, 20, 10, 5, 2, 7, 7, 5, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 29, 50, 65, 70, 98, 150, 133, 156, 183, 210, 208, 195, 235, 235, 193, 217, 209, 221, 222, 181, 181, 169, 155, 171, 144, 128, 151, 113, 105, 80, 57, 44, 22, 21, 16, 11, 10, 8, 3, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 29.9, 33.4, 36.4, 40.1, 43.7, 47.0, 50.7, 54.0, 57.8, 61.2, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 33.3, 66.6, 42.8, 49.9, 55.5, 64.7, 70.9, 73.6, 83.3, 77.7, 92.4]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 7, 12, 27, 34, 31, 38, 30, 36, 53]
Epoch 337 Acc: 95.43 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 693 train Loss: 5588.1 test Loss: 833.7
Epoch 338 Iter 0 subLoss 5311.6 multi 9.96 import weight 0.00
Epoch 338 Iter 1 subLoss 4296.1 multi -1.99 import weight 0.00
Epoch 338 Iter 2 subLoss 4439.3 multi 3.99 import weight 0.00
Epoch 338 Iter 3 subLoss 4642.2 multi 18.91 import weight 0.00
Epoch 338 Iter 4 subLoss 3033.0 multi -1.98 import weight 0.00
Epoch 338 Iter 5 subLoss 3696.4 multi 6.97 import weight 0.00
Epoch 338 Iter 6 subLoss 3125.9 multi 1.00 import weight 0.00
Epoch 338 Iter 7 subLoss 3205.2 multi 9.96 import weight 0.00
Epoch 338 Iter 8 subLoss 3269.3 multi 3.99 import weight 0.00
Epoch 338 Iter 9 subLoss 3336.8 multi 1.00 import weight 0.00
Epoch 338 Iter 10 subLoss 2957.4 multi 12.94 import weight 0.00
Epoch 338 Iter 11 subLoss 3057.3 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0085 / 0.09742 / 16.80
Entropy seen (from low to high)
[28, 60, 238, 199, 211, 447, 554, 446, 369, 280, 197, 196, 249, 299, 248, 169, 131, 102, 96, 72, 64, 69, 33, 52, 45, 42, 40, 30, 27, 19, 29, 21, 16, 19, 10, 5, 5, 4, 7, 5, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 28, 48, 67, 68, 98, 147, 132, 155, 174, 217, 207, 189, 233, 239, 189, 218, 212, 217, 215, 192, 185, 155, 160, 171, 142, 136, 142, 129, 100, 86, 59, 43, 25, 22, 16, 11, 11, 8, 3, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 30.0, 33.4, 36.6, 40.2, 43.6, 47.0, 50.8, 54.1, 57.8, 61.2, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 33.3, 28.5, 45.4, 53.5, 66.6, 68.7, 77.1, 81.2, 80.5, 94.1]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 7, 11, 28, 33, 32, 35, 32, 36, 51]
Epoch 338 Acc: 97.92 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 305 train Loss: 3097.8 test Loss: 368.7
Epoch 339 Iter 0 subLoss 3194.0 multi 12.94 import weight 0.00
Epoch 339 Iter 1 subLoss 2609.1 multi 1.00 import weight 0.00
Epoch 339 Iter 2 subLoss 2891.4 multi 9.96 import weight 0.00
Epoch 339 Iter 3 subLoss 2725.0 multi 1.00 import weight 0.00
Epoch 339 Iter 4 subLoss 2933.7 multi 18.91 import weight 0.00
Epoch 339 Iter 5 subLoss 2896.4 multi 12.94 import weight 0.00
Epoch 339 Iter 6 subLoss 2787.2 multi 3.99 import weight 0.00
Epoch 339 Iter 7 subLoss 3131.4 multi -10.94 import weight 0.00
Epoch 339 Iter 8 subLoss 2916.4 multi 3.99 import weight 0.00
Epoch 339 Iter 9 subLoss 3007.7 multi 1.00 import weight 0.00
Epoch 339 Iter 10 subLoss 3108.3 multi 15.93 import weight 0.00
Epoch 339 Iter 11 subLoss 2998.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0084 / 0.09775 / 16.61
Entropy seen (from low to high)
[28, 63, 237, 204, 214, 470, 557, 435, 365, 277, 191, 204, 264, 288, 239, 174, 119, 109, 91, 70, 66, 61, 35, 52, 46, 42, 41, 30, 21, 22, 30, 19, 15, 18, 10, 5, 4, 5, 8, 4, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 27, 47, 66, 69, 98, 144, 130, 153, 171, 220, 197, 203, 226, 234, 190, 217, 208, 215, 209, 202, 188, 156, 161, 163, 150, 134, 139, 136, 97, 91, 62, 45, 29, 20, 18, 12, 11, 8, 3, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 30.0, 33.5, 36.8, 40.4, 43.7, 47.1, 50.8, 54.1, 57.7, 61.2, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 33.3, 28.5, 45.4, 53.5, 65.6, 67.7, 79.4, 79.4, 79.9, 94.2]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 7, 11, 28, 32, 31, 34, 34, 35, 52]
Epoch 339 Acc: 98.13 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 299 train Loss: 2912.0 test Loss: 319.4
Epoch 340 Iter 0 subLoss 2907.9 multi -1.99 import weight 0.00
Epoch 340 Iter 1 subLoss 3022.4 multi 1.00 import weight 0.00
Epoch 340 Iter 2 subLoss 2593.5 multi -16.91 import weight 0.00
Epoch 340 Iter 3 subLoss 3544.5 multi 1.00 import weight 0.00
Epoch 340 Iter 4 subLoss 3576.8 multi 12.94 import weight 0.00
Epoch 340 Iter 5 subLoss 3069.8 multi -1.99 import weight 0.00
Epoch 340 Iter 6 subLoss 2801.7 multi -7.96 import weight 0.00
Epoch 340 Iter 7 subLoss 2831.4 multi 3.98 import weight 0.00
Epoch 340 Iter 8 subLoss 3163.9 multi -1.99 import weight 0.00
Epoch 340 Iter 9 subLoss 2998.1 multi 3.99 import weight 0.00
Epoch 340 Iter 10 subLoss 2815.6 multi -4.97 import weight 0.00
Epoch 340 Iter 11 subLoss 2443.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0084 / 0.09805 / 17.16
Entropy seen (from low to high)
[29, 64, 243, 203, 219, 487, 554, 437, 364, 271, 192, 204, 267, 293, 231, 165, 120, 111, 78, 74, 69, 53, 41, 50, 47, 36, 42, 30, 23, 21, 30, 19, 14, 16, 10, 5, 5, 4, 8, 4, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 27, 45, 65, 71, 98, 143, 129, 145, 174, 219, 194, 205, 215, 233, 203, 209, 207, 220, 206, 203, 182, 159, 163, 159, 155, 134, 141, 138, 98, 91, 69, 47, 28, 19, 20, 13, 10, 8, 4, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 30.1, 33.6, 37.1, 40.7, 43.6, 47.0, 50.8, 54.2, 57.7, 61.1, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 0.0, 33.3, 24.9, 37.4, 57.1, 67.6, 63.3, 78.7, 78.7, 78.3, 95.6]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 8, 8, 28, 34, 30, 33, 33, 37, 46]
Epoch 340 Acc: 98.07 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 244 train Loss: 3085.3 test Loss: 341.7
Epoch 341 Iter 0 subLoss 2944.2 multi -25.87 import weight 0.00
Epoch 341 Iter 1 subLoss 4984.6 multi 1.00 import weight 0.00
Epoch 341 Iter 2 subLoss 4332.0 multi -4.97 import weight 0.00
Epoch 341 Iter 3 subLoss 6698.8 multi 3.99 import weight 0.00
Epoch 341 Iter 4 subLoss 4702.7 multi -1.98 import weight 0.00
Epoch 341 Iter 5 subLoss 4651.8 multi -4.97 import weight 0.00
Epoch 341 Iter 6 subLoss 6711.8 multi 1.00 import weight 0.00
Epoch 341 Iter 7 subLoss 5160.2 multi 3.99 import weight 0.00
Epoch 341 Iter 8 subLoss 3451.0 multi 9.96 import weight 0.00
Epoch 341 Iter 9 subLoss 3177.6 multi 3.99 import weight 0.00
Epoch 341 Iter 10 subLoss 2999.9 multi 6.97 import weight 0.00
Epoch 341 Iter 11 subLoss 3006.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0083 / 0.09835 / 17.23
Entropy seen (from low to high)
[29, 70, 243, 208, 225, 495, 564, 434, 355, 260, 198, 209, 281, 284, 223, 168, 104, 113, 77, 70, 68, 55, 46, 43, 48, 37, 40, 30, 21, 23, 28, 18, 15, 18, 7, 7, 3, 4, 8, 4, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 28, 42, 67, 72, 93, 143, 127, 138, 184, 212, 189, 204, 224, 218, 212, 204, 212, 221, 197, 211, 175, 160, 166, 151, 159, 136, 145, 136, 105, 95, 69, 50, 30, 17, 19, 17, 9, 9, 4, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 30.2, 33.6, 37.4, 40.5, 43.7, 47.1, 50.7, 54.2, 57.6, 61.0, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 0.0, 33.3, 14.2, 39.9, 61.5, 61.7, 66.6, 80.6, 76.4, 77.7, 95.6]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 7, 10, 26, 34, 30, 31, 34, 36, 46]
Epoch 341 Acc: 98.23 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 300 train Loss: 2942.0 test Loss: 338.3
Epoch 342 Iter 0 subLoss 2973.3 multi -1.99 import weight 0.00
Epoch 342 Iter 1 subLoss 2939.5 multi 21.90 import weight 0.00
Epoch 342 Iter 2 subLoss 2828.9 multi -7.96 import weight 0.00
Epoch 342 Iter 3 subLoss 2769.7 multi 27.87 import weight 0.00
Epoch 342 Iter 4 subLoss 2413.5 multi 12.94 import weight 0.00
Epoch 342 Iter 5 subLoss 2760.4 multi 30.85 import weight 0.00
Epoch 342 Iter 6 subLoss 2837.2 multi 3.99 import weight 0.00
Epoch 342 Iter 7 subLoss 2340.9 multi 3.99 import weight 0.00
Epoch 342 Iter 8 subLoss 2595.5 multi -13.93 import weight 0.00
Epoch 342 Iter 9 subLoss 3129.5 multi 3.99 import weight 0.00
Epoch 342 Iter 10 subLoss 2222.4 multi 3.99 import weight 0.00
Epoch 342 Iter 11 subLoss 3053.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0082 / 0.09867 / 16.99
Entropy seen (from low to high)
[29, 73, 247, 208, 238, 502, 561, 426, 364, 252, 201, 214, 280, 297, 200, 161, 115, 109, 68, 73, 67, 50, 49, 46, 42, 43, 37, 27, 20, 24, 29, 17, 16, 15, 8, 6, 4, 4, 8, 4, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 25, 42, 69, 69, 92, 145, 112, 144, 186, 204, 188, 207, 223, 219, 210, 202, 218, 221, 191, 204, 185, 155, 169, 151, 161, 142, 128, 150, 100, 105, 71, 53, 30, 19, 20, 16, 11, 9, 4, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 30.4, 33.6, 37.7, 40.5, 43.9, 47.0, 50.6, 54.2, 57.5, 61.0, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 0.0, 33.3, 14.2, 49.9, 58.3, 62.8, 68.9, 76.6, 77.1, 77.1, 95.5]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 7, 10, 24, 35, 29, 30, 35, 35, 45]
Epoch 342 Acc: 98.33 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 305 train Loss: 2725.6 test Loss: 286.2
Epoch 343 Iter 0 subLoss 2488.7 multi -4.97 import weight 0.00
Epoch 343 Iter 1 subLoss 3133.5 multi -10.94 import weight 0.00
Epoch 343 Iter 2 subLoss 3107.3 multi 18.91 import weight 0.00
Epoch 343 Iter 3 subLoss 2870.6 multi 9.96 import weight 0.00
Epoch 343 Iter 4 subLoss 2948.8 multi -25.87 import weight 0.00
Epoch 343 Iter 5 subLoss 3390.9 multi -1.99 import weight 0.00
Epoch 343 Iter 6 subLoss 3283.9 multi 6.97 import weight 0.00
Epoch 343 Iter 7 subLoss 2473.5 multi -1.99 import weight 0.00
Epoch 343 Iter 8 subLoss 2938.2 multi 24.88 import weight 1.00
Epoch 343 Iter 9 subLoss 2807.4 multi -4.97 import weight 0.00
Epoch 343 Iter 10 subLoss 3276.2 multi -7.96 import weight 0.00
Epoch 343 Iter 11 subLoss 3300.4 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0083 / 0.09847 / 17.15
Entropy seen (from low to high)
[29, 79, 246, 207, 251, 518, 552, 429, 360, 246, 200, 222, 283, 281, 197, 157, 104, 107, 77, 74, 56, 62, 41, 49, 42, 39, 43, 25, 26, 21, 25, 22, 14, 13, 9, 7, 5, 4, 7, 5, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 25, 44, 66, 70, 96, 143, 119, 145, 177, 218, 169, 213, 224, 227, 216, 200, 213, 222, 193, 211, 181, 162, 162, 155, 151, 145, 127, 141, 87, 114, 72, 46, 36, 19, 20, 17, 10, 9, 5, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 33.5, 37.7, 39.7, 44.0, 47.1, 50.7, 54.3, 57.5, 61.2, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 33.3, 0.0, 28.5, 30.7, 63.6, 62.1, 67.8, 81.2, 76.4, 76.3, 97.5]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 7, 13, 22, 37, 28, 32, 34, 38, 41]
Epoch 343 Acc: 93.23 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 330 train Loss: 9844.0 test Loss: 1490.4
Epoch 344 Iter 0 subLoss 10661.1 multi 1.00 import weight 0.00
Epoch 344 Iter 1 subLoss 7425.9 multi -7.96 import weight 0.00
Epoch 344 Iter 2 subLoss 31154.2 multi -1.99 import weight 0.00
Epoch 344 Iter 3 subLoss 205792.6 multi 1.00 import weight 0.00
Epoch 344 Iter 4 subLoss 14810.9 multi 1.00 import weight 0.00
Epoch 344 Iter 5 subLoss 11170.6 multi -7.96 import weight 0.00
Epoch 344 Iter 6 subLoss 111950.8 multi 1.00 import weight 0.00
Epoch 344 Iter 7 subLoss 9874.7 multi 12.94 import weight 0.00
Epoch 344 Iter 8 subLoss 4657.2 multi -1.98 import weight 0.00
Epoch 344 Iter 9 subLoss 5535.3 multi -13.93 import weight 0.00
Epoch 344 Iter 10 subLoss 20815.7 multi -1.99 import weight 0.00
Epoch 344 Iter 11 subLoss 38099.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0084 / 0.09777 / 17.46
Entropy seen (from low to high)
[25, 64, 228, 189, 215, 364, 488, 430, 405, 326, 229, 256, 265, 325, 222, 168, 126, 110, 85, 73, 68, 61, 41, 46, 51, 37, 42, 30, 25, 21, 26, 26, 16, 13, 10, 6, 5, 4, 6, 7, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 26, 44, 69, 71, 100, 143, 122, 161, 172, 219, 178, 211, 222, 244, 206, 201, 219, 219, 199, 213, 178, 160, 162, 166, 139, 147, 136, 128, 94, 94, 69, 40, 32, 19, 18, 11, 9, 9, 2, 3, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 33.4, 37.7, 39.8, 43.9, 47.1, 50.5, 54.2, 57.5, 61.1, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 33.3, 33.3, 16.6, 35.7, 63.6, 62.1, 67.8, 78.7, 79.4, 74.9, 98.0]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 3, 6, 14, 22, 37, 28, 33, 34, 36, 52]
Epoch 344 Acc: 63.34 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3809 train Loss: 28587.7 test Loss: 5380.6
Epoch 345 Iter 0 subLoss 27902.8 multi 1.00 import weight 0.00
Epoch 345 Iter 1 subLoss 19346.9 multi 3.99 import weight 0.00
Epoch 345 Iter 2 subLoss 5467.7 multi 3.99 import weight 0.00
Epoch 345 Iter 3 subLoss 4989.7 multi 3.99 import weight 0.00
Epoch 345 Iter 4 subLoss 4128.0 multi -10.94 import weight 0.00
Epoch 345 Iter 5 subLoss 5711.1 multi 1.00 import weight 0.00
Epoch 345 Iter 6 subLoss 5268.8 multi 9.96 import weight 0.00
Epoch 345 Iter 7 subLoss 3886.8 multi -1.99 import weight 0.00
Epoch 345 Iter 8 subLoss 3834.2 multi -10.94 import weight 0.00
Epoch 345 Iter 9 subLoss 4555.8 multi 6.97 import weight 0.00
Epoch 345 Iter 10 subLoss 3780.1 multi 6.97 import weight 0.00
Epoch 345 Iter 11 subLoss 4094.9 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0084 / 0.09777 / 16.92
Entropy seen (from low to high)
[26, 66, 228, 190, 222, 376, 488, 435, 404, 324, 222, 265, 270, 312, 217, 167, 120, 107, 87, 71, 66, 61, 44, 40, 52, 40, 41, 27, 26, 22, 24, 27, 17, 13, 9, 6, 6, 3, 6, 7, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 26, 42, 74, 66, 103, 141, 126, 154, 174, 223, 173, 211, 231, 234, 212, 200, 228, 205, 208, 204, 184, 158, 162, 153, 147, 146, 135, 128, 97, 94, 65, 44, 33, 22, 18, 7, 13, 8, 2, 4, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.8, 33.6, 37.3, 39.4, 43.9, 47.1, 50.5, 54.4, 57.6, 61.2, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 33.3, 0.0, 37.4, 35.7, 54.5, 67.5, 67.7, 79.9, 79.4, 74.9, 97.9]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 1, 8, 14, 22, 37, 31, 30, 34, 36, 48]
Epoch 345 Acc: 96.75 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 409 train Loss: 4271.2 test Loss: 572.3
Epoch 346 Iter 0 subLoss 3968.6 multi 21.90 import weight 0.00
Epoch 346 Iter 1 subLoss 3229.5 multi 1.00 import weight 0.00
Epoch 346 Iter 2 subLoss 3205.3 multi 9.96 import weight 0.00
Epoch 346 Iter 3 subLoss 3328.2 multi -10.94 import weight 0.00
Epoch 346 Iter 4 subLoss 3424.9 multi 12.94 import weight 0.00
Epoch 346 Iter 5 subLoss 2683.7 multi 9.96 import weight 0.00
Epoch 346 Iter 6 subLoss 2840.4 multi 3.98 import weight 0.00
Epoch 346 Iter 7 subLoss 3224.3 multi 3.99 import weight 0.00
Epoch 346 Iter 8 subLoss 3299.1 multi 3.99 import weight 0.00
Epoch 346 Iter 9 subLoss 3274.4 multi -4.97 import weight 0.00
Epoch 346 Iter 10 subLoss 3196.4 multi 15.93 import weight 0.00
Epoch 346 Iter 11 subLoss 3316.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0084 / 0.09804 / 16.66
Entropy seen (from low to high)
[28, 65, 233, 196, 226, 379, 496, 438, 400, 314, 239, 254, 291, 296, 215, 160, 114, 109, 81, 68, 66, 59, 45, 40, 51, 44, 36, 28, 25, 22, 23, 27, 16, 12, 10, 6, 6, 4, 6, 6, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 24, 44, 73, 67, 103, 140, 123, 145, 177, 220, 174, 215, 229, 229, 212, 206, 228, 198, 204, 209, 179, 164, 153, 157, 158, 133, 136, 132, 96, 103, 67, 46, 34, 22, 20, 8, 13, 7, 3, 4, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 33.7, 37.5, 39.5, 44.0, 47.2, 50.6, 54.4, 57.5, 61.2, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 33.3, 0.0, 37.4, 35.7, 52.1, 71.4, 64.5, 79.3, 80.5, 74.9, 97.7]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 1, 8, 14, 23, 35, 31, 29, 36, 36, 45]
Epoch 346 Acc: 97.84 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 331 train Loss: 3038.5 test Loss: 350.3
Epoch 347 Iter 0 subLoss 2775.8 multi -28.85 import weight 0.00
Epoch 347 Iter 1 subLoss 3231.4 multi 1.00 import weight 0.00
Epoch 347 Iter 2 subLoss 3304.8 multi -13.93 import weight 0.00
Epoch 347 Iter 3 subLoss 3763.5 multi 9.96 import weight 1.00
Epoch 347 Iter 4 subLoss 3026.3 multi 3.99 import weight 0.00
Epoch 347 Iter 5 subLoss 3188.1 multi -16.91 import weight 0.00
Epoch 347 Iter 6 subLoss 3141.3 multi 6.97 import weight 0.00
Epoch 347 Iter 7 subLoss 3409.7 multi -1.99 import weight 0.00
Epoch 347 Iter 8 subLoss 3381.3 multi 1.00 import weight 0.00
Epoch 347 Iter 9 subLoss 3579.1 multi 15.93 import weight 0.00
Epoch 347 Iter 10 subLoss 3246.8 multi 3.99 import weight 0.00
Epoch 347 Iter 11 subLoss 2809.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0083 / 0.09828 / 15.92
Entropy seen (from low to high)
[29, 66, 234, 197, 237, 381, 511, 425, 408, 311, 230, 254, 309, 287, 204, 160, 109, 109, 79, 71, 62, 60, 42, 45, 46, 44, 38, 26, 23, 23, 21, 28, 16, 10, 11, 7, 5, 4, 6, 6, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 23, 46, 70, 68, 100, 142, 120, 140, 180, 217, 178, 217, 225, 223, 212, 209, 225, 203, 203, 206, 179, 161, 157, 157, 162, 127, 130, 142, 98, 104, 69, 48, 37, 22, 21, 8, 11, 9, 2, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 32.8, 37.7, 39.5, 44.1, 47.2, 50.7, 54.4, 57.6, 61.3, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 39.9, 0.0, 37.4, 49.9, 47.8, 71.4, 69.9, 76.6, 76.4, 78.7, 93.7]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 1, 8, 14, 23, 35, 30, 30, 34, 33, 48]
Epoch 347 Acc: 97.63 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 280 train Loss: 3056.4 test Loss: 386.3
Epoch 348 Iter 0 subLoss 2946.6 multi -25.87 import weight 0.00
Epoch 348 Iter 1 subLoss 2983.4 multi 1.00 import weight 0.00
Epoch 348 Iter 2 subLoss 3571.7 multi 18.91 import weight 0.00
Epoch 348 Iter 3 subLoss 2948.3 multi -22.88 import weight 0.00
Epoch 348 Iter 4 subLoss 3608.3 multi -1.98 import weight 0.00
Epoch 348 Iter 5 subLoss 3605.1 multi 1.00 import weight 0.00
Epoch 348 Iter 6 subLoss 3485.8 multi 12.94 import weight 0.00
Epoch 348 Iter 7 subLoss 3581.3 multi -7.96 import weight 0.00
Epoch 348 Iter 8 subLoss 3405.5 multi 1.00 import weight 0.00
Epoch 348 Iter 9 subLoss 3033.9 multi -4.97 import weight 0.00
Epoch 348 Iter 10 subLoss 3412.7 multi -1.99 import weight 0.00
Epoch 348 Iter 11 subLoss 3081.0 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0083 / 0.09847 / 16.57
Entropy seen (from low to high)
[29, 67, 238, 198, 238, 400, 505, 429, 410, 298, 231, 255, 323, 273, 208, 155, 107, 101, 80, 72, 63, 54, 42, 47, 43, 46, 37, 25, 23, 25, 22, 26, 15, 11, 10, 7, 5, 4, 6, 6, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 23, 44, 69, 68, 102, 133, 128, 140, 180, 211, 181, 214, 226, 222, 216, 205, 226, 198, 201, 205, 182, 161, 161, 143, 171, 126, 138, 141, 97, 100, 79, 46, 40, 20, 24, 8, 11, 9, 2, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 32.9, 37.9, 39.5, 44.1, 47.2, 50.7, 54.3, 57.5, 61.5, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 0.0, 37.4, 49.9, 43.4, 71.4, 73.0, 74.2, 74.9, 79.9, 93.6]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 1, 8, 14, 23, 35, 26, 35, 32, 35, 47]
Epoch 348 Acc: 97.39 BMA: 97.98 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 308 train Loss: 3458.5 test Loss: 446.2
Epoch 349 Iter 0 subLoss 3576.9 multi 21.90 import weight 0.00
Epoch 349 Iter 1 subLoss 3101.4 multi 21.90 import weight 0.00
Epoch 349 Iter 2 subLoss 3090.2 multi 12.94 import weight 0.00
Epoch 349 Iter 3 subLoss 3052.9 multi 3.99 import weight 0.00
Epoch 349 Iter 4 subLoss 2991.7 multi 6.97 import weight 0.00
Epoch 349 Iter 5 subLoss 3176.9 multi 6.97 import weight 0.00
Epoch 349 Iter 6 subLoss 2693.3 multi -4.97 import weight 0.00
Epoch 349 Iter 7 subLoss 2703.4 multi -4.97 import weight 0.00
Epoch 349 Iter 8 subLoss 3115.1 multi -19.90 import weight 0.00
Epoch 349 Iter 9 subLoss 3224.3 multi 6.97 import weight 0.00
Epoch 349 Iter 10 subLoss 3012.4 multi -7.96 import weight 0.00
Epoch 349 Iter 11 subLoss 2703.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0083 / 0.09869 / 17.52
Entropy seen (from low to high)
[29, 69, 243, 199, 240, 414, 514, 431, 403, 291, 230, 258, 330, 265, 198, 158, 102, 99, 82, 69, 66, 47, 43, 47, 44, 46, 34, 23, 24, 27, 23, 25, 13, 11, 10, 8, 4, 2, 7, 6, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 22, 45, 64, 73, 101, 124, 133, 140, 183, 206, 177, 224, 210, 234, 213, 210, 217, 200, 198, 203, 180, 168, 160, 139, 172, 125, 140, 138, 104, 100, 80, 53, 40, 20, 22, 11, 11, 9, 2, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 33.1, 38.2, 39.5, 44.2, 47.2, 50.7, 54.3, 57.5, 61.7, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 0.0, 24.9, 53.3, 38.0, 74.2, 66.6, 79.4, 72.7, 79.4, 93.3]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 1, 8, 15, 21, 35, 27, 34, 33, 34, 45]
Epoch 349 Acc: 97.63 BMA: 97.96 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 270 train Loss: 3281.3 test Loss: 437.3
Epoch 350 Iter 0 subLoss 2930.7 multi 27.87 import weight 1.00
Epoch 350 Iter 1 subLoss 3026.5 multi 3.99 import weight 0.00
Epoch 350 Iter 2 subLoss 2838.9 multi 6.97 import weight 0.00
Epoch 350 Iter 3 subLoss 2801.8 multi 1.00 import weight 0.00
Epoch 350 Iter 4 subLoss 2595.6 multi -10.94 import weight 0.00
Epoch 350 Iter 5 subLoss 3354.3 multi 9.96 import weight 0.00
Epoch 350 Iter 6 subLoss 2596.8 multi -7.96 import weight 0.00
Epoch 350 Iter 7 subLoss 3170.5 multi 9.96 import weight 0.00
Epoch 350 Iter 8 subLoss 2384.9 multi 1.00 import weight 0.00
Epoch 350 Iter 9 subLoss 2822.2 multi -4.97 import weight 0.00
Epoch 350 Iter 10 subLoss 3148.8 multi 9.96 import weight 0.00
Epoch 350 Iter 11 subLoss 2436.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0082 / 0.09896 / 16.79
Entropy seen (from low to high)
[29, 70, 245, 205, 240, 427, 509, 436, 397, 290, 230, 270, 335, 256, 191, 155, 100, 96, 81, 69, 65, 45, 44, 47, 40, 48, 36, 23, 20, 28, 24, 23, 12, 12, 9, 8, 4, 2, 8, 5, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 20, 46, 64, 73, 99, 122, 130, 137, 184, 200, 184, 213, 214, 238, 209, 220, 203, 206, 197, 202, 186, 161, 160, 140, 168, 133, 136, 139, 110, 97, 80, 63, 40, 20, 23, 11, 11, 10, 2, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.9, 29.4, 33.3, 38.4, 39.6, 44.3, 47.2, 50.8, 54.2, 57.5, 61.6, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 0.0, 24.9, 53.3, 40.9, 71.8, 67.8, 79.4, 75.8, 77.1, 91.1]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 1, 8, 15, 22, 32, 28, 34, 29, 35, 45]
Epoch 350 Acc: 98.11 BMA: 97.96 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 243 train Loss: 2860.0 test Loss: 323.0
Epoch 351 Iter 0 subLoss 2421.0 multi -10.94 import weight 0.00
Epoch 351 Iter 1 subLoss 2956.8 multi 3.99 import weight 0.00
Epoch 351 Iter 2 subLoss 3461.9 multi -7.96 import weight 0.00
Epoch 351 Iter 3 subLoss 3255.5 multi -1.99 import weight 0.00
Epoch 351 Iter 4 subLoss 3840.3 multi 1.00 import weight 0.00
Epoch 351 Iter 5 subLoss 2755.3 multi -13.93 import weight 0.00
Epoch 351 Iter 6 subLoss 3746.5 multi -28.85 import weight 0.00
Epoch 351 Iter 7 subLoss 15287.6 multi 1.00 import weight 0.00
Epoch 351 Iter 8 subLoss 9943.4 multi -1.98 import weight 0.00
Epoch 351 Iter 9 subLoss 14433.5 multi -4.97 import weight 0.00
Epoch 351 Iter 10 subLoss 170829.6 multi 1.00 import weight 0.00
Epoch 351 Iter 11 subLoss 18707.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0082 / 0.09884 / 16.96
Entropy seen (from low to high)
[29, 75, 239, 201, 237, 431, 509, 411, 396, 293, 219, 282, 324, 267, 200, 158, 104, 99, 70, 80, 60, 56, 39, 49, 46, 42, 35, 27, 22, 25, 26, 22, 14, 9, 12, 5, 6, 2, 9, 4, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 21, 47, 67, 68, 103, 132, 123, 134, 186, 205, 183, 213, 219, 222, 222, 211, 211, 207, 196, 198, 181, 165, 153, 141, 166, 145, 131, 136, 107, 93, 89, 62, 39, 17, 25, 11, 12, 7, 3, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.6, 29.1, 32.9, 37.5, 39.5, 44.2, 47.2, 50.7, 54.3, 57.6, 61.4, 64.6, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 49.9, 19.9, 56.2, 40.9, 70.9, 72.7, 72.4, 77.4, 79.4, 91.6]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 5, 16, 22, 31, 33, 29, 31, 34, 48]
Epoch 351 Acc: 85.48 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1870 train Loss: 13232.9 test Loss: 2060.6
Epoch 352 Iter 0 subLoss 13237.3 multi -1.99 import weight 0.00
Epoch 352 Iter 1 subLoss 21215.9 multi -4.97 import weight 0.00
Epoch 352 Iter 2 subLoss 92064.0 multi 1.00 import weight 0.00
Epoch 352 Iter 3 subLoss 55713.5 multi 1.00 import weight 0.00
Epoch 352 Iter 4 subLoss 47966.0 multi 1.00 import weight 0.00
Epoch 352 Iter 5 subLoss 43695.2 multi 1.00 import weight 0.00
Epoch 352 Iter 6 subLoss 38731.7 multi 1.00 import weight 0.00
Epoch 352 Iter 7 subLoss 33525.3 multi 1.00 import weight 0.00
Epoch 352 Iter 8 subLoss 30261.2 multi 1.00 import weight 0.00
Epoch 352 Iter 9 subLoss 26218.6 multi 1.00 import weight 0.00
Epoch 352 Iter 10 subLoss 22733.9 multi 3.99 import weight 0.00
Epoch 352 Iter 11 subLoss 13977.1 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0083 / 0.09847 / 16.28
Entropy seen (from low to high)
[29, 78, 241, 188, 191, 433, 542, 415, 387, 292, 205, 290, 313, 260, 222, 158, 123, 91, 71, 81, 67, 50, 46, 50, 41, 48, 32, 33, 19, 22, 29, 24, 16, 9, 11, 6, 6, 2, 8, 4, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 22, 48, 65, 72, 105, 135, 125, 139, 193, 190, 198, 198, 233, 227, 218, 226, 205, 197, 206, 185, 182, 164, 161, 144, 162, 134, 130, 137, 99, 97, 78, 63, 41, 16, 21, 13, 13, 7, 2, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 29.2, 33.0, 37.4, 39.4, 44.3, 47.5, 50.6, 54.2, 57.7, 61.2, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 49.9, 19.9, 52.6, 49.9, 63.3, 74.2, 74.9, 74.1, 81.0, 93.8]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 5, 19, 20, 30, 35, 28, 31, 37, 49]
Epoch 352 Acc: 83.30 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 1397 train Loss: 18339.0 test Loss: 2179.2
Epoch 353 Iter 0 subLoss 18669.2 multi 1.00 import weight 0.00
Epoch 353 Iter 1 subLoss 15085.2 multi 3.99 import weight 0.00
Epoch 353 Iter 2 subLoss 9451.0 multi 1.00 import weight 0.00
Epoch 353 Iter 3 subLoss 9408.3 multi 6.97 import weight 0.00
Epoch 353 Iter 4 subLoss 6135.1 multi -4.97 import weight 0.00
Epoch 353 Iter 5 subLoss 6788.4 multi 1.00 import weight 0.00
Epoch 353 Iter 6 subLoss 6979.3 multi 12.94 import weight 0.00
Epoch 353 Iter 7 subLoss 4046.5 multi 9.96 import weight 0.00
Epoch 353 Iter 8 subLoss 3438.4 multi -10.94 import weight 0.00
Epoch 353 Iter 9 subLoss 4158.3 multi 6.97 import weight 0.00
Epoch 353 Iter 10 subLoss 3433.4 multi -7.96 import weight 0.00
Epoch 353 Iter 11 subLoss 3754.0 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0082 / 0.09873 / 16.24
Entropy seen (from low to high)
[29, 83, 241, 188, 197, 455, 530, 430, 371, 292, 207, 294, 309, 258, 212, 155, 126, 89, 70, 80, 68, 47, 44, 50, 43, 45, 32, 34, 17, 23, 30, 23, 16, 8, 9, 7, 7, 2, 9, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 21, 49, 64, 67, 108, 127, 130, 138, 191, 190, 196, 195, 228, 227, 221, 230, 199, 209, 193, 195, 174, 162, 162, 150, 158, 133, 134, 134, 106, 101, 76, 66, 43, 17, 20, 15, 13, 6, 3, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.2, 29.3, 33.0, 37.5, 39.4, 44.1, 47.4, 50.7, 54.3, 57.6, 61.2, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 24.9, 49.9, 19.9, 43.7, 55.9, 64.2, 74.2, 73.0, 75.7, 81.8, 92.3]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 5, 16, 25, 28, 35, 26, 33, 33, 52]
Epoch 353 Acc: 97.88 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 375 train Loss: 3305.8 test Loss: 363.0
Epoch 354 Iter 0 subLoss 3307.3 multi -10.94 import weight 0.00
Epoch 354 Iter 1 subLoss 3962.5 multi 24.88 import weight 0.00
Epoch 354 Iter 2 subLoss 4070.8 multi -10.94 import weight 0.00
Epoch 354 Iter 3 subLoss 11189.8 multi -1.99 import weight 0.00
Epoch 354 Iter 4 subLoss 29329.2 multi 1.00 import weight 0.00
Epoch 354 Iter 5 subLoss 9819.8 multi 3.99 import weight 0.00
Epoch 354 Iter 6 subLoss 4139.7 multi 6.97 import weight 0.00
Epoch 354 Iter 7 subLoss 3494.1 multi -28.85 import weight 0.00
Epoch 354 Iter 8 subLoss 5529.5 multi -1.99 import weight 0.00
Epoch 354 Iter 9 subLoss 6004.2 multi 1.00 import weight 0.00
Epoch 354 Iter 10 subLoss 5508.9 multi 12.94 import weight 0.00
Epoch 354 Iter 11 subLoss 3821.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0082 / 0.09893 / 16.27
Entropy seen (from low to high)
[29, 87, 242, 187, 204, 471, 525, 433, 368, 283, 209, 297, 315, 254, 206, 154, 123, 88, 67, 79, 65, 48, 45, 50, 42, 46, 31, 31, 19, 20, 33, 23, 14, 8, 9, 7, 7, 2, 9, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 22, 44, 68, 65, 106, 128, 130, 133, 195, 190, 196, 197, 226, 218, 227, 217, 206, 211, 189, 195, 183, 163, 157, 153, 150, 135, 131, 143, 106, 100, 82, 63, 42, 22, 19, 17, 13, 6, 3, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.0, 29.4, 33.1, 37.7, 40.2, 44.2, 47.4, 50.7, 54.3, 57.7, 61.3, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 59.9, 0.0, 46.6, 51.9, 64.2, 73.5, 71.4, 80.6, 79.9, 91.8]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 5, 5, 15, 25, 28, 34, 28, 31, 35, 49]
Epoch 354 Acc: 97.65 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 382 train Loss: 3593.6 test Loss: 412.8
Epoch 355 Iter 0 subLoss 3484.5 multi 15.93 import weight 0.00
Epoch 355 Iter 1 subLoss 3034.3 multi -4.97 import weight 0.00
Epoch 355 Iter 2 subLoss 3270.8 multi -1.98 import weight 0.00
Epoch 355 Iter 3 subLoss 2942.6 multi -22.88 import weight 0.00
Epoch 355 Iter 4 subLoss 5254.7 multi -7.96 import weight 0.00
Epoch 355 Iter 5 subLoss 16189.0 multi 1.00 import weight 0.00
Epoch 355 Iter 6 subLoss 8169.5 multi 3.98 import weight 0.00
Epoch 355 Iter 7 subLoss 4285.9 multi 9.96 import weight 0.00
Epoch 355 Iter 8 subLoss 3059.6 multi 6.97 import weight 0.00
Epoch 355 Iter 9 subLoss 3391.0 multi -1.99 import weight 0.00
Epoch 355 Iter 10 subLoss 3941.1 multi 3.99 import weight 0.00
Epoch 355 Iter 11 subLoss 3546.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0082 / 0.09915 / 15.76
Entropy seen (from low to high)
[29, 89, 247, 186, 213, 492, 513, 434, 361, 276, 221, 296, 322, 255, 186, 155, 119, 90, 66, 83, 58, 47, 46, 50, 39, 50, 28, 28, 20, 20, 33, 23, 13, 9, 8, 6, 8, 2, 9, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 21, 44, 67, 64, 104, 125, 130, 126, 200, 195, 193, 194, 227, 210, 237, 211, 208, 204, 200, 190, 179, 164, 163, 148, 157, 132, 130, 142, 109, 100, 84, 66, 45, 22, 20, 17, 13, 6, 3, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 29.5, 33.1, 37.7, 39.9, 43.9, 47.4, 50.9, 54.5, 57.6, 61.3, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 59.9, 0.0, 46.6, 48.1, 67.8, 72.7, 69.2, 81.2, 79.4, 91.8]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 5, 4, 15, 27, 28, 33, 26, 32, 34, 49]
Epoch 355 Acc: 97.90 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 354 train Loss: 3301.6 test Loss: 349.3
Epoch 356 Iter 0 subLoss 3511.9 multi 6.97 import weight 0.00
Epoch 356 Iter 1 subLoss 2749.0 multi 12.94 import weight 0.00
Epoch 356 Iter 2 subLoss 2822.3 multi -1.99 import weight 0.00
Epoch 356 Iter 3 subLoss 3105.0 multi 21.90 import weight 1.00
Epoch 356 Iter 4 subLoss 2829.1 multi 1.00 import weight 0.00
Epoch 356 Iter 5 subLoss 3250.7 multi 1.00 import weight 0.00
Epoch 356 Iter 6 subLoss 3045.6 multi -1.99 import weight 0.00
Epoch 356 Iter 7 subLoss 3333.2 multi 1.00 import weight 0.00
Epoch 356 Iter 8 subLoss 3093.9 multi 15.93 import weight 0.00
Epoch 356 Iter 9 subLoss 2768.5 multi 30.85 import weight 0.00
Epoch 356 Iter 10 subLoss 2840.7 multi 3.99 import weight 0.00
Epoch 356 Iter 11 subLoss 3554.6 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0081 / 0.09941 / 16.24
Entropy seen (from low to high)
[29, 99, 240, 189, 223, 502, 517, 425, 364, 265, 233, 301, 311, 249, 184, 152, 122, 84, 68, 80, 61, 45, 42, 49, 45, 47, 26, 27, 20, 22, 32, 22, 14, 8, 8, 6, 9, 2, 8, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 18, 45, 63, 68, 105, 125, 124, 126, 188, 207, 189, 193, 232, 203, 235, 211, 209, 205, 195, 192, 182, 161, 160, 146, 159, 145, 129, 133, 113, 98, 91, 69, 45, 26, 19, 19, 12, 7, 3, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.0, 29.7, 32.9, 37.7, 39.8, 44.0, 47.5, 51.0, 54.3, 57.4, 61.3, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 24.9, 79.9, 0.0, 43.7, 46.4, 65.3, 78.5, 70.9, 79.9, 83.3, 88.6]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 5, 4, 16, 28, 26, 28, 31, 30, 36, 44]
Epoch 356 Acc: 98.02 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 355 train Loss: 3496.6 test Loss: 348.4
Epoch 357 Iter 0 subLoss 3520.3 multi -13.93 import weight 0.00
Epoch 357 Iter 1 subLoss 7756.6 multi 1.00 import weight 0.00
Epoch 357 Iter 2 subLoss 4802.3 multi 24.88 import weight 0.00
Epoch 357 Iter 3 subLoss 21715.3 multi 1.00 import weight 0.00
Epoch 357 Iter 4 subLoss 12686.8 multi 6.97 import weight 0.00
Epoch 357 Iter 5 subLoss 4273.9 multi 6.97 import weight 0.00
Epoch 357 Iter 6 subLoss 3297.2 multi 6.97 import weight 0.00
Epoch 357 Iter 7 subLoss 2881.9 multi -10.94 import weight 0.00
Epoch 357 Iter 8 subLoss 2899.5 multi 12.94 import weight 0.00
Epoch 357 Iter 9 subLoss 3221.8 multi 9.96 import weight 0.00
Epoch 357 Iter 10 subLoss 2953.7 multi 3.99 import weight 0.00
Epoch 357 Iter 11 subLoss 2833.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0081 / 0.09962 / 16.12
Entropy seen (from low to high)
[29, 105, 241, 189, 227, 510, 513, 432, 360, 260, 246, 297, 310, 242, 198, 134, 118, 84, 67, 82, 58, 44, 42, 47, 47, 48, 24, 26, 20, 23, 32, 21, 14, 8, 8, 7, 6, 3, 8, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 21, 42, 62, 71, 104, 119, 126, 125, 187, 199, 194, 194, 226, 210, 229, 219, 206, 207, 194, 188, 182, 161, 161, 144, 157, 144, 128, 135, 116, 93, 101, 71, 40, 33, 19, 18, 12, 9, 3, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.2, 29.5, 33.0, 37.6, 39.7, 43.9, 47.2, 50.8, 54.1, 57.3, 61.4, 64.9, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 19.9, 35.7, 57.1, 59.9, 75.8, 71.8, 81.2, 82.3, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 5, 14, 28, 25, 29, 32, 32, 34, 42]
Epoch 357 Acc: 97.98 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 283 train Loss: 2755.1 test Loss: 337.0
Epoch 358 Iter 0 subLoss 2653.5 multi 3.99 import weight 0.00
Epoch 358 Iter 1 subLoss 2039.8 multi 1.00 import weight 0.00
Epoch 358 Iter 2 subLoss 3148.5 multi 12.94 import weight 0.00
Epoch 358 Iter 3 subLoss 3136.6 multi -7.96 import weight 0.00
Epoch 358 Iter 4 subLoss 2945.9 multi -19.90 import weight 0.00
Epoch 358 Iter 5 subLoss 2853.5 multi -13.93 import weight 0.00
Epoch 358 Iter 6 subLoss 5003.0 multi 6.97 import weight 0.00
Epoch 358 Iter 7 subLoss 3277.5 multi 1.00 import weight 0.00
Epoch 358 Iter 8 subLoss 3382.0 multi 3.99 import weight 0.00
Epoch 358 Iter 9 subLoss 2773.1 multi -28.85 import weight 0.00
Epoch 358 Iter 10 subLoss 5381.2 multi 3.98 import weight 0.00
Epoch 358 Iter 11 subLoss 3299.0 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0081 / 0.09977 / 15.90
Entropy seen (from low to high)
[29, 108, 240, 189, 239, 524, 509, 423, 362, 252, 258, 312, 299, 234, 196, 128, 113, 85, 69, 85, 52, 42, 43, 45, 50, 47, 24, 25, 21, 22, 30, 21, 14, 8, 8, 8, 5, 3, 8, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 19, 44, 62, 67, 104, 123, 123, 122, 193, 195, 187, 195, 229, 213, 230, 211, 205, 204, 198, 191, 180, 161, 163, 143, 156, 138, 134, 137, 117, 95, 101, 74, 41, 34, 19, 18, 11, 10, 3, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 29.9, 33.0, 37.5, 40.0, 44.1, 47.5, 50.7, 54.1, 57.4, 61.4, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 19.9, 37.4, 51.8, 54.9, 77.1, 72.4, 84.3, 79.4, 88.0]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 5, 16, 27, 20, 35, 29, 32, 34, 42]
Epoch 358 Acc: 97.51 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 329 train Loss: 3150.7 test Loss: 412.7
Epoch 359 Iter 0 subLoss 3323.9 multi -10.94 import weight 0.00
Epoch 359 Iter 1 subLoss 3207.2 multi 9.96 import weight 0.00
Epoch 359 Iter 2 subLoss 2765.3 multi 33.84 import weight 0.00
Epoch 359 Iter 3 subLoss 3139.1 multi -4.97 import weight 0.00
Epoch 359 Iter 4 subLoss 3231.3 multi -1.99 import weight 0.00
Epoch 359 Iter 5 subLoss 4148.3 multi -4.97 import weight 0.00
Epoch 359 Iter 6 subLoss 4043.6 multi 12.94 import weight 0.00
Epoch 359 Iter 7 subLoss 2835.6 multi 3.99 import weight 0.00
Epoch 359 Iter 8 subLoss 2612.8 multi 1.00 import weight 0.00
Epoch 359 Iter 9 subLoss 2962.4 multi -13.93 import weight 0.00
Epoch 359 Iter 10 subLoss 3083.8 multi -1.99 import weight 0.00
Epoch 359 Iter 11 subLoss 2744.3 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0080 / 0.10001 / 17.35
Entropy seen (from low to high)
[29, 111, 244, 188, 244, 536, 504, 430, 357, 245, 267, 323, 290, 233, 191, 128, 106, 82, 72, 75, 57, 40, 46, 43, 50, 46, 24, 24, 19, 23, 29, 20, 15, 7, 9, 7, 6, 3, 7, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 18, 42, 61, 66, 104, 120, 127, 120, 190, 188, 193, 192, 231, 206, 230, 218, 208, 196, 201, 188, 171, 171, 166, 144, 145, 147, 136, 134, 123, 92, 106, 73, 44, 37, 18, 19, 11, 10, 4, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 30.0, 33.2, 37.6, 40.2, 44.0, 47.4, 50.5, 54.2, 57.5, 61.4, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 19.9, 26.6, 53.8, 59.0, 76.4, 73.3, 82.7, 83.3, 87.8]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 5, 15, 26, 22, 34, 30, 29, 36, 41]
Epoch 359 Acc: 98.23 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 274 train Loss: 2899.4 test Loss: 315.5
Epoch 360 Iter 0 subLoss 2527.7 multi 9.96 import weight 0.00
Epoch 360 Iter 1 subLoss 2910.0 multi -1.98 import weight 0.00
Epoch 360 Iter 2 subLoss 2450.8 multi -7.96 import weight 0.00
Epoch 360 Iter 3 subLoss 2808.5 multi 3.98 import weight 0.00
Epoch 360 Iter 4 subLoss 3040.5 multi 1.00 import weight 0.00
Epoch 360 Iter 5 subLoss 2721.9 multi 3.99 import weight 0.00
Epoch 360 Iter 6 subLoss 3094.1 multi 15.93 import weight 0.00
Epoch 360 Iter 7 subLoss 2666.8 multi -7.96 import weight 0.00
Epoch 360 Iter 8 subLoss 2709.7 multi 1.00 import weight 0.00
Epoch 360 Iter 9 subLoss 2863.8 multi -1.99 import weight 0.00
Epoch 360 Iter 10 subLoss 2896.4 multi 15.93 import weight 0.00
Epoch 360 Iter 11 subLoss 2127.0 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0080 / 0.10026 / 16.83
Entropy seen (from low to high)
[29, 113, 245, 189, 253, 546, 506, 432, 344, 251, 265, 325, 291, 232, 186, 122, 108, 78, 73, 71, 55, 42, 47, 43, 47, 44, 26, 24, 19, 24, 27, 21, 13, 9, 7, 8, 4, 5, 6, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 16, 45, 57, 67, 100, 120, 127, 118, 188, 187, 195, 189, 226, 210, 224, 227, 201, 196, 199, 191, 169, 174, 164, 140, 150, 153, 131, 135, 131, 92, 103, 78, 45, 39, 19, 18, 11, 10, 5, 3, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.2, 33.3, 37.7, 40.0, 43.7, 47.2, 50.6, 54.2, 57.5, 61.3, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 24.9, 21.4, 51.8, 60.8, 74.9, 77.4, 77.7, 84.2, 85.3]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 4, 4, 14, 27, 23, 32, 31, 27, 38, 41]
Epoch 360 Acc: 98.05 BMA: 98.07 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 212 train Loss: 2852.4 test Loss: 366.9
Epoch 361 Iter 0 subLoss 2508.2 multi 1.00 import weight 0.00
Epoch 361 Iter 1 subLoss 2511.3 multi -4.97 import weight 0.00
Epoch 361 Iter 2 subLoss 2706.7 multi 3.99 import weight 0.00
Epoch 361 Iter 3 subLoss 2689.3 multi 12.94 import weight 0.00
Epoch 361 Iter 4 subLoss 2873.4 multi 9.96 import weight 0.00
Epoch 361 Iter 5 subLoss 3113.5 multi -19.90 import weight 0.00
Epoch 361 Iter 6 subLoss 2696.7 multi -4.97 import weight 0.00
Epoch 361 Iter 7 subLoss 3586.6 multi -7.96 import weight 0.00
Epoch 361 Iter 8 subLoss 5687.6 multi -13.93 import weight 0.00
Epoch 361 Iter 9 subLoss 111803.0 multi 1.00 import weight 0.00
Epoch 361 Iter 10 subLoss 6856.2 multi -1.98 import weight 0.00
Epoch 361 Iter 11 subLoss 9826.0 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0080 / 0.10004 / 16.70
Entropy seen (from low to high)
[15, 68, 186, 230, 266, 558, 514, 444, 339, 260, 285, 329, 296, 230, 184, 128, 105, 70, 74, 80, 53, 37, 54, 41, 45, 46, 27, 23, 22, 24, 22, 23, 12, 11, 6, 8, 5, 4, 6, 4, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 18, 43, 57, 63, 103, 114, 134, 118, 192, 186, 195, 192, 230, 206, 228, 217, 206, 201, 203, 191, 173, 168, 162, 145, 148, 160, 125, 140, 114, 104, 102, 79, 49, 33, 17, 13, 13, 7, 1, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.1, 33.8, 37.4, 40.3, 44.2, 47.4, 50.5, 54.3, 57.6, 61.7, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 33.3, 99.9, 14.2, 28.5, 54.1, 58.3, 74.9, 76.4, 81.4, 82.3, 86.3]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 7, 14, 24, 24, 32, 34, 27, 34, 44]
Epoch 361 Acc: 83.69 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 982 train Loss: 16107.5 test Loss: 2864.1
Epoch 362 Iter 0 subLoss 17292.0 multi 6.97 import weight 0.00
Epoch 362 Iter 1 subLoss 3502.8 multi 18.91 import weight 0.00
Epoch 362 Iter 2 subLoss 3208.5 multi 12.94 import weight 0.00
Epoch 362 Iter 3 subLoss 2761.4 multi 36.82 import weight 0.00
Epoch 362 Iter 4 subLoss 2360.9 multi 3.99 import weight 0.00
Epoch 362 Iter 5 subLoss 2535.5 multi 3.98 import weight 0.00
Epoch 362 Iter 6 subLoss 2753.8 multi -16.91 import weight 0.00
Epoch 362 Iter 7 subLoss 3405.2 multi 1.00 import weight 0.00
Epoch 362 Iter 8 subLoss 3251.6 multi 3.98 import weight 0.00
Epoch 362 Iter 9 subLoss 2878.0 multi 12.94 import weight 0.00
Epoch 362 Iter 10 subLoss 2459.6 multi -4.97 import weight 0.00
Epoch 362 Iter 11 subLoss 3028.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0079 / 0.10031 / 16.18
Entropy seen (from low to high)
[15, 70, 185, 233, 283, 562, 508, 446, 333, 259, 286, 342, 285, 236, 175, 122, 105, 69, 75, 80, 49, 40, 52, 39, 45, 44, 28, 24, 20, 24, 22, 22, 12, 12, 5, 8, 5, 4, 6, 4, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 17, 44, 50, 68, 104, 110, 133, 117, 188, 187, 193, 190, 229, 207, 228, 224, 201, 195, 207, 185, 182, 162, 161, 149, 145, 164, 130, 133, 124, 104, 99, 79, 57, 32, 18, 13, 12, 7, 2, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.1, 33.9, 37.5, 40.1, 44.0, 47.4, 50.4, 54.3, 57.5, 61.5, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 33.3, 99.9, 16.6, 33.3, 52.1, 58.3, 78.1, 72.7, 80.7, 84.8, 85.1]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 6, 15, 23, 24, 32, 33, 26, 33, 47]
Epoch 362 Acc: 98.07 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 302 train Loss: 2787.6 test Loss: 325.6
Epoch 363 Iter 0 subLoss 2754.8 multi -13.93 import weight 0.00
Epoch 363 Iter 1 subLoss 3514.2 multi 6.97 import weight 0.00
Epoch 363 Iter 2 subLoss 3057.6 multi 3.99 import weight 0.00
Epoch 363 Iter 3 subLoss 3113.5 multi -16.91 import weight 0.00
Epoch 363 Iter 4 subLoss 2873.1 multi 15.93 import weight 0.00
Epoch 363 Iter 5 subLoss 2419.5 multi 15.93 import weight 0.00
Epoch 363 Iter 6 subLoss 2869.6 multi 1.00 import weight 0.00
Epoch 363 Iter 7 subLoss 2983.6 multi 3.99 import weight 0.00
Epoch 363 Iter 8 subLoss 2926.2 multi -13.93 import weight 0.00
Epoch 363 Iter 9 subLoss 2965.2 multi -10.94 import weight 0.00
Epoch 363 Iter 10 subLoss 2629.0 multi 9.96 import weight 0.00
Epoch 363 Iter 11 subLoss 2929.4 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0079 / 0.10056 / 16.08
Entropy seen (from low to high)
[15, 75, 185, 233, 286, 573, 507, 451, 326, 253, 303, 342, 277, 235, 167, 126, 98, 66, 77, 78, 50, 40, 49, 40, 45, 42, 28, 24, 20, 26, 25, 18, 10, 13, 5, 7, 5, 4, 6, 4, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 17, 42, 51, 68, 102, 111, 128, 121, 178, 189, 194, 191, 229, 203, 223, 224, 203, 200, 200, 192, 173, 166, 160, 152, 149, 157, 130, 136, 130, 109, 97, 78, 59, 35, 17, 14, 13, 6, 3, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 34.1, 37.5, 40.4, 44.1, 47.2, 50.4, 54.4, 57.5, 61.3, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 33.3, 99.9, 14.2, 30.7, 47.8, 59.9, 78.7, 79.3, 70.3, 84.8, 85.4]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 7, 13, 23, 25, 33, 29, 27, 33, 48]
Epoch 363 Acc: 98.02 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 292 train Loss: 3056.9 test Loss: 345.0
Epoch 364 Iter 0 subLoss 2662.8 multi -4.97 import weight 0.00
Epoch 364 Iter 1 subLoss 2926.5 multi -7.96 import weight 0.00
Epoch 364 Iter 2 subLoss 3962.3 multi 27.87 import weight 0.00
Epoch 364 Iter 3 subLoss 3989.8 multi -1.99 import weight 0.00
Epoch 364 Iter 4 subLoss 3787.8 multi 9.96 import weight 0.00
Epoch 364 Iter 5 subLoss 2618.7 multi 3.99 import weight 0.00
Epoch 364 Iter 6 subLoss 2704.4 multi 3.98 import weight 0.00
Epoch 364 Iter 7 subLoss 2547.5 multi -13.93 import weight 0.00
Epoch 364 Iter 8 subLoss 2765.0 multi 33.84 import weight 1.00
Epoch 364 Iter 9 subLoss 3296.4 multi 12.94 import weight 1.00
Epoch 364 Iter 10 subLoss 2032.0 multi 3.98 import weight 0.00
Epoch 364 Iter 11 subLoss 2602.7 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0079 / 0.10079 / 15.47
Entropy seen (from low to high)
[15, 78, 185, 238, 289, 595, 500, 447, 325, 253, 307, 339, 272, 230, 165, 123, 100, 61, 82, 76, 50, 37, 48, 46, 38, 45, 23, 25, 21, 27, 23, 18, 8, 14, 5, 7, 5, 4, 6, 4, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 17, 41, 51, 67, 95, 116, 126, 122, 181, 180, 196, 194, 221, 210, 218, 224, 208, 192, 197, 192, 177, 170, 161, 147, 150, 162, 126, 130, 139, 104, 103, 80, 60, 37, 19, 15, 13, 6, 3, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.1, 33.9, 36.9, 40.4, 44.1, 47.2, 50.4, 54.4, 57.4, 61.4, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 0.0, 99.9, 14.2, 41.6, 47.8, 55.9, 81.8, 79.3, 70.3, 81.2, 84.7]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 4, 7, 12, 23, 25, 33, 29, 27, 32, 46]
Epoch 364 Acc: 97.98 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 260 train Loss: 2653.0 test Loss: 330.9
Epoch 365 Iter 0 subLoss 2340.9 multi 6.97 import weight 0.00
Epoch 365 Iter 1 subLoss 2083.1 multi 1.00 import weight 0.00
Epoch 365 Iter 2 subLoss 2580.3 multi 9.96 import weight 0.00
Epoch 365 Iter 3 subLoss 2537.7 multi 6.97 import weight 0.00
Epoch 365 Iter 4 subLoss 2715.7 multi -7.96 import weight 0.00
Epoch 365 Iter 5 subLoss 2195.6 multi 1.00 import weight 0.00
Epoch 365 Iter 6 subLoss 3239.2 multi 1.00 import weight 0.00
Epoch 365 Iter 7 subLoss 2817.1 multi -13.93 import weight 0.00
Epoch 365 Iter 8 subLoss 2929.4 multi -4.97 import weight 0.00
Epoch 365 Iter 9 subLoss 2662.9 multi -1.99 import weight 0.00
Epoch 365 Iter 10 subLoss 3097.3 multi 18.91 import weight 0.00
Epoch 365 Iter 11 subLoss 2680.4 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0078 / 0.10103 / 16.16
Entropy seen (from low to high)
[15, 78, 189, 238, 295, 602, 501, 452, 320, 255, 306, 343, 272, 221, 171, 117, 93, 64, 79, 71, 50, 37, 49, 44, 43, 41, 21, 26, 20, 30, 21, 19, 6, 14, 5, 8, 4, 5, 5, 4, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 17, 39, 51, 68, 93, 116, 121, 125, 172, 187, 195, 193, 221, 205, 217, 224, 206, 198, 197, 191, 169, 178, 159, 145, 155, 157, 137, 126, 137, 104, 107, 85, 61, 37, 19, 16, 13, 5, 4, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 32.8, 36.6, 40.4, 44.0, 47.1, 50.4, 54.6, 57.6, 61.4, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 79.9, 28.5, 45.4, 39.1, 61.5, 82.3, 78.5, 63.9, 84.8, 84.4]
[0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 5, 7, 11, 23, 26, 34, 28, 25, 33, 45]
Epoch 365 Acc: 98.17 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 268 train Loss: 2670.5 test Loss: 295.5
Epoch 366 Iter 0 subLoss 2425.1 multi -10.94 import weight 0.00
Epoch 366 Iter 1 subLoss 2945.4 multi -16.91 import weight 0.00
Epoch 366 Iter 2 subLoss 3005.8 multi -4.97 import weight 0.00
Epoch 366 Iter 3 subLoss 3316.1 multi -1.99 import weight 0.00
Epoch 366 Iter 4 subLoss 3328.3 multi -10.94 import weight 0.00
Epoch 366 Iter 5 subLoss 5373.4 multi -1.98 import weight 0.00
Epoch 366 Iter 6 subLoss 6435.0 multi -7.96 import weight 0.00
Epoch 366 Iter 7 subLoss 29318.9 multi -1.99 import weight 0.00
Epoch 366 Iter 8 subLoss 141205.8 multi 1.00 import weight 0.00
Epoch 366 Iter 9 subLoss 26252.9 multi 1.00 import weight 0.00
Epoch 366 Iter 10 subLoss 16741.5 multi 1.00 import weight 0.00
Epoch 366 Iter 11 subLoss 11625.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0078 / 0.10088 / 16.29
Entropy seen (from low to high)
[15, 78, 188, 232, 310, 590, 493, 432, 330, 252, 331, 341, 287, 201, 173, 124, 91, 66, 77, 73, 46, 46, 43, 42, 45, 40, 22, 27, 16, 31, 20, 19, 10, 12, 3, 9, 5, 5, 5, 4, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 17, 40, 52, 66, 93, 118, 122, 126, 167, 183, 203, 186, 230, 201, 222, 224, 203, 202, 198, 199, 167, 166, 162, 144, 148, 166, 140, 134, 116, 107, 113, 85, 62, 36, 18, 15, 10, 6, 3, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.1, 33.0, 36.8, 40.6, 44.4, 47.1, 50.2, 54.4, 57.6, 61.4, 64.5, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 12.4, 36.3, 45.4, 55.9, 78.7, 78.1, 68.1, 85.2, 84.0]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 8, 11, 22, 25, 33, 32, 22, 34, 44]
Epoch 366 Acc: 93.11 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1162 train Loss: 8985.7 test Loss: 1083.0
Epoch 367 Iter 0 subLoss 9129.8 multi 1.00 import weight 0.00
Epoch 367 Iter 1 subLoss 8097.6 multi 1.00 import weight 0.00
Epoch 367 Iter 2 subLoss 5716.7 multi 3.99 import weight 0.00
Epoch 367 Iter 3 subLoss 3598.1 multi -22.88 import weight 0.00
Epoch 367 Iter 4 subLoss 13793.6 multi 3.99 import weight 0.00
Epoch 367 Iter 5 subLoss 4620.1 multi -4.97 import weight 0.00
Epoch 367 Iter 6 subLoss 7632.8 multi 3.99 import weight 0.00
Epoch 367 Iter 7 subLoss 5324.6 multi 6.97 import weight 0.00
Epoch 367 Iter 8 subLoss 3602.4 multi 1.00 import weight 0.00
Epoch 367 Iter 9 subLoss 3397.3 multi -1.98 import weight 0.00
Epoch 367 Iter 10 subLoss 3556.1 multi -10.94 import weight 0.00
Epoch 367 Iter 11 subLoss 5094.3 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0078 / 0.10092 / 16.45
Entropy seen (from low to high)
[15, 81, 189, 237, 314, 596, 489, 439, 320, 253, 342, 343, 276, 202, 165, 119, 94, 60, 88, 64, 45, 47, 43, 44, 44, 37, 24, 26, 15, 32, 19, 19, 10, 12, 3, 9, 4, 6, 5, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 17, 40, 50, 67, 95, 114, 127, 120, 167, 182, 205, 189, 231, 204, 216, 225, 200, 207, 201, 193, 169, 161, 155, 152, 149, 167, 137, 129, 121, 111, 110, 87, 63, 37, 18, 15, 9, 7, 3, 4, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 32.2, 36.7, 40.7, 44.1, 47.0, 50.4, 54.5, 57.6, 61.4, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 83.3, 0.0, 39.9, 45.8, 62.9, 77.4, 74.1, 72.7, 85.7, 83.7]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 6, 6, 10, 24, 27, 31, 31, 22, 35, 43]
Epoch 367 Acc: 97.76 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 509 train Loss: 3647.6 test Loss: 363.1
Epoch 368 Iter 0 subLoss 3368.5 multi 3.99 import weight 0.00
Epoch 368 Iter 1 subLoss 3084.7 multi 1.00 import weight 0.00
Epoch 368 Iter 2 subLoss 3281.5 multi -1.99 import weight 0.00
Epoch 368 Iter 3 subLoss 3902.8 multi 1.00 import weight 0.00
Epoch 368 Iter 4 subLoss 3274.6 multi 3.99 import weight 0.00
Epoch 368 Iter 5 subLoss 3388.8 multi 6.97 import weight 0.00
Epoch 368 Iter 6 subLoss 3002.5 multi -1.99 import weight 0.00
Epoch 368 Iter 7 subLoss 2751.1 multi -10.94 import weight 0.00
Epoch 368 Iter 8 subLoss 3320.6 multi -7.96 import weight 0.00
Epoch 368 Iter 9 subLoss 4169.6 multi -10.94 import weight 0.00
Epoch 368 Iter 10 subLoss 5967.3 multi -4.97 import weight 0.00
Epoch 368 Iter 11 subLoss 9082.1 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0078 / 0.10038 / 17.29
Entropy seen (from low to high)
[14, 71, 171, 210, 350, 602, 495, 438, 312, 258, 345, 343, 272, 206, 165, 123, 93, 62, 77, 73, 48, 47, 40, 49, 44, 35, 24, 27, 18, 23, 27, 20, 9, 12, 3, 8, 4, 8, 4, 2, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 19, 44, 51, 67, 92, 115, 131, 127, 171, 185, 196, 199, 232, 207, 218, 220, 200, 212, 196, 195, 178, 154, 152, 154, 166, 145, 148, 117, 122, 112, 104, 81, 62, 35, 14, 16, 5, 6, 5, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 32.0, 36.2, 40.3, 43.8, 46.8, 50.3, 54.4, 57.6, 61.6, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 79.9, 0.0, 24.9, 51.9, 60.7, 77.4, 71.8, 75.9, 89.9, 83.3]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 5, 7, 8, 25, 28, 31, 32, 25, 30, 48]
Epoch 368 Acc: 86.73 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 908 train Loss: 14819.4 test Loss: 2105.0
Epoch 369 Iter 0 subLoss 14457.8 multi 1.00 import weight 0.00
Epoch 369 Iter 1 subLoss 9710.5 multi -1.99 import weight 0.00
Epoch 369 Iter 2 subLoss 15188.0 multi 3.99 import weight 0.00
Epoch 369 Iter 3 subLoss 5948.5 multi -4.97 import weight 0.00
Epoch 369 Iter 4 subLoss 7193.2 multi -13.93 import weight 0.00
Epoch 369 Iter 5 subLoss 32546.2 multi 1.00 import weight 0.00
Epoch 369 Iter 6 subLoss 26631.0 multi -1.99 import weight 0.00
Epoch 369 Iter 7 subLoss 37218.6 multi -1.99 import weight 0.00
Epoch 369 Iter 8 subLoss 48327.2 multi 1.00 import weight 0.00
Epoch 369 Iter 9 subLoss 41807.1 multi 1.00 import weight 0.00
Epoch 369 Iter 10 subLoss 36356.7 multi 1.00 import weight 0.00
Epoch 369 Iter 11 subLoss 30315.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0079 / 0.09994 / 16.96
Entropy seen (from low to high)
[13, 64, 158, 207, 356, 517, 468, 423, 353, 286, 356, 355, 283, 208, 178, 129, 95, 75, 71, 76, 48, 49, 45, 48, 41, 37, 27, 25, 19, 24, 26, 21, 10, 12, 3, 8, 4, 8, 4, 2, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 20, 44, 59, 61, 97, 116, 135, 133, 169, 188, 195, 200, 226, 219, 224, 212, 205, 204, 198, 200, 175, 162, 133, 165, 172, 138, 150, 109, 118, 115, 91, 88, 58, 34, 15, 13, 1, 9, 5, 0, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 32.9, 36.3, 40.0, 43.7, 46.9, 50.3, 54.3, 57.6, 61.7, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 74.9, 0.0, 22.2, 53.5, 59.9, 74.9, 71.8, 81.4, 89.9, 82.6]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 6, 9, 28, 25, 32, 32, 27, 30, 46]
Epoch 369 Acc: 73.13 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3031 train Loss: 27857.3 test Loss: 3464.1
Epoch 370 Iter 0 subLoss 26144.7 multi 1.00 import weight 0.00
Epoch 370 Iter 1 subLoss 23498.3 multi 1.00 import weight 0.00
Epoch 370 Iter 2 subLoss 19151.9 multi 1.00 import weight 0.00
Epoch 370 Iter 3 subLoss 16878.5 multi -1.99 import weight 0.00
Epoch 370 Iter 4 subLoss 22630.1 multi 1.00 import weight 0.00
Epoch 370 Iter 5 subLoss 18646.2 multi 3.99 import weight 0.00
Epoch 370 Iter 6 subLoss 10165.1 multi -4.97 import weight 0.00
Epoch 370 Iter 7 subLoss 14967.0 multi 3.98 import weight 0.00
Epoch 370 Iter 8 subLoss 9010.9 multi -1.98 import weight 0.00
Epoch 370 Iter 9 subLoss 10814.5 multi -1.99 import weight 0.00
Epoch 370 Iter 10 subLoss 12217.5 multi 3.99 import weight 0.00
Epoch 370 Iter 11 subLoss 9328.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0079 / 0.09963 / 17.09
Entropy seen (from low to high)
[13, 64, 161, 211, 373, 507, 469, 433, 337, 283, 357, 355, 281, 206, 171, 132, 97, 71, 68, 79, 50, 52, 44, 40, 46, 40, 26, 26, 19, 21, 29, 21, 7, 12, 4, 9, 4, 8, 4, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 22, 42, 62, 64, 101, 114, 131, 138, 180, 177, 197, 194, 234, 215, 226, 212, 217, 194, 200, 206, 158, 165, 144, 178, 154, 146, 136, 118, 121, 104, 92, 86, 54, 35, 13, 12, 1, 8, 5, 0, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.2, 32.8, 36.2, 39.9, 44.0, 47.0, 50.5, 54.3, 57.7, 61.6, 64.5, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 74.9, 0.0, 19.9, 55.1, 62.4, 74.9, 70.9, 77.7, 88.8, 84.3]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 6, 10, 29, 24, 32, 31, 27, 27, 51]
Epoch 370 Acc: 95.80 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 932 train Loss: 9964.0 test Loss: 713.3
Epoch 371 Iter 0 subLoss 9493.9 multi -4.97 import weight 0.00
Epoch 371 Iter 1 subLoss 12570.1 multi 15.93 import weight 0.00
Epoch 371 Iter 2 subLoss 6003.3 multi 3.99 import weight 0.00
Epoch 371 Iter 3 subLoss 4751.4 multi -10.94 import weight 0.00
Epoch 371 Iter 4 subLoss 6505.3 multi 6.97 import weight 0.00
Epoch 371 Iter 5 subLoss 5260.6 multi 9.96 import weight 0.00
Epoch 371 Iter 6 subLoss 4308.9 multi 3.98 import weight 0.00
Epoch 371 Iter 7 subLoss 3731.9 multi 24.88 import weight 0.00
Epoch 371 Iter 8 subLoss 3390.2 multi -1.99 import weight 0.00
Epoch 371 Iter 9 subLoss 3418.4 multi -1.98 import weight 0.00
Epoch 371 Iter 10 subLoss 2845.0 multi 1.00 import weight 0.00
Epoch 371 Iter 11 subLoss 3476.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0078 / 0.09983 / 17.05
Entropy seen (from low to high)
[13, 68, 162, 216, 380, 510, 477, 420, 337, 291, 359, 355, 270, 207, 162, 135, 95, 68, 69, 76, 48, 56, 43, 41, 45, 41, 22, 26, 19, 23, 27, 23, 6, 11, 5, 8, 4, 8, 4, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 21, 41, 64, 61, 98, 112, 136, 138, 170, 181, 205, 190, 230, 217, 221, 209, 221, 195, 196, 208, 159, 164, 143, 179, 152, 141, 138, 125, 117, 110, 91, 90, 54, 37, 15, 12, 1, 8, 5, 0, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 32.9, 36.2, 39.8, 43.9, 47.0, 50.5, 54.1, 57.7, 61.5, 64.1, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 74.9, 0.0, 11.1, 56.6, 60.8, 74.1, 72.7, 78.2, 85.7, 83.3]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 4, 6, 9, 30, 23, 31, 33, 23, 28, 54]
Epoch 371 Acc: 97.86 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 347 train Loss: 3207.5 test Loss: 351.5
Epoch 372 Iter 0 subLoss 2861.7 multi 3.99 import weight 0.00
Epoch 372 Iter 1 subLoss 3013.0 multi -10.94 import weight 0.00
Epoch 372 Iter 2 subLoss 3148.5 multi 9.96 import weight 0.00
Epoch 372 Iter 3 subLoss 2954.1 multi 1.00 import weight 0.00
Epoch 372 Iter 4 subLoss 3096.6 multi 18.91 import weight 0.00
Epoch 372 Iter 5 subLoss 3285.0 multi -1.99 import weight 0.00
Epoch 372 Iter 6 subLoss 2768.7 multi 33.84 import weight 1.00
Epoch 372 Iter 7 subLoss 3192.1 multi 15.93 import weight 0.00
Epoch 372 Iter 8 subLoss 2760.0 multi 36.82 import weight 1.00
Epoch 372 Iter 9 subLoss 2484.8 multi -4.97 import weight 0.00
Epoch 372 Iter 10 subLoss 3186.6 multi -19.90 import weight 0.00
Epoch 372 Iter 11 subLoss 2899.3 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0078 / 0.10005 / 17.03
Entropy seen (from low to high)
[13, 70, 164, 218, 389, 514, 480, 417, 346, 284, 354, 361, 262, 208, 162, 128, 93, 67, 71, 77, 44, 53, 42, 44, 50, 35, 20, 27, 20, 22, 29, 20, 7, 11, 4, 8, 4, 8, 4, 2, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 21, 40, 62, 63, 95, 112, 138, 132, 166, 185, 203, 192, 229, 214, 222, 214, 214, 194, 199, 207, 162, 157, 149, 174, 152, 144, 142, 127, 115, 111, 89, 92, 59, 34, 19, 13, 1, 8, 5, 0, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.4, 31.9, 36.0, 39.8, 43.8, 46.9, 50.5, 54.2, 57.7, 61.4, 64.3, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 0.0, 12.4, 53.3, 58.3, 77.4, 74.1, 75.9, 85.7, 84.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 6, 8, 30, 24, 31, 31, 25, 28, 52]
Epoch 372 Acc: 98.02 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 289 train Loss: 2814.4 test Loss: 339.0
Epoch 373 Iter 0 subLoss 2588.2 multi 12.94 import weight 0.00
Epoch 373 Iter 1 subLoss 2632.3 multi -13.93 import weight 0.00
Epoch 373 Iter 2 subLoss 3089.2 multi 3.98 import weight 0.00
Epoch 373 Iter 3 subLoss 2932.0 multi 18.91 import weight 0.00
Epoch 373 Iter 4 subLoss 2336.3 multi 1.00 import weight 0.00
Epoch 373 Iter 5 subLoss 2542.9 multi -13.93 import weight 0.00
Epoch 373 Iter 6 subLoss 3192.1 multi 15.93 import weight 0.00
Epoch 373 Iter 7 subLoss 2393.6 multi -7.96 import weight 0.00
Epoch 373 Iter 8 subLoss 2667.2 multi 1.00 import weight 0.00
Epoch 373 Iter 9 subLoss 2403.9 multi 1.00 import weight 0.00
Epoch 373 Iter 10 subLoss 2684.0 multi 18.91 import weight 0.00
Epoch 373 Iter 11 subLoss 2681.4 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0078 / 0.10029 / 16.28
Entropy seen (from low to high)
[14, 70, 169, 220, 399, 518, 479, 408, 348, 296, 351, 359, 258, 210, 158, 119, 93, 70, 78, 62, 50, 49, 42, 44, 50, 33, 22, 26, 20, 23, 28, 18, 8, 10, 4, 9, 4, 7, 4, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 19, 41, 61, 60, 97, 111, 134, 129, 172, 182, 201, 192, 230, 210, 222, 218, 204, 197, 196, 207, 170, 158, 149, 166, 154, 146, 142, 122, 119, 122, 86, 89, 64, 39, 18, 12, 3, 8, 5, 0, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.2, 32.1, 36.1, 40.0, 44.2, 47.1, 50.7, 54.3, 57.6, 61.3, 64.3, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 14.2, 24.9, 53.3, 56.5, 83.3, 76.6, 69.2, 85.7, 83.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 5, 7, 8, 30, 23, 30, 30, 26, 28, 50]
Epoch 373 Acc: 98.37 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 268 train Loss: 2661.5 test Loss: 285.5
Epoch 374 Iter 0 subLoss 2517.8 multi -1.98 import weight 0.00
Epoch 374 Iter 1 subLoss 2142.9 multi 1.00 import weight 0.00
Epoch 374 Iter 2 subLoss 2395.8 multi -4.97 import weight 0.00
Epoch 374 Iter 3 subLoss 2642.4 multi -4.97 import weight 0.00
Epoch 374 Iter 4 subLoss 3046.5 multi 3.98 import weight 0.00
Epoch 374 Iter 5 subLoss 2745.2 multi 18.91 import weight 0.00
Epoch 374 Iter 6 subLoss 2307.0 multi 9.96 import weight 0.00
Epoch 374 Iter 7 subLoss 2243.2 multi 3.99 import weight 0.00
Epoch 374 Iter 8 subLoss 2675.7 multi -4.97 import weight 0.00
Epoch 374 Iter 9 subLoss 2519.3 multi 1.00 import weight 0.00
Epoch 374 Iter 10 subLoss 2408.6 multi 1.00 import weight 0.00
Epoch 374 Iter 11 subLoss 2608.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0077 / 0.10053 / 15.64
Entropy seen (from low to high)
[14, 71, 173, 218, 419, 518, 471, 414, 345, 300, 366, 342, 254, 211, 150, 121, 90, 65, 82, 59, 48, 53, 41, 40, 50, 34, 21, 26, 20, 23, 26, 19, 11, 7, 4, 9, 4, 7, 4, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 20, 38, 61, 56, 97, 115, 134, 126, 165, 191, 190, 191, 235, 213, 214, 223, 197, 202, 197, 208, 168, 157, 142, 167, 159, 154, 130, 126, 125, 123, 83, 93, 68, 36, 22, 13, 3, 7, 6, 0, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 32.8, 36.2, 40.0, 44.2, 47.1, 50.8, 54.4, 57.6, 61.3, 64.4, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 14.2, 12.4, 49.9, 56.5, 86.2, 73.3, 65.3, 89.6, 81.2]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 5, 7, 8, 30, 23, 29, 30, 26, 29, 48]
Epoch 374 Acc: 98.23 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 260 train Loss: 2558.3 test Loss: 294.5
Epoch 375 Iter 0 subLoss 2572.5 multi 1.00 import weight 0.00
Epoch 375 Iter 1 subLoss 2068.6 multi 1.00 import weight 0.00
Epoch 375 Iter 2 subLoss 2928.4 multi -1.98 import weight 0.00
Epoch 375 Iter 3 subLoss 2812.1 multi -10.94 import weight 0.00
Epoch 375 Iter 4 subLoss 2664.6 multi 3.98 import weight 0.00
Epoch 375 Iter 5 subLoss 2415.9 multi 12.94 import weight 0.00
Epoch 375 Iter 6 subLoss 2213.5 multi 1.00 import weight 0.00
Epoch 375 Iter 7 subLoss 2216.3 multi 3.99 import weight 0.00
Epoch 375 Iter 8 subLoss 2488.9 multi -1.99 import weight 0.00
Epoch 375 Iter 9 subLoss 2737.9 multi -16.91 import weight 0.00
Epoch 375 Iter 10 subLoss 2746.7 multi 18.91 import weight 0.00
Epoch 375 Iter 11 subLoss 2585.9 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0077 / 0.10077 / 15.36
Entropy seen (from low to high)
[14, 72, 175, 220, 434, 517, 465, 418, 341, 312, 378, 331, 245, 207, 148, 120, 84, 66, 82, 59, 45, 55, 39, 40, 51, 34, 18, 27, 20, 25, 24, 18, 13, 5, 6, 7, 5, 6, 4, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 18, 38, 59, 58, 97, 108, 139, 121, 165, 192, 188, 195, 230, 215, 215, 216, 198, 205, 191, 200, 179, 155, 148, 164, 158, 157, 130, 127, 125, 120, 91, 92, 70, 38, 24, 11, 5, 7, 6, 0, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.1, 33.0, 36.3, 39.9, 44.3, 47.1, 50.9, 54.4, 57.6, 61.3, 64.4, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 14.2, 24.9, 46.6, 60.8, 84.6, 75.7, 59.9, 93.1, 80.4]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 5, 7, 8, 30, 23, 26, 33, 25, 29, 46]
Epoch 375 Acc: 98.40 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 258 train Loss: 2470.6 test Loss: 275.4
Epoch 376 Iter 0 subLoss 2270.4 multi 1.00 import weight 0.00
Epoch 376 Iter 1 subLoss 2804.1 multi 6.97 import weight 0.00
Epoch 376 Iter 2 subLoss 2581.6 multi 15.93 import weight 0.00
Epoch 376 Iter 3 subLoss 2424.4 multi -10.94 import weight 0.00
Epoch 376 Iter 4 subLoss 2171.0 multi 1.00 import weight 0.00
Epoch 376 Iter 5 subLoss 2402.6 multi 3.99 import weight 0.00
Epoch 376 Iter 6 subLoss 2175.5 multi 3.99 import weight 0.00
Epoch 376 Iter 7 subLoss 2608.2 multi -1.99 import weight 0.00
Epoch 376 Iter 8 subLoss 2636.2 multi -10.94 import weight 0.00
Epoch 376 Iter 9 subLoss 2178.4 multi 6.97 import weight 0.00
Epoch 376 Iter 10 subLoss 2878.3 multi 12.94 import weight 0.00
Epoch 376 Iter 11 subLoss 2431.1 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0076 / 0.10102 / 16.58
Entropy seen (from low to high)
[14, 73, 178, 222, 444, 525, 455, 415, 345, 317, 379, 334, 239, 199, 151, 118, 79, 64, 87, 56, 48, 52, 33, 46, 49, 31, 17, 27, 21, 25, 23, 18, 13, 6, 5, 8, 3, 7, 4, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 18, 38, 55, 57, 99, 108, 135, 124, 163, 186, 192, 191, 227, 215, 213, 220, 201, 202, 189, 206, 178, 146, 149, 161, 163, 160, 139, 120, 124, 123, 94, 95, 72, 39, 24, 10, 6, 7, 6, 0, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 33.1, 36.5, 39.8, 44.4, 47.3, 51.1, 54.3, 57.6, 61.2, 64.4, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 14.2, 24.9, 40.6, 64.9, 84.6, 78.7, 58.3, 89.9, 79.0]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 5, 7, 8, 32, 20, 26, 33, 24, 30, 43]
Epoch 376 Acc: 98.44 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 243 train Loss: 2527.2 test Loss: 267.3
Epoch 377 Iter 0 subLoss 2715.3 multi -4.97 import weight 0.00
Epoch 377 Iter 1 subLoss 3069.0 multi -10.94 import weight 0.00
Epoch 377 Iter 2 subLoss 2639.5 multi -7.96 import weight 0.00
Epoch 377 Iter 3 subLoss 2935.1 multi 18.91 import weight 1.00
Epoch 377 Iter 4 subLoss 2345.0 multi 6.97 import weight 0.00
Epoch 377 Iter 5 subLoss 2799.5 multi 18.91 import weight 0.00
Epoch 377 Iter 6 subLoss 2605.9 multi 1.00 import weight 0.00
Epoch 377 Iter 7 subLoss 2906.7 multi -4.97 import weight 0.00
Epoch 377 Iter 8 subLoss 2841.7 multi 3.98 import weight 0.00
Epoch 377 Iter 9 subLoss 2393.9 multi -1.99 import weight 0.00
Epoch 377 Iter 10 subLoss 1839.6 multi 1.00 import weight 0.00
Epoch 377 Iter 11 subLoss 2124.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0076 / 0.10126 / 15.72
Entropy seen (from low to high)
[14, 76, 181, 228, 448, 523, 471, 408, 330, 325, 387, 332, 231, 196, 152, 116, 75, 67, 85, 54, 46, 53, 30, 49, 48, 28, 21, 23, 21, 25, 27, 14, 13, 6, 6, 7, 3, 7, 4, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 17, 38, 53, 61, 95, 108, 128, 126, 161, 184, 188, 196, 227, 205, 221, 214, 209, 200, 188, 203, 181, 149, 150, 154, 164, 159, 148, 118, 127, 121, 100, 94, 71, 42, 25, 11, 6, 7, 5, 1, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 33.2, 36.5, 39.8, 44.4, 47.3, 50.9, 54.3, 57.7, 61.3, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.9, 28.5, 12.4, 41.9, 57.8, 85.7, 78.7, 62.4, 86.6, 79.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 5, 7, 8, 31, 19, 28, 33, 24, 30, 40]
Epoch 377 Acc: 98.15 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 212 train Loss: 2422.6 test Loss: 302.0
Epoch 378 Iter 0 subLoss 2435.0 multi 1.00 import weight 0.00
Epoch 378 Iter 1 subLoss 2622.8 multi 9.96 import weight 0.00
Epoch 378 Iter 2 subLoss 2923.7 multi 1.00 import weight 0.00
Epoch 378 Iter 3 subLoss 2238.3 multi 1.00 import weight 0.00
Epoch 378 Iter 4 subLoss 2316.0 multi 1.00 import weight 0.00
Epoch 378 Iter 5 subLoss 2455.4 multi -1.99 import weight 0.00
Epoch 378 Iter 6 subLoss 2058.7 multi 1.00 import weight 0.00
Epoch 378 Iter 7 subLoss 2554.6 multi -4.97 import weight 0.00
Epoch 378 Iter 8 subLoss 2632.9 multi -7.96 import weight 0.00
Epoch 378 Iter 9 subLoss 2520.6 multi 3.99 import weight 0.00
Epoch 378 Iter 10 subLoss 2573.4 multi 3.99 import weight 0.00
Epoch 378 Iter 11 subLoss 2522.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0076 / 0.10149 / 14.35
Entropy seen (from low to high)
[14, 78, 184, 228, 462, 529, 467, 404, 326, 338, 380, 327, 236, 189, 153, 110, 74, 66, 86, 52, 48, 49, 37, 44, 45, 28, 23, 22, 20, 27, 26, 12, 13, 6, 6, 7, 3, 9, 2, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 15, 40, 53, 59, 97, 105, 124, 128, 157, 185, 188, 196, 222, 211, 218, 218, 202, 205, 183, 200, 177, 158, 152, 152, 166, 158, 147, 122, 127, 116, 106, 98, 69, 46, 24, 12, 6, 6, 6, 1, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 33.5, 36.9, 39.6, 44.2, 47.2, 50.8, 54.3, 57.7, 61.3, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 19.9, 33.3, 46.6, 52.6, 82.1, 81.8, 62.4, 86.6, 82.4]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 6, 5, 9, 30, 19, 28, 33, 24, 30, 40]
Epoch 378 Acc: 98.29 BMA: 98.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 252 train Loss: 2465.4 test Loss: 292.9
Epoch 379 Iter 0 subLoss 2234.2 multi 3.99 import weight 0.00
Epoch 379 Iter 1 subLoss 2430.4 multi 3.99 import weight 0.00
Epoch 379 Iter 2 subLoss 2534.7 multi 3.99 import weight 0.00
Epoch 379 Iter 3 subLoss 2314.7 multi 3.98 import weight 0.00
Epoch 379 Iter 4 subLoss 2147.0 multi 3.99 import weight 0.00
Epoch 379 Iter 5 subLoss 2810.2 multi -10.94 import weight 0.00
Epoch 379 Iter 6 subLoss 2348.2 multi 9.96 import weight 0.00
Epoch 379 Iter 7 subLoss 2581.0 multi 15.93 import weight 0.00
Epoch 379 Iter 8 subLoss 2162.7 multi 1.00 import weight 0.00
Epoch 379 Iter 9 subLoss 2115.2 multi 6.97 import weight 0.00
Epoch 379 Iter 10 subLoss 2232.3 multi 6.97 import weight 0.00
Epoch 379 Iter 11 subLoss 2206.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0075 / 0.10172 / 13.82
Entropy seen (from low to high)
[14, 80, 184, 229, 479, 529, 458, 415, 321, 342, 386, 314, 234, 187, 154, 104, 75, 65, 85, 53, 47, 49, 37, 44, 45, 26, 23, 23, 20, 29, 23, 11, 12, 6, 6, 7, 3, 9, 2, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 15, 41, 47, 61, 99, 102, 120, 132, 152, 184, 189, 192, 225, 211, 205, 226, 203, 205, 188, 194, 181, 154, 155, 152, 170, 154, 147, 127, 119, 119, 109, 101, 67, 54, 23, 11, 8, 6, 6, 1, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.8, 33.7, 37.1, 39.9, 44.2, 47.3, 50.9, 54.3, 57.8, 61.4, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.8, 24.9, 33.3, 49.9, 49.9, 82.1, 78.7, 69.2, 85.7, 79.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 7, 4, 9, 30, 18, 28, 33, 26, 28, 40]
Epoch 379 Acc: 98.46 BMA: 98.02 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 220 train Loss: 2403.0 test Loss: 269.3
Epoch 380 Iter 0 subLoss 2133.6 multi -7.96 import weight 0.00
Epoch 380 Iter 1 subLoss 1976.0 multi 1.00 import weight 0.00
Epoch 380 Iter 2 subLoss 2223.3 multi 1.00 import weight 0.00
Epoch 380 Iter 3 subLoss 2447.8 multi -10.94 import weight 0.00
Epoch 380 Iter 4 subLoss 2295.8 multi -1.99 import weight 0.00
Epoch 380 Iter 5 subLoss 2400.5 multi 3.99 import weight 0.00
Epoch 380 Iter 6 subLoss 2147.8 multi 3.98 import weight 0.00
Epoch 380 Iter 7 subLoss 2587.5 multi 18.91 import weight 0.00
Epoch 380 Iter 8 subLoss 2151.2 multi -7.96 import weight 0.00
Epoch 380 Iter 9 subLoss 2146.1 multi 6.97 import weight 0.00
Epoch 380 Iter 10 subLoss 2335.1 multi 3.99 import weight 0.00
Epoch 380 Iter 11 subLoss 2308.3 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0075 / 0.10196 / 14.42
Entropy seen (from low to high)
[14, 81, 190, 234, 488, 525, 455, 417, 321, 342, 385, 315, 227, 187, 153, 102, 74, 65, 83, 53, 48, 49, 40, 42, 43, 24, 23, 22, 21, 28, 24, 10, 12, 6, 6, 7, 4, 9, 1, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 15, 41, 48, 59, 97, 99, 117, 137, 150, 183, 187, 187, 229, 215, 198, 226, 202, 204, 191, 188, 190, 150, 156, 155, 172, 149, 147, 129, 122, 117, 109, 106, 69, 55, 23, 12, 8, 6, 6, 1, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.8, 33.8, 37.1, 39.9, 44.2, 47.3, 50.8, 54.2, 57.6, 61.2, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.1, 24.9, 33.3, 51.7, 49.9, 78.5, 80.6, 67.8, 86.2, 81.0]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 7, 4, 9, 29, 18, 28, 31, 28, 29, 37]
Epoch 380 Acc: 98.50 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 230 train Loss: 2342.2 test Loss: 262.4
Epoch 381 Iter 0 subLoss 2252.6 multi -4.97 import weight 0.00
Epoch 381 Iter 1 subLoss 2359.7 multi -13.93 import weight 0.00
Epoch 381 Iter 2 subLoss 2345.5 multi 9.96 import weight 0.00
Epoch 381 Iter 3 subLoss 2688.3 multi 21.90 import weight 0.00
Epoch 381 Iter 4 subLoss 2737.2 multi -13.93 import weight 0.00
Epoch 381 Iter 5 subLoss 2720.9 multi 1.00 import weight 0.00
Epoch 381 Iter 6 subLoss 2695.0 multi -13.93 import weight 0.00
Epoch 381 Iter 7 subLoss 6380.8 multi -1.99 import weight 0.00
Epoch 381 Iter 8 subLoss 9323.4 multi 1.00 import weight 0.00
Epoch 381 Iter 9 subLoss 6126.1 multi 9.96 import weight 0.00
Epoch 381 Iter 10 subLoss 2410.4 multi 9.96 import weight 0.00
Epoch 381 Iter 11 subLoss 2724.9 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0074 / 0.10217 / 14.34
Entropy seen (from low to high)
[14, 82, 192, 242, 495, 532, 453, 422, 313, 340, 389, 313, 222, 188, 145, 101, 69, 71, 79, 51, 48, 49, 43, 40, 40, 24, 26, 22, 18, 28, 24, 10, 12, 6, 7, 6, 4, 9, 1, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 16, 40, 46, 58, 97, 99, 116, 133, 153, 177, 187, 190, 231, 212, 196, 228, 204, 197, 191, 191, 186, 155, 154, 157, 166, 153, 144, 136, 122, 121, 106, 103, 75, 54, 26, 14, 7, 7, 6, 1, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 34.1, 37.1, 39.9, 44.2, 47.2, 50.8, 54.3, 57.8, 61.3, 65.1, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.1, 24.9, 33.3, 53.5, 47.3, 79.3, 79.9, 67.8, 86.2, 78.3]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 7, 4, 9, 28, 19, 29, 30, 28, 29, 37]
Epoch 381 Acc: 98.33 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 272 train Loss: 2287.8 test Loss: 301.2
Epoch 382 Iter 0 subLoss 2543.3 multi -13.93 import weight 0.00
Epoch 382 Iter 1 subLoss 2678.7 multi -4.97 import weight 0.00
Epoch 382 Iter 2 subLoss 2207.5 multi 1.00 import weight 0.00
Epoch 382 Iter 3 subLoss 2332.3 multi 6.97 import weight 0.00
Epoch 382 Iter 4 subLoss 2097.9 multi -1.99 import weight 0.00
Epoch 382 Iter 5 subLoss 2273.7 multi 3.99 import weight 0.00
Epoch 382 Iter 6 subLoss 2511.9 multi 3.99 import weight 0.00
Epoch 382 Iter 7 subLoss 2955.0 multi 3.99 import weight 0.00
Epoch 382 Iter 8 subLoss 2452.2 multi -1.99 import weight 0.00
Epoch 382 Iter 9 subLoss 2167.8 multi 1.00 import weight 0.00
Epoch 382 Iter 10 subLoss 2387.5 multi 3.99 import weight 0.00
Epoch 382 Iter 11 subLoss 2367.8 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0074 / 0.10238 / 15.50
Entropy seen (from low to high)
[14, 85, 191, 248, 505, 535, 448, 419, 314, 352, 383, 309, 220, 191, 138, 99, 70, 69, 78, 51, 47, 48, 42, 38, 41, 22, 26, 21, 19, 29, 25, 10, 10, 6, 7, 6, 4, 9, 1, 2, 5, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 16, 40, 43, 59, 96, 96, 119, 133, 149, 176, 192, 188, 224, 214, 198, 225, 206, 193, 190, 194, 185, 158, 151, 155, 169, 156, 141, 139, 118, 126, 106, 98, 84, 54, 27, 16, 7, 7, 6, 1, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 34.3, 37.1, 39.8, 44.1, 47.2, 50.7, 54.3, 57.8, 61.3, 65.1, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.1, 24.9, 24.9, 57.1, 42.1, 79.9, 79.3, 67.8, 83.3, 79.9]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 7, 4, 8, 28, 19, 30, 29, 28, 30, 35]
Epoch 382 Acc: 98.31 BMA: 98.03 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 236 train Loss: 2308.3 test Loss: 294.3
Epoch 383 Iter 0 subLoss 2933.1 multi 18.91 import weight 1.00
Epoch 383 Iter 1 subLoss 2234.6 multi 6.97 import weight 0.00
Epoch 383 Iter 2 subLoss 2419.6 multi 12.94 import weight 0.00
Epoch 383 Iter 3 subLoss 1846.0 multi -1.99 import weight 0.00
Epoch 383 Iter 4 subLoss 1790.3 multi 1.00 import weight 0.00
Epoch 383 Iter 5 subLoss 2048.1 multi -7.96 import weight 0.00
Epoch 383 Iter 6 subLoss 2393.5 multi -1.99 import weight 0.00
Epoch 383 Iter 7 subLoss 1818.3 multi 1.00 import weight 0.00
Epoch 383 Iter 8 subLoss 2477.6 multi 1.00 import weight 0.00
Epoch 383 Iter 9 subLoss 2548.6 multi -10.94 import weight 0.00
Epoch 383 Iter 10 subLoss 2309.8 multi 12.94 import weight 0.00
Epoch 383 Iter 11 subLoss 2040.0 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0074 / 0.10258 / 14.56
Entropy seen (from low to high)
[15, 88, 190, 253, 518, 536, 447, 414, 318, 354, 381, 309, 219, 181, 140, 92, 68, 76, 71, 50, 49, 46, 42, 41, 41, 21, 23, 23, 18, 31, 22, 12, 9, 6, 6, 7, 3, 9, 1, 2, 5, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 16, 38, 42, 61, 95, 95, 117, 134, 148, 172, 191, 191, 225, 214, 196, 223, 211, 190, 186, 192, 181, 170, 145, 155, 161, 163, 138, 141, 125, 122, 110, 97, 88, 54, 30, 17, 7, 7, 6, 1, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 33.9, 36.8, 39.7, 44.1, 47.2, 50.7, 54.3, 57.9, 61.4, 65.1, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 24.9, 24.9, 55.5, 44.9, 79.3, 79.9, 66.6, 82.7, 80.5]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 8, 4, 8, 27, 20, 29, 30, 27, 29, 36]
Epoch 383 Acc: 98.13 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 204 train Loss: 2353.9 test Loss: 295.4
Epoch 384 Iter 0 subLoss 2706.1 multi 3.99 import weight 0.00
Epoch 384 Iter 1 subLoss 2357.2 multi -13.93 import weight 0.00
Epoch 384 Iter 2 subLoss 2888.9 multi -19.90 import weight 0.00
Epoch 384 Iter 3 subLoss 4188.8 multi -13.93 import weight 0.00
Epoch 384 Iter 4 subLoss 9585.4 multi 9.96 import weight 0.00
Epoch 384 Iter 5 subLoss 5068.3 multi 6.97 import weight 0.00
Epoch 384 Iter 6 subLoss 3151.5 multi -19.90 import weight 0.00
Epoch 384 Iter 7 subLoss 5873.3 multi -1.98 import weight 0.00
Epoch 384 Iter 8 subLoss 7586.7 multi 1.00 import weight 0.00
Epoch 384 Iter 9 subLoss 6458.7 multi 3.98 import weight 0.00
Epoch 384 Iter 10 subLoss 4363.5 multi 24.88 import weight 0.00
Epoch 384 Iter 11 subLoss 2543.2 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0073 / 0.10273 / 13.18
Entropy seen (from low to high)
[15, 88, 195, 255, 521, 541, 452, 415, 315, 359, 378, 308, 214, 174, 138, 94, 65, 80, 66, 53, 48, 43, 42, 41, 39, 21, 25, 21, 19, 30, 21, 13, 9, 6, 6, 7, 3, 9, 1, 2, 5, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 16, 38, 42, 59, 95, 95, 115, 136, 148, 166, 196, 190, 225, 214, 196, 222, 209, 190, 182, 191, 184, 171, 143, 154, 161, 153, 153, 137, 122, 127, 114, 95, 91, 55, 31, 18, 7, 7, 4, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 33.8, 37.1, 40.0, 44.1, 47.3, 50.9, 54.2, 57.9, 61.5, 65.2, 68.6]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 0.0, 44.4, 33.3, 24.9, 49.9, 47.3, 77.7, 78.7, 69.2, 82.7, 80.5]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 9, 3, 8, 28, 19, 27, 33, 26, 29, 36]
Epoch 384 Acc: 97.84 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 254 train Loss: 3109.8 test Loss: 356.3
Epoch 385 Iter 0 subLoss 2946.4 multi -22.88 import weight 0.00
Epoch 385 Iter 1 subLoss 6426.6 multi 1.00 import weight 0.00
Epoch 385 Iter 2 subLoss 5564.8 multi 1.00 import weight 0.00
Epoch 385 Iter 3 subLoss 5267.8 multi 12.94 import weight 0.00
Epoch 385 Iter 4 subLoss 2677.1 multi -1.99 import weight 0.00
Epoch 385 Iter 5 subLoss 3153.7 multi -16.91 import weight 0.00
Epoch 385 Iter 6 subLoss 3710.8 multi -1.99 import weight 0.00
Epoch 385 Iter 7 subLoss 4800.6 multi 27.87 import weight 0.00
Epoch 385 Iter 8 subLoss 15632.3 multi 1.00 import weight 0.00
Epoch 385 Iter 9 subLoss 7750.3 multi 3.99 import weight 0.00
Epoch 385 Iter 10 subLoss 4136.3 multi 9.96 import weight 0.00
Epoch 385 Iter 11 subLoss 2656.9 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0073 / 0.10295 / 14.58
Entropy seen (from low to high)
[15, 90, 196, 263, 523, 540, 462, 409, 321, 359, 384, 291, 218, 176, 123, 95, 68, 83, 62, 51, 46, 44, 40, 45, 35, 22, 24, 23, 19, 28, 22, 12, 9, 6, 6, 7, 3, 9, 1, 2, 5, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 15, 39, 36, 64, 95, 94, 111, 139, 149, 164, 190, 192, 221, 214, 199, 218, 209, 186, 185, 194, 181, 170, 146, 149, 163, 154, 155, 136, 126, 122, 120, 93, 96, 59, 32, 16, 9, 5, 6, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.5, 33.6, 36.9, 39.7, 44.3, 47.4, 51.0, 54.1, 57.8, 61.3, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 0.0, 37.4, 49.9, 22.2, 59.2, 47.3, 75.9, 79.4, 74.9, 74.9, 84.2]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 8, 4, 9, 27, 19, 25, 34, 24, 28, 38]
Epoch 385 Acc: 98.25 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 265 train Loss: 2495.8 test Loss: 286.6
Epoch 386 Iter 0 subLoss 2633.6 multi -4.97 import weight 0.00
Epoch 386 Iter 1 subLoss 2808.8 multi 6.97 import weight 0.00
Epoch 386 Iter 2 subLoss 2391.1 multi 1.00 import weight 0.00
Epoch 386 Iter 3 subLoss 2548.4 multi -4.97 import weight 0.00
Epoch 386 Iter 4 subLoss 2177.0 multi 3.99 import weight 0.00
Epoch 386 Iter 5 subLoss 2203.4 multi 3.98 import weight 0.00
Epoch 386 Iter 6 subLoss 2514.7 multi 6.97 import weight 0.00
Epoch 386 Iter 7 subLoss 2723.7 multi 6.97 import weight 0.00
Epoch 386 Iter 8 subLoss 2369.5 multi 3.98 import weight 0.00
Epoch 386 Iter 9 subLoss 2105.6 multi -1.99 import weight 0.00
Epoch 386 Iter 10 subLoss 2080.9 multi 3.99 import weight 0.00
Epoch 386 Iter 11 subLoss 2444.0 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0073 / 0.10318 / 13.94
Entropy seen (from low to high)
[15, 94, 197, 270, 528, 535, 466, 406, 317, 372, 380, 287, 219, 169, 129, 86, 69, 85, 59, 47, 50, 41, 43, 45, 32, 21, 24, 22, 22, 26, 22, 12, 8, 6, 7, 6, 3, 9, 1, 2, 5, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 14, 37, 36, 68, 92, 94, 107, 144, 137, 172, 189, 186, 222, 216, 201, 211, 210, 180, 191, 198, 179, 167, 149, 150, 161, 151, 158, 137, 123, 128, 122, 93, 98, 59, 36, 15, 10, 4, 7, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.5, 33.6, 37.0, 39.8, 44.0, 47.2, 50.8, 54.1, 57.8, 61.4, 64.9, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 0.0, 37.4, 49.9, 28.5, 53.5, 44.4, 78.5, 78.1, 71.9, 80.7, 81.5]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 8, 4, 7, 28, 18, 28, 32, 25, 26, 38]
Epoch 386 Acc: 98.05 BMA: 98.07 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 244 train Loss: 2473.1 test Loss: 313.6
Epoch 387 Iter 0 subLoss 2836.7 multi 6.97 import weight 0.00
Epoch 387 Iter 1 subLoss 2216.1 multi -1.99 import weight 0.00
Epoch 387 Iter 2 subLoss 2648.5 multi -13.93 import weight 0.00
Epoch 387 Iter 3 subLoss 2275.2 multi 6.97 import weight 0.00
Epoch 387 Iter 4 subLoss 2847.3 multi 3.99 import weight 0.00
Epoch 387 Iter 5 subLoss 2582.9 multi 21.90 import weight 0.00
Epoch 387 Iter 6 subLoss 2365.6 multi 6.97 import weight 0.00
Epoch 387 Iter 7 subLoss 2117.2 multi 6.97 import weight 0.00
Epoch 387 Iter 8 subLoss 2239.4 multi 9.96 import weight 0.00
Epoch 387 Iter 9 subLoss 2492.3 multi -4.97 import weight 0.00
Epoch 387 Iter 10 subLoss 2212.7 multi 1.00 import weight 0.00
Epoch 387 Iter 11 subLoss 2236.4 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0073 / 0.10341 / 13.05
Entropy seen (from low to high)
[15, 96, 195, 284, 529, 532, 473, 403, 315, 379, 381, 277, 223, 162, 128, 87, 62, 87, 61, 47, 47, 40, 42, 44, 34, 22, 22, 24, 21, 28, 18, 14, 6, 7, 5, 7, 3, 9, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 6, 13, 36, 37, 68, 88, 92, 111, 138, 139, 169, 192, 189, 216, 211, 205, 204, 212, 187, 191, 191, 177, 171, 152, 146, 163, 153, 157, 140, 125, 122, 128, 94, 98, 62, 39, 15, 10, 3, 8, 3, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.4, 33.6, 37.2, 41.0, 44.8, 47.2, 50.6, 54.1, 57.8, 61.3, 65.0, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 0.0, 33.3, 39.9, 66.6, 47.9, 47.3, 78.5, 78.1, 71.9, 81.4, 80.5]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 9, 5, 6, 25, 19, 28, 32, 25, 27, 36]
Epoch 387 Acc: 98.38 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 223 train Loss: 2395.8 test Loss: 277.0
Epoch 388 Iter 0 subLoss 2696.3 multi -10.94 import weight 0.00
Epoch 388 Iter 1 subLoss 2752.0 multi -13.93 import weight 0.00
Epoch 388 Iter 2 subLoss 3299.2 multi 9.96 import weight 0.00
Epoch 388 Iter 3 subLoss 2033.3 multi 6.97 import weight 0.00
Epoch 388 Iter 4 subLoss 2483.2 multi -1.99 import weight 0.00
Epoch 388 Iter 5 subLoss 2551.2 multi -13.93 import weight 0.00
Epoch 388 Iter 6 subLoss 2273.4 multi 9.96 import weight 0.00
Epoch 388 Iter 7 subLoss 2218.8 multi 3.99 import weight 0.00
Epoch 388 Iter 8 subLoss 2675.2 multi 1.00 import weight 0.00
Epoch 388 Iter 9 subLoss 2080.6 multi 6.97 import weight 0.00
Epoch 388 Iter 10 subLoss 2779.4 multi -40.79 import weight 0.00
Epoch 388 Iter 11 subLoss 2683.7 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0072 / 0.10363 / 13.17
Entropy seen (from low to high)
[15, 98, 194, 288, 535, 537, 472, 404, 311, 381, 383, 272, 226, 156, 125, 85, 65, 86, 61, 45, 47, 39, 42, 46, 31, 23, 22, 23, 23, 24, 19, 15, 5, 7, 6, 6, 3, 9, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 13, 38, 36, 65, 85, 93, 109, 140, 137, 166, 193, 189, 216, 204, 214, 199, 210, 192, 188, 181, 184, 177, 152, 144, 156, 160, 156, 141, 125, 127, 124, 96, 102, 62, 40, 16, 12, 3, 7, 4, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.9, 31.0, 33.7, 37.2, 41.0, 44.8, 47.3, 50.7, 54.2, 57.8, 61.1, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 33.3, 39.9, 66.6, 47.9, 47.3, 79.3, 75.8, 79.1, 71.4, 83.7]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 9, 5, 6, 25, 19, 29, 29, 24, 28, 37]
Epoch 388 Acc: 98.35 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 268 train Loss: 2511.7 test Loss: 283.9
Epoch 389 Iter 0 subLoss 2284.1 multi -7.96 import weight 0.00
Epoch 389 Iter 1 subLoss 2893.8 multi 18.91 import weight 0.00
Epoch 389 Iter 2 subLoss 2340.5 multi 9.96 import weight 0.00
Epoch 389 Iter 3 subLoss 2157.0 multi -7.96 import weight 0.00
Epoch 389 Iter 4 subLoss 2790.9 multi 21.90 import weight 0.00
Epoch 389 Iter 5 subLoss 2516.9 multi 9.96 import weight 0.00
Epoch 389 Iter 6 subLoss 2530.3 multi 6.97 import weight 0.00
Epoch 389 Iter 7 subLoss 2406.7 multi 1.00 import weight 0.00
Epoch 389 Iter 8 subLoss 2360.0 multi 9.96 import weight 0.00
Epoch 389 Iter 9 subLoss 2258.3 multi -1.99 import weight 0.00
Epoch 389 Iter 10 subLoss 1704.3 multi 1.00 import weight 0.00
Epoch 389 Iter 11 subLoss 2316.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0072 / 0.10384 / 14.38
Entropy seen (from low to high)
[16, 101, 192, 295, 544, 534, 474, 403, 311, 388, 377, 277, 210, 161, 126, 78, 67, 89, 55, 43, 48, 39, 43, 45, 30, 24, 20, 23, 23, 26, 16, 15, 5, 8, 5, 6, 3, 9, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 5, 14, 36, 37, 64, 82, 94, 107, 136, 143, 164, 183, 200, 206, 210, 208, 202, 210, 191, 190, 179, 182, 184, 149, 147, 152, 161, 157, 141, 125, 126, 129, 96, 97, 68, 40, 20, 12, 3, 7, 4, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.9, 31.1, 33.6, 37.1, 41.0, 44.8, 47.2, 50.8, 54.3, 57.8, 61.1, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 33.3, 39.9, 66.6, 49.9, 42.8, 82.1, 74.0, 80.7, 70.3, 86.1]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 9, 5, 6, 24, 21, 28, 27, 26, 27, 36]
Epoch 389 Acc: 98.38 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 231 train Loss: 2292.7 test Loss: 273.1
Epoch 390 Iter 0 subLoss 2145.2 multi 9.96 import weight 0.00
Epoch 390 Iter 1 subLoss 2842.9 multi 6.97 import weight 0.00
Epoch 390 Iter 2 subLoss 1960.2 multi 1.00 import weight 0.00
Epoch 390 Iter 3 subLoss 2121.1 multi -1.99 import weight 0.00
Epoch 390 Iter 4 subLoss 2223.6 multi -4.97 import weight 0.00
Epoch 390 Iter 5 subLoss 2511.8 multi 12.94 import weight 0.00
Epoch 390 Iter 6 subLoss 2428.9 multi -13.93 import weight 0.00
Epoch 390 Iter 7 subLoss 2554.0 multi -10.94 import weight 0.00
Epoch 390 Iter 8 subLoss 4812.7 multi 1.00 import weight 0.00
Epoch 390 Iter 9 subLoss 3273.0 multi 6.97 import weight 0.00
Epoch 390 Iter 10 subLoss 2377.0 multi -16.91 import weight 0.00
Epoch 390 Iter 11 subLoss 2691.3 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0072 / 0.10392 / 14.06
Entropy seen (from low to high)
[16, 102, 197, 294, 548, 535, 482, 395, 316, 391, 380, 264, 212, 156, 126, 77, 62, 91, 55, 46, 44, 45, 38, 44, 30, 24, 20, 25, 20, 28, 16, 14, 5, 7, 6, 6, 3, 9, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 14, 33, 41, 62, 80, 94, 108, 132, 141, 167, 183, 202, 211, 205, 204, 202, 208, 195, 195, 179, 182, 179, 150, 146, 151, 168, 153, 139, 129, 122, 128, 101, 96, 68, 43, 20, 12, 2, 8, 4, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 31.0, 33.7, 37.0, 40.6, 44.6, 47.0, 50.5, 54.3, 58.1, 61.5, 64.9, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 37.4, 49.9, 74.9, 41.6, 52.1, 79.3, 79.9, 74.9, 70.8, 86.1]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 8, 6, 4, 24, 23, 29, 30, 24, 24, 36]
Epoch 390 Acc: 96.59 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 269 train Loss: 5096.1 test Loss: 615.2
Epoch 391 Iter 0 subLoss 5225.6 multi -10.94 import weight 0.00
Epoch 391 Iter 1 subLoss 39847.0 multi 1.00 import weight 0.00
Epoch 391 Iter 2 subLoss 9655.8 multi 1.00 import weight 0.00
Epoch 391 Iter 3 subLoss 7136.6 multi 3.99 import weight 0.00
Epoch 391 Iter 4 subLoss 3388.8 multi 9.96 import weight 0.00
Epoch 391 Iter 5 subLoss 2586.3 multi 24.88 import weight 0.00
Epoch 391 Iter 6 subLoss 2302.1 multi 15.93 import weight 0.00
Epoch 391 Iter 7 subLoss 2272.9 multi 12.94 import weight 0.00
Epoch 391 Iter 8 subLoss 2668.8 multi 3.99 import weight 0.00
Epoch 391 Iter 9 subLoss 2127.9 multi 1.00 import weight 0.00
Epoch 391 Iter 10 subLoss 2039.3 multi 9.96 import weight 0.00
Epoch 391 Iter 11 subLoss 2475.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0071 / 0.10412 / 14.39
Entropy seen (from low to high)
[16, 103, 201, 299, 558, 533, 483, 388, 328, 396, 376, 256, 207, 157, 121, 71, 66, 92, 53, 46, 42, 44, 42, 38, 30, 28, 19, 21, 22, 29, 14, 15, 4, 8, 5, 6, 3, 9, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 14, 31, 41, 63, 77, 97, 102, 137, 139, 163, 184, 197, 210, 210, 206, 196, 210, 194, 196, 176, 183, 177, 154, 140, 153, 173, 150, 142, 127, 125, 126, 105, 98, 69, 42, 23, 13, 2, 8, 4, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.6, 31.1, 33.5, 37.0, 40.7, 44.9, 47.3, 50.7, 54.5, 58.0, 61.3, 64.7, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 37.4, 49.9, 71.4, 38.0, 55.9, 76.9, 82.7, 71.9, 69.5, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 8, 6, 7, 21, 25, 26, 29, 25, 23, 35]
Epoch 391 Acc: 98.29 BMA: 98.05 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 247 train Loss: 2286.3 test Loss: 275.7
Epoch 392 Iter 0 subLoss 2257.0 multi 1.00 import weight 0.00
Epoch 392 Iter 1 subLoss 2351.9 multi -13.93 import weight 0.00
Epoch 392 Iter 2 subLoss 2355.3 multi -10.94 import weight 0.00
Epoch 392 Iter 3 subLoss 2849.3 multi 9.96 import weight 0.00
Epoch 392 Iter 4 subLoss 1631.6 multi 1.00 import weight 0.00
Epoch 392 Iter 5 subLoss 2981.3 multi 6.97 import weight 0.00
Epoch 392 Iter 6 subLoss 2299.6 multi -1.98 import weight 0.00
Epoch 392 Iter 7 subLoss 2511.3 multi 15.93 import weight 0.00
Epoch 392 Iter 8 subLoss 2171.9 multi 6.97 import weight 0.00
Epoch 392 Iter 9 subLoss 2441.7 multi -4.97 import weight 0.00
Epoch 392 Iter 10 subLoss 2113.5 multi 9.96 import weight 0.00
Epoch 392 Iter 11 subLoss 2159.4 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0071 / 0.10432 / 13.81
Entropy seen (from low to high)
[16, 106, 201, 308, 557, 541, 489, 380, 330, 405, 365, 252, 202, 159, 115, 73, 64, 90, 53, 47, 40, 47, 42, 36, 28, 29, 19, 20, 22, 31, 13, 15, 3, 9, 4, 6, 3, 9, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 13, 31, 40, 65, 76, 95, 104, 134, 140, 156, 185, 201, 208, 210, 210, 188, 212, 194, 194, 178, 183, 176, 153, 140, 158, 172, 144, 148, 128, 124, 125, 109, 102, 68, 41, 24, 13, 3, 7, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 31.3, 33.5, 37.0, 40.7, 44.8, 47.1, 50.6, 54.3, 57.8, 61.1, 64.6, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 37.4, 49.9, 66.6, 42.8, 55.9, 76.9, 81.4, 76.9, 66.6, 86.1]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 8, 6, 6, 21, 25, 26, 27, 26, 24, 36]
Epoch 392 Acc: 98.48 BMA: 98.09 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 215 train Loss: 2225.9 test Loss: 261.3
Epoch 393 Iter 0 subLoss 1824.5 multi -1.99 import weight 0.00
Epoch 393 Iter 1 subLoss 1946.4 multi 1.00 import weight 0.00
Epoch 393 Iter 2 subLoss 2239.2 multi 12.94 import weight 0.00
Epoch 393 Iter 3 subLoss 2748.9 multi 18.91 import weight 0.00
Epoch 393 Iter 4 subLoss 2582.5 multi 27.87 import weight 0.00
Epoch 393 Iter 5 subLoss 2129.2 multi 1.00 import weight 0.00
Epoch 393 Iter 6 subLoss 2214.6 multi 6.97 import weight 0.00
Epoch 393 Iter 7 subLoss 1705.9 multi 3.99 import weight 0.00
Epoch 393 Iter 8 subLoss 1979.5 multi 1.00 import weight 0.00
Epoch 393 Iter 9 subLoss 2107.1 multi 1.00 import weight 0.00
Epoch 393 Iter 10 subLoss 2255.6 multi 3.98 import weight 0.00
Epoch 393 Iter 11 subLoss 2166.0 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0071 / 0.10452 / 15.12
Entropy seen (from low to high)
[16, 109, 201, 317, 567, 544, 480, 383, 332, 408, 358, 250, 200, 154, 115, 67, 67, 88, 53, 49, 43, 42, 40, 38, 25, 31, 19, 18, 24, 30, 13, 14, 4, 8, 5, 5, 3, 9, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 13, 31, 38, 63, 77, 95, 102, 131, 141, 156, 183, 205, 203, 213, 207, 188, 212, 195, 189, 185, 177, 178, 156, 136, 160, 165, 153, 146, 126, 127, 123, 114, 104, 72, 40, 25, 13, 3, 7, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.6, 31.4, 33.3, 37.0, 40.9, 44.8, 47.0, 50.4, 54.2, 57.8, 61.1, 64.6, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 0.0, 37.4, 49.9, 83.3, 36.8, 55.9, 77.7, 82.1, 75.9, 66.6, 86.1]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 8, 6, 6, 19, 25, 27, 28, 25, 24, 36]
Epoch 393 Acc: 98.58 BMA: 98.13 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 216 train Loss: 2127.6 test Loss: 241.7
Epoch 394 Iter 0 subLoss 2318.9 multi 1.00 import weight 0.00
Epoch 394 Iter 1 subLoss 2399.6 multi 3.98 import weight 0.00
Epoch 394 Iter 2 subLoss 2271.6 multi 15.93 import weight 0.00
Epoch 394 Iter 3 subLoss 2051.6 multi -1.98 import weight 0.00
Epoch 394 Iter 4 subLoss 2044.6 multi -7.96 import weight 0.00
Epoch 394 Iter 5 subLoss 2131.6 multi -13.93 import weight 0.00
Epoch 394 Iter 6 subLoss 2344.1 multi 12.94 import weight 0.00
Epoch 394 Iter 7 subLoss 2307.2 multi 15.93 import weight 0.00
Epoch 394 Iter 8 subLoss 1792.2 multi 3.99 import weight 0.00
Epoch 394 Iter 9 subLoss 1935.1 multi 1.00 import weight 0.00
Epoch 394 Iter 10 subLoss 2291.6 multi 1.00 import weight 0.00
Epoch 394 Iter 11 subLoss 1924.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0070 / 0.10472 / 16.08
Entropy seen (from low to high)
[16, 113, 200, 323, 575, 540, 482, 381, 338, 409, 351, 254, 191, 157, 110, 68, 69, 84, 48, 52, 42, 42, 38, 40, 24, 32, 19, 17, 24, 29, 15, 11, 5, 8, 6, 4, 4, 8, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 14, 31, 37, 62, 77, 97, 102, 122, 148, 154, 177, 207, 199, 212, 209, 191, 209, 197, 189, 178, 183, 180, 152, 138, 162, 166, 149, 143, 135, 123, 126, 110, 112, 71, 41, 27, 12, 3, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 0.0, 32.4, 36.8, 40.4, 44.2, 47.1, 50.5, 54.2, 57.7, 61.1, 64.6, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 28.5, 66.6, 83.3, 33.3, 58.3, 76.9, 85.1, 74.0, 66.6, 85.2]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 7, 6, 6, 21, 24, 26, 27, 27, 24, 34]
Epoch 394 Acc: 98.58 BMA: 98.13 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 192 train Loss: 2238.1 test Loss: 235.2
Epoch 395 Iter 0 subLoss 2134.5 multi -10.94 import weight 0.00
Epoch 395 Iter 1 subLoss 2163.4 multi 1.00 import weight 0.00
Epoch 395 Iter 2 subLoss 2324.6 multi -16.91 import weight 0.00
Epoch 395 Iter 3 subLoss 2315.9 multi 1.00 import weight 0.00
Epoch 395 Iter 4 subLoss 2312.0 multi 3.99 import weight 0.00
Epoch 395 Iter 5 subLoss 2018.4 multi 1.00 import weight 0.00
Epoch 395 Iter 6 subLoss 2517.3 multi 18.91 import weight 0.00
Epoch 395 Iter 7 subLoss 2163.7 multi 3.99 import weight 0.00
Epoch 395 Iter 8 subLoss 2200.0 multi 6.97 import weight 0.00
Epoch 395 Iter 9 subLoss 1979.9 multi 3.98 import weight 0.00
Epoch 395 Iter 10 subLoss 1925.2 multi 3.99 import weight 0.00
Epoch 395 Iter 11 subLoss 2317.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0070 / 0.10491 / 15.53
Entropy seen (from low to high)
[16, 115, 205, 325, 582, 540, 480, 380, 341, 415, 342, 249, 195, 152, 104, 72, 72, 79, 50, 50, 42, 41, 38, 38, 26, 29, 21, 16, 24, 29, 15, 11, 6, 7, 7, 3, 4, 8, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 14, 28, 39, 61, 77, 94, 105, 118, 154, 148, 178, 197, 208, 208, 208, 190, 213, 199, 188, 176, 181, 183, 149, 141, 159, 164, 149, 140, 144, 121, 128, 113, 108, 74, 46, 27, 12, 3, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 0.0, 32.4, 36.7, 40.5, 44.4, 47.2, 50.5, 54.2, 57.6, 61.1, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 28.5, 66.6, 71.4, 34.9, 58.3, 75.9, 84.6, 74.9, 67.9, 85.2]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 7, 6, 7, 20, 24, 25, 26, 28, 25, 34]
Epoch 395 Acc: 98.52 BMA: 98.15 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 231 train Loss: 2081.3 test Loss: 246.0
Epoch 396 Iter 0 subLoss 1893.5 multi 1.00 import weight 0.00
Epoch 396 Iter 1 subLoss 2316.2 multi 9.96 import weight 0.00
Epoch 396 Iter 2 subLoss 1983.7 multi -7.96 import weight 0.00
Epoch 396 Iter 3 subLoss 2294.3 multi 3.99 import weight 0.00
Epoch 396 Iter 4 subLoss 2352.9 multi -10.94 import weight 0.00
Epoch 396 Iter 5 subLoss 2366.8 multi 3.98 import weight 0.00
Epoch 396 Iter 6 subLoss 1883.5 multi 1.00 import weight 0.00
Epoch 396 Iter 7 subLoss 2193.7 multi 3.99 import weight 0.00
Epoch 396 Iter 8 subLoss 2058.4 multi -1.99 import weight 0.00
Epoch 396 Iter 9 subLoss 1930.8 multi -1.98 import weight 0.00
Epoch 396 Iter 10 subLoss 2257.1 multi 6.97 import weight 0.00
Epoch 396 Iter 11 subLoss 2278.1 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0070 / 0.10511 / 15.09
Entropy seen (from low to high)
[16, 119, 204, 335, 582, 540, 478, 385, 343, 419, 334, 247, 202, 141, 103, 68, 76, 76, 50, 47, 43, 40, 39, 39, 25, 27, 21, 16, 25, 28, 15, 11, 6, 7, 7, 4, 5, 6, 1, 3, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 14, 27, 39, 62, 72, 95, 109, 118, 150, 149, 174, 193, 212, 201, 216, 188, 212, 191, 193, 178, 184, 177, 155, 137, 159, 157, 150, 148, 147, 117, 130, 113, 110, 75, 51, 26, 11, 5, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 0.0, 32.6, 36.7, 39.9, 44.1, 47.4, 50.6, 54.1, 57.4, 61.1, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 28.5, 49.9, 72.7, 38.8, 54.1, 73.9, 84.6, 76.6, 70.8, 82.8]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 7, 4, 11, 18, 24, 23, 26, 30, 24, 35]
Epoch 396 Acc: 98.52 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 227 train Loss: 2128.7 test Loss: 226.9
Epoch 397 Iter 0 subLoss 2559.9 multi -7.96 import weight 0.00
Epoch 397 Iter 1 subLoss 2191.3 multi 6.97 import weight 0.00
Epoch 397 Iter 2 subLoss 2543.0 multi -4.97 import weight 0.00
Epoch 397 Iter 3 subLoss 2171.2 multi 1.00 import weight 0.00
Epoch 397 Iter 4 subLoss 2242.4 multi -13.93 import weight 0.00
Epoch 397 Iter 5 subLoss 2236.5 multi 15.93 import weight 0.00
Epoch 397 Iter 6 subLoss 2213.9 multi 6.97 import weight 0.00
Epoch 397 Iter 7 subLoss 1973.3 multi 6.97 import weight 0.00
Epoch 397 Iter 8 subLoss 2217.5 multi 9.96 import weight 0.00
Epoch 397 Iter 9 subLoss 2237.6 multi 18.91 import weight 0.00
Epoch 397 Iter 10 subLoss 1769.6 multi 1.00 import weight 0.00
Epoch 397 Iter 11 subLoss 2123.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0069 / 0.10530 / 17.05
Entropy seen (from low to high)
[16, 120, 207, 341, 587, 553, 469, 381, 355, 411, 335, 238, 200, 142, 98, 69, 80, 67, 52, 47, 41, 43, 36, 40, 26, 24, 24, 14, 25, 27, 16, 10, 9, 4, 8, 3, 5, 6, 1, 4, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 14, 26, 40, 62, 71, 93, 105, 119, 153, 144, 173, 199, 205, 201, 220, 182, 213, 192, 187, 183, 189, 170, 156, 141, 157, 149, 157, 153, 143, 119, 132, 113, 109, 79, 54, 26, 11, 5, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 0.0, 33.5, 37.0, 40.0, 44.0, 47.6, 50.8, 54.1, 57.4, 61.1, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 16.6, 49.9, 79.9, 33.3, 59.0, 72.7, 84.6, 74.1, 73.9, 83.3]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 6, 4, 10, 21, 22, 22, 26, 31, 23, 36]
Epoch 397 Acc: 98.50 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 212 train Loss: 2019.4 test Loss: 233.4
Epoch 398 Iter 0 subLoss 1666.4 multi 1.00 import weight 0.00
Epoch 398 Iter 1 subLoss 1780.8 multi 1.00 import weight 0.00
Epoch 398 Iter 2 subLoss 2324.3 multi -25.87 import weight 0.00
Epoch 398 Iter 3 subLoss 1979.6 multi 9.96 import weight 0.00
Epoch 398 Iter 4 subLoss 2448.2 multi -1.99 import weight 0.00
Epoch 398 Iter 5 subLoss 1990.0 multi -10.94 import weight 0.00
Epoch 398 Iter 6 subLoss 2374.9 multi -16.91 import weight 0.00
Epoch 398 Iter 7 subLoss 3672.6 multi 12.94 import weight 0.00
Epoch 398 Iter 8 subLoss 2613.3 multi -4.97 import weight 0.00
Epoch 398 Iter 9 subLoss 2850.1 multi -25.87 import weight 0.00
Epoch 398 Iter 10 subLoss 12591.3 multi 1.00 import weight 0.00
Epoch 398 Iter 11 subLoss 10627.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0070 / 0.10508 / 17.12
Entropy seen (from low to high)
[16, 121, 210, 350, 591, 554, 468, 368, 351, 398, 338, 235, 198, 140, 114, 63, 72, 81, 49, 52, 36, 51, 31, 38, 30, 24, 21, 17, 25, 27, 16, 9, 7, 6, 8, 3, 5, 6, 1, 4, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 12, 27, 40, 64, 70, 95, 103, 123, 149, 141, 178, 197, 204, 201, 220, 179, 216, 197, 189, 181, 186, 182, 153, 143, 167, 156, 158, 151, 137, 120, 128, 112, 96, 76, 51, 25, 12, 6, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 0.0, 33.8, 37.0, 39.4, 44.1, 47.6, 50.6, 54.2, 57.6, 61.3, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 0.0, 66.6, 72.7, 33.3, 55.9, 76.1, 82.7, 75.8, 71.4, 84.2]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 3, 6, 11, 18, 25, 21, 29, 29, 21, 38]
Epoch 398 Acc: 91.44 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1062 train Loss: 8784.4 test Loss: 1794.3
Epoch 399 Iter 0 subLoss 7906.0 multi 3.98 import weight 0.00
Epoch 399 Iter 1 subLoss 4586.3 multi 18.91 import weight 0.00
Epoch 399 Iter 2 subLoss 2447.2 multi 1.00 import weight 0.00
Epoch 399 Iter 3 subLoss 2205.6 multi 3.99 import weight 0.00
Epoch 399 Iter 4 subLoss 1803.0 multi -4.97 import weight 0.00
Epoch 399 Iter 5 subLoss 2336.1 multi 3.98 import weight 0.00
Epoch 399 Iter 6 subLoss 2335.4 multi 6.97 import weight 0.00
Epoch 399 Iter 7 subLoss 1997.6 multi -4.97 import weight 0.00
Epoch 399 Iter 8 subLoss 2513.1 multi 21.90 import weight 0.00
Epoch 399 Iter 9 subLoss 2373.5 multi -13.93 import weight 0.00
Epoch 399 Iter 10 subLoss 2347.4 multi 9.96 import weight 0.00
Epoch 399 Iter 11 subLoss 2285.7 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0069 / 0.10517 / 15.70
Entropy seen (from low to high)
[16, 123, 212, 357, 589, 556, 468, 364, 363, 398, 333, 226, 202, 133, 114, 65, 71, 82, 47, 52, 35, 49, 33, 38, 29, 24, 21, 16, 25, 28, 15, 10, 7, 6, 9, 2, 5, 6, 1, 5, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 11, 27, 39, 63, 67, 98, 105, 117, 153, 136, 174, 209, 199, 201, 220, 184, 214, 194, 191, 179, 189, 178, 155, 142, 168, 157, 153, 150, 137, 122, 128, 114, 95, 77, 54, 26, 11, 7, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 0.0, 33.8, 36.3, 39.3, 44.0, 47.6, 50.6, 54.0, 57.5, 61.2, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 0.0, 57.1, 63.6, 35.2, 53.8, 73.6, 83.8, 74.0, 73.9, 83.7]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 2, 7, 11, 17, 26, 19, 31, 27, 23, 37]
Epoch 399 Acc: 97.98 BMA: 98.19 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 228 train Loss: 2559.3 test Loss: 350.1
Epoch 400 Iter 0 subLoss 2590.7 multi -31.84 import weight 0.00
Epoch 400 Iter 1 subLoss 10922.9 multi 3.99 import weight 0.00
Epoch 400 Iter 2 subLoss 2908.4 multi -4.97 import weight 0.00
Epoch 400 Iter 3 subLoss 3353.4 multi 12.94 import weight 0.00
Epoch 400 Iter 4 subLoss 2364.6 multi 6.97 import weight 0.00
Epoch 400 Iter 5 subLoss 2345.1 multi 12.94 import weight 0.00
Epoch 400 Iter 6 subLoss 1742.2 multi 1.00 import weight 0.00
Epoch 400 Iter 7 subLoss 1936.6 multi 1.00 import weight 0.00
Epoch 400 Iter 8 subLoss 2533.6 multi 9.96 import weight 0.00
Epoch 400 Iter 9 subLoss 1924.8 multi 6.97 import weight 0.00
Epoch 400 Iter 10 subLoss 2305.1 multi 12.94 import weight 0.00
Epoch 400 Iter 11 subLoss 1852.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0069 / 0.10532 / 16.05
Entropy seen (from low to high)
[16, 125, 215, 363, 595, 555, 469, 362, 368, 397, 329, 231, 193, 133, 108, 65, 73, 81, 44, 50, 39, 46, 35, 36, 27, 25, 21, 16, 26, 27, 14, 10, 8, 5, 9, 3, 4, 6, 2, 4, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 4, 10, 27, 40, 61, 68, 97, 104, 120, 146, 138, 175, 210, 194, 200, 221, 183, 213, 190, 198, 175, 192, 175, 157, 141, 168, 156, 154, 153, 135, 127, 125, 113, 96, 81, 55, 26, 12, 8, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 0.0, 33.8, 36.2, 39.5, 44.0, 47.5, 50.6, 54.0, 57.4, 61.1, 64.5, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 0.0, 57.1, 63.6, 31.2, 55.5, 73.6, 82.7, 74.9, 74.9, 83.3]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 2, 7, 11, 16, 27, 19, 29, 28, 24, 36]
Epoch 400 Acc: 98.52 BMA: 98.21 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 185 train Loss: 2163.2 test Loss: 254.9
Epoch 401 Iter 0 subLoss 2063.0 multi -4.97 import weight 0.00
Epoch 401 Iter 1 subLoss 2407.9 multi 1.00 import weight 0.00
Epoch 401 Iter 2 subLoss 2183.5 multi -16.91 import weight 0.00
Epoch 401 Iter 3 subLoss 2259.0 multi 6.97 import weight 0.00
Epoch 401 Iter 4 subLoss 2290.3 multi 3.99 import weight 0.00
Epoch 401 Iter 5 subLoss 2252.8 multi 9.96 import weight 0.00
Epoch 401 Iter 6 subLoss 2301.7 multi 12.94 import weight 0.00
Epoch 401 Iter 7 subLoss 1920.8 multi 9.96 import weight 0.00
Epoch 401 Iter 8 subLoss 2201.2 multi 6.97 import weight 0.00
Epoch 401 Iter 9 subLoss 1908.9 multi -1.99 import weight 0.00
Epoch 401 Iter 10 subLoss 1699.6 multi 1.00 import weight 0.00
Epoch 401 Iter 11 subLoss 1939.8 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0069 / 0.10548 / 15.34
Entropy seen (from low to high)
[16, 125, 218, 368, 599, 554, 472, 364, 363, 404, 321, 235, 184, 141, 99, 66, 79, 76, 41, 47, 42, 44, 37, 35, 27, 24, 21, 16, 27, 26, 14, 9, 9, 4, 9, 3, 4, 6, 2, 4, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 11, 26, 40, 61, 67, 95, 102, 121, 147, 137, 172, 206, 200, 192, 231, 185, 208, 183, 203, 181, 187, 173, 159, 139, 171, 154, 153, 148, 141, 126, 124, 118, 95, 86, 57, 25, 13, 8, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 0.0, 33.8, 36.9, 39.3, 44.0, 47.8, 50.8, 54.3, 57.6, 61.2, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 33.3, 39.9, 61.5, 31.2, 55.5, 78.9, 82.1, 74.9, 70.8, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 3, 5, 13, 16, 27, 19, 28, 28, 24, 35]
Epoch 401 Acc: 98.66 BMA: 98.21 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 193 train Loss: 2106.1 test Loss: 233.3
Epoch 402 Iter 0 subLoss 2154.5 multi -4.97 import weight 0.00
Epoch 402 Iter 1 subLoss 2290.2 multi 6.97 import weight 0.00
Epoch 402 Iter 2 subLoss 2119.2 multi 9.96 import weight 0.00
Epoch 402 Iter 3 subLoss 1915.8 multi -1.99 import weight 0.00
Epoch 402 Iter 4 subLoss 2000.4 multi -1.99 import weight 0.00
Epoch 402 Iter 5 subLoss 1961.9 multi 3.99 import weight 0.00
Epoch 402 Iter 6 subLoss 2210.7 multi 6.97 import weight 0.00
Epoch 402 Iter 7 subLoss 1826.6 multi 1.00 import weight 0.00
Epoch 402 Iter 8 subLoss 1904.0 multi 1.00 import weight 0.00
Epoch 402 Iter 9 subLoss 1813.6 multi 1.00 import weight 0.00
Epoch 402 Iter 10 subLoss 2191.1 multi 6.97 import weight 0.00
Epoch 402 Iter 11 subLoss 1999.0 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0068 / 0.10565 / 14.49
Entropy seen (from low to high)
[16, 127, 221, 369, 613, 553, 466, 365, 364, 403, 320, 239, 179, 137, 96, 64, 84, 71, 41, 47, 42, 42, 38, 34, 28, 23, 21, 15, 27, 27, 13, 9, 9, 5, 8, 3, 4, 6, 2, 4, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 12, 24, 42, 60, 69, 93, 101, 120, 148, 138, 167, 205, 206, 189, 223, 186, 207, 187, 204, 177, 188, 171, 160, 144, 163, 151, 155, 155, 141, 124, 126, 119, 99, 89, 53, 29, 13, 8, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.6, 0.0, 33.9, 36.8, 39.5, 43.9, 47.6, 50.7, 54.2, 57.6, 61.0, 64.5, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 33.3, 39.9, 49.9, 31.2, 57.6, 76.1, 81.4, 73.0, 74.0, 84.3]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 3, 5, 12, 16, 26, 21, 27, 26, 27, 32]
Epoch 402 Acc: 98.66 BMA: 98.21 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 199 train Loss: 2057.4 test Loss: 240.9
Epoch 403 Iter 0 subLoss 2236.5 multi 21.90 import weight 0.00
Epoch 403 Iter 1 subLoss 2118.9 multi 12.94 import weight 0.00
Epoch 403 Iter 2 subLoss 1855.1 multi 1.00 import weight 0.00
Epoch 403 Iter 3 subLoss 2243.1 multi -19.90 import weight 0.00
Epoch 403 Iter 4 subLoss 2182.9 multi -13.93 import weight 0.00
Epoch 403 Iter 5 subLoss 2268.4 multi -25.87 import weight 0.00
Epoch 403 Iter 6 subLoss 4139.5 multi 12.94 import weight 0.00
Epoch 403 Iter 7 subLoss 2320.5 multi -22.88 import weight 0.00
Epoch 403 Iter 8 subLoss 6201.4 multi 3.99 import weight 0.00
Epoch 403 Iter 9 subLoss 2843.1 multi 12.94 import weight 0.00
Epoch 403 Iter 10 subLoss 1852.7 multi 3.98 import weight 0.00
Epoch 403 Iter 11 subLoss 2276.5 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0068 / 0.10580 / 14.71
Entropy seen (from low to high)
[16, 129, 221, 381, 614, 549, 476, 358, 368, 406, 320, 230, 179, 132, 90, 66, 96, 59, 41, 45, 43, 44, 36, 33, 31, 22, 19, 15, 27, 26, 13, 9, 10, 4, 8, 3, 4, 6, 2, 4, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 11, 24, 42, 59, 69, 91, 104, 118, 149, 138, 161, 209, 201, 190, 224, 184, 205, 195, 199, 178, 191, 165, 166, 141, 165, 146, 158, 150, 150, 114, 131, 125, 97, 91, 53, 31, 13, 8, 8, 5, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 0.0, 33.9, 36.7, 39.6, 43.9, 47.6, 50.6, 54.1, 57.7, 61.1, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 33.3, 39.9, 58.3, 31.2, 51.9, 81.8, 80.7, 71.4, 76.9, 82.7]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 3, 5, 12, 16, 25, 22, 26, 28, 26, 29]
Epoch 403 Acc: 98.62 BMA: 98.23 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 227 train Loss: 2193.9 test Loss: 230.7
Epoch 404 Iter 0 subLoss 2147.4 multi 6.97 import weight 0.00
Epoch 404 Iter 1 subLoss 1523.9 multi 1.00 import weight 0.00
Epoch 404 Iter 2 subLoss 2288.5 multi -13.93 import weight 0.00
Epoch 404 Iter 3 subLoss 2183.4 multi -10.94 import weight 0.00
Epoch 404 Iter 4 subLoss 2498.6 multi -4.97 import weight 0.00
Epoch 404 Iter 5 subLoss 3080.0 multi 6.97 import weight 0.00
Epoch 404 Iter 6 subLoss 2509.4 multi -1.99 import weight 0.00
Epoch 404 Iter 7 subLoss 2437.6 multi 3.99 import weight 0.00
Epoch 404 Iter 8 subLoss 1964.6 multi 6.97 import weight 0.00
Epoch 404 Iter 9 subLoss 1758.3 multi -1.99 import weight 0.00
Epoch 404 Iter 10 subLoss 1919.5 multi -1.98 import weight 0.00
Epoch 404 Iter 11 subLoss 2349.9 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0068 / 0.10597 / 15.98
Entropy seen (from low to high)
[16, 132, 227, 385, 615, 555, 468, 360, 374, 400, 317, 232, 172, 137, 82, 65, 100, 57, 43, 39, 45, 45, 36, 30, 33, 20, 20, 15, 29, 24, 13, 9, 9, 4, 8, 3, 4, 7, 1, 4, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 10, 24, 42, 58, 68, 92, 103, 116, 143, 146, 159, 200, 211, 185, 222, 190, 205, 193, 197, 181, 186, 156, 183, 138, 162, 147, 155, 155, 145, 113, 141, 123, 91, 93, 59, 31, 15, 8, 7, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 0.0, 33.6, 36.3, 39.7, 44.1, 47.7, 50.6, 53.9, 57.5, 61.1, 64.6, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 49.9, 59.9, 58.3, 24.9, 54.1, 79.9, 85.7, 67.8, 73.0, 86.2]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 5, 12, 16, 24, 20, 28, 28, 26, 29]
Epoch 404 Acc: 98.72 BMA: 98.25 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 234 train Loss: 2100.4 test Loss: 229.4
Epoch 405 Iter 0 subLoss 1749.0 multi 3.99 import weight 0.00
Epoch 405 Iter 1 subLoss 2029.5 multi 1.00 import weight 0.00
Epoch 405 Iter 2 subLoss 1683.3 multi 1.00 import weight 0.00
Epoch 405 Iter 3 subLoss 2366.2 multi 9.96 import weight 0.00
Epoch 405 Iter 4 subLoss 1912.3 multi 1.00 import weight 0.00
Epoch 405 Iter 5 subLoss 2383.4 multi -1.98 import weight 0.00
Epoch 405 Iter 6 subLoss 2209.8 multi 6.97 import weight 0.00
Epoch 405 Iter 7 subLoss 1582.2 multi 1.00 import weight 0.00
Epoch 405 Iter 8 subLoss 1824.3 multi 1.00 import weight 0.00
Epoch 405 Iter 9 subLoss 1973.0 multi 6.97 import weight 0.00
Epoch 405 Iter 10 subLoss 2285.4 multi -10.94 import weight 0.00
Epoch 405 Iter 11 subLoss 2390.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0067 / 0.10616 / 14.45
Entropy seen (from low to high)
[17, 133, 227, 399, 615, 553, 474, 356, 381, 397, 312, 227, 170, 133, 84, 62, 103, 53, 45, 38, 48, 40, 38, 28, 32, 22, 19, 19, 25, 24, 12, 10, 9, 3, 8, 3, 4, 7, 1, 4, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 10, 23, 40, 60, 65, 90, 106, 115, 141, 149, 158, 197, 213, 180, 222, 188, 202, 203, 192, 175, 188, 165, 170, 145, 152, 154, 158, 157, 142, 119, 140, 121, 95, 92, 65, 31, 15, 7, 7, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 0.0, 33.6, 36.3, 39.8, 44.2, 47.7, 50.7, 53.8, 57.6, 61.2, 64.5, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 49.9, 59.9, 41.6, 31.2, 54.1, 84.2, 82.7, 67.8, 70.8, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 5, 12, 16, 24, 19, 29, 28, 24, 28]
Epoch 405 Acc: 98.75 BMA: 98.25 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 239 train Loss: 2001.5 test Loss: 222.1
Epoch 406 Iter 0 subLoss 1692.8 multi 1.00 import weight 0.00
Epoch 406 Iter 1 subLoss 2578.3 multi 6.97 import weight 0.00
Epoch 406 Iter 2 subLoss 1792.6 multi 3.98 import weight 0.00
Epoch 406 Iter 3 subLoss 1541.7 multi 1.00 import weight 0.00
Epoch 406 Iter 4 subLoss 1897.2 multi 1.00 import weight 0.00
Epoch 406 Iter 5 subLoss 2593.9 multi -28.85 import weight 0.00
Epoch 406 Iter 6 subLoss 2085.7 multi 9.96 import weight 0.00
Epoch 406 Iter 7 subLoss 1876.0 multi 1.00 import weight 0.00
Epoch 406 Iter 8 subLoss 2073.1 multi -4.97 import weight 0.00
Epoch 406 Iter 9 subLoss 2169.2 multi 3.98 import weight 0.00
Epoch 406 Iter 10 subLoss 1914.2 multi 3.99 import weight 0.00
Epoch 406 Iter 11 subLoss 1785.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0067 / 0.10635 / 15.48
Entropy seen (from low to high)
[17, 138, 229, 401, 621, 550, 476, 358, 381, 398, 305, 228, 166, 132, 81, 65, 100, 55, 43, 39, 48, 40, 34, 29, 34, 19, 22, 17, 25, 23, 13, 9, 9, 4, 7, 3, 4, 7, 1, 4, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 9, 21, 42, 54, 68, 89, 105, 118, 137, 151, 150, 198, 216, 181, 215, 196, 197, 201, 194, 178, 186, 164, 174, 137, 159, 149, 166, 149, 145, 122, 138, 118, 100, 95, 67, 33, 15, 7, 7, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 0.0, 33.6, 36.3, 40.0, 44.1, 47.6, 50.7, 53.8, 57.7, 61.2, 64.5, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 49.9, 59.9, 45.4, 23.5, 56.5, 84.9, 82.1, 68.9, 73.9, 82.1]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 5, 11, 17, 23, 20, 28, 29, 23, 28]
Epoch 406 Acc: 98.70 BMA: 98.23 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 178 train Loss: 2061.5 test Loss: 224.6
Epoch 407 Iter 0 subLoss 2426.1 multi -10.94 import weight 0.00
Epoch 407 Iter 1 subLoss 1980.0 multi 9.96 import weight 0.00
Epoch 407 Iter 2 subLoss 2095.6 multi -7.96 import weight 0.00
Epoch 407 Iter 3 subLoss 2205.9 multi 9.96 import weight 0.00
Epoch 407 Iter 4 subLoss 1966.8 multi 9.96 import weight 0.00
Epoch 407 Iter 5 subLoss 1872.4 multi 3.99 import weight 0.00
Epoch 407 Iter 6 subLoss 1897.5 multi 3.98 import weight 0.00
Epoch 407 Iter 7 subLoss 1901.2 multi -1.99 import weight 0.00
Epoch 407 Iter 8 subLoss 2561.2 multi 1.00 import weight 0.00
Epoch 407 Iter 9 subLoss 2137.8 multi -10.94 import weight 0.00
Epoch 407 Iter 10 subLoss 2254.0 multi 9.96 import weight 0.00
Epoch 407 Iter 11 subLoss 1652.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0067 / 0.10654 / 14.85
Entropy seen (from low to high)
[18, 138, 231, 418, 622, 544, 475, 357, 390, 392, 307, 217, 167, 133, 78, 63, 97, 56, 45, 37, 48, 39, 35, 29, 32, 20, 21, 19, 23, 23, 13, 9, 10, 3, 7, 3, 6, 5, 1, 4, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 8, 21, 43, 53, 67, 88, 103, 122, 132, 151, 142, 207, 208, 188, 216, 187, 202, 202, 190, 180, 186, 164, 170, 140, 165, 145, 168, 149, 140, 127, 132, 122, 98, 105, 65, 34, 16, 8, 7, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 0.0, 33.6, 36.3, 39.7, 44.1, 47.6, 50.8, 53.9, 57.8, 61.2, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 66.6, 49.9, 49.9, 49.9, 29.4, 56.5, 84.9, 82.1, 67.8, 73.9, 82.7]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 4, 12, 17, 23, 20, 28, 28, 23, 29]
Epoch 407 Acc: 98.64 BMA: 98.25 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 165 train Loss: 1986.9 test Loss: 225.8
Epoch 408 Iter 0 subLoss 1622.7 multi 1.00 import weight 0.00
Epoch 408 Iter 1 subLoss 2017.4 multi 1.00 import weight 0.00
Epoch 408 Iter 2 subLoss 1637.4 multi 1.00 import weight 0.00
Epoch 408 Iter 3 subLoss 2313.2 multi 6.97 import weight 0.00
Epoch 408 Iter 4 subLoss 1948.7 multi -7.96 import weight 0.00
Epoch 408 Iter 5 subLoss 2379.9 multi -16.91 import weight 0.00
Epoch 408 Iter 6 subLoss 2611.8 multi -1.99 import weight 0.00
Epoch 408 Iter 7 subLoss 2916.5 multi -4.97 import weight 0.00
Epoch 408 Iter 8 subLoss 3999.1 multi 15.93 import weight 0.00
Epoch 408 Iter 9 subLoss 2632.3 multi -1.99 import weight 0.00
Epoch 408 Iter 10 subLoss 2856.3 multi -25.87 import weight 0.00
Epoch 408 Iter 11 subLoss 13169.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0067 / 0.10640 / 14.37
Entropy seen (from low to high)
[18, 140, 230, 432, 628, 534, 476, 359, 387, 387, 306, 212, 170, 129, 79, 65, 94, 56, 46, 37, 45, 38, 38, 29, 33, 20, 22, 17, 24, 24, 11, 10, 10, 3, 7, 2, 7, 5, 2, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 9, 20, 43, 56, 63, 89, 104, 117, 137, 151, 148, 196, 212, 191, 215, 188, 198, 208, 195, 173, 181, 171, 175, 140, 165, 153, 165, 142, 153, 117, 133, 124, 93, 96, 66, 33, 17, 8, 7, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.3, 0.0, 32.8, 35.6, 39.7, 44.2, 47.7, 50.9, 53.8, 57.8, 61.1, 64.5, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 49.9, 59.9, 45.4, 33.3, 62.4, 72.2, 86.2, 70.3, 67.9, 86.2]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 4, 5, 11, 18, 24, 18, 29, 27, 25, 29]
Epoch 408 Acc: 94.08 BMA: 98.27 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1316 train Loss: 8977.2 test Loss: 1289.3
Epoch 409 Iter 0 subLoss 9547.3 multi 6.97 import weight 0.00
Epoch 409 Iter 1 subLoss 2125.8 multi 1.00 import weight 0.00
Epoch 409 Iter 2 subLoss 2388.0 multi -1.99 import weight 0.00
Epoch 409 Iter 3 subLoss 2484.6 multi -1.98 import weight 0.00
Epoch 409 Iter 4 subLoss 2313.1 multi 9.96 import weight 0.00
Epoch 409 Iter 5 subLoss 1901.7 multi 1.00 import weight 0.00
Epoch 409 Iter 6 subLoss 2276.4 multi 21.90 import weight 0.00
Epoch 409 Iter 7 subLoss 1862.7 multi -7.96 import weight 0.00
Epoch 409 Iter 8 subLoss 2110.9 multi 15.93 import weight 0.00
Epoch 409 Iter 9 subLoss 2075.6 multi -1.98 import weight 0.00
Epoch 409 Iter 10 subLoss 2384.1 multi 1.00 import weight 0.00
Epoch 409 Iter 11 subLoss 2260.3 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0067 / 0.10656 / 15.84
Entropy seen (from low to high)
[18, 142, 233, 439, 627, 537, 481, 350, 397, 383, 299, 215, 167, 124, 78, 67, 92, 54, 46, 39, 45, 38, 37, 29, 32, 19, 23, 14, 28, 21, 14, 8, 10, 4, 6, 2, 7, 5, 2, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 9, 20, 41, 58, 59, 93, 101, 117, 135, 147, 153, 190, 215, 192, 211, 190, 198, 202, 199, 173, 186, 170, 172, 142, 160, 156, 163, 144, 154, 118, 136, 118, 99, 99, 61, 37, 19, 8, 7, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.2, 0.0, 32.9, 35.6, 39.8, 44.3, 47.5, 50.9, 53.9, 57.9, 61.1, 64.5, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 49.9, 59.9, 49.9, 23.5, 62.9, 76.4, 86.2, 70.3, 67.9, 86.2]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 4, 5, 10, 17, 27, 17, 29, 27, 25, 29]
Epoch 409 Acc: 98.54 BMA: 98.27 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 226 train Loss: 2454.7 test Loss: 256.3
Epoch 410 Iter 0 subLoss 2798.6 multi 24.88 import weight 0.00
Epoch 410 Iter 1 subLoss 2395.0 multi 1.00 import weight 0.00
Epoch 410 Iter 2 subLoss 2435.4 multi 3.98 import weight 0.00
Epoch 410 Iter 3 subLoss 1661.0 multi 1.00 import weight 0.00
Epoch 410 Iter 4 subLoss 2417.9 multi 9.96 import weight 0.00
Epoch 410 Iter 5 subLoss 1898.8 multi 6.97 import weight 0.00
Epoch 410 Iter 6 subLoss 1851.7 multi 6.97 import weight 0.00
Epoch 410 Iter 7 subLoss 1953.4 multi -4.97 import weight 0.00
Epoch 410 Iter 8 subLoss 1899.6 multi 9.96 import weight 0.00
Epoch 410 Iter 9 subLoss 2249.3 multi -16.91 import weight 0.00
Epoch 410 Iter 10 subLoss 2521.3 multi -10.94 import weight 0.00
Epoch 410 Iter 11 subLoss 1999.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0066 / 0.10670 / 15.79
Entropy seen (from low to high)
[18, 144, 236, 445, 630, 536, 483, 346, 402, 387, 291, 214, 165, 116, 81, 65, 93, 53, 46, 36, 50, 35, 38, 26, 34, 20, 22, 14, 27, 21, 14, 9, 9, 5, 5, 4, 5, 5, 2, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 8, 20, 41, 57, 61, 85, 107, 118, 134, 140, 159, 188, 217, 190, 204, 192, 201, 200, 199, 175, 184, 169, 170, 147, 160, 152, 171, 136, 157, 123, 132, 122, 99, 99, 63, 38, 18, 9, 7, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.1, 0.0, 33.0, 35.5, 39.4, 44.0, 47.5, 51.0, 53.9, 57.8, 61.1, 64.4, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 49.9, 49.9, 54.5, 23.5, 59.2, 87.4, 82.7, 67.8, 70.8, 86.6]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 4, 4, 11, 17, 27, 16, 29, 28, 24, 30]
Epoch 410 Acc: 98.35 BMA: 98.29 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 199 train Loss: 2531.9 test Loss: 276.9
Epoch 411 Iter 0 subLoss 2229.1 multi -13.93 import weight 0.00
Epoch 411 Iter 1 subLoss 3224.7 multi 12.94 import weight 0.00
Epoch 411 Iter 2 subLoss 2043.2 multi -4.97 import weight 0.00
Epoch 411 Iter 3 subLoss 2400.2 multi -1.98 import weight 0.00
Epoch 411 Iter 4 subLoss 2061.7 multi -1.99 import weight 0.00
Epoch 411 Iter 5 subLoss 2073.0 multi -1.99 import weight 0.00
Epoch 411 Iter 6 subLoss 2871.7 multi 15.93 import weight 0.00
Epoch 411 Iter 7 subLoss 1963.2 multi 9.96 import weight 0.00
Epoch 411 Iter 8 subLoss 2179.0 multi 1.00 import weight 0.00
Epoch 411 Iter 9 subLoss 1733.1 multi 1.00 import weight 0.00
Epoch 411 Iter 10 subLoss 2173.8 multi 3.99 import weight 0.00
Epoch 411 Iter 11 subLoss 1903.3 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0066 / 0.10686 / 15.16
Entropy seen (from low to high)
[18, 146, 241, 450, 641, 533, 475, 348, 403, 394, 274, 217, 165, 113, 81, 65, 91, 54, 46, 32, 51, 36, 36, 26, 36, 19, 21, 16, 25, 21, 15, 8, 9, 6, 4, 3, 6, 5, 2, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 8, 20, 41, 58, 61, 83, 107, 113, 137, 141, 157, 179, 220, 195, 206, 191, 197, 202, 194, 179, 181, 175, 167, 148, 158, 155, 163, 141, 154, 128, 131, 124, 101, 101, 61, 39, 20, 9, 7, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.0, 0.0, 33.6, 35.8, 39.5, 44.0, 47.4, 51.0, 53.8, 57.9, 61.1, 64.5, 68.6]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 66.6, 49.9, 49.9, 27.7, 59.9, 83.3, 78.5, 69.9, 73.9, 86.6]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 3, 4, 10, 18, 25, 18, 28, 30, 23, 30]
Epoch 411 Acc: 98.77 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 190 train Loss: 2089.0 test Loss: 221.0
Epoch 412 Iter 0 subLoss 1970.8 multi 6.97 import weight 0.00
Epoch 412 Iter 1 subLoss 1753.2 multi -1.98 import weight 0.00
Epoch 412 Iter 2 subLoss 2143.9 multi 6.97 import weight 0.00
Epoch 412 Iter 3 subLoss 1959.5 multi -1.98 import weight 0.00
Epoch 412 Iter 4 subLoss 2538.2 multi 9.96 import weight 0.00
Epoch 412 Iter 5 subLoss 1942.3 multi -4.97 import weight 0.00
Epoch 412 Iter 6 subLoss 2106.5 multi 1.00 import weight 0.00
Epoch 412 Iter 7 subLoss 1949.1 multi -1.99 import weight 0.00
Epoch 412 Iter 8 subLoss 2482.2 multi 1.00 import weight 0.00
Epoch 412 Iter 9 subLoss 1784.3 multi 6.97 import weight 0.00
Epoch 412 Iter 10 subLoss 2085.3 multi 3.99 import weight 0.00
Epoch 412 Iter 11 subLoss 1599.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0066 / 0.10702 / 13.96
Entropy seen (from low to high)
[18, 146, 244, 465, 637, 535, 470, 347, 417, 385, 268, 222, 159, 112, 75, 67, 91, 54, 48, 30, 50, 36, 34, 27, 35, 20, 20, 17, 24, 21, 16, 8, 8, 6, 4, 3, 6, 6, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 8, 20, 42, 51, 64, 86, 105, 107, 141, 139, 152, 184, 214, 200, 204, 199, 192, 203, 192, 175, 188, 169, 172, 140, 162, 158, 160, 144, 150, 131, 131, 125, 100, 106, 61, 37, 24, 9, 7, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.9, 0.0, 33.6, 36.5, 40.0, 43.9, 47.3, 50.9, 53.9, 57.9, 61.2, 64.6, 68.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 49.9, 66.6, 44.4, 33.3, 57.6, 83.3, 78.5, 69.9, 73.9, 86.2]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 3, 9, 18, 26, 18, 28, 30, 23, 29]
Epoch 412 Acc: 98.77 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 159 train Loss: 2069.6 test Loss: 214.5
Epoch 413 Iter 0 subLoss 1994.1 multi 3.99 import weight 0.00
Epoch 413 Iter 1 subLoss 2057.1 multi -1.99 import weight 0.00
Epoch 413 Iter 2 subLoss 2204.4 multi 12.94 import weight 0.00
Epoch 413 Iter 3 subLoss 1876.8 multi 3.98 import weight 0.00
Epoch 413 Iter 4 subLoss 1965.6 multi 9.96 import weight 0.00
Epoch 413 Iter 5 subLoss 1833.5 multi -4.97 import weight 0.00
Epoch 413 Iter 6 subLoss 2354.2 multi -16.91 import weight 0.00
Epoch 413 Iter 7 subLoss 1972.6 multi 6.97 import weight 0.00
Epoch 413 Iter 8 subLoss 2303.7 multi 12.94 import weight 0.00
Epoch 413 Iter 9 subLoss 1965.0 multi 12.94 import weight 0.00
Epoch 413 Iter 10 subLoss 1928.0 multi 1.00 import weight 0.00
Epoch 413 Iter 11 subLoss 2046.8 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0065 / 0.10720 / 14.89
Entropy seen (from low to high)
[18, 147, 247, 473, 641, 542, 462, 347, 425, 380, 256, 232, 147, 113, 75, 65, 90, 55, 47, 31, 52, 32, 34, 27, 35, 19, 20, 18, 25, 19, 17, 7, 8, 6, 4, 3, 6, 6, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 8, 20, 41, 50, 65, 83, 105, 103, 144, 140, 144, 187, 215, 200, 204, 199, 193, 209, 178, 176, 188, 172, 173, 145, 156, 162, 154, 150, 144, 141, 124, 126, 105, 106, 63, 38, 23, 11, 6, 7, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 0.0, 33.6, 36.5, 40.2, 44.0, 47.3, 50.9, 53.8, 58.0, 61.2, 64.7, 68.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 74.9, 66.6, 44.4, 29.4, 55.5, 88.2, 79.3, 69.9, 73.9, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 3, 9, 17, 27, 17, 29, 30, 23, 28]
Epoch 413 Acc: 98.70 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 204 train Loss: 1976.4 test Loss: 218.0
Epoch 414 Iter 0 subLoss 1534.2 multi -1.99 import weight 0.00
Epoch 414 Iter 1 subLoss 2270.0 multi 21.90 import weight 0.00
Epoch 414 Iter 2 subLoss 2114.1 multi 15.93 import weight 0.00
Epoch 414 Iter 3 subLoss 1689.3 multi 3.99 import weight 0.00
Epoch 414 Iter 4 subLoss 1644.3 multi -4.97 import weight 0.00
Epoch 414 Iter 5 subLoss 2046.6 multi 1.00 import weight 0.00
Epoch 414 Iter 6 subLoss 1825.5 multi 3.99 import weight 0.00
Epoch 414 Iter 7 subLoss 1765.8 multi -1.98 import weight 0.00
Epoch 414 Iter 8 subLoss 2334.4 multi 6.97 import weight 0.00
Epoch 414 Iter 9 subLoss 2007.6 multi -7.96 import weight 0.00
Epoch 414 Iter 10 subLoss 2148.4 multi 9.96 import weight 0.00
Epoch 414 Iter 11 subLoss 2020.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0065 / 0.10736 / 15.65
Entropy seen (from low to high)
[18, 151, 247, 486, 639, 543, 460, 346, 427, 380, 255, 226, 144, 111, 74, 69, 91, 48, 50, 36, 45, 34, 33, 29, 32, 19, 20, 17, 26, 18, 17, 7, 8, 6, 4, 3, 6, 6, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 7, 21, 38, 52, 66, 81, 106, 102, 138, 147, 139, 187, 212, 202, 201, 202, 186, 213, 180, 178, 187, 172, 165, 151, 157, 166, 147, 152, 148, 136, 130, 126, 105, 108, 64, 37, 24, 12, 5, 8, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 0.0, 33.5, 36.6, 40.3, 44.1, 47.3, 51.1, 54.1, 58.1, 61.4, 64.7, 68.6]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 74.9, 66.6, 33.3, 29.4, 57.1, 93.7, 73.3, 72.4, 77.2, 85.1]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 3, 9, 17, 28, 16, 30, 29, 22, 27]
Epoch 414 Acc: 98.87 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 202 train Loss: 2046.5 test Loss: 194.7
Epoch 415 Iter 0 subLoss 2011.1 multi 1.00 import weight 0.00
Epoch 415 Iter 1 subLoss 2093.3 multi -7.96 import weight 0.00
Epoch 415 Iter 2 subLoss 2127.0 multi -1.99 import weight 0.00
Epoch 415 Iter 3 subLoss 1957.8 multi -4.97 import weight 0.00
Epoch 415 Iter 4 subLoss 2219.4 multi 1.00 import weight 0.00
Epoch 415 Iter 5 subLoss 2522.2 multi -7.96 import weight 0.00
Epoch 415 Iter 6 subLoss 2296.3 multi 3.99 import weight 0.00
Epoch 415 Iter 7 subLoss 2019.7 multi 3.99 import weight 0.00
Epoch 415 Iter 8 subLoss 2007.3 multi -4.97 import weight 0.00
Epoch 415 Iter 9 subLoss 2660.6 multi 6.97 import weight 0.00
Epoch 415 Iter 10 subLoss 2150.9 multi -10.94 import weight 0.00
Epoch 415 Iter 11 subLoss 2392.8 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0065 / 0.10750 / 14.38
Entropy seen (from low to high)
[18, 153, 248, 493, 645, 538, 463, 347, 435, 369, 257, 222, 141, 114, 71, 68, 87, 47, 51, 35, 46, 37, 27, 33, 28, 23, 17, 17, 27, 17, 18, 7, 7, 6, 4, 5, 4, 6, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 7, 20, 39, 51, 63, 85, 103, 102, 139, 148, 133, 185, 214, 205, 192, 207, 182, 219, 173, 185, 184, 176, 163, 158, 146, 168, 148, 156, 142, 141, 127, 130, 108, 103, 68, 39, 24, 12, 5, 8, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 0.0, 33.5, 36.7, 39.7, 43.9, 47.4, 51.1, 54.1, 58.0, 61.2, 64.8, 68.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 74.9, 49.9, 39.9, 35.2, 59.2, 88.2, 74.0, 71.8, 77.2, 84.6]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 2, 10, 17, 27, 17, 27, 32, 22, 26]
Epoch 415 Acc: 98.35 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 239 train Loss: 2335.8 test Loss: 253.2
Epoch 416 Iter 0 subLoss 2040.4 multi 3.99 import weight 0.00
Epoch 416 Iter 1 subLoss 2028.9 multi -1.99 import weight 0.00
Epoch 416 Iter 2 subLoss 2556.9 multi -7.96 import weight 0.00
Epoch 416 Iter 3 subLoss 2912.7 multi -1.99 import weight 0.00
Epoch 416 Iter 4 subLoss 2616.9 multi 1.00 import weight 0.00
Epoch 416 Iter 5 subLoss 2656.9 multi 3.98 import weight 0.00
Epoch 416 Iter 6 subLoss 2307.4 multi 12.94 import weight 0.00
Epoch 416 Iter 7 subLoss 1480.9 multi 1.00 import weight 0.00
Epoch 416 Iter 8 subLoss 2133.9 multi -13.93 import weight 0.00
Epoch 416 Iter 9 subLoss 1755.7 multi 1.00 import weight 0.00
Epoch 416 Iter 10 subLoss 2342.4 multi 15.93 import weight 0.00
Epoch 416 Iter 11 subLoss 2318.4 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0065 / 0.10766 / 14.21
Entropy seen (from low to high)
[18, 155, 250, 504, 647, 531, 462, 360, 427, 367, 261, 212, 145, 110, 68, 68, 88, 45, 49, 41, 42, 36, 27, 33, 29, 24, 14, 21, 25, 15, 18, 8, 6, 6, 4, 5, 4, 6, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 7, 20, 39, 49, 63, 87, 99, 102, 139, 148, 131, 187, 210, 208, 184, 218, 177, 219, 175, 180, 179, 187, 160, 155, 145, 167, 153, 157, 144, 134, 135, 126, 111, 103, 68, 43, 24, 12, 3, 10, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.9, 0.0, 33.4, 36.8, 39.8, 44.0, 47.4, 51.1, 54.1, 58.1, 61.2, 64.6, 68.6]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 74.9, 49.9, 39.9, 35.2, 57.6, 88.8, 70.3, 74.1, 76.1, 85.1]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 4, 2, 10, 17, 26, 18, 27, 31, 21, 27]
Epoch 416 Acc: 98.93 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 231 train Loss: 1995.5 test Loss: 184.5
Epoch 417 Iter 0 subLoss 1620.9 multi 3.99 import weight 0.00
Epoch 417 Iter 1 subLoss 1871.1 multi 6.97 import weight 0.00
Epoch 417 Iter 2 subLoss 2363.6 multi 9.96 import weight 0.00
Epoch 417 Iter 3 subLoss 1785.2 multi 9.96 import weight 0.00
Epoch 417 Iter 4 subLoss 1766.0 multi -1.99 import weight 0.00
Epoch 417 Iter 5 subLoss 1991.3 multi 6.97 import weight 0.00
Epoch 417 Iter 6 subLoss 1367.2 multi 1.00 import weight 0.00
Epoch 417 Iter 7 subLoss 2276.7 multi 24.88 import weight 0.00
Epoch 417 Iter 8 subLoss 2133.7 multi -10.94 import weight 0.00
Epoch 417 Iter 9 subLoss 2013.4 multi 3.99 import weight 0.00
Epoch 417 Iter 10 subLoss 1961.2 multi 12.94 import weight 0.00
Epoch 417 Iter 11 subLoss 1990.8 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10782 / 14.43
Entropy seen (from low to high)
[19, 159, 250, 512, 652, 534, 448, 370, 426, 367, 249, 215, 144, 108, 66, 74, 85, 42, 51, 40, 43, 34, 28, 31, 30, 22, 15, 22, 23, 15, 18, 9, 5, 6, 4, 5, 4, 6, 1, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 6, 21, 39, 49, 62, 86, 99, 100, 139, 139, 137, 184, 213, 207, 188, 218, 174, 219, 171, 181, 183, 184, 161, 155, 147, 169, 146, 150, 152, 135, 133, 129, 112, 105, 71, 44, 24, 13, 3, 10, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 31.3, 34.4, 37.0, 39.9, 44.2, 47.6, 51.1, 54.0, 58.0, 61.2, 64.5, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 49.9, 36.3, 37.4, 55.9, 88.8, 70.3, 71.8, 78.9, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 4, 2, 11, 16, 25, 18, 27, 32, 19, 28]
Epoch 417 Acc: 98.77 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 199 train Loss: 1957.4 test Loss: 208.9
Epoch 418 Iter 0 subLoss 2123.0 multi 1.00 import weight 0.00
Epoch 418 Iter 1 subLoss 1584.3 multi 3.99 import weight 0.00
Epoch 418 Iter 2 subLoss 2033.2 multi 3.98 import weight 0.00
Epoch 418 Iter 3 subLoss 1432.1 multi 1.00 import weight 0.00
Epoch 418 Iter 4 subLoss 1919.3 multi -1.98 import weight 0.00
Epoch 418 Iter 5 subLoss 2131.9 multi -10.94 import weight 0.00
Epoch 418 Iter 6 subLoss 1655.0 multi 1.00 import weight 0.00
Epoch 418 Iter 7 subLoss 1975.0 multi 3.99 import weight 0.00
Epoch 418 Iter 8 subLoss 1552.7 multi -1.99 import weight 0.00
Epoch 418 Iter 9 subLoss 1762.0 multi 1.00 import weight 0.00
Epoch 418 Iter 10 subLoss 1933.1 multi -1.98 import weight 0.00
Epoch 418 Iter 11 subLoss 1997.4 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10798 / 14.48
Entropy seen (from low to high)
[19, 163, 252, 521, 651, 535, 452, 363, 425, 367, 248, 210, 147, 103, 66, 77, 80, 48, 44, 43, 41, 36, 26, 31, 30, 21, 17, 23, 21, 14, 18, 9, 5, 6, 4, 5, 4, 6, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 6, 21, 40, 47, 63, 83, 98, 105, 134, 140, 138, 181, 210, 204, 192, 219, 171, 217, 173, 182, 185, 184, 161, 152, 149, 165, 146, 151, 155, 138, 133, 124, 120, 102, 73, 46, 23, 14, 3, 10, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 31.1, 34.3, 36.8, 39.5, 44.3, 47.5, 51.0, 54.0, 58.0, 61.1, 64.5, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 33.3, 36.3, 33.3, 51.9, 89.4, 65.3, 74.1, 80.9, 85.1]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 3, 11, 15, 25, 19, 26, 31, 21, 27]
Epoch 418 Acc: 98.75 BMA: 98.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 199 train Loss: 1957.5 test Loss: 206.8
Epoch 419 Iter 0 subLoss 1936.2 multi 1.00 import weight 0.00
Epoch 419 Iter 1 subLoss 1540.3 multi 1.00 import weight 0.00
Epoch 419 Iter 2 subLoss 1968.9 multi 15.93 import weight 0.00
Epoch 419 Iter 3 subLoss 2131.6 multi -7.96 import weight 0.00
Epoch 419 Iter 4 subLoss 1749.1 multi 3.98 import weight 0.00
Epoch 419 Iter 5 subLoss 1834.4 multi -4.97 import weight 0.00
Epoch 419 Iter 6 subLoss 1816.7 multi 3.98 import weight 0.00
Epoch 419 Iter 7 subLoss 1568.6 multi -1.99 import weight 0.00
Epoch 419 Iter 8 subLoss 2121.3 multi 3.99 import weight 0.00
Epoch 419 Iter 9 subLoss 1836.7 multi -1.99 import weight 0.00
Epoch 419 Iter 10 subLoss 1673.6 multi -4.97 import weight 0.00
Epoch 419 Iter 11 subLoss 1700.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10814 / 14.56
Entropy seen (from low to high)
[19, 166, 254, 524, 654, 539, 443, 371, 429, 364, 250, 198, 149, 103, 65, 78, 76, 48, 43, 44, 41, 34, 25, 33, 28, 22, 17, 21, 23, 14, 17, 9, 5, 6, 4, 6, 3, 6, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 6, 20, 39, 47, 65, 77, 100, 107, 132, 143, 134, 181, 210, 205, 189, 215, 173, 213, 179, 182, 183, 180, 163, 151, 154, 164, 145, 152, 157, 137, 132, 120, 125, 101, 75, 52, 23, 13, 4, 9, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.9, 30.9, 34.3, 37.0, 39.6, 44.3, 47.5, 51.0, 53.9, 57.8, 61.0, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 99.9, 33.3, 36.3, 35.7, 47.9, 89.4, 66.6, 73.5, 80.9, 83.9]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 3, 11, 14, 25, 19, 24, 34, 21, 25]
Epoch 419 Acc: 98.70 BMA: 98.33 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 170 train Loss: 1865.2 test Loss: 209.3
Epoch 420 Iter 0 subLoss 1784.7 multi 12.94 import weight 0.00
Epoch 420 Iter 1 subLoss 2222.7 multi -13.93 import weight 0.00
Epoch 420 Iter 2 subLoss 2177.8 multi 6.97 import weight 0.00
Epoch 420 Iter 3 subLoss 2232.6 multi 18.91 import weight 0.00
Epoch 420 Iter 4 subLoss 2103.5 multi 1.00 import weight 0.00
Epoch 420 Iter 5 subLoss 1922.5 multi 1.00 import weight 0.00
Epoch 420 Iter 6 subLoss 1510.5 multi 1.00 import weight 0.00
Epoch 420 Iter 7 subLoss 1768.5 multi 3.99 import weight 0.00
Epoch 420 Iter 8 subLoss 1478.6 multi 1.00 import weight 0.00
Epoch 420 Iter 9 subLoss 2145.7 multi 1.00 import weight 0.00
Epoch 420 Iter 10 subLoss 1804.0 multi -4.97 import weight 0.00
Epoch 420 Iter 11 subLoss 1905.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10831 / 15.28
Entropy seen (from low to high)
[19, 167, 258, 539, 648, 540, 445, 368, 437, 359, 246, 195, 146, 96, 66, 85, 70, 52, 39, 48, 37, 34, 24, 34, 26, 22, 17, 22, 22, 15, 16, 10, 4, 7, 3, 6, 3, 6, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 6, 19, 39, 46, 62, 80, 100, 107, 129, 139, 141, 178, 206, 209, 189, 213, 173, 215, 178, 183, 176, 182, 157, 158, 157, 158, 147, 154, 150, 147, 126, 129, 122, 96, 81, 55, 23, 14, 4, 8, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.9, 30.8, 33.4, 36.6, 39.6, 44.4, 47.4, 51.1, 54.0, 57.6, 60.9, 64.5, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 59.9, 37.4, 49.9, 84.2, 68.1, 73.5, 82.6, 86.3]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 4, 3, 10, 16, 24, 19, 22, 34, 23, 22]
Epoch 420 Acc: 98.77 BMA: 98.33 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 190 train Loss: 1860.1 test Loss: 189.9
Epoch 421 Iter 0 subLoss 2109.7 multi 3.99 import weight 0.00
Epoch 421 Iter 1 subLoss 1566.0 multi 1.00 import weight 0.00
Epoch 421 Iter 2 subLoss 1776.8 multi -13.93 import weight 0.00
Epoch 421 Iter 3 subLoss 1910.4 multi -1.99 import weight 0.00
Epoch 421 Iter 4 subLoss 1808.7 multi -1.99 import weight 0.00
Epoch 421 Iter 5 subLoss 1524.5 multi 1.00 import weight 0.00
Epoch 421 Iter 6 subLoss 1931.3 multi 1.00 import weight 0.00
Epoch 421 Iter 7 subLoss 1677.0 multi -1.98 import weight 0.00
Epoch 421 Iter 8 subLoss 2009.0 multi -10.94 import weight 0.00
Epoch 421 Iter 9 subLoss 1915.2 multi 1.00 import weight 0.00
Epoch 421 Iter 10 subLoss 2082.7 multi 6.97 import weight 0.00
Epoch 421 Iter 11 subLoss 1932.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10846 / 15.07
Entropy seen (from low to high)
[19, 167, 263, 547, 642, 544, 449, 374, 432, 355, 250, 192, 136, 94, 68, 94, 62, 50, 40, 47, 37, 34, 24, 37, 21, 25, 18, 20, 20, 15, 17, 9, 4, 8, 2, 6, 3, 6, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 38, 49, 62, 79, 99, 105, 131, 135, 147, 175, 202, 206, 186, 213, 183, 210, 179, 183, 176, 180, 153, 165, 151, 156, 155, 156, 147, 147, 120, 136, 123, 98, 81, 56, 25, 14, 3, 9, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.0, 30.6, 33.3, 36.8, 39.7, 44.4, 47.3, 51.1, 53.9, 57.6, 61.0, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 66.6, 43.7, 49.9, 84.2, 69.5, 71.4, 84.9, 86.9]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 4, 3, 9, 16, 24, 19, 23, 35, 20, 23]
Epoch 421 Acc: 98.79 BMA: 98.33 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 193 train Loss: 1954.4 test Loss: 187.5
Epoch 422 Iter 0 subLoss 2163.0 multi 3.99 import weight 0.00
Epoch 422 Iter 1 subLoss 2168.3 multi 6.97 import weight 0.00
Epoch 422 Iter 2 subLoss 1854.5 multi 9.96 import weight 0.00
Epoch 422 Iter 3 subLoss 2285.1 multi -16.91 import weight 0.00
Epoch 422 Iter 4 subLoss 1911.8 multi 3.99 import weight 0.00
Epoch 422 Iter 5 subLoss 1738.1 multi 3.99 import weight 0.00
Epoch 422 Iter 6 subLoss 1631.3 multi 1.00 import weight 0.00
Epoch 422 Iter 7 subLoss 1680.2 multi 1.00 import weight 0.00
Epoch 422 Iter 8 subLoss 1948.6 multi -10.94 import weight 0.00
Epoch 422 Iter 9 subLoss 1664.1 multi 1.00 import weight 0.00
Epoch 422 Iter 10 subLoss 1808.7 multi 1.00 import weight 0.00
Epoch 422 Iter 11 subLoss 1666.7 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10862 / 16.35
Entropy seen (from low to high)
[19, 170, 270, 552, 635, 561, 434, 379, 431, 352, 246, 191, 139, 88, 71, 93, 58, 53, 39, 49, 33, 34, 24, 37, 21, 24, 20, 19, 20, 15, 16, 9, 4, 9, 2, 6, 2, 6, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 36, 51, 60, 81, 94, 107, 129, 136, 140, 178, 202, 212, 179, 211, 179, 219, 173, 189, 172, 176, 161, 163, 148, 160, 149, 155, 152, 147, 122, 136, 126, 101, 81, 56, 26, 13, 4, 9, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.1, 30.4, 33.1, 37.0, 40.3, 44.7, 47.2, 50.9, 53.8, 57.8, 61.1, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 24.9, 57.1, 52.9, 42.8, 82.6, 72.7, 71.4, 84.9, 86.3]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 4, 4, 7, 17, 21, 23, 22, 35, 20, 22]
Epoch 422 Acc: 98.79 BMA: 98.33 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 166 train Loss: 1842.0 test Loss: 183.9
Epoch 423 Iter 0 subLoss 2082.9 multi 9.96 import weight 0.00
Epoch 423 Iter 1 subLoss 1648.0 multi -4.97 import weight 0.00
Epoch 423 Iter 2 subLoss 2253.2 multi 9.96 import weight 0.00
Epoch 423 Iter 3 subLoss 1770.2 multi -10.94 import weight 0.00
Epoch 423 Iter 4 subLoss 1807.3 multi 3.99 import weight 0.00
Epoch 423 Iter 5 subLoss 1925.4 multi -4.97 import weight 0.00
Epoch 423 Iter 6 subLoss 1768.1 multi 6.97 import weight 0.00
Epoch 423 Iter 7 subLoss 2180.0 multi -16.91 import weight 0.00
Epoch 423 Iter 8 subLoss 2641.7 multi -13.93 import weight 0.00
Epoch 423 Iter 9 subLoss 8995.9 multi 6.97 import weight 0.00
Epoch 423 Iter 10 subLoss 4692.4 multi 3.98 import weight 0.00
Epoch 423 Iter 11 subLoss 2679.2 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10865 / 13.72
Entropy seen (from low to high)
[19, 174, 268, 566, 633, 561, 427, 386, 437, 341, 245, 191, 135, 90, 70, 90, 59, 53, 33, 52, 32, 33, 29, 34, 21, 24, 19, 19, 20, 18, 14, 9, 4, 8, 3, 6, 3, 5, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 36, 51, 59, 78, 94, 110, 127, 134, 144, 174, 206, 211, 180, 210, 177, 220, 173, 190, 177, 175, 156, 165, 147, 164, 153, 148, 154, 139, 131, 128, 128, 98, 87, 57, 24, 14, 5, 8, 4, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.3, 30.2, 33.1, 36.5, 39.5, 44.3, 47.2, 50.9, 54.0, 57.8, 61.0, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 49.9, 49.9, 52.1, 73.9, 74.9, 72.9, 84.2, 86.3]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 4, 8, 16, 23, 23, 20, 37, 19, 22]
Epoch 423 Acc: 97.35 BMA: 98.35 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 267 train Loss: 3015.0 test Loss: 523.2
Epoch 424 Iter 0 subLoss 2438.7 multi 6.97 import weight 0.00
Epoch 424 Iter 1 subLoss 2481.3 multi 3.99 import weight 0.00
Epoch 424 Iter 2 subLoss 2387.0 multi 3.99 import weight 0.00
Epoch 424 Iter 3 subLoss 2005.6 multi -7.96 import weight 0.00
Epoch 424 Iter 4 subLoss 2045.0 multi 3.99 import weight 0.00
Epoch 424 Iter 5 subLoss 1998.8 multi 15.93 import weight 0.00
Epoch 424 Iter 6 subLoss 1916.3 multi 6.97 import weight 0.00
Epoch 424 Iter 7 subLoss 1732.6 multi 6.97 import weight 0.00
Epoch 424 Iter 8 subLoss 1820.0 multi -4.97 import weight 0.00
Epoch 424 Iter 9 subLoss 2189.1 multi -13.93 import weight 0.00
Epoch 424 Iter 10 subLoss 2130.6 multi -7.96 import weight 0.00
Epoch 424 Iter 11 subLoss 2498.8 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10875 / 15.02
Entropy seen (from low to high)
[19, 175, 273, 574, 625, 568, 425, 389, 435, 339, 243, 192, 126, 93, 69, 93, 55, 53, 34, 49, 35, 32, 28, 33, 22, 25, 18, 18, 22, 15, 16, 10, 2, 9, 4, 5, 4, 4, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 35, 53, 58, 79, 91, 107, 132, 134, 139, 181, 201, 214, 175, 210, 172, 225, 174, 191, 172, 170, 163, 167, 144, 158, 159, 151, 150, 143, 124, 132, 130, 98, 89, 59, 26, 13, 5, 8, 4, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.2, 30.3, 33.3, 37.1, 40.3, 44.8, 47.2, 50.8, 53.8, 57.6, 61.0, 64.6, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 74.9, 49.9, 49.9, 46.6, 45.4, 81.8, 72.7, 72.2, 84.9, 86.3]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 4, 4, 8, 15, 22, 22, 22, 36, 20, 22]
Epoch 424 Acc: 97.61 BMA: 98.35 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 249 train Loss: 4697.4 test Loss: 390.7
Epoch 425 Iter 0 subLoss 4314.6 multi -19.90 import weight 0.00
Epoch 425 Iter 1 subLoss 90744.7 multi 1.00 import weight 0.00
Epoch 425 Iter 2 subLoss 24109.2 multi 1.00 import weight 0.00
Epoch 425 Iter 3 subLoss 16360.5 multi 3.99 import weight 0.00
Epoch 425 Iter 4 subLoss 6856.1 multi 1.00 import weight 0.00
Epoch 425 Iter 5 subLoss 6900.6 multi 15.93 import weight 0.00
Epoch 425 Iter 6 subLoss 2874.7 multi 18.91 import weight 0.00
Epoch 425 Iter 7 subLoss 2491.7 multi -7.96 import weight 0.00
Epoch 425 Iter 8 subLoss 2935.0 multi 21.90 import weight 1.00
Epoch 425 Iter 9 subLoss 2898.9 multi 21.90 import weight 0.00
Epoch 425 Iter 10 subLoss 2316.6 multi 9.96 import weight 0.00
Epoch 425 Iter 11 subLoss 2105.3 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10888 / 16.45
Entropy seen (from low to high)
[19, 180, 271, 585, 624, 568, 425, 388, 439, 335, 244, 181, 132, 88, 70, 90, 56, 52, 35, 50, 34, 32, 27, 33, 22, 24, 19, 19, 20, 16, 15, 10, 2, 10, 3, 5, 4, 4, 2, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 33, 54, 56, 81, 92, 105, 132, 133, 140, 174, 206, 206, 180, 211, 173, 220, 179, 190, 173, 167, 166, 166, 143, 154, 162, 154, 143, 144, 126, 130, 134, 99, 94, 57, 27, 14, 6, 8, 4, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.2, 30.7, 33.5, 36.7, 39.5, 44.4, 47.2, 50.6, 53.8, 57.7, 61.1, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 74.9, 37.4, 49.9, 39.9, 83.3, 72.7, 72.2, 84.9, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 4, 8, 16, 20, 24, 22, 36, 20, 21]
Epoch 425 Acc: 98.79 BMA: 98.35 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 210 train Loss: 2017.0 test Loss: 226.6
Epoch 426 Iter 0 subLoss 2013.5 multi 1.00 import weight 0.00
Epoch 426 Iter 1 subLoss 1953.3 multi -4.97 import weight 0.00
Epoch 426 Iter 2 subLoss 1974.7 multi 3.99 import weight 0.00
Epoch 426 Iter 3 subLoss 1999.0 multi 18.91 import weight 0.00
Epoch 426 Iter 4 subLoss 2163.1 multi 9.96 import weight 0.00
Epoch 426 Iter 5 subLoss 2015.7 multi 3.99 import weight 0.00
Epoch 426 Iter 6 subLoss 2034.3 multi 6.97 import weight 0.00
Epoch 426 Iter 7 subLoss 1838.9 multi 1.00 import weight 0.00
Epoch 426 Iter 8 subLoss 1980.4 multi -25.87 import weight 0.00
Epoch 426 Iter 9 subLoss 1862.5 multi -10.94 import weight 0.00
Epoch 426 Iter 10 subLoss 2899.9 multi 24.88 import weight 0.00
Epoch 426 Iter 11 subLoss 2550.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10898 / 14.43
Entropy seen (from low to high)
[19, 183, 271, 592, 629, 562, 421, 396, 438, 328, 241, 183, 129, 88, 70, 91, 56, 50, 33, 52, 32, 33, 29, 34, 22, 21, 21, 20, 18, 16, 15, 10, 2, 9, 3, 6, 4, 4, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 33, 53, 58, 80, 90, 105, 137, 131, 135, 180, 201, 203, 186, 204, 176, 219, 177, 194, 170, 168, 160, 167, 151, 153, 154, 159, 146, 139, 122, 138, 128, 105, 91, 62, 29, 15, 5, 8, 4, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.1, 31.0, 33.7, 36.7, 40.0, 44.8, 47.0, 50.6, 54.0, 57.8, 61.1, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 59.9, 37.4, 46.1, 49.9, 82.6, 72.7, 71.4, 84.9, 85.7]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 5, 8, 13, 24, 23, 22, 35, 20, 21]
Epoch 426 Acc: 96.36 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 255 train Loss: 3886.9 test Loss: 532.3
Epoch 427 Iter 0 subLoss 3524.3 multi -13.93 import weight 0.00
Epoch 427 Iter 1 subLoss 15792.6 multi 3.99 import weight 0.00
Epoch 427 Iter 2 subLoss 3574.1 multi 24.88 import weight 0.00
Epoch 427 Iter 3 subLoss 2770.4 multi -37.81 import weight 0.00
Epoch 427 Iter 4 subLoss 28949.2 multi 3.99 import weight 0.00
Epoch 427 Iter 5 subLoss 4422.4 multi -1.99 import weight 0.00
Epoch 427 Iter 6 subLoss 4928.8 multi 15.93 import weight 0.00
Epoch 427 Iter 7 subLoss 2561.9 multi -1.99 import weight 0.00
Epoch 427 Iter 8 subLoss 3402.8 multi -1.99 import weight 0.00
Epoch 427 Iter 9 subLoss 3054.2 multi 3.99 import weight 0.00
Epoch 427 Iter 10 subLoss 2564.8 multi 1.00 import weight 0.00
Epoch 427 Iter 11 subLoss 2586.1 multi 27.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10911 / 16.32
Entropy seen (from low to high)
[20, 182, 276, 598, 631, 561, 420, 394, 441, 329, 235, 186, 127, 84, 71, 88, 58, 46, 36, 49, 33, 35, 26, 36, 19, 23, 20, 21, 17, 18, 14, 9, 3, 8, 3, 6, 4, 4, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 33, 53, 57, 78, 91, 107, 135, 128, 135, 181, 195, 204, 189, 208, 172, 218, 177, 190, 175, 171, 163, 156, 151, 158, 157, 155, 144, 143, 123, 137, 129, 103, 93, 64, 31, 13, 7, 8, 4, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.3, 31.0, 33.9, 36.7, 39.5, 44.3, 46.8, 50.5, 54.0, 58.0, 61.2, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 74.9, 24.9, 53.8, 45.8, 83.3, 69.5, 73.5, 84.9, 84.9]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 4, 8, 13, 24, 24, 23, 34, 20, 20]
Epoch 427 Acc: 98.23 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 27.87 Pidx 258 train Loss: 2581.0 test Loss: 282.3
Epoch 428 Iter 0 subLoss 2464.0 multi -1.99 import weight 0.00
Epoch 428 Iter 1 subLoss 2420.4 multi -10.94 import weight 0.00
Epoch 428 Iter 2 subLoss 3698.0 multi 9.96 import weight 0.00
Epoch 428 Iter 3 subLoss 2071.6 multi 1.00 import weight 0.00
Epoch 428 Iter 4 subLoss 1836.8 multi 3.98 import weight 0.00
Epoch 428 Iter 5 subLoss 1763.3 multi 9.96 import weight 0.00
Epoch 428 Iter 6 subLoss 2144.6 multi 1.00 import weight 0.00
Epoch 428 Iter 7 subLoss 2509.4 multi -4.97 import weight 0.00
Epoch 428 Iter 8 subLoss 1864.1 multi -7.96 import weight 0.00
Epoch 428 Iter 9 subLoss 2502.4 multi -1.98 import weight 0.00
Epoch 428 Iter 10 subLoss 2443.5 multi -4.97 import weight 0.00
Epoch 428 Iter 11 subLoss 2523.6 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10922 / 16.30
Entropy seen (from low to high)
[20, 184, 275, 607, 632, 562, 419, 395, 444, 326, 235, 179, 123, 83, 75, 85, 59, 48, 33, 48, 34, 35, 25, 35, 21, 22, 21, 20, 17, 18, 15, 8, 4, 7, 3, 6, 3, 5, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 17, 32, 54, 56, 79, 91, 109, 132, 125, 139, 177, 195, 201, 194, 212, 168, 217, 178, 188, 173, 162, 176, 153, 154, 154, 162, 153, 135, 150, 127, 134, 126, 108, 93, 64, 34, 12, 8, 7, 5, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.3, 30.9, 34.1, 36.8, 39.5, 44.3, 47.1, 50.6, 53.9, 58.0, 61.2, 64.8, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 74.9, 24.9, 49.9, 45.4, 83.9, 69.5, 73.5, 84.9, 84.9]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 4, 8, 14, 22, 25, 23, 34, 20, 20]
Epoch 428 Acc: 98.09 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 252 train Loss: 2502.4 test Loss: 328.7
Epoch 429 Iter 0 subLoss 1944.8 multi -7.96 import weight 0.00
Epoch 429 Iter 1 subLoss 2788.5 multi -4.97 import weight 0.00
Epoch 429 Iter 2 subLoss 3262.2 multi -1.99 import weight 0.00
Epoch 429 Iter 3 subLoss 3392.6 multi -1.99 import weight 0.00
Epoch 429 Iter 4 subLoss 4338.1 multi -1.98 import weight 0.00
Epoch 429 Iter 5 subLoss 5251.2 multi -4.97 import weight 0.00
Epoch 429 Iter 6 subLoss 13029.1 multi 1.00 import weight 0.00
Epoch 429 Iter 7 subLoss 6983.1 multi -16.91 import weight 0.00
Epoch 429 Iter 8 subLoss 232606.0 multi 1.00 import weight 0.00
Epoch 429 Iter 9 subLoss 27822.6 multi 3.99 import weight 0.00
Epoch 429 Iter 10 subLoss 12990.1 multi -1.99 import weight 0.00
Epoch 429 Iter 11 subLoss 24087.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10886 / 14.41
Entropy seen (from low to high)
[5, 88, 261, 610, 686, 589, 418, 397, 435, 341, 242, 182, 136, 85, 73, 86, 61, 50, 35, 50, 33, 35, 27, 32, 21, 26, 19, 19, 20, 16, 14, 12, 2, 8, 3, 6, 3, 5, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 17, 34, 52, 59, 77, 97, 105, 137, 127, 135, 184, 204, 190, 194, 212, 174, 210, 189, 183, 177, 176, 163, 160, 147, 154, 155, 153, 149, 149, 121, 133, 133, 101, 88, 57, 29, 14, 5, 9, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 30.8, 34.1, 36.8, 39.4, 44.4, 47.2, 50.6, 54.0, 58.1, 61.1, 64.7, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 74.9, 33.3, 46.1, 49.9, 77.7, 71.4, 73.5, 85.7, 80.9]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 4, 9, 13, 22, 27, 21, 34, 21, 21]
Epoch 429 Acc: 83.60 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2408 train Loss: 15833.6 test Loss: 2529.8
Epoch 430 Iter 0 subLoss 14895.2 multi -4.97 import weight 0.00
Epoch 430 Iter 1 subLoss 42658.1 multi 1.00 import weight 0.00
Epoch 430 Iter 2 subLoss 34721.7 multi 3.99 import weight 0.00
Epoch 430 Iter 3 subLoss 17432.2 multi 1.00 import weight 0.00
Epoch 430 Iter 4 subLoss 14179.1 multi 1.00 import weight 0.00
Epoch 430 Iter 5 subLoss 11999.1 multi -1.99 import weight 0.00
Epoch 430 Iter 6 subLoss 15243.6 multi -1.99 import weight 0.00
Epoch 430 Iter 7 subLoss 20244.5 multi 1.00 import weight 0.00
Epoch 430 Iter 8 subLoss 17891.3 multi 6.97 import weight 0.00
Epoch 430 Iter 9 subLoss 6234.7 multi 3.99 import weight 0.00
Epoch 430 Iter 10 subLoss 4977.9 multi 6.97 import weight 0.00
Epoch 430 Iter 11 subLoss 3407.3 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10883 / 15.43
Entropy seen (from low to high)
[5, 88, 266, 619, 687, 583, 416, 402, 436, 335, 241, 181, 130, 85, 77, 80, 65, 48, 36, 52, 31, 35, 29, 31, 20, 24, 19, 23, 17, 17, 16, 10, 2, 8, 3, 6, 3, 5, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 35, 49, 59, 82, 91, 108, 138, 126, 137, 180, 206, 194, 198, 201, 176, 215, 184, 182, 183, 171, 165, 158, 149, 154, 159, 150, 150, 142, 123, 133, 137, 95, 91, 56, 30, 13, 6, 9, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 30.7, 34.1, 36.7, 39.4, 44.3, 46.9, 50.4, 54.0, 58.1, 61.1, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 74.9, 33.3, 54.5, 45.8, 77.7, 72.7, 72.7, 84.9, 82.6]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 4, 9, 11, 24, 27, 22, 33, 20, 23]
Epoch 430 Acc: 97.59 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 340 train Loss: 3738.7 test Loss: 486.4
Epoch 431 Iter 0 subLoss 3804.6 multi -10.94 import weight 0.00
Epoch 431 Iter 1 subLoss 4743.1 multi -4.97 import weight 0.00
Epoch 431 Iter 2 subLoss 6561.1 multi 9.96 import weight 0.00
Epoch 431 Iter 3 subLoss 3572.7 multi 27.87 import weight 0.00
Epoch 431 Iter 4 subLoss 2477.5 multi 3.98 import weight 0.00
Epoch 431 Iter 5 subLoss 2493.0 multi -4.97 import weight 0.00
Epoch 431 Iter 6 subLoss 3009.9 multi 1.00 import weight 0.00
Epoch 431 Iter 7 subLoss 2595.5 multi -28.85 import weight 0.00
Epoch 431 Iter 8 subLoss 6627.4 multi 9.96 import weight 0.00
Epoch 431 Iter 9 subLoss 3680.2 multi -1.99 import weight 0.00
Epoch 431 Iter 10 subLoss 3753.0 multi 24.88 import weight 0.00
Epoch 431 Iter 11 subLoss 3115.6 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10868 / 14.54
Entropy seen (from low to high)
[5, 90, 240, 438, 783, 645, 451, 414, 431, 344, 240, 180, 130, 84, 73, 78, 68, 51, 34, 51, 28, 39, 30, 27, 25, 23, 18, 22, 20, 14, 18, 8, 4, 7, 4, 6, 2, 6, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 17, 38, 51, 57, 80, 90, 114, 139, 126, 143, 178, 208, 200, 192, 195, 182, 214, 186, 183, 185, 163, 161, 158, 150, 161, 156, 152, 144, 144, 126, 128, 131, 95, 90, 63, 25, 13, 8, 7, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 31.1, 34.2, 36.7, 39.6, 44.2, 47.1, 50.6, 54.2, 58.2, 61.2, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 74.9, 33.3, 49.9, 49.9, 77.7, 68.1, 76.4, 83.3, 83.9]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 4, 9, 12, 24, 27, 22, 34, 18, 25]
Epoch 431 Acc: 81.81 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 311 train Loss: 20632.3 test Loss: 2650.6
Epoch 432 Iter 0 subLoss 20072.2 multi 1.00 import weight 0.00
Epoch 432 Iter 1 subLoss 5804.1 multi -1.98 import weight 0.00
Epoch 432 Iter 2 subLoss 10659.9 multi 3.99 import weight 0.00
Epoch 432 Iter 3 subLoss 2978.1 multi -4.97 import weight 0.00
Epoch 432 Iter 4 subLoss 2947.2 multi -22.88 import weight 0.00
Epoch 432 Iter 5 subLoss 5709.7 multi -4.97 import weight 0.00
Epoch 432 Iter 6 subLoss 11873.1 multi 3.99 import weight 0.00
Epoch 432 Iter 7 subLoss 3806.8 multi -7.96 import weight 0.00
Epoch 432 Iter 8 subLoss 4976.6 multi 9.96 import weight 0.00
Epoch 432 Iter 9 subLoss 2826.0 multi -4.97 import weight 0.00
Epoch 432 Iter 10 subLoss 2954.5 multi 1.00 import weight 0.00
Epoch 432 Iter 11 subLoss 3583.3 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10871 / 14.65
Entropy seen (from low to high)
[5, 92, 242, 446, 791, 633, 454, 415, 432, 341, 237, 177, 131, 83, 73, 72, 68, 53, 34, 52, 32, 34, 31, 26, 25, 23, 16, 23, 21, 14, 19, 7, 5, 6, 4, 6, 2, 6, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 38, 50, 58, 81, 89, 114, 140, 126, 141, 176, 206, 210, 179, 199, 189, 206, 193, 181, 181, 170, 157, 158, 145, 168, 149, 151, 142, 148, 124, 123, 134, 100, 92, 63, 25, 14, 8, 7, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 31.1, 34.2, 36.7, 39.8, 44.1, 47.3, 50.7, 54.2, 58.2, 61.1, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 74.9, 33.3, 46.1, 54.1, 75.9, 69.5, 79.4, 77.7, 84.6]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 4, 9, 13, 24, 25, 23, 34, 18, 26]
Epoch 432 Acc: 97.59 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 358 train Loss: 3510.0 test Loss: 425.0
Epoch 433 Iter 0 subLoss 3589.8 multi -7.96 import weight 0.00
Epoch 433 Iter 1 subLoss 4367.9 multi 27.87 import weight 0.00
Epoch 433 Iter 2 subLoss 2899.2 multi 27.87 import weight 0.00
Epoch 433 Iter 3 subLoss 2202.7 multi 15.93 import weight 0.00
Epoch 433 Iter 4 subLoss 2402.3 multi -1.99 import weight 0.00
Epoch 433 Iter 5 subLoss 2833.4 multi 6.97 import weight 0.00
Epoch 433 Iter 6 subLoss 1907.0 multi 3.99 import weight 0.00
Epoch 433 Iter 7 subLoss 2203.5 multi 18.91 import weight 0.00
Epoch 433 Iter 8 subLoss 2191.2 multi -1.98 import weight 0.00
Epoch 433 Iter 9 subLoss 2237.2 multi 21.90 import weight 0.00
Epoch 433 Iter 10 subLoss 2023.3 multi -7.96 import weight 0.00
Epoch 433 Iter 11 subLoss 2309.9 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10885 / 15.08
Entropy seen (from low to high)
[5, 95, 240, 458, 795, 628, 455, 419, 435, 331, 236, 176, 129, 78, 77, 80, 58, 51, 35, 52, 31, 34, 32, 26, 26, 20, 20, 20, 20, 15, 18, 7, 5, 6, 4, 6, 2, 6, 3, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 18, 34, 55, 53, 84, 89, 112, 139, 130, 141, 174, 205, 206, 184, 199, 184, 205, 191, 183, 183, 166, 160, 156, 151, 160, 155, 147, 147, 144, 126, 126, 134, 100, 91, 64, 28, 15, 7, 7, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.6, 0.0, 33.0, 36.6, 39.9, 44.1, 47.3, 50.7, 54.2, 58.3, 61.2, 64.6, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 74.9, 33.3, 53.8, 47.8, 76.9, 70.8, 78.7, 76.4, 84.6]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 4, 9, 13, 23, 26, 24, 33, 17, 26]
Epoch 433 Acc: 98.48 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 230 train Loss: 2181.2 test Loss: 240.7
Epoch 434 Iter 0 subLoss 2262.1 multi -25.87 import weight 0.00
Epoch 434 Iter 1 subLoss 2483.1 multi 3.99 import weight 0.00
Epoch 434 Iter 2 subLoss 2304.5 multi 18.91 import weight 0.00
Epoch 434 Iter 3 subLoss 2272.8 multi 24.88 import weight 0.00
Epoch 434 Iter 4 subLoss 2120.6 multi 6.97 import weight 0.00
Epoch 434 Iter 5 subLoss 2537.1 multi 6.97 import weight 0.00
Epoch 434 Iter 6 subLoss 1680.4 multi 3.99 import weight 0.00
Epoch 434 Iter 7 subLoss 2156.1 multi -13.93 import weight 0.00
Epoch 434 Iter 8 subLoss 1874.3 multi 3.99 import weight 0.00
Epoch 434 Iter 9 subLoss 1973.9 multi 6.97 import weight 0.00
Epoch 434 Iter 10 subLoss 2237.0 multi 24.88 import weight 0.00
Epoch 434 Iter 11 subLoss 1783.9 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10899 / 14.26
Entropy seen (from low to high)
[5, 96, 246, 462, 801, 629, 448, 427, 427, 330, 235, 174, 123, 82, 75, 82, 58, 51, 32, 50, 31, 36, 30, 27, 25, 20, 21, 20, 19, 16, 17, 7, 5, 7, 3, 6, 3, 4, 5, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 17, 34, 53, 56, 83, 90, 110, 139, 130, 137, 175, 201, 210, 184, 197, 178, 213, 192, 181, 177, 160, 170, 158, 152, 159, 158, 143, 145, 142, 131, 124, 138, 101, 87, 71, 28, 15, 7, 7, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 0.0, 33.2, 36.6, 40.0, 44.1, 47.5, 50.8, 54.0, 58.0, 61.2, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 74.9, 33.3, 49.9, 52.3, 71.9, 70.8, 79.4, 77.7, 84.6]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 4, 9, 14, 21, 25, 24, 34, 18, 26]
Epoch 434 Acc: 98.79 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 178 train Loss: 1990.8 test Loss: 208.6
Epoch 435 Iter 0 subLoss 2341.0 multi 18.91 import weight 0.00
Epoch 435 Iter 1 subLoss 1727.5 multi 1.00 import weight 0.00
Epoch 435 Iter 2 subLoss 1740.7 multi 1.00 import weight 0.00
Epoch 435 Iter 3 subLoss 2059.8 multi -10.94 import weight 0.00
Epoch 435 Iter 4 subLoss 1747.1 multi 3.99 import weight 0.00
Epoch 435 Iter 5 subLoss 1808.4 multi 6.97 import weight 0.00
Epoch 435 Iter 6 subLoss 1917.6 multi 6.97 import weight 0.00
Epoch 435 Iter 7 subLoss 1848.7 multi -13.93 import weight 0.00
Epoch 435 Iter 8 subLoss 1738.3 multi 6.97 import weight 0.00
Epoch 435 Iter 9 subLoss 2083.7 multi 9.96 import weight 0.00
Epoch 435 Iter 10 subLoss 1943.6 multi -4.97 import weight 0.00
Epoch 435 Iter 11 subLoss 1914.3 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10914 / 15.35
Entropy seen (from low to high)
[5, 99, 244, 477, 805, 621, 453, 421, 428, 326, 235, 173, 121, 81, 74, 81, 57, 51, 35, 47, 31, 35, 31, 26, 27, 20, 19, 20, 19, 18, 15, 8, 4, 7, 3, 6, 4, 3, 5, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 17, 32, 55, 53, 85, 90, 108, 137, 130, 139, 171, 205, 205, 184, 196, 179, 207, 197, 182, 178, 159, 174, 154, 150, 158, 159, 146, 146, 143, 132, 123, 137, 104, 89, 70, 29, 16, 6, 7, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.8, 0.0, 33.5, 36.6, 40.1, 44.2, 47.4, 50.5, 53.9, 57.9, 61.1, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 74.9, 44.4, 53.8, 42.1, 71.4, 71.4, 81.0, 76.4, 84.6]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 4, 9, 13, 19, 28, 21, 37, 17, 26]
Epoch 435 Acc: 98.72 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 191 train Loss: 1933.8 test Loss: 214.0
Epoch 436 Iter 0 subLoss 1828.7 multi 1.00 import weight 0.00
Epoch 436 Iter 1 subLoss 1950.3 multi -7.96 import weight 0.00
Epoch 436 Iter 2 subLoss 1798.5 multi -7.96 import weight 0.00
Epoch 436 Iter 3 subLoss 2169.8 multi 9.96 import weight 0.00
Epoch 436 Iter 4 subLoss 1873.5 multi 6.97 import weight 0.00
Epoch 436 Iter 5 subLoss 1675.6 multi -4.97 import weight 0.00
Epoch 436 Iter 6 subLoss 1907.9 multi 6.97 import weight 0.00
Epoch 436 Iter 7 subLoss 1897.5 multi 12.94 import weight 0.00
Epoch 436 Iter 8 subLoss 2073.7 multi 3.99 import weight 0.00
Epoch 436 Iter 9 subLoss 2300.2 multi 21.90 import weight 0.00
Epoch 436 Iter 10 subLoss 1819.2 multi -4.97 import weight 0.00
Epoch 436 Iter 11 subLoss 2217.7 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10927 / 15.85
Entropy seen (from low to high)
[5, 100, 246, 488, 809, 615, 447, 432, 427, 324, 232, 170, 118, 79, 75, 83, 55, 49, 35, 46, 31, 36, 29, 27, 26, 21, 18, 21, 20, 16, 15, 9, 4, 5, 4, 6, 4, 3, 5, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 17, 33, 54, 53, 84, 89, 108, 133, 132, 142, 166, 203, 201, 189, 197, 177, 203, 198, 186, 176, 159, 176, 152, 146, 164, 156, 150, 140, 150, 132, 122, 138, 102, 92, 71, 31, 17, 5, 8, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 0.0, 32.4, 36.3, 40.1, 44.2, 47.2, 50.5, 54.0, 58.1, 61.1, 64.6, 68.6]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 74.9, 49.9, 57.1, 42.1, 72.4, 64.9, 83.3, 77.7, 84.6]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 4, 4, 8, 14, 19, 29, 20, 36, 18, 26]
Epoch 436 Acc: 98.60 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 221 train Loss: 2012.5 test Loss: 222.2
Epoch 437 Iter 0 subLoss 2233.7 multi 27.87 import weight 0.00
Epoch 437 Iter 1 subLoss 2184.1 multi -10.94 import weight 0.00
Epoch 437 Iter 2 subLoss 3079.5 multi -7.96 import weight 0.00
Epoch 437 Iter 3 subLoss 5120.6 multi -1.99 import weight 0.00
Epoch 437 Iter 4 subLoss 7409.5 multi 1.00 import weight 0.00
Epoch 437 Iter 5 subLoss 5433.1 multi -4.97 import weight 0.00
Epoch 437 Iter 6 subLoss 12577.7 multi 18.91 import weight 0.00
Epoch 437 Iter 7 subLoss 27075.5 multi -1.98 import weight 0.00
Epoch 437 Iter 8 subLoss 86878.7 multi 1.00 import weight 0.00
Epoch 437 Iter 9 subLoss 25331.2 multi 1.00 import weight 0.00
Epoch 437 Iter 10 subLoss 17306.8 multi -7.96 import weight 0.00
Epoch 437 Iter 11 subLoss 67745.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10892 / 14.84
Entropy seen (from low to high)
[5, 87, 233, 449, 647, 614, 517, 472, 463, 341, 251, 190, 127, 83, 74, 77, 62, 59, 34, 46, 32, 36, 30, 27, 25, 23, 16, 21, 20, 17, 16, 9, 5, 5, 4, 5, 3, 5, 5, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 17, 36, 51, 57, 83, 91, 109, 144, 123, 142, 179, 198, 204, 191, 193, 192, 194, 192, 182, 186, 157, 174, 150, 150, 162, 157, 152, 135, 152, 120, 126, 142, 101, 89, 66, 30, 14, 6, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.7, 0.0, 32.3, 36.2, 39.9, 44.2, 47.4, 50.8, 54.1, 58.2, 61.1, 64.4, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 74.9, 55.5, 49.9, 47.6, 73.0, 73.9, 77.4, 80.9, 83.3]
[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 4, 4, 9, 14, 21, 26, 23, 31, 21, 24]
Epoch 437 Acc: 57.87 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 6774 train Loss: 47614.7 test Loss: 7879.9
Epoch 438 Iter 0 subLoss 46939.4 multi 1.00 import weight 0.00
Epoch 438 Iter 1 subLoss 40282.0 multi 1.00 import weight 0.00
Epoch 438 Iter 2 subLoss 35787.4 multi 1.00 import weight 0.00
Epoch 438 Iter 3 subLoss 34335.1 multi 1.00 import weight 0.00
Epoch 438 Iter 4 subLoss 31962.3 multi 1.00 import weight 0.00
Epoch 438 Iter 5 subLoss 29332.7 multi -1.99 import weight 0.00
Epoch 438 Iter 6 subLoss 33764.8 multi 1.00 import weight 0.00
Epoch 438 Iter 7 subLoss 32154.1 multi -1.99 import weight 0.00
Epoch 438 Iter 8 subLoss 36465.1 multi 1.00 import weight 0.00
Epoch 438 Iter 9 subLoss 36665.3 multi 1.00 import weight 0.00
Epoch 438 Iter 10 subLoss 31184.4 multi 1.00 import weight 0.00
Epoch 438 Iter 11 subLoss 30080.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10863 / 15.26
Entropy seen (from low to high)
[5, 86, 233, 454, 585, 494, 524, 532, 492, 369, 252, 202, 147, 89, 77, 80, 61, 57, 41, 46, 34, 37, 29, 29, 23, 25, 15, 20, 23, 16, 17, 7, 7, 5, 4, 5, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 17, 37, 51, 61, 81, 94, 104, 147, 125, 150, 174, 200, 201, 201, 195, 186, 196, 198, 177, 179, 173, 164, 156, 147, 160, 152, 154, 137, 148, 126, 124, 135, 98, 86, 68, 28, 11, 7, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.1, 32.5, 36.2, 39.8, 44.3, 47.4, 50.7, 54.0, 58.0, 61.0, 64.3, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 74.9, 59.9, 46.1, 47.6, 75.9, 71.9, 76.6, 80.9, 84.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 10, 13, 21, 25, 25, 30, 21, 26]
Epoch 438 Acc: 65.52 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3008 train Loss: 28476.2 test Loss: 5400.4
Epoch 439 Iter 0 subLoss 27583.3 multi 1.00 import weight 0.00
Epoch 439 Iter 1 subLoss 25094.4 multi 1.00 import weight 0.00
Epoch 439 Iter 2 subLoss 25418.0 multi 1.00 import weight 0.00
Epoch 439 Iter 3 subLoss 20110.2 multi 6.97 import weight 0.00
Epoch 439 Iter 4 subLoss 6980.2 multi -13.93 import weight 0.00
Epoch 439 Iter 5 subLoss 18436.3 multi 1.00 import weight 0.00
Epoch 439 Iter 6 subLoss 16970.3 multi 3.99 import weight 0.00
Epoch 439 Iter 7 subLoss 10856.6 multi 1.00 import weight 0.00
Epoch 439 Iter 8 subLoss 10273.6 multi -4.97 import weight 0.00
Epoch 439 Iter 9 subLoss 14652.6 multi -1.99 import weight 0.00
Epoch 439 Iter 10 subLoss 16534.0 multi -1.98 import weight 0.00
Epoch 439 Iter 11 subLoss 21161.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10829 / 14.30
Entropy seen (from low to high)
[5, 85, 224, 457, 555, 405, 448, 596, 510, 405, 280, 214, 155, 101, 84, 81, 67, 60, 41, 44, 33, 42, 29, 30, 25, 24, 15, 22, 22, 17, 17, 8, 6, 7, 3, 5, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 19, 38, 52, 58, 84, 94, 115, 139, 126, 155, 178, 198, 201, 201, 197, 186, 202, 194, 189, 165, 171, 162, 169, 140, 163, 152, 144, 146, 148, 120, 124, 130, 99, 83, 63, 28, 10, 5, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.3, 32.5, 36.1, 39.7, 44.3, 47.5, 50.7, 53.9, 58.0, 61.1, 64.5, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 74.9, 54.5, 46.1, 54.9, 73.0, 70.8, 72.7, 89.4, 81.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 11, 13, 20, 26, 24, 33, 19, 27]
Epoch 439 Acc: 62.52 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 2116 train Loss: 25166.8 test Loss: 4890.8
Epoch 440 Iter 0 subLoss 24717.3 multi 1.00 import weight 0.00
Epoch 440 Iter 1 subLoss 21749.0 multi 1.00 import weight 0.00
Epoch 440 Iter 2 subLoss 21268.8 multi 3.99 import weight 0.00
Epoch 440 Iter 3 subLoss 13999.1 multi 3.99 import weight 0.00
Epoch 440 Iter 4 subLoss 9714.6 multi 1.00 import weight 0.00
Epoch 440 Iter 5 subLoss 9793.5 multi 1.00 import weight 0.00
Epoch 440 Iter 6 subLoss 8900.0 multi 3.99 import weight 0.00
Epoch 440 Iter 7 subLoss 7091.7 multi 1.00 import weight 0.00
Epoch 440 Iter 8 subLoss 5911.2 multi 3.99 import weight 0.00
Epoch 440 Iter 9 subLoss 5158.5 multi 3.98 import weight 0.00
Epoch 440 Iter 10 subLoss 5172.5 multi 1.00 import weight 0.00
Epoch 440 Iter 11 subLoss 4747.1 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10816 / 14.51
Entropy seen (from low to high)
[5, 85, 225, 467, 556, 400, 449, 603, 490, 403, 278, 210, 164, 101, 87, 73, 75, 56, 46, 37, 37, 44, 27, 30, 26, 24, 17, 22, 20, 19, 17, 8, 5, 8, 3, 5, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 19, 37, 54, 57, 84, 95, 114, 144, 123, 159, 181, 193, 198, 211, 200, 182, 208, 187, 189, 163, 175, 153, 181, 137, 156, 154, 146, 145, 146, 117, 124, 130, 95, 84, 64, 28, 9, 6, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 33.9, 36.5, 39.7, 44.3, 47.5, 50.8, 53.9, 58.0, 61.1, 64.5, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 74.9, 54.5, 46.1, 57.1, 71.9, 71.9, 72.7, 88.8, 82.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 11, 13, 21, 25, 25, 33, 18, 29]
Epoch 440 Acc: 94.30 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 474 train Loss: 5111.3 test Loss: 914.6
Epoch 441 Iter 0 subLoss 5316.7 multi 12.94 import weight 0.00
Epoch 441 Iter 1 subLoss 3686.5 multi 1.00 import weight 0.00
Epoch 441 Iter 2 subLoss 3348.3 multi 3.99 import weight 0.00
Epoch 441 Iter 3 subLoss 3424.4 multi 9.96 import weight 0.00
Epoch 441 Iter 4 subLoss 3381.4 multi 12.94 import weight 0.00
Epoch 441 Iter 5 subLoss 3095.3 multi 15.93 import weight 0.00
Epoch 441 Iter 6 subLoss 2566.1 multi 3.98 import weight 0.00
Epoch 441 Iter 7 subLoss 2248.5 multi -25.87 import weight 0.00
Epoch 441 Iter 8 subLoss 3553.3 multi -7.96 import weight 0.00
Epoch 441 Iter 9 subLoss 4525.8 multi -7.96 import weight 0.00
Epoch 441 Iter 10 subLoss 10058.9 multi 1.00 import weight 0.00
Epoch 441 Iter 11 subLoss 5651.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10814 / 14.97
Entropy seen (from low to high)
[5, 85, 228, 472, 558, 398, 448, 607, 488, 396, 276, 211, 164, 101, 86, 73, 75, 55, 47, 37, 38, 43, 28, 28, 28, 24, 15, 23, 22, 17, 16, 9, 5, 8, 3, 5, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 19, 38, 53, 57, 84, 92, 122, 140, 125, 160, 177, 197, 195, 212, 198, 184, 211, 184, 187, 166, 176, 150, 179, 141, 149, 160, 138, 149, 149, 112, 124, 132, 96, 83, 66, 27, 10, 6, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.8, 33.0, 36.1, 39.7, 44.2, 47.5, 50.9, 54.1, 58.1, 61.1, 64.4, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 74.9, 54.5, 46.1, 60.8, 66.6, 71.9, 79.9, 79.9, 85.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 11, 13, 23, 24, 25, 30, 20, 28]
Epoch 441 Acc: 96.46 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 565 train Loss: 3716.0 test Loss: 600.6
Epoch 442 Iter 0 subLoss 3254.4 multi 6.97 import weight 0.00
Epoch 442 Iter 1 subLoss 3076.6 multi -4.97 import weight 0.00
Epoch 442 Iter 2 subLoss 3365.6 multi 3.99 import weight 0.00
Epoch 442 Iter 3 subLoss 3271.9 multi 6.97 import weight 0.00
Epoch 442 Iter 4 subLoss 2815.6 multi -10.94 import weight 0.00
Epoch 442 Iter 5 subLoss 3658.4 multi -10.94 import weight 0.00
Epoch 442 Iter 6 subLoss 3885.4 multi 1.00 import weight 0.00
Epoch 442 Iter 7 subLoss 3868.2 multi 1.00 import weight 0.00
Epoch 442 Iter 8 subLoss 3630.2 multi 12.94 import weight 0.00
Epoch 442 Iter 9 subLoss 3063.9 multi -10.94 import weight 0.00
Epoch 442 Iter 10 subLoss 3669.8 multi 1.00 import weight 0.00
Epoch 442 Iter 11 subLoss 2998.1 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10815 / 14.80
Entropy seen (from low to high)
[5, 88, 229, 478, 557, 399, 455, 607, 483, 391, 278, 212, 157, 104, 80, 75, 74, 56, 47, 35, 42, 39, 30, 26, 27, 27, 13, 21, 24, 17, 15, 10, 5, 8, 3, 5, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 19, 35, 56, 57, 84, 93, 122, 140, 123, 164, 176, 198, 196, 211, 194, 182, 216, 181, 188, 169, 178, 148, 174, 141, 146, 162, 139, 142, 156, 110, 124, 134, 97, 80, 68, 28, 11, 6, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.9, 34.1, 36.5, 39.7, 44.2, 47.4, 51.0, 54.2, 58.0, 61.0, 64.3, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 74.9, 54.5, 49.9, 61.5, 63.6, 70.8, 80.6, 78.9, 86.2]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 11, 12, 26, 22, 24, 31, 19, 29]
Epoch 442 Acc: 97.12 BMA: 98.38 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 299 train Loss: 3252.2 test Loss: 484.7
Epoch 443 Iter 0 subLoss 3546.3 multi 6.97 import weight 0.00
Epoch 443 Iter 1 subLoss 3161.7 multi -4.97 import weight 0.00
Epoch 443 Iter 2 subLoss 3187.3 multi -16.91 import weight 0.00
Epoch 443 Iter 3 subLoss 4142.2 multi -7.96 import weight 0.00
Epoch 443 Iter 4 subLoss 4037.4 multi -7.96 import weight 0.00
Epoch 443 Iter 5 subLoss 7110.0 multi 6.97 import weight 0.00
Epoch 443 Iter 6 subLoss 3262.2 multi -1.99 import weight 0.00
Epoch 443 Iter 7 subLoss 3671.1 multi 12.94 import weight 0.00
Epoch 443 Iter 8 subLoss 3332.4 multi -4.97 import weight 0.00
Epoch 443 Iter 9 subLoss 3715.7 multi 1.00 import weight 0.00
Epoch 443 Iter 10 subLoss 3403.7 multi 1.00 import weight 0.00
Epoch 443 Iter 11 subLoss 3282.5 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10811 / 15.34
Entropy seen (from low to high)
[6, 88, 233, 482, 559, 399, 454, 612, 487, 377, 277, 210, 156, 103, 82, 71, 76, 56, 46, 34, 43, 39, 29, 28, 27, 24, 16, 21, 24, 16, 15, 11, 5, 8, 3, 5, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 20, 34, 56, 58, 89, 88, 123, 144, 117, 172, 170, 194, 201, 209, 193, 185, 216, 188, 177, 172, 177, 149, 170, 150, 138, 159, 147, 141, 149, 110, 127, 132, 98, 80, 69, 28, 12, 6, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.9, 34.2, 36.5, 39.8, 44.2, 47.3, 50.9, 54.4, 58.1, 61.1, 64.3, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 74.9, 63.6, 54.5, 60.7, 63.6, 70.8, 79.9, 78.9, 86.2]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 11, 11, 28, 22, 24, 30, 19, 29]
Epoch 443 Acc: 96.77 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 328 train Loss: 3638.1 test Loss: 536.1
Epoch 444 Iter 0 subLoss 3879.5 multi 6.97 import weight 0.00
Epoch 444 Iter 1 subLoss 3783.6 multi 12.94 import weight 0.00
Epoch 444 Iter 2 subLoss 2620.2 multi 3.99 import weight 0.00
Epoch 444 Iter 3 subLoss 3046.3 multi 6.97 import weight 0.00
Epoch 444 Iter 4 subLoss 2996.7 multi 6.97 import weight 0.00
Epoch 444 Iter 5 subLoss 3091.1 multi 18.91 import weight 0.00
Epoch 444 Iter 6 subLoss 2912.5 multi 1.00 import weight 0.00
Epoch 444 Iter 7 subLoss 2804.8 multi 3.98 import weight 0.00
Epoch 444 Iter 8 subLoss 2626.2 multi 6.97 import weight 0.00
Epoch 444 Iter 9 subLoss 2564.6 multi 6.97 import weight 0.00
Epoch 444 Iter 10 subLoss 2992.3 multi 9.96 import weight 0.00
Epoch 444 Iter 11 subLoss 2059.5 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10818 / 15.32
Entropy seen (from low to high)
[6, 92, 236, 489, 558, 390, 464, 618, 476, 381, 274, 206, 159, 96, 83, 74, 74, 54, 47, 33, 41, 42, 27, 27, 27, 26, 14, 22, 23, 17, 14, 11, 5, 8, 3, 5, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 5, 21, 33, 55, 61, 86, 91, 121, 142, 118, 168, 174, 198, 200, 205, 195, 187, 216, 179, 182, 172, 180, 147, 171, 146, 139, 162, 142, 140, 152, 107, 130, 124, 106, 81, 69, 30, 12, 6, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.0, 34.4, 36.5, 39.8, 44.2, 47.4, 51.1, 54.5, 57.9, 61.0, 64.3, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 74.9, 63.6, 49.9, 62.0, 68.4, 66.6, 79.9, 79.9, 86.2]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 11, 12, 29, 19, 24, 30, 20, 29]
Epoch 444 Acc: 98.19 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 205 train Loss: 2477.2 test Loss: 306.7
Epoch 445 Iter 0 subLoss 2543.4 multi -10.94 import weight 0.00
Epoch 445 Iter 1 subLoss 3092.2 multi 21.90 import weight 0.00
Epoch 445 Iter 2 subLoss 2403.7 multi 1.00 import weight 0.00
Epoch 445 Iter 3 subLoss 2483.7 multi 6.97 import weight 0.00
Epoch 445 Iter 4 subLoss 2264.7 multi -22.88 import weight 0.00
Epoch 445 Iter 5 subLoss 2744.6 multi 21.90 import weight 0.00
Epoch 445 Iter 6 subLoss 2575.1 multi -4.97 import weight 0.00
Epoch 445 Iter 7 subLoss 2460.3 multi 1.00 import weight 0.00
Epoch 445 Iter 8 subLoss 2559.5 multi -4.97 import weight 0.00
Epoch 445 Iter 9 subLoss 3117.5 multi -10.94 import weight 0.00
Epoch 445 Iter 10 subLoss 3536.7 multi -1.99 import weight 0.00
Epoch 445 Iter 11 subLoss 4199.6 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10803 / 15.25
Entropy seen (from low to high)
[6, 92, 240, 498, 549, 392, 459, 608, 476, 385, 269, 214, 154, 97, 86, 76, 65, 60, 47, 34, 43, 38, 34, 24, 29, 24, 14, 24, 23, 17, 13, 11, 4, 9, 2, 6, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 20, 34, 59, 62, 82, 95, 120, 141, 121, 172, 170, 198, 200, 209, 188, 193, 214, 181, 176, 170, 183, 153, 169, 149, 127, 164, 148, 144, 140, 112, 130, 124, 96, 88, 69, 30, 10, 6, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.4, 33.6, 36.2, 39.7, 44.2, 47.3, 51.0, 54.4, 57.8, 61.3, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 74.9, 63.6, 58.3, 58.6, 64.7, 71.4, 77.4, 83.3, 84.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 11, 12, 29, 17, 28, 31, 18, 26]
Epoch 445 Acc: 89.67 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 419 train Loss: 9040.6 test Loss: 1579.8
Epoch 446 Iter 0 subLoss 8823.6 multi 1.00 import weight 0.00
Epoch 446 Iter 1 subLoss 6353.3 multi -1.99 import weight 0.00
Epoch 446 Iter 2 subLoss 9821.7 multi 1.00 import weight 0.00
Epoch 446 Iter 3 subLoss 6535.2 multi 9.96 import weight 0.00
Epoch 446 Iter 4 subLoss 3431.5 multi -7.96 import weight 0.00
Epoch 446 Iter 5 subLoss 4859.9 multi -19.90 import weight 0.00
Epoch 446 Iter 6 subLoss 121073.2 multi 1.00 import weight 0.00
Epoch 446 Iter 7 subLoss 6329.6 multi 3.99 import weight 0.00
Epoch 446 Iter 8 subLoss 4683.2 multi 3.99 import weight 0.00
Epoch 446 Iter 9 subLoss 4110.5 multi -10.94 import weight 0.00
Epoch 446 Iter 10 subLoss 5597.9 multi 18.91 import weight 0.00
Epoch 446 Iter 11 subLoss 3827.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0064 / 0.10803 / 15.38
Entropy seen (from low to high)
[6, 93, 243, 502, 551, 393, 462, 611, 467, 386, 264, 212, 156, 95, 84, 75, 69, 56, 49, 33, 42, 40, 34, 24, 28, 22, 18, 23, 21, 17, 14, 11, 5, 8, 3, 5, 3, 4, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 21, 32, 60, 59, 86, 95, 125, 139, 117, 171, 174, 195, 198, 204, 202, 189, 216, 179, 175, 168, 185, 149, 170, 150, 129, 161, 143, 145, 145, 110, 128, 126, 97, 88, 68, 31, 11, 6, 6, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.4, 33.9, 36.2, 39.7, 44.2, 47.3, 51.0, 54.5, 57.8, 61.3, 64.6, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 74.9, 63.6, 58.3, 59.9, 62.4, 71.4, 80.6, 77.7, 85.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 11, 12, 30, 16, 28, 31, 18, 28]
Epoch 446 Acc: 97.70 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 382 train Loss: 2885.4 test Loss: 405.8
Epoch 447 Iter 0 subLoss 3157.6 multi -13.93 import weight 0.00
Epoch 447 Iter 1 subLoss 3348.7 multi 3.98 import weight 0.00
Epoch 447 Iter 2 subLoss 3601.6 multi 3.99 import weight 0.00
Epoch 447 Iter 3 subLoss 2939.8 multi 24.88 import weight 1.00
Epoch 447 Iter 4 subLoss 2724.6 multi 9.96 import weight 0.00
Epoch 447 Iter 5 subLoss 2674.4 multi 1.00 import weight 0.00
Epoch 447 Iter 6 subLoss 2427.0 multi -7.96 import weight 0.00
Epoch 447 Iter 7 subLoss 2567.8 multi 6.97 import weight 0.00
Epoch 447 Iter 8 subLoss 2296.3 multi 3.99 import weight 0.00
Epoch 447 Iter 9 subLoss 2330.2 multi 9.96 import weight 0.00
Epoch 447 Iter 10 subLoss 2298.7 multi 6.97 import weight 0.00
Epoch 447 Iter 11 subLoss 2186.8 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10812 / 15.19
Entropy seen (from low to high)
[6, 95, 246, 507, 554, 391, 466, 615, 463, 382, 260, 212, 154, 93, 87, 76, 64, 59, 46, 31, 44, 40, 33, 24, 27, 23, 19, 22, 21, 16, 14, 11, 5, 8, 3, 5, 2, 5, 6, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 19, 34, 58, 61, 87, 93, 123, 138, 122, 170, 174, 195, 193, 208, 199, 186, 216, 179, 182, 167, 175, 154, 174, 149, 125, 161, 145, 141, 146, 113, 128, 129, 98, 87, 66, 34, 11, 6, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.5, 34.2, 36.2, 39.8, 44.3, 47.2, 50.9, 54.4, 57.7, 61.2, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 74.9, 63.6, 54.5, 61.2, 59.9, 70.3, 81.8, 77.7, 84.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 11, 11, 31, 15, 27, 33, 18, 26]
Epoch 447 Acc: 98.37 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 218 train Loss: 2444.3 test Loss: 283.3
Epoch 448 Iter 0 subLoss 2628.1 multi 9.96 import weight 0.00
Epoch 448 Iter 1 subLoss 2078.5 multi 6.97 import weight 0.00
Epoch 448 Iter 2 subLoss 2183.8 multi -4.97 import weight 0.00
Epoch 448 Iter 3 subLoss 1993.3 multi 18.91 import weight 0.00
Epoch 448 Iter 4 subLoss 2241.4 multi -22.88 import weight 0.00
Epoch 448 Iter 5 subLoss 2448.7 multi -1.99 import weight 0.00
Epoch 448 Iter 6 subLoss 2376.2 multi -16.91 import weight 0.00
Epoch 448 Iter 7 subLoss 2801.5 multi 6.97 import weight 0.00
Epoch 448 Iter 8 subLoss 2547.9 multi -7.96 import weight 0.00
Epoch 448 Iter 9 subLoss 3122.1 multi -7.96 import weight 0.00
Epoch 448 Iter 10 subLoss 3996.2 multi 18.91 import weight 0.00
Epoch 448 Iter 11 subLoss 2638.8 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10814 / 15.30
Entropy seen (from low to high)
[6, 95, 251, 514, 555, 392, 464, 616, 466, 381, 256, 206, 151, 92, 87, 76, 65, 58, 47, 29, 46, 39, 34, 21, 27, 25, 18, 21, 22, 16, 14, 11, 5, 8, 3, 4, 3, 6, 5, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 20, 32, 58, 61, 88, 93, 122, 132, 129, 167, 174, 194, 198, 211, 195, 185, 214, 176, 189, 166, 178, 154, 168, 146, 130, 160, 143, 143, 146, 115, 129, 124, 100, 89, 65, 34, 12, 6, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 34.2, 36.7, 40.2, 44.3, 47.2, 50.9, 54.5, 57.7, 61.2, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 79.9, 66.6, 63.6, 45.4, 64.5, 62.4, 69.2, 80.6, 79.9, 83.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 11, 11, 31, 16, 26, 31, 20, 25]
Epoch 448 Acc: 98.17 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 263 train Loss: 2917.7 test Loss: 333.3
Epoch 449 Iter 0 subLoss 2665.8 multi 6.97 import weight 0.00
Epoch 449 Iter 1 subLoss 2593.4 multi -25.87 import weight 0.00
Epoch 449 Iter 2 subLoss 3976.1 multi -22.88 import weight 0.00
Epoch 449 Iter 3 subLoss 73183.7 multi 1.00 import weight 0.00
Epoch 449 Iter 4 subLoss 17599.1 multi 3.99 import weight 0.00
Epoch 449 Iter 5 subLoss 4058.7 multi -1.99 import weight 0.00
Epoch 449 Iter 6 subLoss 4218.9 multi 9.96 import weight 0.00
Epoch 449 Iter 7 subLoss 2725.7 multi 12.94 import weight 0.00
Epoch 449 Iter 8 subLoss 2470.5 multi 3.99 import weight 0.00
Epoch 449 Iter 9 subLoss 3129.3 multi -4.97 import weight 0.00
Epoch 449 Iter 10 subLoss 2473.3 multi 6.97 import weight 0.00
Epoch 449 Iter 11 subLoss 2668.4 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10821 / 16.22
Entropy seen (from low to high)
[6, 99, 250, 523, 549, 392, 474, 614, 469, 374, 254, 208, 146, 94, 82, 78, 62, 60, 45, 29, 48, 37, 33, 22, 27, 25, 17, 22, 22, 15, 14, 12, 5, 7, 3, 4, 3, 6, 5, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 20, 32, 56, 65, 87, 91, 119, 135, 128, 168, 176, 196, 191, 212, 192, 192, 205, 178, 192, 167, 178, 151, 168, 148, 129, 159, 143, 146, 145, 116, 127, 122, 101, 91, 65, 36, 13, 6, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 34.4, 36.7, 40.4, 44.4, 47.2, 51.0, 54.6, 57.8, 61.2, 64.5, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 79.9, 66.6, 63.6, 45.4, 64.5, 64.7, 67.9, 80.6, 78.9, 91.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 11, 11, 31, 17, 25, 31, 19, 24]
Epoch 449 Acc: 98.27 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 266 train Loss: 2473.8 test Loss: 289.1
Epoch 450 Iter 0 subLoss 2191.3 multi -7.96 import weight 0.00
Epoch 450 Iter 1 subLoss 2360.8 multi 12.94 import weight 0.00
Epoch 450 Iter 2 subLoss 2338.8 multi 12.94 import weight 0.00
Epoch 450 Iter 3 subLoss 2686.1 multi 12.94 import weight 0.00
Epoch 450 Iter 4 subLoss 2718.9 multi -4.97 import weight 0.00
Epoch 450 Iter 5 subLoss 2282.0 multi -16.91 import weight 0.00
Epoch 450 Iter 6 subLoss 2437.5 multi 3.99 import weight 0.00
Epoch 450 Iter 7 subLoss 2813.0 multi -13.93 import weight 0.00
Epoch 450 Iter 8 subLoss 4432.8 multi 3.98 import weight 0.00
Epoch 450 Iter 9 subLoss 3006.6 multi -4.97 import weight 0.00
Epoch 450 Iter 10 subLoss 3342.3 multi 6.97 import weight 0.00
Epoch 450 Iter 11 subLoss 2301.1 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10829 / 15.92
Entropy seen (from low to high)
[7, 101, 248, 533, 547, 397, 472, 616, 467, 370, 248, 210, 146, 94, 79, 79, 64, 57, 44, 31, 46, 40, 32, 21, 26, 24, 19, 21, 22, 15, 14, 13, 5, 5, 4, 4, 3, 6, 5, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 19, 33, 55, 66, 89, 87, 120, 134, 127, 166, 177, 199, 192, 209, 189, 196, 202, 182, 191, 170, 170, 150, 170, 146, 139, 151, 140, 148, 147, 117, 126, 122, 106, 87, 68, 35, 15, 6, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 34.6, 36.7, 40.5, 44.3, 47.0, 50.9, 54.4, 57.9, 61.3, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 79.9, 66.6, 59.9, 49.9, 62.0, 68.4, 67.9, 80.6, 78.9, 91.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 10, 12, 29, 19, 25, 31, 19, 24]
Epoch 450 Acc: 98.38 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 230 train Loss: 2354.7 test Loss: 265.8
Epoch 451 Iter 0 subLoss 2225.1 multi -13.93 import weight 0.00
Epoch 451 Iter 1 subLoss 2734.1 multi -25.87 import weight 0.00
Epoch 451 Iter 2 subLoss 3318.4 multi 1.00 import weight 0.00
Epoch 451 Iter 3 subLoss 3074.7 multi -4.97 import weight 0.00
Epoch 451 Iter 4 subLoss 3748.5 multi -28.85 import weight 0.00
Epoch 451 Iter 5 subLoss 32827.0 multi 1.00 import weight 0.00
Epoch 451 Iter 6 subLoss 8530.7 multi 3.99 import weight 0.00
Epoch 451 Iter 7 subLoss 4892.0 multi 3.99 import weight 0.00
Epoch 451 Iter 8 subLoss 3830.3 multi -13.93 import weight 0.00
Epoch 451 Iter 9 subLoss 5310.0 multi 3.99 import weight 0.00
Epoch 451 Iter 10 subLoss 4517.8 multi 12.94 import weight 0.00
Epoch 451 Iter 11 subLoss 3232.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10826 / 15.85
Entropy seen (from low to high)
[7, 105, 254, 533, 548, 398, 481, 607, 467, 361, 253, 207, 144, 93, 79, 80, 63, 55, 47, 29, 46, 39, 32, 21, 27, 22, 21, 22, 19, 15, 15, 13, 5, 5, 4, 4, 3, 6, 5, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 19, 32, 56, 67, 90, 86, 119, 132, 133, 165, 178, 195, 200, 200, 188, 206, 201, 180, 184, 172, 174, 149, 171, 146, 137, 151, 139, 148, 145, 114, 128, 124, 106, 86, 68, 36, 15, 6, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 34.9, 36.7, 40.5, 44.3, 47.1, 50.9, 54.5, 57.8, 61.3, 64.5, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 79.9, 66.6, 59.9, 49.9, 63.3, 66.6, 67.9, 80.6, 77.7, 91.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 10, 12, 30, 18, 25, 31, 18, 24]
Epoch 451 Acc: 98.21 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 323 train Loss: 3346.0 test Loss: 336.7
Epoch 452 Iter 0 subLoss 2780.4 multi -1.99 import weight 0.00
Epoch 452 Iter 1 subLoss 3353.7 multi 6.97 import weight 0.00
Epoch 452 Iter 2 subLoss 3448.1 multi -16.91 import weight 0.00
Epoch 452 Iter 3 subLoss 3327.8 multi -7.96 import weight 0.00
Epoch 452 Iter 4 subLoss 4158.9 multi 3.98 import weight 0.00
Epoch 452 Iter 5 subLoss 3773.6 multi -28.85 import weight 0.00
Epoch 452 Iter 6 subLoss 9282.2 multi 3.98 import weight 0.00
Epoch 452 Iter 7 subLoss 5973.9 multi 1.00 import weight 0.00
Epoch 452 Iter 8 subLoss 5472.8 multi 3.98 import weight 0.00
Epoch 452 Iter 9 subLoss 5259.7 multi -1.99 import weight 0.00
Epoch 452 Iter 10 subLoss 5582.2 multi 1.00 import weight 0.00
Epoch 452 Iter 11 subLoss 5270.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10812 / 16.00
Entropy seen (from low to high)
[7, 108, 254, 539, 542, 406, 492, 595, 464, 358, 255, 205, 140, 88, 84, 77, 65, 53, 47, 30, 47, 39, 31, 23, 26, 21, 23, 21, 20, 15, 15, 12, 6, 5, 3, 5, 3, 6, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 17, 33, 58, 65, 93, 88, 115, 136, 133, 171, 179, 186, 207, 204, 188, 207, 203, 177, 181, 170, 175, 157, 163, 147, 130, 153, 139, 147, 143, 117, 122, 123, 107, 86, 68, 37, 15, 6, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 0.0, 36.4, 40.6, 44.2, 47.2, 50.9, 54.4, 57.9, 61.2, 64.4, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 83.3, 66.6, 59.9, 46.1, 67.8, 59.9, 70.8, 77.4, 83.3, 91.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 3, 10, 13, 28, 20, 24, 31, 18, 23]
Epoch 452 Acc: 97.26 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 527 train Loss: 5271.4 test Loss: 495.3
Epoch 453 Iter 0 subLoss 5193.3 multi -7.96 import weight 0.00
Epoch 453 Iter 1 subLoss 6130.7 multi -4.97 import weight 0.00
Epoch 453 Iter 2 subLoss 6959.1 multi 1.00 import weight 0.00
Epoch 453 Iter 3 subLoss 7286.5 multi 1.00 import weight 0.00
Epoch 453 Iter 4 subLoss 7683.5 multi 9.96 import weight 0.00
Epoch 453 Iter 5 subLoss 4329.6 multi -1.99 import weight 0.00
Epoch 453 Iter 6 subLoss 4934.3 multi -28.85 import weight 0.00
Epoch 453 Iter 7 subLoss 10497.4 multi 1.00 import weight 0.00
Epoch 453 Iter 8 subLoss 9134.1 multi -4.97 import weight 0.00
Epoch 453 Iter 9 subLoss 15701.8 multi -1.99 import weight 0.00
Epoch 453 Iter 10 subLoss 22698.8 multi -1.99 import weight 0.00
Epoch 453 Iter 11 subLoss 37273.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10778 / 16.32
Entropy seen (from low to high)
[7, 109, 256, 538, 540, 341, 439, 588, 493, 394, 274, 216, 153, 84, 95, 77, 68, 53, 49, 27, 51, 36, 33, 25, 26, 22, 25, 20, 18, 16, 16, 11, 8, 5, 3, 5, 3, 6, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 33, 60, 64, 91, 92, 115, 140, 132, 177, 175, 195, 200, 209, 192, 214, 199, 174, 183, 175, 168, 170, 154, 138, 134, 155, 145, 146, 134, 121, 121, 121, 97, 88, 64, 30, 18, 4, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 34.9, 36.7, 40.4, 44.2, 47.2, 50.9, 54.5, 57.8, 61.3, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 79.9, 66.6, 59.9, 46.1, 66.6, 61.1, 67.9, 81.2, 83.3, 91.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 10, 13, 30, 18, 25, 32, 18, 24]
Epoch 453 Acc: 80.79 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3727 train Loss: 28262.7 test Loss: 3294.4
Epoch 454 Iter 0 subLoss 25999.3 multi 1.00 import weight 0.00
Epoch 454 Iter 1 subLoss 21909.2 multi 1.00 import weight 0.00
Epoch 454 Iter 2 subLoss 17552.8 multi 1.00 import weight 0.00
Epoch 454 Iter 3 subLoss 14516.4 multi -1.99 import weight 0.00
Epoch 454 Iter 4 subLoss 20355.9 multi 1.00 import weight 0.00
Epoch 454 Iter 5 subLoss 15555.7 multi 1.00 import weight 0.00
Epoch 454 Iter 6 subLoss 13857.7 multi 1.00 import weight 0.00
Epoch 454 Iter 7 subLoss 11758.1 multi 6.97 import weight 0.00
Epoch 454 Iter 8 subLoss 7491.9 multi -1.98 import weight 0.00
Epoch 454 Iter 9 subLoss 8530.6 multi 6.97 import weight 0.00
Epoch 454 Iter 10 subLoss 6711.0 multi 3.99 import weight 0.00
Epoch 454 Iter 11 subLoss 6383.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10760 / 16.12
Entropy seen (from low to high)
[7, 109, 259, 540, 542, 345, 442, 582, 489, 394, 271, 209, 156, 85, 96, 74, 65, 55, 55, 26, 47, 40, 31, 29, 23, 22, 25, 20, 19, 16, 15, 12, 8, 4, 4, 4, 4, 6, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 34, 58, 66, 90, 95, 122, 137, 136, 171, 179, 191, 204, 213, 190, 216, 199, 172, 189, 161, 174, 172, 154, 139, 132, 150, 149, 145, 130, 122, 121, 121, 92, 87, 64, 32, 17, 4, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 34.9, 36.9, 41.5, 44.2, 47.3, 50.8, 54.3, 57.8, 61.2, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 99.9, 54.5, 49.9, 68.9, 57.8, 67.9, 81.2, 83.3, 91.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 6, 2, 11, 12, 29, 19, 25, 32, 18, 23]
Epoch 454 Acc: 96.98 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 638 train Loss: 6052.9 test Loss: 568.0
Epoch 455 Iter 0 subLoss 5546.9 multi 3.99 import weight 0.00
Epoch 455 Iter 1 subLoss 5338.1 multi -13.93 import weight 0.00
Epoch 455 Iter 2 subLoss 7949.1 multi -4.97 import weight 0.00
Epoch 455 Iter 3 subLoss 8238.1 multi 1.00 import weight 0.00
Epoch 455 Iter 4 subLoss 7261.9 multi 1.00 import weight 0.00
Epoch 455 Iter 5 subLoss 7230.9 multi -7.96 import weight 0.00
Epoch 455 Iter 6 subLoss 9308.2 multi 6.97 import weight 0.00
Epoch 455 Iter 7 subLoss 7619.5 multi 3.98 import weight 0.00
Epoch 455 Iter 8 subLoss 6778.1 multi 1.00 import weight 0.00
Epoch 455 Iter 9 subLoss 6637.0 multi -7.96 import weight 0.00
Epoch 455 Iter 10 subLoss 8180.0 multi -7.96 import weight 0.00
Epoch 455 Iter 11 subLoss 10647.4 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10727 / 16.32
Entropy seen (from low to high)
[7, 111, 261, 542, 538, 350, 438, 570, 483, 396, 265, 218, 150, 94, 94, 78, 62, 57, 55, 31, 41, 43, 34, 25, 24, 26, 24, 17, 20, 17, 17, 10, 10, 5, 3, 4, 4, 6, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 34, 62, 65, 90, 95, 125, 142, 140, 170, 179, 192, 218, 190, 193, 230, 194, 175, 196, 155, 175, 176, 156, 124, 142, 148, 147, 140, 129, 117, 126, 117, 88, 87, 60, 31, 18, 2, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 34.9, 37.2, 41.5, 44.1, 47.3, 50.7, 54.3, 57.7, 61.2, 64.5, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 59.9, 99.9, 54.5, 49.9, 69.9, 55.5, 69.2, 78.1, 88.2, 91.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 5, 2, 11, 12, 30, 18, 26, 32, 17, 24]
Epoch 455 Acc: 93.46 BMA: 98.40 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1064 train Loss: 11133.1 test Loss: 1209.4
Epoch 456 Iter 0 subLoss 10786.3 multi 9.96 import weight 0.00
Epoch 456 Iter 1 subLoss 7221.6 multi 9.96 import weight 0.00
Epoch 456 Iter 2 subLoss 6003.4 multi 6.97 import weight 0.00
Epoch 456 Iter 3 subLoss 5608.3 multi -22.88 import weight 0.00
Epoch 456 Iter 4 subLoss 7461.6 multi 6.97 import weight 0.00
Epoch 456 Iter 5 subLoss 6624.1 multi 12.94 import weight 0.00
Epoch 456 Iter 6 subLoss 5688.1 multi -10.94 import weight 0.00
Epoch 456 Iter 7 subLoss 6230.1 multi 6.97 import weight 0.00
Epoch 456 Iter 8 subLoss 5342.5 multi -7.96 import weight 0.00
Epoch 456 Iter 9 subLoss 6477.6 multi -1.98 import weight 0.00
Epoch 456 Iter 10 subLoss 6087.3 multi 9.96 import weight 0.00
Epoch 456 Iter 11 subLoss 5987.5 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10713 / 16.41
Entropy seen (from low to high)
[7, 114, 259, 553, 535, 347, 439, 577, 478, 390, 265, 213, 148, 100, 88, 81, 64, 55, 57, 29, 43, 43, 34, 24, 24, 25, 24, 18, 20, 17, 17, 9, 10, 5, 4, 4, 4, 6, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 34, 62, 68, 87, 96, 125, 139, 152, 166, 181, 192, 219, 189, 191, 236, 184, 186, 186, 158, 179, 179, 148, 134, 138, 147, 146, 139, 124, 118, 127, 114, 84, 90, 59, 31, 18, 2, 7, 0, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.5, 34.7, 36.8, 41.6, 43.9, 47.2, 50.6, 54.4, 57.9, 61.1, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 99.9, 59.9, 53.8, 68.9, 57.1, 71.9, 76.6, 89.4, 87.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 6, 2, 10, 13, 29, 21, 25, 30, 19, 24]
Epoch 456 Acc: 96.65 BMA: 98.42 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 598 train Loss: 5797.5 test Loss: 573.8
Epoch 457 Iter 0 subLoss 5362.4 multi 1.00 import weight 0.00
Epoch 457 Iter 1 subLoss 5613.6 multi -4.97 import weight 0.00
Epoch 457 Iter 2 subLoss 5172.5 multi 3.98 import weight 0.00
Epoch 457 Iter 3 subLoss 6008.7 multi 9.96 import weight 0.00
Epoch 457 Iter 4 subLoss 5145.6 multi -1.98 import weight 0.00
Epoch 457 Iter 5 subLoss 5175.4 multi 6.97 import weight 0.00
Epoch 457 Iter 6 subLoss 4748.3 multi 1.00 import weight 0.00
Epoch 457 Iter 7 subLoss 4985.1 multi 1.00 import weight 0.00
Epoch 457 Iter 8 subLoss 4027.8 multi 6.97 import weight 0.00
Epoch 457 Iter 9 subLoss 4629.3 multi -1.99 import weight 0.00
Epoch 457 Iter 10 subLoss 4467.8 multi 6.97 import weight 0.00
Epoch 457 Iter 11 subLoss 3759.0 multi 24.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10715 / 16.01
Entropy seen (from low to high)
[8, 115, 263, 555, 536, 348, 444, 577, 478, 389, 260, 218, 138, 99, 88, 82, 61, 56, 55, 29, 43, 45, 32, 24, 25, 25, 26, 15, 20, 16, 18, 9, 10, 5, 4, 4, 4, 6, 4, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 18, 36, 59, 67, 86, 101, 123, 139, 149, 169, 183, 188, 219, 190, 195, 235, 185, 183, 185, 158, 182, 168, 156, 131, 141, 146, 143, 140, 126, 119, 124, 117, 86, 86, 60, 33, 17, 3, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 34.7, 36.8, 41.7, 43.8, 47.4, 50.7, 54.4, 58.0, 61.2, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 99.9, 55.5, 62.4, 66.6, 59.0, 70.8, 77.4, 88.8, 84.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 6, 2, 9, 16, 27, 22, 24, 31, 18, 26]
Epoch 457 Acc: 97.80 BMA: 98.42 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 24.88 Pidx 375 train Loss: 3838.3 test Loss: 340.3
Epoch 458 Iter 0 subLoss 3989.2 multi -1.99 import weight 0.00
Epoch 458 Iter 1 subLoss 3442.7 multi -13.93 import weight 0.00
Epoch 458 Iter 2 subLoss 4509.9 multi -16.91 import weight 0.00
Epoch 458 Iter 3 subLoss 8046.8 multi 3.98 import weight 0.00
Epoch 458 Iter 4 subLoss 4702.6 multi -1.99 import weight 0.00
Epoch 458 Iter 5 subLoss 5401.0 multi 3.98 import weight 0.00
Epoch 458 Iter 6 subLoss 4176.4 multi 27.87 import weight 0.00
Epoch 458 Iter 7 subLoss 3769.6 multi 3.99 import weight 0.00
Epoch 458 Iter 8 subLoss 3618.5 multi 1.00 import weight 0.00
Epoch 458 Iter 9 subLoss 3509.4 multi 21.90 import weight 0.00
Epoch 458 Iter 10 subLoss 3592.6 multi -25.87 import weight 0.00
Epoch 458 Iter 11 subLoss 4299.2 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10712 / 16.51
Entropy seen (from low to high)
[8, 115, 266, 565, 525, 354, 443, 586, 475, 381, 261, 215, 136, 101, 86, 81, 62, 56, 53, 30, 43, 43, 34, 24, 25, 24, 26, 16, 19, 18, 17, 9, 11, 4, 4, 4, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 7, 17, 35, 59, 69, 85, 98, 129, 135, 147, 172, 179, 190, 221, 191, 198, 233, 180, 183, 194, 156, 173, 174, 156, 130, 143, 146, 141, 139, 126, 117, 125, 117, 88, 84, 61, 34, 16, 4, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.8, 34.7, 36.8, 41.5, 44.0, 47.4, 50.6, 54.4, 58.1, 61.1, 64.2, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 66.6, 99.9, 54.5, 66.6, 65.5, 60.8, 70.8, 77.7, 84.9, 88.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 6, 2, 11, 12, 29, 23, 24, 27, 20, 26]
Epoch 458 Acc: 96.91 BMA: 98.42 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 429 train Loss: 4402.4 test Loss: 497.3
Epoch 459 Iter 0 subLoss 3844.9 multi 1.00 import weight 0.00
Epoch 459 Iter 1 subLoss 4912.6 multi -4.97 import weight 0.00
Epoch 459 Iter 2 subLoss 5871.9 multi 1.00 import weight 0.00
Epoch 459 Iter 3 subLoss 5113.3 multi 1.00 import weight 0.00
Epoch 459 Iter 4 subLoss 4786.6 multi 6.97 import weight 0.00
Epoch 459 Iter 5 subLoss 4471.7 multi 1.00 import weight 0.00
Epoch 459 Iter 6 subLoss 4086.6 multi 15.93 import weight 0.00
Epoch 459 Iter 7 subLoss 2792.0 multi 21.90 import weight 0.00
Epoch 459 Iter 8 subLoss 2560.9 multi 9.96 import weight 0.00
Epoch 459 Iter 9 subLoss 2688.6 multi 15.93 import weight 0.00
Epoch 459 Iter 10 subLoss 2353.6 multi -19.90 import weight 0.00
Epoch 459 Iter 11 subLoss 2898.0 multi 30.85 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10719 / 16.47
Entropy seen (from low to high)
[9, 117, 266, 578, 514, 355, 461, 580, 472, 373, 263, 212, 139, 95, 84, 81, 63, 55, 52, 31, 43, 43, 33, 25, 24, 24, 27, 16, 18, 18, 17, 9, 12, 3, 4, 4, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 7, 17, 35, 59, 67, 84, 101, 125, 138, 149, 169, 181, 192, 215, 191, 202, 230, 181, 184, 191, 153, 181, 167, 155, 131, 143, 143, 146, 143, 122, 119, 122, 122, 86, 83, 64, 33, 17, 4, 6, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.8, 34.6, 36.5, 40.0, 43.7, 47.3, 50.6, 54.4, 58.0, 61.2, 64.4, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 59.9, 99.9, 54.5, 69.2, 65.5, 59.0, 71.9, 78.5, 84.2, 88.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 2, 11, 13, 29, 22, 25, 28, 19, 26]
Epoch 459 Acc: 98.50 BMA: 98.44 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 30.85 Pidx 289 train Loss: 2836.0 test Loss: 262.1
Epoch 460 Iter 0 subLoss 2879.2 multi 21.90 import weight 0.00
Epoch 460 Iter 1 subLoss 3216.8 multi -22.88 import weight 0.00
Epoch 460 Iter 2 subLoss 2952.2 multi 3.98 import weight 0.00
Epoch 460 Iter 3 subLoss 3084.0 multi 1.00 import weight 0.00
Epoch 460 Iter 4 subLoss 2525.5 multi -1.99 import weight 0.00
Epoch 460 Iter 5 subLoss 2558.6 multi -4.97 import weight 0.00
Epoch 460 Iter 6 subLoss 2879.7 multi 24.88 import weight 0.00
Epoch 460 Iter 7 subLoss 3035.0 multi -4.97 import weight 0.00
Epoch 460 Iter 8 subLoss 3516.7 multi 6.97 import weight 0.00
Epoch 460 Iter 9 subLoss 2588.8 multi 27.87 import weight 0.00
Epoch 460 Iter 10 subLoss 2423.2 multi -4.97 import weight 0.00
Epoch 460 Iter 11 subLoss 2609.8 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10730 / 16.33
Entropy seen (from low to high)
[9, 118, 272, 578, 513, 356, 466, 584, 466, 378, 253, 215, 133, 96, 85, 80, 62, 58, 49, 29, 44, 45, 31, 25, 24, 23, 27, 16, 18, 18, 17, 9, 12, 3, 4, 4, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 7, 17, 33, 60, 69, 83, 100, 121, 140, 150, 166, 188, 185, 211, 197, 200, 227, 183, 187, 186, 158, 177, 167, 154, 136, 139, 142, 147, 144, 124, 116, 122, 122, 90, 84, 63, 36, 17, 4, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 34.6, 36.6, 40.2, 43.8, 47.4, 50.7, 54.4, 58.0, 61.2, 64.5, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 59.9, 99.9, 54.5, 64.2, 67.8, 59.0, 69.5, 80.6, 83.3, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 2, 11, 14, 28, 22, 23, 31, 18, 25]
Epoch 460 Acc: 98.48 BMA: 98.46 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 260 train Loss: 2700.6 test Loss: 272.6
Epoch 461 Iter 0 subLoss 2620.6 multi 12.94 import weight 0.00
Epoch 461 Iter 1 subLoss 2198.4 multi -4.97 import weight 0.00
Epoch 461 Iter 2 subLoss 2412.4 multi 3.98 import weight 0.00
Epoch 461 Iter 3 subLoss 2176.9 multi -1.99 import weight 0.00
Epoch 461 Iter 4 subLoss 2736.9 multi -22.88 import weight 0.00
Epoch 461 Iter 5 subLoss 3056.7 multi 3.98 import weight 0.00
Epoch 461 Iter 6 subLoss 2599.1 multi -25.87 import weight 0.00
Epoch 461 Iter 7 subLoss 4106.1 multi 15.93 import weight 0.00
Epoch 461 Iter 8 subLoss 2779.7 multi -34.82 import weight 0.00
Epoch 461 Iter 9 subLoss 8643.8 multi 6.97 import weight 0.00
Epoch 461 Iter 10 subLoss 3566.3 multi -1.98 import weight 0.00
Epoch 461 Iter 11 subLoss 3705.5 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10730 / 16.35
Entropy seen (from low to high)
[9, 119, 275, 584, 510, 356, 468, 590, 460, 373, 260, 211, 130, 94, 85, 79, 64, 57, 50, 26, 44, 43, 33, 25, 25, 22, 27, 15, 19, 18, 17, 9, 12, 3, 4, 4, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 7, 17, 33, 62, 66, 85, 96, 129, 137, 153, 161, 188, 185, 209, 198, 203, 228, 183, 182, 188, 159, 177, 168, 149, 135, 143, 144, 142, 146, 120, 119, 121, 123, 91, 84, 63, 37, 17, 4, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 34.7, 36.5, 40.8, 44.0, 47.4, 50.6, 54.5, 58.0, 60.9, 64.3, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 59.9, 99.9, 49.9, 64.2, 66.6, 58.3, 71.4, 79.3, 85.7, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 10, 14, 27, 24, 21, 29, 21, 25]
Epoch 461 Acc: 97.12 BMA: 98.44 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 370 train Loss: 4067.3 test Loss: 462.8
Epoch 462 Iter 0 subLoss 3876.4 multi 9.96 import weight 0.00
Epoch 462 Iter 1 subLoss 2795.6 multi 24.88 import weight 0.00
Epoch 462 Iter 2 subLoss 2964.0 multi -19.90 import weight 0.00
Epoch 462 Iter 3 subLoss 2124.3 multi 9.96 import weight 0.00
Epoch 462 Iter 4 subLoss 2388.0 multi 3.98 import weight 0.00
Epoch 462 Iter 5 subLoss 2765.2 multi 36.82 import weight 0.00
Epoch 462 Iter 6 subLoss 2413.0 multi 6.97 import weight 0.00
Epoch 462 Iter 7 subLoss 2872.8 multi 27.87 import weight 0.00
Epoch 462 Iter 8 subLoss 2220.8 multi -10.94 import weight 0.00
Epoch 462 Iter 9 subLoss 2401.5 multi 3.99 import weight 0.00
Epoch 462 Iter 10 subLoss 2296.3 multi 6.97 import weight 0.00
Epoch 462 Iter 11 subLoss 2228.7 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10740 / 16.22
Entropy seen (from low to high)
[11, 118, 281, 590, 504, 355, 480, 590, 454, 375, 252, 212, 130, 91, 83, 81, 64, 55, 50, 27, 45, 42, 31, 25, 24, 22, 28, 15, 19, 18, 16, 9, 12, 3, 5, 3, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 33, 60, 68, 87, 93, 127, 139, 152, 161, 187, 184, 206, 206, 196, 220, 193, 176, 192, 157, 180, 164, 151, 137, 138, 148, 140, 152, 118, 119, 119, 125, 93, 83, 62, 40, 18, 4, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.1, 34.9, 36.8, 40.9, 43.9, 47.3, 50.6, 54.4, 58.0, 61.1, 64.3, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 99.9, 55.5, 59.9, 66.6, 56.5, 72.7, 79.9, 84.2, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 3, 9, 15, 27, 23, 22, 30, 19, 25]
Epoch 462 Acc: 98.68 BMA: 98.48 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 222 train Loss: 2457.6 test Loss: 247.4
Epoch 463 Iter 0 subLoss 2624.1 multi 15.93 import weight 0.00
Epoch 463 Iter 1 subLoss 2304.3 multi 18.91 import weight 0.00
Epoch 463 Iter 2 subLoss 2005.9 multi -13.93 import weight 0.00
Epoch 463 Iter 3 subLoss 2097.5 multi -13.93 import weight 0.00
Epoch 463 Iter 4 subLoss 2750.9 multi -16.91 import weight 0.00
Epoch 463 Iter 5 subLoss 6013.0 multi -22.88 import weight 0.00
Epoch 463 Iter 6 subLoss 293978.1 multi 1.00 import weight 0.00
Epoch 463 Iter 7 subLoss 16678.9 multi 1.00 import weight 0.00
Epoch 463 Iter 8 subLoss 13012.5 multi 1.00 import weight 0.00
Epoch 463 Iter 9 subLoss 10453.2 multi 1.00 import weight 0.00
Epoch 463 Iter 10 subLoss 8939.6 multi 1.00 import weight 0.00
Epoch 463 Iter 11 subLoss 7049.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10734 / 16.34
Entropy seen (from low to high)
[11, 120, 282, 595, 505, 351, 479, 600, 440, 373, 250, 215, 128, 89, 84, 83, 65, 52, 54, 26, 44, 42, 32, 26, 25, 21, 28, 14, 19, 18, 17, 9, 11, 4, 5, 3, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 34, 59, 68, 86, 97, 124, 140, 153, 164, 187, 184, 205, 204, 199, 220, 192, 177, 193, 153, 185, 158, 155, 136, 142, 135, 147, 151, 118, 117, 124, 115, 98, 84, 63, 39, 18, 4, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.1, 34.8, 36.7, 40.9, 44.0, 47.3, 50.4, 54.4, 57.9, 60.9, 64.3, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 99.9, 49.9, 61.5, 66.6, 58.3, 71.4, 82.7, 82.6, 87.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 3, 10, 13, 27, 24, 21, 29, 23, 24]
Epoch 463 Acc: 95.21 BMA: 98.50 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 704 train Loss: 4898.4 test Loss: 695.9
Epoch 464 Iter 0 subLoss 4822.8 multi -13.93 import weight 0.00
Epoch 464 Iter 1 subLoss 11683.8 multi 3.99 import weight 0.00
Epoch 464 Iter 2 subLoss 6354.1 multi 1.00 import weight 0.00
Epoch 464 Iter 3 subLoss 6336.2 multi -1.99 import weight 0.00
Epoch 464 Iter 4 subLoss 6483.9 multi 6.97 import weight 0.00
Epoch 464 Iter 5 subLoss 4233.9 multi 6.97 import weight 0.00
Epoch 464 Iter 6 subLoss 3788.2 multi 12.94 import weight 0.00
Epoch 464 Iter 7 subLoss 2579.4 multi -7.96 import weight 0.00
Epoch 464 Iter 8 subLoss 2804.4 multi 3.99 import weight 0.00
Epoch 464 Iter 9 subLoss 3226.9 multi 12.94 import weight 0.00
Epoch 464 Iter 10 subLoss 3095.9 multi 21.90 import weight 0.00
Epoch 464 Iter 11 subLoss 2439.5 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10744 / 16.69
Entropy seen (from low to high)
[11, 124, 283, 598, 506, 351, 482, 599, 443, 374, 246, 206, 131, 92, 76, 85, 64, 51, 55, 25, 44, 42, 32, 29, 22, 23, 26, 16, 18, 17, 18, 8, 12, 3, 5, 3, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 33, 60, 68, 87, 96, 121, 141, 152, 160, 192, 184, 205, 204, 197, 222, 193, 174, 184, 163, 177, 166, 151, 140, 138, 136, 146, 151, 122, 117, 122, 116, 99, 87, 63, 39, 16, 6, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 34.6, 36.4, 41.0, 44.0, 47.3, 50.5, 54.3, 57.8, 61.0, 64.4, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 79.9, 99.9, 59.9, 53.8, 66.6, 56.5, 72.7, 83.3, 81.8, 87.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 10, 13, 27, 23, 22, 30, 22, 24]
Epoch 464 Acc: 98.15 BMA: 98.50 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 243 train Loss: 2632.5 test Loss: 307.2
Epoch 465 Iter 0 subLoss 2579.0 multi -4.97 import weight 0.00
Epoch 465 Iter 1 subLoss 2674.3 multi -1.99 import weight 0.00
Epoch 465 Iter 2 subLoss 2537.1 multi 6.97 import weight 0.00
Epoch 465 Iter 3 subLoss 3065.0 multi -10.94 import weight 0.00
Epoch 465 Iter 4 subLoss 2347.9 multi 15.93 import weight 0.00
Epoch 465 Iter 5 subLoss 2652.3 multi 3.99 import weight 0.00
Epoch 465 Iter 6 subLoss 2263.7 multi -19.90 import weight 0.00
Epoch 465 Iter 7 subLoss 2637.9 multi -10.94 import weight 0.00
Epoch 465 Iter 8 subLoss 2990.9 multi 12.94 import weight 0.00
Epoch 465 Iter 9 subLoss 2342.3 multi 18.91 import weight 0.00
Epoch 465 Iter 10 subLoss 3252.8 multi 9.96 import weight 0.00
Epoch 465 Iter 11 subLoss 2529.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10755 / 16.25
Entropy seen (from low to high)
[11, 124, 287, 602, 505, 353, 488, 603, 435, 376, 241, 203, 133, 90, 72, 85, 64, 55, 52, 25, 43, 41, 33, 28, 23, 21, 27, 16, 18, 18, 18, 7, 12, 3, 6, 2, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 33, 60, 68, 88, 95, 117, 144, 150, 162, 186, 188, 204, 205, 192, 225, 190, 171, 189, 161, 180, 164, 152, 141, 134, 141, 141, 150, 126, 119, 120, 119, 101, 85, 63, 40, 18, 6, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 34.7, 36.8, 41.1, 43.9, 47.1, 50.5, 54.3, 57.7, 61.1, 64.5, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 99.9, 55.5, 61.5, 64.2, 54.5, 73.9, 83.3, 81.8, 87.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 3, 9, 13, 28, 22, 23, 30, 22, 24]
Epoch 465 Acc: 98.23 BMA: 98.50 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 252 train Loss: 2432.2 test Loss: 289.1
Epoch 466 Iter 0 subLoss 2466.7 multi 3.98 import weight 0.00
Epoch 466 Iter 1 subLoss 2584.2 multi 24.88 import weight 0.00
Epoch 466 Iter 2 subLoss 2660.6 multi 9.96 import weight 0.00
Epoch 466 Iter 3 subLoss 2039.1 multi 6.97 import weight 0.00
Epoch 466 Iter 4 subLoss 2478.5 multi 6.97 import weight 0.00
Epoch 466 Iter 5 subLoss 2094.3 multi -10.94 import weight 0.00
Epoch 466 Iter 6 subLoss 2291.9 multi 9.96 import weight 0.00
Epoch 466 Iter 7 subLoss 2467.4 multi 6.97 import weight 0.00
Epoch 466 Iter 8 subLoss 2331.9 multi 15.93 import weight 0.00
Epoch 466 Iter 9 subLoss 1818.5 multi -1.99 import weight 0.00
Epoch 466 Iter 10 subLoss 2275.3 multi 21.90 import weight 0.00
Epoch 466 Iter 11 subLoss 2418.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10768 / 16.63
Entropy seen (from low to high)
[11, 126, 286, 610, 501, 358, 495, 597, 442, 368, 235, 203, 132, 93, 73, 82, 65, 53, 51, 24, 45, 40, 32, 27, 23, 21, 27, 16, 20, 16, 18, 8, 11, 3, 6, 2, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 31, 62, 68, 89, 94, 119, 141, 143, 169, 183, 190, 200, 203, 194, 223, 189, 171, 190, 166, 177, 164, 150, 142, 135, 134, 149, 153, 125, 115, 120, 122, 98, 90, 64, 41, 18, 6, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.8, 34.7, 36.8, 40.7, 43.9, 47.5, 50.7, 54.2, 57.7, 61.2, 64.4, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 74.9, 99.9, 63.6, 49.9, 69.2, 52.3, 74.9, 83.3, 80.9, 87.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 4, 2, 11, 14, 26, 21, 24, 30, 21, 25]
Epoch 466 Acc: 98.58 BMA: 98.50 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 241 train Loss: 2246.9 test Loss: 255.6
Epoch 467 Iter 0 subLoss 2086.1 multi 6.97 import weight 0.00
Epoch 467 Iter 1 subLoss 2051.6 multi -4.97 import weight 0.00
Epoch 467 Iter 2 subLoss 2291.0 multi 12.94 import weight 0.00
Epoch 467 Iter 3 subLoss 2016.4 multi 3.99 import weight 0.00
Epoch 467 Iter 4 subLoss 2144.4 multi 3.99 import weight 0.00
Epoch 467 Iter 5 subLoss 2129.9 multi 12.94 import weight 0.00
Epoch 467 Iter 6 subLoss 2042.7 multi 1.00 import weight 0.00
Epoch 467 Iter 7 subLoss 2250.2 multi 6.97 import weight 0.00
Epoch 467 Iter 8 subLoss 1947.9 multi -1.98 import weight 0.00
Epoch 467 Iter 9 subLoss 2095.9 multi -10.94 import weight 0.00
Epoch 467 Iter 10 subLoss 2704.2 multi 1.00 import weight 0.00
Epoch 467 Iter 11 subLoss 2245.3 multi -19.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10780 / 17.48
Entropy seen (from low to high)
[11, 129, 288, 618, 495, 358, 500, 599, 439, 360, 239, 199, 135, 89, 74, 80, 66, 53, 50, 25, 45, 40, 31, 27, 21, 22, 28, 17, 18, 18, 16, 9, 11, 2, 6, 2, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 31, 61, 69, 87, 93, 117, 143, 141, 169, 187, 185, 201, 202, 192, 225, 189, 166, 187, 174, 174, 165, 150, 146, 135, 133, 146, 153, 122, 124, 116, 121, 103, 90, 66, 41, 18, 6, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.7, 34.3, 36.5, 40.8, 43.9, 47.3, 50.6, 54.1, 57.7, 61.3, 64.6, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 79.9, 99.9, 59.9, 57.1, 67.9, 52.1, 70.8, 83.8, 84.9, 91.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 2, 10, 14, 25, 23, 24, 31, 20, 24]
Epoch 467 Acc: 98.50 BMA: 98.50 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -19.90 Pidx 224 train Loss: 2238.0 test Loss: 260.7
Epoch 468 Iter 0 subLoss 2338.0 multi 18.91 import weight 0.00
Epoch 468 Iter 1 subLoss 2144.2 multi 6.97 import weight 0.00
Epoch 468 Iter 2 subLoss 1908.8 multi 6.97 import weight 0.00
Epoch 468 Iter 3 subLoss 1927.7 multi -10.94 import weight 0.00
Epoch 468 Iter 4 subLoss 1936.7 multi 1.00 import weight 0.00
Epoch 468 Iter 5 subLoss 1814.9 multi 1.00 import weight 0.00
Epoch 468 Iter 6 subLoss 2452.0 multi -16.91 import weight 0.00
Epoch 468 Iter 7 subLoss 2722.3 multi 12.94 import weight 0.00
Epoch 468 Iter 8 subLoss 1699.6 multi -4.97 import weight 0.00
Epoch 468 Iter 9 subLoss 2064.6 multi -10.94 import weight 0.00
Epoch 468 Iter 10 subLoss 2617.9 multi 1.00 import weight 0.00
Epoch 468 Iter 11 subLoss 2306.7 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10792 / 18.45
Entropy seen (from low to high)
[11, 129, 292, 627, 497, 349, 507, 596, 444, 355, 240, 193, 133, 90, 71, 81, 65, 53, 54, 22, 43, 41, 31, 27, 23, 21, 26, 18, 19, 18, 16, 7, 12, 1, 6, 2, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 17, 32, 60, 66, 85, 99, 112, 149, 135, 167, 187, 187, 200, 205, 186, 226, 191, 166, 180, 183, 166, 168, 151, 147, 138, 128, 145, 152, 124, 128, 111, 127, 101, 92, 66, 43, 18, 7, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.6, 34.2, 36.1, 40.1, 43.7, 47.0, 50.6, 54.2, 57.7, 61.3, 64.6, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 99.9, 55.5, 71.4, 65.3, 52.1, 69.5, 84.3, 84.9, 95.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 3, 9, 14, 26, 23, 23, 32, 20, 22]
Epoch 468 Acc: 98.66 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 230 train Loss: 2127.6 test Loss: 231.1
Epoch 469 Iter 0 subLoss 2222.0 multi -4.97 import weight 0.00
Epoch 469 Iter 1 subLoss 1990.0 multi 21.90 import weight 0.00
Epoch 469 Iter 2 subLoss 2035.4 multi 9.96 import weight 0.00
Epoch 469 Iter 3 subLoss 1952.6 multi -7.96 import weight 0.00
Epoch 469 Iter 4 subLoss 2078.0 multi 6.97 import weight 0.00
Epoch 469 Iter 5 subLoss 2188.9 multi -4.97 import weight 0.00
Epoch 469 Iter 6 subLoss 2264.2 multi -19.90 import weight 0.00
Epoch 469 Iter 7 subLoss 2175.3 multi 1.00 import weight 0.00
Epoch 469 Iter 8 subLoss 2703.0 multi 3.98 import weight 0.00
Epoch 469 Iter 9 subLoss 2327.3 multi -31.84 import weight 0.00
Epoch 469 Iter 10 subLoss 2229.5 multi -1.98 import weight 0.00
Epoch 469 Iter 11 subLoss 2913.6 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10803 / 17.67
Entropy seen (from low to high)
[12, 128, 294, 633, 502, 345, 510, 593, 451, 349, 239, 191, 128, 92, 70, 81, 66, 51, 52, 23, 43, 42, 30, 28, 21, 22, 26, 17, 19, 18, 16, 8, 11, 1, 6, 2, 4, 5, 5, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 18, 31, 60, 67, 82, 100, 112, 148, 134, 166, 191, 186, 195, 209, 185, 219, 194, 168, 183, 180, 168, 171, 145, 147, 141, 126, 146, 150, 124, 130, 114, 125, 100, 95, 65, 46, 18, 7, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.4, 34.1, 36.1, 39.3, 43.6, 47.1, 50.7, 54.2, 57.7, 61.3, 64.7, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.9, 99.9, 59.9, 64.2, 65.3, 59.0, 69.5, 81.8, 84.9, 94.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 2, 10, 14, 26, 22, 23, 33, 20, 20]
Epoch 469 Acc: 98.66 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 291 train Loss: 2381.9 test Loss: 232.3
Epoch 470 Iter 0 subLoss 2176.2 multi 3.98 import weight 0.00
Epoch 470 Iter 1 subLoss 2258.5 multi 6.97 import weight 0.00
Epoch 470 Iter 2 subLoss 2756.6 multi -13.93 import weight 0.00
Epoch 470 Iter 3 subLoss 2701.7 multi 6.97 import weight 0.00
Epoch 470 Iter 4 subLoss 2233.6 multi 15.93 import weight 0.00
Epoch 470 Iter 5 subLoss 2024.0 multi -7.96 import weight 0.00
Epoch 470 Iter 6 subLoss 2145.7 multi 9.96 import weight 0.00
Epoch 470 Iter 7 subLoss 1904.3 multi 9.96 import weight 0.00
Epoch 470 Iter 8 subLoss 2066.4 multi -7.96 import weight 0.00
Epoch 470 Iter 9 subLoss 1905.0 multi 12.94 import weight 0.00
Epoch 470 Iter 10 subLoss 2272.3 multi 21.90 import weight 0.00
Epoch 470 Iter 11 subLoss 2376.0 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10815 / 17.66
Entropy seen (from low to high)
[12, 131, 292, 638, 501, 356, 514, 589, 448, 341, 241, 191, 126, 90, 72, 78, 66, 52, 49, 23, 45, 42, 28, 29, 20, 23, 25, 17, 19, 19, 15, 9, 10, 1, 6, 2, 3, 7, 4, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 18, 31, 60, 64, 81, 104, 111, 146, 133, 163, 196, 181, 193, 216, 184, 220, 191, 169, 180, 182, 164, 173, 149, 145, 140, 122, 147, 155, 119, 135, 111, 127, 101, 97, 66, 47, 18, 7, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.3, 34.0, 36.2, 39.4, 43.4, 47.1, 50.8, 54.2, 57.7, 61.4, 64.6, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 99.9, 55.5, 66.6, 69.2, 59.0, 65.2, 81.8, 88.8, 89.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 2, 9, 15, 26, 22, 23, 33, 18, 20]
Epoch 470 Acc: 98.25 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 237 train Loss: 2537.6 test Loss: 306.6
Epoch 471 Iter 0 subLoss 2251.3 multi 9.96 import weight 0.00
Epoch 471 Iter 1 subLoss 2019.5 multi 6.97 import weight 0.00
Epoch 471 Iter 2 subLoss 2363.4 multi 12.94 import weight 0.00
Epoch 471 Iter 3 subLoss 1854.3 multi 9.96 import weight 0.00
Epoch 471 Iter 4 subLoss 1828.2 multi -4.97 import weight 0.00
Epoch 471 Iter 5 subLoss 2484.1 multi 1.00 import weight 0.00
Epoch 471 Iter 6 subLoss 2143.7 multi 12.94 import weight 0.00
Epoch 471 Iter 7 subLoss 1872.1 multi 9.96 import weight 0.00
Epoch 471 Iter 8 subLoss 1866.5 multi -7.96 import weight 0.00
Epoch 471 Iter 9 subLoss 1997.0 multi 24.88 import weight 0.00
Epoch 471 Iter 10 subLoss 2424.5 multi -10.94 import weight 0.00
Epoch 471 Iter 11 subLoss 2088.1 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10826 / 17.60
Entropy seen (from low to high)
[12, 133, 295, 645, 494, 360, 524, 584, 448, 339, 237, 191, 121, 90, 73, 79, 63, 50, 51, 22, 45, 40, 30, 28, 21, 22, 25, 19, 18, 19, 14, 10, 9, 1, 6, 2, 3, 8, 3, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 18, 32, 58, 65, 79, 105, 110, 146, 131, 163, 196, 179, 195, 214, 187, 218, 191, 172, 178, 182, 161, 175, 148, 144, 140, 125, 145, 156, 121, 131, 114, 128, 102, 94, 69, 48, 19, 7, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.2, 34.2, 36.2, 39.4, 43.5, 47.0, 50.7, 54.1, 57.7, 61.3, 64.5, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 74.9, 99.9, 55.5, 71.4, 65.3, 63.6, 62.4, 79.9, 86.3, 94.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 2, 9, 14, 26, 22, 24, 30, 22, 19]
Epoch 471 Acc: 98.64 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 208 train Loss: 2071.7 test Loss: 227.9
Epoch 472 Iter 0 subLoss 2469.0 multi 6.97 import weight 0.00
Epoch 472 Iter 1 subLoss 1573.5 multi -4.97 import weight 0.00
Epoch 472 Iter 2 subLoss 1963.4 multi 9.96 import weight 0.00
Epoch 472 Iter 3 subLoss 1885.3 multi -16.91 import weight 0.00
Epoch 472 Iter 4 subLoss 2409.0 multi 6.97 import weight 0.00
Epoch 472 Iter 5 subLoss 1934.2 multi 3.99 import weight 0.00
Epoch 472 Iter 6 subLoss 2060.8 multi -4.97 import weight 0.00
Epoch 472 Iter 7 subLoss 2073.9 multi 3.99 import weight 0.00
Epoch 472 Iter 8 subLoss 1968.5 multi 12.94 import weight 0.00
Epoch 472 Iter 9 subLoss 2025.5 multi -7.96 import weight 0.00
Epoch 472 Iter 10 subLoss 2251.4 multi 12.94 import weight 0.00
Epoch 472 Iter 11 subLoss 1750.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10839 / 18.12
Entropy seen (from low to high)
[12, 135, 297, 647, 497, 358, 533, 585, 445, 334, 238, 189, 116, 89, 76, 76, 62, 53, 47, 25, 44, 41, 29, 28, 21, 24, 22, 18, 18, 19, 14, 10, 9, 1, 6, 2, 3, 8, 3, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 19, 32, 58, 62, 82, 102, 110, 143, 133, 161, 196, 175, 197, 218, 188, 214, 195, 169, 178, 186, 147, 184, 147, 145, 141, 128, 144, 154, 124, 126, 119, 127, 103, 95, 68, 51, 19, 7, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.1, 34.4, 36.3, 39.5, 43.5, 47.1, 50.8, 54.0, 57.6, 61.3, 64.5, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 99.9, 55.5, 66.6, 66.6, 63.6, 62.4, 80.6, 85.7, 94.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 2, 9, 15, 24, 22, 24, 31, 21, 20]
Epoch 472 Acc: 98.74 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 175 train Loss: 2047.5 test Loss: 207.6
Epoch 473 Iter 0 subLoss 2136.8 multi -13.93 import weight 0.00
Epoch 473 Iter 1 subLoss 2478.2 multi 3.99 import weight 0.00
Epoch 473 Iter 2 subLoss 2145.3 multi 12.94 import weight 0.00
Epoch 473 Iter 3 subLoss 1645.9 multi -1.99 import weight 0.00
Epoch 473 Iter 4 subLoss 1611.9 multi 1.00 import weight 0.00
Epoch 473 Iter 5 subLoss 2094.1 multi -10.94 import weight 0.00
Epoch 473 Iter 6 subLoss 2137.1 multi -10.94 import weight 0.00
Epoch 473 Iter 7 subLoss 2252.2 multi 15.93 import weight 0.00
Epoch 473 Iter 8 subLoss 2185.7 multi -7.96 import weight 0.00
Epoch 473 Iter 9 subLoss 2078.2 multi 6.97 import weight 0.00
Epoch 473 Iter 10 subLoss 2344.3 multi 15.93 import weight 0.00
Epoch 473 Iter 11 subLoss 2141.6 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10851 / 17.55
Entropy seen (from low to high)
[13, 135, 300, 654, 494, 364, 543, 575, 447, 323, 239, 185, 119, 88, 79, 73, 61, 50, 48, 27, 44, 41, 27, 27, 22, 24, 21, 19, 18, 18, 14, 11, 8, 1, 6, 3, 3, 7, 3, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 19, 32, 58, 58, 83, 103, 111, 138, 133, 166, 189, 181, 198, 213, 189, 217, 186, 172, 184, 184, 145, 186, 148, 143, 139, 129, 144, 155, 125, 126, 120, 127, 102, 94, 73, 50, 20, 7, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.0, 34.6, 36.4, 39.6, 43.6, 47.0, 50.6, 53.7, 57.5, 61.4, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 99.9, 55.5, 64.2, 65.2, 68.1, 61.5, 79.9, 86.3, 90.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 2, 9, 14, 23, 22, 26, 30, 22, 21]
Epoch 473 Acc: 98.79 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 214 train Loss: 2041.3 test Loss: 202.5
Epoch 474 Iter 0 subLoss 1674.7 multi -1.99 import weight 0.00
Epoch 474 Iter 1 subLoss 2239.7 multi 18.91 import weight 0.00
Epoch 474 Iter 2 subLoss 2153.1 multi -28.85 import weight 0.00
Epoch 474 Iter 3 subLoss 2238.8 multi 21.90 import weight 0.00
Epoch 474 Iter 4 subLoss 2405.5 multi 9.96 import weight 0.00
Epoch 474 Iter 5 subLoss 1988.8 multi -25.87 import weight 0.00
Epoch 474 Iter 6 subLoss 2158.0 multi -25.87 import weight 0.00
Epoch 474 Iter 7 subLoss 3422.0 multi 12.94 import weight 0.00
Epoch 474 Iter 8 subLoss 2032.4 multi 6.97 import weight 0.00
Epoch 474 Iter 9 subLoss 2039.7 multi 9.96 import weight 0.00
Epoch 474 Iter 10 subLoss 1486.5 multi 1.00 import weight 0.00
Epoch 474 Iter 11 subLoss 1847.0 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10859 / 17.50
Entropy seen (from low to high)
[13, 135, 306, 660, 486, 372, 548, 567, 449, 321, 241, 180, 116, 88, 82, 70, 61, 52, 43, 28, 44, 42, 27, 25, 22, 26, 19, 20, 19, 16, 14, 11, 8, 2, 5, 3, 3, 7, 4, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 20, 31, 58, 58, 81, 100, 114, 134, 136, 162, 195, 176, 198, 209, 190, 221, 187, 176, 180, 183, 151, 173, 155, 142, 143, 129, 139, 156, 127, 126, 122, 125, 100, 96, 76, 50, 21, 7, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.9, 34.8, 36.5, 39.7, 43.6, 47.1, 50.5, 53.7, 57.6, 61.3, 64.4, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 99.9, 55.5, 64.2, 61.9, 70.8, 61.5, 78.5, 91.3, 86.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 2, 9, 14, 21, 24, 26, 28, 23, 22]
Epoch 474 Acc: 98.77 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 184 train Loss: 2190.7 test Loss: 203.7
Epoch 475 Iter 0 subLoss 2295.1 multi 15.93 import weight 0.00
Epoch 475 Iter 1 subLoss 1796.2 multi -4.97 import weight 0.00
Epoch 475 Iter 2 subLoss 2396.5 multi 1.00 import weight 0.00
Epoch 475 Iter 3 subLoss 2063.6 multi -1.99 import weight 0.00
Epoch 475 Iter 4 subLoss 2140.8 multi 15.93 import weight 0.00
Epoch 475 Iter 5 subLoss 1651.7 multi -1.99 import weight 0.00
Epoch 475 Iter 6 subLoss 2151.6 multi -25.87 import weight 0.00
Epoch 475 Iter 7 subLoss 2232.3 multi 24.88 import weight 0.00
Epoch 475 Iter 8 subLoss 1769.8 multi 9.96 import weight 0.00
Epoch 475 Iter 9 subLoss 1449.1 multi -1.99 import weight 0.00
Epoch 475 Iter 10 subLoss 2422.5 multi -7.96 import weight 0.00
Epoch 475 Iter 11 subLoss 2273.2 multi 24.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10870 / 17.00
Entropy seen (from low to high)
[13, 137, 310, 662, 486, 375, 555, 554, 459, 318, 241, 175, 113, 88, 79, 71, 60, 53, 42, 29, 45, 38, 30, 24, 22, 26, 18, 20, 19, 18, 12, 11, 8, 2, 5, 4, 1, 8, 4, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 20, 30, 59, 57, 81, 102, 113, 131, 139, 158, 196, 177, 200, 204, 191, 218, 187, 178, 180, 183, 147, 171, 162, 141, 143, 125, 139, 161, 125, 129, 117, 127, 105, 96, 76, 50, 22, 7, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.8, 0.0, 36.4, 39.7, 43.7, 47.3, 50.7, 53.7, 57.6, 61.3, 64.4, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 55.5, 56.2, 66.6, 71.9, 61.5, 77.7, 91.6, 83.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 2, 9, 16, 18, 25, 26, 27, 24, 24]
Epoch 475 Acc: 98.75 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 24.88 Pidx 227 train Loss: 2074.1 test Loss: 197.9
Epoch 476 Iter 0 subLoss 1844.6 multi -7.96 import weight 0.00
Epoch 476 Iter 1 subLoss 2090.1 multi -7.96 import weight 0.00
Epoch 476 Iter 2 subLoss 2378.2 multi -16.91 import weight 0.00
Epoch 476 Iter 3 subLoss 3556.2 multi -7.96 import weight 0.00
Epoch 476 Iter 4 subLoss 7231.8 multi -7.96 import weight 0.00
Epoch 476 Iter 5 subLoss 104762.2 multi 1.00 import weight 0.00
Epoch 476 Iter 6 subLoss 5034.3 multi 15.93 import weight 0.00
Epoch 476 Iter 7 subLoss 2547.9 multi -7.96 import weight 0.00
Epoch 476 Iter 8 subLoss 3010.2 multi -13.93 import weight 0.00
Epoch 476 Iter 9 subLoss 6734.4 multi 1.00 import weight 0.00
Epoch 476 Iter 10 subLoss 5162.1 multi 3.99 import weight 0.00
Epoch 476 Iter 11 subLoss 3700.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10866 / 16.52
Entropy seen (from low to high)
[13, 139, 313, 664, 481, 379, 565, 548, 457, 306, 244, 175, 108, 93, 75, 74, 58, 57, 42, 27, 42, 41, 30, 25, 23, 26, 17, 19, 21, 18, 13, 10, 8, 3, 4, 3, 2, 8, 4, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 19, 32, 57, 59, 79, 105, 115, 130, 138, 163, 196, 176, 204, 193, 196, 214, 196, 169, 183, 181, 149, 172, 159, 146, 135, 128, 137, 159, 127, 126, 121, 126, 103, 98, 78, 49, 21, 8, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 0.0, 36.5, 39.6, 43.7, 47.2, 50.5, 53.8, 57.5, 61.2, 64.6, 68.6]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 55.5, 59.9, 63.1, 69.2, 60.8, 79.9, 91.6, 79.1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 2, 9, 15, 19, 26, 23, 30, 24, 24]
Epoch 476 Acc: 97.31 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 370 train Loss: 4671.7 test Loss: 485.4
Epoch 477 Iter 0 subLoss 4712.8 multi -4.97 import weight 0.00
Epoch 477 Iter 1 subLoss 7479.6 multi 1.00 import weight 0.00
Epoch 477 Iter 2 subLoss 6104.7 multi 3.99 import weight 0.00
Epoch 477 Iter 3 subLoss 3688.0 multi 1.00 import weight 0.00
Epoch 477 Iter 4 subLoss 3832.8 multi -10.94 import weight 0.00
Epoch 477 Iter 5 subLoss 5692.5 multi 9.96 import weight 0.00
Epoch 477 Iter 6 subLoss 3329.8 multi -4.97 import weight 0.00
Epoch 477 Iter 7 subLoss 3230.5 multi 1.00 import weight 0.00
Epoch 477 Iter 8 subLoss 2826.7 multi -7.96 import weight 0.00
Epoch 477 Iter 9 subLoss 3570.3 multi 27.87 import weight 0.00
Epoch 477 Iter 10 subLoss 2749.8 multi 18.91 import weight 0.00
Epoch 477 Iter 11 subLoss 2224.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0060 / 0.10875 / 16.88
Entropy seen (from low to high)
[13, 143, 317, 661, 477, 386, 570, 544, 459, 305, 242, 173, 106, 93, 75, 74, 59, 53, 42, 30, 40, 42, 28, 26, 21, 26, 19, 17, 22, 17, 14, 9, 8, 3, 4, 3, 3, 7, 4, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 18, 32, 57, 60, 77, 107, 113, 128, 140, 162, 199, 173, 203, 194, 201, 210, 188, 174, 182, 185, 152, 167, 160, 143, 135, 129, 136, 163, 118, 133, 122, 125, 106, 93, 82, 51, 21, 8, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 0.0, 36.6, 39.7, 43.9, 47.4, 50.6, 53.8, 57.6, 61.3, 64.6, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 59.9, 57.1, 63.1, 69.2, 60.8, 79.9, 91.3, 82.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 2, 10, 14, 19, 26, 23, 30, 23, 23]
Epoch 477 Acc: 98.44 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 222 train Loss: 2293.1 test Loss: 256.9
Epoch 478 Iter 0 subLoss 2228.3 multi 3.99 import weight 0.00
Epoch 478 Iter 1 subLoss 2042.1 multi -4.97 import weight 0.00
Epoch 478 Iter 2 subLoss 1823.3 multi -1.99 import weight 0.00
Epoch 478 Iter 3 subLoss 2487.4 multi 1.00 import weight 0.00
Epoch 478 Iter 4 subLoss 2128.2 multi 15.93 import weight 0.00
Epoch 478 Iter 5 subLoss 2495.7 multi -13.93 import weight 0.00
Epoch 478 Iter 6 subLoss 2317.7 multi -4.97 import weight 0.00
Epoch 478 Iter 7 subLoss 2331.9 multi 18.91 import weight 0.00
Epoch 478 Iter 8 subLoss 1695.8 multi -1.99 import weight 0.00
Epoch 478 Iter 9 subLoss 2421.3 multi -4.97 import weight 0.00
Epoch 478 Iter 10 subLoss 1779.0 multi -16.91 import weight 0.00
Epoch 478 Iter 11 subLoss 2972.4 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0060 / 0.10873 / 16.88
Entropy seen (from low to high)
[14, 145, 316, 664, 474, 395, 568, 544, 458, 302, 239, 173, 107, 92, 76, 73, 59, 54, 40, 30, 41, 39, 30, 25, 23, 24, 19, 18, 22, 15, 15, 10, 6, 4, 4, 4, 2, 7, 4, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 18, 33, 56, 58, 84, 100, 118, 124, 142, 162, 196, 176, 199, 198, 200, 210, 190, 171, 184, 183, 155, 165, 162, 143, 131, 133, 137, 159, 120, 131, 125, 120, 109, 89, 83, 53, 22, 8, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 0.0, 36.7, 39.6, 44.0, 47.3, 50.5, 53.7, 57.5, 61.3, 64.5, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 49.9, 64.2, 64.7, 66.6, 63.9, 78.5, 91.6, 82.6]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 2, 10, 14, 17, 27, 25, 28, 24, 23]
Epoch 478 Acc: 97.35 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 297 train Loss: 3864.7 test Loss: 445.9
Epoch 479 Iter 0 subLoss 3624.6 multi -7.96 import weight 0.00
Epoch 479 Iter 1 subLoss 14832.8 multi -1.99 import weight 0.00
Epoch 479 Iter 2 subLoss 70157.1 multi 1.00 import weight 0.00
Epoch 479 Iter 3 subLoss 8541.4 multi -10.94 import weight 0.00
Epoch 479 Iter 4 subLoss 126434.1 multi 1.00 import weight 0.00
Epoch 479 Iter 5 subLoss 37515.6 multi 1.00 import weight 0.00
Epoch 479 Iter 6 subLoss 31344.2 multi 1.00 import weight 0.00
Epoch 479 Iter 7 subLoss 29082.3 multi 1.00 import weight 0.00
Epoch 479 Iter 8 subLoss 26715.6 multi 1.00 import weight 0.00
Epoch 479 Iter 9 subLoss 24745.0 multi -1.99 import weight 0.00
Epoch 479 Iter 10 subLoss 28077.0 multi 1.00 import weight 0.00
Epoch 479 Iter 11 subLoss 26336.8 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10853 / 16.58
Entropy seen (from low to high)
[11, 143, 290, 670, 478, 301, 516, 625, 497, 315, 257, 179, 111, 93, 81, 69, 62, 51, 43, 29, 43, 37, 31, 25, 23, 23, 20, 18, 23, 13, 17, 10, 6, 3, 5, 3, 4, 6, 4, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 18, 33, 55, 61, 79, 104, 118, 125, 140, 166, 193, 179, 200, 202, 195, 215, 194, 169, 184, 183, 161, 163, 161, 140, 131, 131, 145, 149, 130, 122, 126, 117, 104, 96, 74, 53, 27, 4, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 34.9, 37.3, 40.6, 43.8, 47.3, 50.4, 53.8, 57.6, 61.5, 64.7, 68.5]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 99.9, 44.4, 62.4, 66.6, 66.6, 65.2, 80.6, 90.9, 79.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 1, 9, 16, 15, 30, 23, 31, 22, 25]
Epoch 479 Acc: 72.87 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 2633 train Loss: 37752.6 test Loss: 5721.8
Epoch 480 Iter 0 subLoss 37487.8 multi 3.99 import weight 0.00
Epoch 480 Iter 1 subLoss 25976.9 multi 6.97 import weight 0.00
Epoch 480 Iter 2 subLoss 22191.0 multi 1.00 import weight 0.00
Epoch 480 Iter 3 subLoss 18767.9 multi 1.00 import weight 0.00
Epoch 480 Iter 4 subLoss 19014.4 multi 1.00 import weight 0.00
Epoch 480 Iter 5 subLoss 16281.7 multi 1.00 import weight 0.00
Epoch 480 Iter 6 subLoss 14882.3 multi 3.98 import weight 0.00
Epoch 480 Iter 7 subLoss 6636.9 multi -7.96 import weight 0.00
Epoch 480 Iter 8 subLoss 15882.7 multi -1.99 import weight 0.00
Epoch 480 Iter 9 subLoss 19861.0 multi 1.00 import weight 0.00
Epoch 480 Iter 10 subLoss 17730.6 multi 1.00 import weight 0.00
Epoch 480 Iter 11 subLoss 16966.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10843 / 16.62
Entropy seen (from low to high)
[11, 145, 294, 672, 473, 277, 460, 637, 526, 328, 267, 188, 107, 95, 81, 74, 58, 59, 42, 27, 41, 39, 31, 25, 24, 22, 20, 18, 22, 15, 16, 10, 6, 3, 5, 3, 3, 8, 3, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 17, 34, 55, 62, 80, 103, 115, 129, 137, 169, 196, 182, 201, 196, 195, 217, 194, 169, 185, 181, 155, 173, 157, 143, 125, 134, 144, 148, 131, 125, 123, 115, 101, 97, 75, 52, 28, 4, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 34.8, 37.3, 40.6, 44.0, 47.4, 50.6, 53.8, 57.6, 61.6, 64.8, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 99.9, 49.9, 59.9, 64.7, 67.8, 66.6, 81.2, 89.4, 80.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 1, 10, 15, 17, 28, 24, 32, 19, 26]
Epoch 480 Acc: 82.97 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1696 train Loss: 15034.2 test Loss: 2124.6
Epoch 481 Iter 0 subLoss 16126.1 multi 1.00 import weight 0.00
Epoch 481 Iter 1 subLoss 13236.7 multi 1.00 import weight 0.00
Epoch 481 Iter 2 subLoss 10535.4 multi -4.97 import weight 0.00
Epoch 481 Iter 3 subLoss 19557.0 multi 1.00 import weight 0.00
Epoch 481 Iter 4 subLoss 18781.4 multi 1.00 import weight 0.00
Epoch 481 Iter 5 subLoss 16457.3 multi 6.97 import weight 0.00
Epoch 481 Iter 6 subLoss 6738.1 multi 3.99 import weight 0.00
Epoch 481 Iter 7 subLoss 4507.5 multi -13.93 import weight 0.00
Epoch 481 Iter 8 subLoss 8502.5 multi -4.97 import weight 0.00
Epoch 481 Iter 9 subLoss 14140.7 multi -1.99 import weight 0.00
Epoch 481 Iter 10 subLoss 17893.8 multi 9.96 import weight 0.00
Epoch 481 Iter 11 subLoss 7119.7 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10844 / 16.62
Entropy seen (from low to high)
[11, 146, 300, 677, 463, 277, 478, 623, 526, 329, 264, 187, 108, 93, 83, 71, 58, 57, 43, 28, 40, 39, 32, 24, 24, 22, 20, 18, 21, 16, 16, 10, 6, 3, 5, 3, 3, 8, 3, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 17, 34, 56, 59, 83, 103, 116, 126, 140, 166, 197, 179, 204, 194, 198, 219, 191, 169, 184, 179, 157, 172, 162, 141, 121, 139, 142, 149, 127, 123, 127, 115, 101, 94, 78, 53, 28, 4, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 34.7, 37.4, 41.3, 44.2, 47.5, 50.7, 53.8, 57.6, 61.4, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 49.9, 55.5, 56.2, 68.7, 67.8, 65.2, 80.6, 89.9, 81.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 2, 9, 16, 16, 28, 23, 31, 20, 27]
Epoch 481 Acc: 98.42 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 711 train Loss: 3805.9 test Loss: 306.4
Epoch 482 Iter 0 subLoss 4385.8 multi 6.97 import weight 0.00
Epoch 482 Iter 1 subLoss 3299.0 multi 9.96 import weight 0.00
Epoch 482 Iter 2 subLoss 2494.4 multi -10.94 import weight 0.00
Epoch 482 Iter 3 subLoss 3199.4 multi 15.93 import weight 0.00
Epoch 482 Iter 4 subLoss 3226.9 multi 15.93 import weight 0.00
Epoch 482 Iter 5 subLoss 2354.4 multi -25.87 import weight 0.00
Epoch 482 Iter 6 subLoss 3006.0 multi -4.97 import weight 0.00
Epoch 482 Iter 7 subLoss 3511.1 multi 9.96 import weight 0.00
Epoch 482 Iter 8 subLoss 2411.2 multi 3.98 import weight 0.00
Epoch 482 Iter 9 subLoss 2635.4 multi -7.96 import weight 0.00
Epoch 482 Iter 10 subLoss 2460.4 multi 9.96 import weight 0.00
Epoch 482 Iter 11 subLoss 2538.5 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10851 / 16.56
Entropy seen (from low to high)
[11, 146, 305, 679, 461, 279, 488, 628, 514, 325, 266, 182, 109, 90, 87, 67, 61, 53, 44, 27, 38, 42, 32, 23, 25, 21, 20, 17, 23, 16, 15, 9, 7, 3, 5, 3, 3, 8, 3, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 18, 33, 56, 59, 86, 97, 116, 129, 137, 165, 201, 174, 206, 196, 194, 221, 193, 167, 187, 176, 156, 172, 163, 141, 124, 133, 143, 150, 126, 124, 128, 117, 100, 95, 75, 57, 28, 4, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.8, 34.8, 37.4, 41.4, 44.0, 47.4, 50.6, 53.8, 57.7, 61.4, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 49.9, 62.4, 52.9, 66.6, 68.9, 65.2, 79.9, 90.4, 81.4]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 2, 8, 17, 15, 29, 23, 30, 21, 27]
Epoch 482 Acc: 98.33 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 253 train Loss: 2480.0 test Loss: 284.4
Epoch 483 Iter 0 subLoss 2027.1 multi -4.97 import weight 0.00
Epoch 483 Iter 1 subLoss 2527.7 multi 3.98 import weight 0.00
Epoch 483 Iter 2 subLoss 2364.4 multi 12.94 import weight 0.00
Epoch 483 Iter 3 subLoss 2418.7 multi 6.97 import weight 0.00
Epoch 483 Iter 4 subLoss 2097.4 multi -4.97 import weight 0.00
Epoch 483 Iter 5 subLoss 2065.1 multi 1.00 import weight 0.00
Epoch 483 Iter 6 subLoss 1997.7 multi 24.88 import weight 0.00
Epoch 483 Iter 7 subLoss 2414.3 multi 9.96 import weight 0.00
Epoch 483 Iter 8 subLoss 2278.1 multi 27.87 import weight 0.00
Epoch 483 Iter 9 subLoss 2459.3 multi -13.93 import weight 0.00
Epoch 483 Iter 10 subLoss 2140.7 multi 18.91 import weight 0.00
Epoch 483 Iter 11 subLoss 2016.2 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0060 / 0.10862 / 17.10
Entropy seen (from low to high)
[11, 146, 309, 682, 463, 279, 496, 625, 515, 330, 255, 177, 106, 91, 88, 66, 59, 54, 45, 25, 38, 43, 31, 24, 25, 22, 18, 17, 23, 16, 15, 9, 7, 4, 4, 3, 3, 8, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 19, 32, 56, 58, 86, 97, 115, 129, 136, 167, 199, 173, 206, 191, 199, 220, 192, 166, 182, 186, 145, 172, 171, 137, 127, 130, 144, 153, 129, 122, 126, 117, 104, 94, 74, 61, 28, 4, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.9, 34.8, 37.5, 41.0, 43.8, 47.3, 50.5, 53.9, 57.8, 61.3, 64.3, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 99.9, 66.6, 56.2, 62.4, 68.9, 62.4, 80.7, 91.3, 82.1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 1, 9, 16, 16, 29, 24, 26, 23, 28]
Epoch 483 Acc: 98.91 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 201 train Loss: 2162.8 test Loss: 211.3
Epoch 484 Iter 0 subLoss 2028.9 multi -4.97 import weight 0.00
Epoch 484 Iter 1 subLoss 1877.1 multi 9.96 import weight 0.00
Epoch 484 Iter 2 subLoss 2191.5 multi -7.96 import weight 0.00
Epoch 484 Iter 3 subLoss 2384.8 multi 1.00 import weight 0.00
Epoch 484 Iter 4 subLoss 1734.3 multi 9.96 import weight 0.00
Epoch 484 Iter 5 subLoss 1988.2 multi -22.88 import weight 0.00
Epoch 484 Iter 6 subLoss 2288.6 multi -25.87 import weight 0.00
Epoch 484 Iter 7 subLoss 2980.7 multi 3.99 import weight 0.00
Epoch 484 Iter 8 subLoss 2652.1 multi 6.97 import weight 0.00
Epoch 484 Iter 9 subLoss 2076.9 multi 3.99 import weight 0.00
Epoch 484 Iter 10 subLoss 2446.7 multi -4.97 import weight 0.00
Epoch 484 Iter 11 subLoss 2607.9 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0060 / 0.10866 / 17.04
Entropy seen (from low to high)
[11, 146, 318, 685, 456, 283, 502, 625, 505, 331, 255, 176, 107, 88, 88, 66, 58, 53, 45, 25, 39, 42, 31, 23, 24, 23, 18, 18, 23, 15, 15, 9, 7, 4, 4, 3, 3, 8, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 17, 33, 57, 57, 87, 96, 118, 125, 133, 170, 200, 167, 216, 188, 199, 222, 184, 170, 190, 179, 145, 176, 167, 135, 129, 127, 142, 152, 135, 122, 126, 112, 104, 99, 75, 60, 28, 5, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.9, 0.0, 37.1, 41.2, 43.8, 47.3, 50.8, 54.0, 57.9, 61.3, 64.4, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 66.6, 56.2, 61.1, 70.3, 62.4, 79.9, 91.6, 82.1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 1, 9, 16, 18, 27, 24, 25, 24, 28]
Epoch 484 Acc: 98.27 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 260 train Loss: 2694.4 test Loss: 275.8
Epoch 485 Iter 0 subLoss 2436.7 multi -1.99 import weight 0.00
Epoch 485 Iter 1 subLoss 2868.6 multi 1.00 import weight 0.00
Epoch 485 Iter 2 subLoss 2159.2 multi -25.87 import weight 0.00
Epoch 485 Iter 3 subLoss 7196.9 multi -10.94 import weight 0.00
Epoch 485 Iter 4 subLoss 99756.0 multi 1.00 import weight 0.00
Epoch 485 Iter 5 subLoss 7947.1 multi -1.98 import weight 0.00
Epoch 485 Iter 6 subLoss 11112.7 multi 1.00 import weight 0.00
Epoch 485 Iter 7 subLoss 8852.4 multi -1.98 import weight 0.00
Epoch 485 Iter 8 subLoss 12091.3 multi 12.94 import weight 0.00
Epoch 485 Iter 9 subLoss 3532.5 multi 1.00 import weight 0.00
Epoch 485 Iter 10 subLoss 3866.2 multi 3.99 import weight 0.00
Epoch 485 Iter 11 subLoss 2426.6 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0060 / 0.10870 / 17.33
Entropy seen (from low to high)
[11, 148, 321, 688, 453, 286, 504, 628, 497, 335, 252, 171, 109, 86, 91, 64, 54, 56, 44, 27, 35, 43, 32, 23, 24, 24, 17, 18, 22, 16, 15, 9, 7, 4, 4, 3, 3, 8, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 17, 33, 57, 56, 88, 96, 117, 128, 134, 170, 201, 167, 208, 189, 207, 212, 190, 171, 191, 173, 148, 171, 169, 139, 126, 129, 136, 153, 139, 120, 127, 111, 105, 97, 78, 59, 29, 6, 4, 3, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.1, 0.0, 37.1, 41.7, 44.4, 47.3, 50.8, 54.0, 57.8, 61.3, 64.4, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 85.7, 56.2, 57.8, 71.9, 63.9, 79.9, 91.6, 82.1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 3, 7, 16, 19, 25, 25, 25, 24, 28]
Epoch 485 Acc: 97.74 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 242 train Loss: 3992.5 test Loss: 394.5
Epoch 486 Iter 0 subLoss 3671.4 multi 15.93 import weight 0.00
Epoch 486 Iter 1 subLoss 2818.0 multi -13.93 import weight 0.00
Epoch 486 Iter 2 subLoss 3267.3 multi -1.98 import weight 0.00
Epoch 486 Iter 3 subLoss 3060.3 multi -7.96 import weight 0.00
Epoch 486 Iter 4 subLoss 4377.6 multi -16.91 import weight 0.00
Epoch 486 Iter 5 subLoss 15497.7 multi 1.00 import weight 0.00
Epoch 486 Iter 6 subLoss 9214.1 multi 6.97 import weight 0.00
Epoch 486 Iter 7 subLoss 2821.9 multi -7.96 import weight 0.00
Epoch 486 Iter 8 subLoss 3749.1 multi -25.87 import weight 0.00
Epoch 486 Iter 9 subLoss 24347.2 multi -1.99 import weight 0.00
Epoch 486 Iter 10 subLoss 57597.9 multi 1.00 import weight 0.00
Epoch 486 Iter 11 subLoss 31328.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10850 / 17.39
Entropy seen (from low to high)
[9, 132, 282, 643, 489, 309, 415, 651, 539, 350, 279, 177, 111, 94, 88, 70, 58, 59, 41, 27, 34, 45, 31, 23, 25, 24, 18, 18, 20, 17, 17, 8, 7, 2, 6, 3, 2, 9, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 19, 30, 60, 57, 88, 102, 116, 124, 138, 170, 203, 164, 209, 192, 206, 219, 186, 170, 195, 172, 144, 176, 162, 137, 128, 138, 131, 142, 141, 123, 122, 110, 109, 98, 75, 59, 28, 4, 5, 2, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.0, 0.0, 37.1, 41.6, 44.5, 47.4, 50.9, 54.1, 57.8, 61.2, 64.3, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 74.9, 59.9, 63.6, 68.1, 63.9, 74.9, 95.9, 82.1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 3, 8, 15, 22, 22, 25, 24, 25, 28]
Epoch 486 Acc: 71.78 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3132 train Loss: 20266.5 test Loss: 4588.1
Epoch 487 Iter 0 subLoss 19046.4 multi -1.99 import weight 0.00
Epoch 487 Iter 1 subLoss 36799.7 multi 3.99 import weight 0.00
Epoch 487 Iter 2 subLoss 7923.6 multi -1.99 import weight 0.00
Epoch 487 Iter 3 subLoss 11765.5 multi -7.96 import weight 0.00
Epoch 487 Iter 4 subLoss 31832.4 multi -1.99 import weight 0.00
Epoch 487 Iter 5 subLoss 44640.4 multi 1.00 import weight 0.00
Epoch 487 Iter 6 subLoss 37573.1 multi 1.00 import weight 0.00
Epoch 487 Iter 7 subLoss 32379.3 multi 1.00 import weight 0.00
Epoch 487 Iter 8 subLoss 28043.0 multi 1.00 import weight 0.00
Epoch 487 Iter 9 subLoss 24268.7 multi 1.00 import weight 0.00
Epoch 487 Iter 10 subLoss 22543.5 multi 1.00 import weight 0.00
Epoch 487 Iter 11 subLoss 19944.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0061 / 0.10825 / 17.47
Entropy seen (from low to high)
[9, 131, 270, 590, 509, 329, 341, 599, 596, 390, 290, 187, 122, 103, 89, 73, 62, 54, 48, 28, 35, 43, 33, 24, 23, 26, 18, 15, 21, 19, 17, 9, 7, 1, 6, 4, 2, 8, 4, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 19, 32, 58, 60, 86, 102, 120, 122, 144, 172, 201, 161, 220, 187, 217, 211, 185, 169, 197, 170, 141, 179, 165, 132, 131, 137, 131, 151, 129, 119, 128, 100, 111, 97, 74, 57, 27, 6, 5, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.0, 0.0, 37.1, 41.5, 44.4, 47.4, 50.9, 54.1, 57.8, 61.3, 64.3, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 74.9, 56.2, 66.6, 68.1, 65.3, 75.9, 95.6, 82.1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 3, 8, 16, 21, 22, 26, 25, 23, 28]
Epoch 487 Acc: 72.54 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1994 train Loss: 18063.2 test Loss: 4245.9
Epoch 488 Iter 0 subLoss 17818.9 multi 1.00 import weight 0.00
Epoch 488 Iter 1 subLoss 16701.2 multi -4.97 import weight 0.00
Epoch 488 Iter 2 subLoss 25118.7 multi -1.99 import weight 0.00
Epoch 488 Iter 3 subLoss 29945.9 multi 1.00 import weight 0.00
Epoch 488 Iter 4 subLoss 26843.5 multi 1.00 import weight 0.00
Epoch 488 Iter 5 subLoss 24929.6 multi 1.00 import weight 0.00
Epoch 488 Iter 6 subLoss 22203.1 multi 1.00 import weight 0.00
Epoch 488 Iter 7 subLoss 19789.2 multi 1.00 import weight 0.00
Epoch 488 Iter 8 subLoss 17312.2 multi -1.99 import weight 0.00
Epoch 488 Iter 9 subLoss 21220.6 multi -1.99 import weight 0.00
Epoch 488 Iter 10 subLoss 24821.6 multi -1.99 import weight 0.00
Epoch 488 Iter 11 subLoss 30254.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10795 / 17.59
Entropy seen (from low to high)
[7, 114, 251, 479, 540, 386, 319, 521, 625, 438, 321, 202, 138, 107, 91, 81, 55, 60, 54, 29, 30, 47, 31, 25, 25, 26, 18, 14, 24, 17, 18, 9, 7, 2, 6, 4, 2, 8, 4, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 4, 19, 33, 58, 60, 90, 101, 116, 126, 148, 172, 206, 157, 222, 199, 211, 214, 186, 168, 198, 166, 150, 174, 174, 115, 132, 140, 130, 151, 128, 115, 128, 98, 115, 87, 71, 61, 24, 6, 4, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.9, 34.9, 37.4, 41.4, 44.3, 47.5, 51.1, 54.1, 57.7, 61.3, 64.5, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 33.3, 74.9, 55.5, 66.6, 69.9, 62.9, 79.9, 95.9, 79.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 8, 18, 21, 20, 27, 25, 25, 25]
Epoch 488 Acc: 64.49 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3025 train Loss: 27795.8 test Loss: 5837.3
Epoch 489 Iter 0 subLoss 27446.9 multi 1.00 import weight 0.00
Epoch 489 Iter 1 subLoss 24398.7 multi -1.99 import weight 0.00
Epoch 489 Iter 2 subLoss 28992.6 multi 3.99 import weight 0.00
Epoch 489 Iter 3 subLoss 20345.2 multi 1.00 import weight 0.00
Epoch 489 Iter 4 subLoss 18966.7 multi 1.00 import weight 0.00
Epoch 489 Iter 5 subLoss 18043.6 multi 1.00 import weight 0.00
Epoch 489 Iter 6 subLoss 15952.8 multi 1.00 import weight 0.00
Epoch 489 Iter 7 subLoss 16535.5 multi 1.00 import weight 0.00
Epoch 489 Iter 8 subLoss 15308.8 multi 3.99 import weight 0.00
Epoch 489 Iter 9 subLoss 10863.7 multi 1.00 import weight 0.00
Epoch 489 Iter 10 subLoss 10178.0 multi 1.00 import weight 0.00
Epoch 489 Iter 11 subLoss 10074.3 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10775 / 17.79
Entropy seen (from low to high)
[7, 115, 251, 474, 531, 394, 310, 450, 592, 480, 346, 233, 142, 111, 95, 82, 58, 56, 57, 31, 33, 35, 40, 26, 26, 26, 19, 14, 22, 19, 17, 9, 6, 4, 6, 4, 1, 9, 4, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 20, 31, 58, 61, 92, 103, 115, 126, 151, 173, 204, 154, 236, 192, 212, 220, 183, 167, 193, 171, 153, 172, 169, 115, 136, 140, 123, 156, 126, 116, 121, 97, 117, 83, 71, 61, 24, 6, 4, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.9, 34.8, 37.4, 41.3, 44.3, 47.5, 51.0, 54.1, 57.7, 61.2, 64.6, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 33.3, 74.9, 55.5, 66.6, 69.9, 64.2, 79.1, 96.2, 79.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 8, 18, 21, 20, 28, 24, 27, 25]
Epoch 489 Acc: 76.65 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 1007 train Loss: 14376.9 test Loss: 3446.8
Epoch 490 Iter 0 subLoss 14306.1 multi 3.99 import weight 0.00
Epoch 490 Iter 1 subLoss 12074.1 multi 3.99 import weight 0.00
Epoch 490 Iter 2 subLoss 8836.4 multi 3.98 import weight 0.00
Epoch 490 Iter 3 subLoss 5824.8 multi -13.93 import weight 0.00
Epoch 490 Iter 4 subLoss 11741.2 multi -1.99 import weight 0.00
Epoch 490 Iter 5 subLoss 13217.2 multi 1.00 import weight 0.00
Epoch 490 Iter 6 subLoss 12605.0 multi -1.99 import weight 0.00
Epoch 490 Iter 7 subLoss 13571.4 multi -1.99 import weight 0.00
Epoch 490 Iter 8 subLoss 14987.1 multi -1.99 import weight 0.00
Epoch 490 Iter 9 subLoss 17396.0 multi 1.00 import weight 0.00
Epoch 490 Iter 10 subLoss 15606.2 multi 3.99 import weight 0.00
Epoch 490 Iter 11 subLoss 12763.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10756 / 17.46
Entropy seen (from low to high)
[7, 112, 254, 473, 529, 386, 317, 419, 536, 498, 374, 252, 155, 113, 96, 88, 56, 60, 53, 35, 29, 37, 42, 27, 25, 28, 18, 14, 22, 20, 17, 8, 6, 5, 6, 4, 1, 9, 4, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 21, 30, 58, 64, 92, 104, 115, 128, 151, 173, 204, 163, 232, 192, 215, 219, 181, 169, 191, 167, 156, 174, 170, 111, 136, 143, 124, 151, 129, 115, 114, 95, 118, 81, 71, 61, 25, 7, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.8, 34.7, 37.3, 41.2, 44.4, 47.6, 50.9, 54.1, 57.8, 61.2, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 33.3, 77.7, 52.9, 66.6, 66.6, 66.6, 79.9, 96.2, 76.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 9, 17, 21, 21, 27, 25, 27, 26]
Epoch 490 Acc: 80.46 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1276 train Loss: 12443.0 test Loss: 2881.1
Epoch 491 Iter 0 subLoss 12374.2 multi 6.97 import weight 0.00
Epoch 491 Iter 1 subLoss 7961.4 multi 21.90 import weight 0.00
Epoch 491 Iter 2 subLoss 4656.5 multi 1.00 import weight 0.00
Epoch 491 Iter 3 subLoss 5062.3 multi 9.96 import weight 0.00
Epoch 491 Iter 4 subLoss 3063.3 multi -4.97 import weight 0.00
Epoch 491 Iter 5 subLoss 2366.4 multi 15.93 import weight 0.00
Epoch 491 Iter 6 subLoss 3045.4 multi 6.97 import weight 0.00
Epoch 491 Iter 7 subLoss 2521.6 multi 6.97 import weight 0.00
Epoch 491 Iter 8 subLoss 1937.7 multi 6.97 import weight 0.00
Epoch 491 Iter 9 subLoss 2597.6 multi -25.87 import weight 0.00
Epoch 491 Iter 10 subLoss 3072.9 multi -10.94 import weight 0.00
Epoch 491 Iter 11 subLoss 3171.9 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10763 / 17.37
Entropy seen (from low to high)
[8, 113, 256, 476, 527, 392, 318, 419, 539, 501, 364, 253, 149, 114, 98, 83, 57, 60, 51, 35, 29, 37, 42, 29, 23, 28, 19, 13, 23, 19, 18, 8, 5, 5, 6, 4, 1, 9, 4, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 21, 30, 58, 65, 94, 98, 121, 129, 149, 170, 206, 165, 229, 189, 215, 219, 181, 167, 190, 171, 152, 177, 169, 112, 133, 145, 123, 149, 131, 113, 119, 95, 117, 84, 69, 61, 27, 7, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.8, 34.9, 37.3, 41.1, 44.4, 47.4, 50.8, 54.2, 57.8, 61.3, 64.7, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 99.9, 33.3, 77.7, 53.3, 62.4, 69.9, 66.6, 80.7, 96.1, 75.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 9, 15, 24, 20, 27, 26, 26, 25]
Epoch 491 Acc: 98.00 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 317 train Loss: 2809.3 test Loss: 342.9
Epoch 492 Iter 0 subLoss 2286.8 multi -22.88 import weight 0.00
Epoch 492 Iter 1 subLoss 3045.7 multi 9.96 import weight 0.00
Epoch 492 Iter 2 subLoss 2865.9 multi 3.99 import weight 0.00
Epoch 492 Iter 3 subLoss 2510.4 multi 15.93 import weight 0.00
Epoch 492 Iter 4 subLoss 3550.2 multi -4.97 import weight 0.00
Epoch 492 Iter 5 subLoss 2970.3 multi -1.99 import weight 0.00
Epoch 492 Iter 6 subLoss 3165.4 multi -4.97 import weight 0.00
Epoch 492 Iter 7 subLoss 2826.7 multi -4.97 import weight 0.00
Epoch 492 Iter 8 subLoss 3188.2 multi -16.91 import weight 0.00
Epoch 492 Iter 9 subLoss 3880.4 multi -1.98 import weight 0.00
Epoch 492 Iter 10 subLoss 4709.5 multi 1.00 import weight 0.00
Epoch 492 Iter 11 subLoss 4669.3 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10768 / 17.86
Entropy seen (from low to high)
[8, 114, 258, 481, 531, 383, 324, 422, 542, 499, 362, 247, 144, 116, 97, 83, 56, 60, 52, 36, 27, 37, 44, 27, 24, 29, 15, 16, 20, 21, 18, 8, 5, 5, 6, 4, 1, 9, 4, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 21, 30, 56, 69, 90, 101, 122, 126, 151, 171, 205, 160, 234, 185, 214, 221, 185, 164, 192, 165, 154, 175, 169, 115, 129, 147, 120, 153, 131, 110, 121, 95, 116, 87, 69, 63, 27, 7, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 0.0, 37.0, 41.0, 44.4, 47.5, 50.8, 54.2, 57.9, 61.4, 64.7, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 77.7, 56.2, 60.8, 69.9, 71.4, 75.9, 96.1, 79.1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 3, 9, 16, 23, 20, 28, 25, 26, 24]
Epoch 492 Acc: 97.06 BMA: 98.52 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 466 train Loss: 4189.9 test Loss: 493.6
Epoch 493 Iter 0 subLoss 4077.5 multi -7.96 import weight 0.00
Epoch 493 Iter 1 subLoss 6403.9 multi 3.98 import weight 0.00
Epoch 493 Iter 2 subLoss 3774.4 multi -28.85 import weight 0.00
Epoch 493 Iter 3 subLoss 20244.5 multi 3.99 import weight 0.00
Epoch 493 Iter 4 subLoss 6526.8 multi -1.99 import weight 0.00
Epoch 493 Iter 5 subLoss 6355.6 multi 3.98 import weight 0.00
Epoch 493 Iter 6 subLoss 5426.4 multi 1.00 import weight 0.00
Epoch 493 Iter 7 subLoss 4856.0 multi -16.91 import weight 0.00
Epoch 493 Iter 8 subLoss 11142.8 multi -1.99 import weight 0.00
Epoch 493 Iter 9 subLoss 16826.8 multi 1.00 import weight 0.00
Epoch 493 Iter 10 subLoss 11539.3 multi 1.00 import weight 0.00
Epoch 493 Iter 11 subLoss 9259.9 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10762 / 18.06
Entropy seen (from low to high)
[8, 115, 258, 489, 529, 376, 319, 424, 548, 489, 353, 251, 144, 118, 99, 87, 55, 61, 54, 37, 28, 37, 43, 26, 25, 29, 18, 14, 20, 22, 16, 9, 5, 5, 6, 4, 1, 9, 3, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 20, 31, 58, 67, 93, 99, 121, 129, 154, 166, 204, 158, 241, 186, 213, 223, 181, 161, 193, 165, 159, 171, 170, 118, 125, 149, 119, 151, 133, 107, 119, 98, 113, 92, 66, 63, 27, 7, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 0.0, 36.7, 40.3, 44.4, 47.5, 50.9, 54.3, 58.0, 61.4, 64.8, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 77.7, 56.2, 62.4, 69.9, 71.4, 74.9, 96.4, 79.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 4, 9, 16, 24, 20, 28, 24, 28, 20]
Epoch 493 Acc: 88.38 BMA: 98.54 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 925 train Loss: 13954.9 test Loss: 2056.2
Epoch 494 Iter 0 subLoss 12789.2 multi -1.99 import weight 0.00
Epoch 494 Iter 1 subLoss 23000.3 multi -1.99 import weight 0.00
Epoch 494 Iter 2 subLoss 69712.2 multi 1.00 import weight 0.00
Epoch 494 Iter 3 subLoss 16140.3 multi -4.97 import weight 0.00
Epoch 494 Iter 4 subLoss 42291.1 multi 1.00 import weight 0.00
Epoch 494 Iter 5 subLoss 25529.7 multi 1.00 import weight 0.00
Epoch 494 Iter 6 subLoss 21113.8 multi 1.00 import weight 0.00
Epoch 494 Iter 7 subLoss 14143.4 multi 1.00 import weight 0.00
Epoch 494 Iter 8 subLoss 14195.2 multi 3.98 import weight 0.00
Epoch 494 Iter 9 subLoss 8107.1 multi -1.98 import weight 0.00
Epoch 494 Iter 10 subLoss 8958.2 multi 9.96 import weight 0.00
Epoch 494 Iter 11 subLoss 4704.1 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0063 / 0.10762 / 18.09
Entropy seen (from low to high)
[8, 115, 259, 493, 529, 374, 322, 425, 554, 487, 350, 245, 147, 114, 103, 80, 57, 59, 55, 38, 26, 36, 47, 25, 25, 29, 17, 15, 20, 22, 15, 10, 6, 4, 6, 4, 1, 9, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 5, 20, 31, 58, 66, 96, 101, 119, 131, 157, 163, 202, 163, 236, 192, 210, 220, 178, 163, 193, 166, 158, 169, 164, 122, 126, 147, 118, 154, 133, 103, 123, 97, 113, 94, 67, 62, 28, 7, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 0.0, 36.7, 40.3, 44.3, 47.5, 50.9, 54.4, 57.9, 61.3, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 77.7, 62.4, 59.9, 68.4, 70.3, 79.1, 90.3, 83.3]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 4, 9, 16, 25, 19, 27, 24, 31, 18]
Epoch 494 Acc: 96.52 BMA: 98.54 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 470 train Loss: 4299.5 test Loss: 589.3
Epoch 495 Iter 0 subLoss 4151.0 multi 6.97 import weight 0.00
Epoch 495 Iter 1 subLoss 3409.7 multi 3.98 import weight 0.00
Epoch 495 Iter 2 subLoss 3576.0 multi 30.85 import weight 0.00
Epoch 495 Iter 3 subLoss 2728.3 multi 15.93 import weight 0.00
Epoch 495 Iter 4 subLoss 2846.1 multi 12.94 import weight 0.00
Epoch 495 Iter 5 subLoss 2693.6 multi -13.93 import weight 0.00
Epoch 495 Iter 6 subLoss 2619.7 multi 1.00 import weight 0.00
Epoch 495 Iter 7 subLoss 3207.2 multi 6.97 import weight 0.00
Epoch 495 Iter 8 subLoss 2525.9 multi 6.97 import weight 0.00
Epoch 495 Iter 9 subLoss 3261.9 multi 1.00 import weight 0.00
Epoch 495 Iter 10 subLoss 2615.3 multi 3.99 import weight 0.00
Epoch 495 Iter 11 subLoss 2548.9 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10768 / 18.06
Entropy seen (from low to high)
[8, 118, 259, 498, 531, 367, 331, 428, 554, 490, 347, 237, 143, 118, 99, 79, 58, 57, 55, 37, 27, 38, 44, 25, 25, 29, 17, 15, 20, 22, 17, 8, 6, 4, 6, 4, 1, 9, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 20, 30, 57, 69, 97, 97, 120, 130, 154, 165, 203, 163, 236, 191, 210, 221, 178, 164, 191, 166, 156, 171, 162, 124, 124, 148, 117, 157, 133, 101, 120, 101, 115, 93, 64, 65, 29, 7, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.6, 0.0, 36.7, 40.3, 44.3, 47.6, 50.9, 54.3, 57.9, 61.4, 64.8, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 77.7, 58.8, 60.8, 69.9, 70.3, 75.9, 92.8, 84.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 4, 9, 17, 23, 20, 27, 25, 28, 20]
Epoch 495 Acc: 98.42 BMA: 98.54 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 254 train Loss: 2877.3 test Loss: 265.2
Epoch 496 Iter 0 subLoss 2176.2 multi 6.97 import weight 0.00
Epoch 496 Iter 1 subLoss 2884.2 multi -31.84 import weight 0.00
Epoch 496 Iter 2 subLoss 3646.4 multi -16.91 import weight 0.00
Epoch 496 Iter 3 subLoss 3821.4 multi 9.96 import weight 0.00
Epoch 496 Iter 4 subLoss 2925.6 multi -7.96 import weight 0.00
Epoch 496 Iter 5 subLoss 3075.1 multi -7.96 import weight 0.00
Epoch 496 Iter 6 subLoss 4268.0 multi 1.00 import weight 0.00
Epoch 496 Iter 7 subLoss 3931.3 multi -1.99 import weight 0.00
Epoch 496 Iter 8 subLoss 4467.8 multi 9.96 import weight 0.00
Epoch 496 Iter 9 subLoss 3348.2 multi 9.96 import weight 0.00
Epoch 496 Iter 10 subLoss 2313.4 multi -1.99 import weight 0.00
Epoch 496 Iter 11 subLoss 3118.9 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10773 / 18.07
Entropy seen (from low to high)
[8, 119, 261, 504, 526, 367, 342, 422, 563, 486, 344, 236, 138, 117, 102, 73, 58, 57, 55, 36, 26, 43, 39, 27, 24, 29, 17, 15, 20, 22, 16, 10, 5, 4, 6, 4, 1, 9, 3, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 20, 30, 56, 69, 97, 98, 119, 132, 150, 169, 200, 166, 236, 190, 212, 216, 176, 172, 182, 167, 161, 166, 162, 127, 123, 143, 122, 148, 141, 99, 123, 99, 116, 95, 63, 68, 28, 8, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 0.0, 36.7, 40.3, 44.3, 47.6, 50.8, 54.2, 57.9, 61.2, 64.7, 68.0]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 77.7, 58.8, 59.0, 71.4, 69.2, 79.9, 92.8, 80.9]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 4, 9, 17, 22, 21, 26, 25, 28, 21]
Epoch 496 Acc: 98.21 BMA: 98.54 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 311 train Loss: 3169.7 test Loss: 317.2
Epoch 497 Iter 0 subLoss 3341.4 multi 12.94 import weight 0.00
Epoch 497 Iter 1 subLoss 3023.4 multi 3.99 import weight 0.00
Epoch 497 Iter 2 subLoss 2476.3 multi 3.99 import weight 0.00
Epoch 497 Iter 3 subLoss 2616.4 multi 6.97 import weight 0.00
Epoch 497 Iter 4 subLoss 2451.3 multi -13.93 import weight 0.00
Epoch 497 Iter 5 subLoss 2717.5 multi -10.94 import weight 0.00
Epoch 497 Iter 6 subLoss 3349.6 multi 15.93 import weight 0.00
Epoch 497 Iter 7 subLoss 2845.9 multi 15.93 import weight 0.00
Epoch 497 Iter 8 subLoss 2526.9 multi 9.96 import weight 0.00
Epoch 497 Iter 9 subLoss 2645.5 multi -19.90 import weight 0.00
Epoch 497 Iter 10 subLoss 3333.8 multi -7.96 import weight 0.00
Epoch 497 Iter 11 subLoss 3097.4 multi 24.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10778 / 18.53
Entropy seen (from low to high)
[8, 122, 261, 506, 526, 366, 350, 423, 563, 490, 331, 238, 135, 116, 102, 73, 59, 56, 55, 35, 25, 44, 40, 25, 24, 29, 16, 16, 20, 22, 17, 9, 5, 5, 5, 4, 1, 9, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 21, 30, 55, 69, 97, 99, 121, 129, 149, 172, 201, 164, 231, 194, 213, 213, 177, 172, 186, 161, 153, 170, 165, 133, 117, 140, 125, 150, 137, 102, 124, 98, 116, 96, 64, 69, 26, 10, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 0.0, 36.8, 40.3, 44.4, 47.6, 50.9, 54.1, 58.0, 61.4, 64.7, 67.8]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 77.7, 58.8, 54.5, 74.9, 65.5, 86.9, 92.5, 85.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 4, 9, 17, 22, 20, 29, 23, 27, 21]
Epoch 497 Acc: 97.82 BMA: 98.56 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 24.88 Pidx 309 train Loss: 3119.2 test Loss: 403.4
Epoch 498 Iter 0 subLoss 3058.6 multi 1.00 import weight 0.00
Epoch 498 Iter 1 subLoss 2669.0 multi 9.96 import weight 0.00
Epoch 498 Iter 2 subLoss 2438.6 multi -1.98 import weight 0.00
Epoch 498 Iter 3 subLoss 2930.7 multi 24.88 import weight 1.00
Epoch 498 Iter 4 subLoss 2863.7 multi 6.97 import weight 0.00
Epoch 498 Iter 5 subLoss 2535.3 multi -1.99 import weight 0.00
Epoch 498 Iter 6 subLoss 2721.5 multi 15.93 import weight 0.00
Epoch 498 Iter 7 subLoss 2524.6 multi 12.94 import weight 0.00
Epoch 498 Iter 8 subLoss 2262.4 multi -28.85 import weight 0.00
Epoch 498 Iter 9 subLoss 2268.2 multi -25.87 import weight 0.00
Epoch 498 Iter 10 subLoss 3444.4 multi -10.94 import weight 0.00
Epoch 498 Iter 11 subLoss 4082.4 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10783 / 18.47
Entropy seen (from low to high)
[8, 126, 257, 511, 523, 376, 348, 428, 563, 486, 334, 229, 136, 115, 97, 73, 60, 56, 57, 32, 26, 45, 39, 25, 25, 27, 16, 16, 21, 21, 16, 10, 5, 5, 5, 4, 1, 9, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 21, 30, 54, 71, 98, 96, 123, 128, 153, 170, 200, 161, 235, 190, 213, 216, 172, 174, 189, 157, 151, 171, 168, 126, 124, 138, 124, 149, 138, 103, 123, 98, 118, 97, 65, 68, 27, 10, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7, 0.0, 36.8, 40.3, 44.5, 47.6, 50.9, 54.1, 58.0, 61.4, 64.7, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 77.7, 58.8, 54.5, 74.9, 66.6, 86.3, 92.3, 85.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 4, 9, 17, 22, 20, 30, 22, 26, 21]
Epoch 498 Acc: 98.05 BMA: 98.56 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 408 train Loss: 2711.9 test Loss: 322.7
Epoch 499 Iter 0 subLoss 3531.2 multi 3.99 import weight 0.00
Epoch 499 Iter 1 subLoss 2851.2 multi -28.85 import weight 0.00
Epoch 499 Iter 2 subLoss 2761.6 multi 33.84 import weight 0.00
Epoch 499 Iter 3 subLoss 2266.7 multi -22.88 import weight 0.00
Epoch 499 Iter 4 subLoss 3100.4 multi -1.98 import weight 0.00
Epoch 499 Iter 5 subLoss 3647.2 multi -13.93 import weight 0.00
Epoch 499 Iter 6 subLoss 3871.3 multi 9.96 import weight 0.00
Epoch 499 Iter 7 subLoss 2653.9 multi 6.97 import weight 0.00
Epoch 499 Iter 8 subLoss 2874.0 multi 21.90 import weight 0.00
Epoch 499 Iter 9 subLoss 2289.2 multi -19.90 import weight 0.00
Epoch 499 Iter 10 subLoss 3274.5 multi 1.00 import weight 0.00
Epoch 499 Iter 11 subLoss 3076.5 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0062 / 0.10786 / 18.46
Entropy seen (from low to high)
[9, 125, 263, 511, 525, 371, 353, 423, 582, 472, 333, 226, 137, 114, 96, 72, 61, 55, 57, 31, 29, 42, 40, 24, 26, 25, 16, 16, 20, 22, 17, 9, 5, 5, 5, 4, 2, 8, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 3, 6, 22, 30, 53, 69, 99, 98, 122, 127, 151, 177, 198, 162, 232, 186, 214, 221, 172, 175, 185, 158, 148, 173, 164, 132, 117, 138, 129, 145, 136, 108, 121, 99, 118, 97, 65, 69, 29, 10, 2, 1, 0, 0, 0, 0]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.8, 0.0, 37.0, 40.2, 44.4, 47.5, 50.9, 54.2, 58.1, 61.5, 64.7, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 49.9, 74.9, 63.1, 52.3, 74.9, 67.7, 85.7, 92.3, 85.7]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 4, 8, 19, 21, 20, 31, 21, 26, 21]
Epoch 499 Acc: 98.13 BMA: 98.56 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 307 train Loss: 3151.6 test Loss: 310.9
Sampling Time used: 11327.3
Grad mul
