Namespace(N=50000, T=0.1, batch=5000, c='csghmc', classes=5, div=10, filters=16, gpu=0, hidden=10, ifprint=1.0, ifsave=1.0, lr=1e-06, part=1000000, seed=5503, sn=500, stepsize=0.01, warm=0.5, wdecay=25, zeta=30000.0)
adjust the learning rate 2.000e-06 weight decay 1.200e+01
(16, 1, 5, 5)
(16,)
(32, 16, 5, 5)
(32,)
(10, 1568)
(10,)
(5, 10)
(5,)
Current Theta
tensor([1.0000e-06, 1.0000e-06, 1.0000e-06,  ..., 1.0000e-06, 1.0000e-06,
        1.0000e-06], device='cuda:0')
Epoch 0 Iter 0 subLoss 48555.8 multi 1.00 import weight 1.00
Epoch 0 Iter 1 subLoss 48234.8 multi 1.00 import weight 1.00
Epoch 0 Iter 2 subLoss 48148.3 multi 1.00 import weight 1.00
Epoch 0 Iter 3 subLoss 47962.0 multi 1.00 import weight 1.00
Epoch 0 Iter 4 subLoss 47836.1 multi 1.00 import weight 1.00
Epoch 0 Iter 5 subLoss 47770.1 multi 1.00 import weight 1.00
Epoch 0 Iter 6 subLoss 47644.6 multi 1.00 import weight 1.00
Epoch 0 Iter 7 subLoss 47402.1 multi 1.00 import weight 1.00
Epoch 0 Iter 8 subLoss 47268.5 multi 1.00 import weight 1.00
Epoch 0 Iter 9 subLoss 46974.8 multi 1.00 import weight 1.00
Epoch 0 Iter 10 subLoss 46749.7 multi 1.00 import weight 1.00
Epoch 0 Iter 11 subLoss 46572.9 multi 1.00 import weight 1.00
Epoch 0 Acc: 56.45 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 4657 train Loss: 47264.0 test Loss: 7695.3
Epoch 1 Iter 0 subLoss 46297.7 multi 1.00 import weight 1.00
Epoch 1 Iter 1 subLoss 45861.3 multi 1.00 import weight 1.00
Epoch 1 Iter 2 subLoss 45694.1 multi 1.00 import weight 1.00
Epoch 1 Iter 3 subLoss 45133.8 multi 1.00 import weight 1.00
Epoch 1 Iter 4 subLoss 44901.4 multi 1.00 import weight 1.00
Epoch 1 Iter 5 subLoss 44577.0 multi 1.00 import weight 1.00
Epoch 1 Iter 6 subLoss 45049.3 multi 1.00 import weight 1.00
Epoch 1 Iter 7 subLoss 46280.9 multi 1.00 import weight 1.00
Epoch 1 Iter 8 subLoss 46760.0 multi 1.00 import weight 1.00
Epoch 1 Iter 9 subLoss 45333.2 multi 1.00 import weight 1.00
Epoch 1 Iter 10 subLoss 42911.1 multi 1.00 import weight 1.00
Epoch 1 Iter 11 subLoss 42119.6 multi 1.00 import weight 1.00
Epoch 1 Acc: 61.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 4211 train Loss: 42147.9 test Loss: 6295.4
Epoch 2 Iter 0 subLoss 41430.7 multi 1.00 import weight 1.00
Epoch 2 Iter 1 subLoss 40672.8 multi 1.00 import weight 1.00
Epoch 2 Iter 2 subLoss 40103.2 multi 1.00 import weight 1.00
Epoch 2 Iter 3 subLoss 40680.0 multi -1.99 import weight 1.00
Epoch 2 Iter 4 subLoss 63000.0 multi 1.00 import weight 1.00
Epoch 2 Iter 5 subLoss 46196.3 multi 1.00 import weight 1.00
Epoch 2 Iter 6 subLoss 45062.4 multi 1.00 import weight 1.00
Epoch 2 Iter 7 subLoss 44049.8 multi 1.00 import weight 1.00
Epoch 2 Iter 8 subLoss 43217.5 multi 1.00 import weight 1.00
Epoch 2 Iter 9 subLoss 42449.2 multi 1.00 import weight 1.00
Epoch 2 Iter 10 subLoss 41765.0 multi 1.00 import weight 1.00
Epoch 2 Iter 11 subLoss 41306.3 multi 1.00 import weight 1.00
Epoch 2 Acc: 55.28 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 1.00 Pidx 4130 train Loss: 41474.7 test Loss: 6150.4
Epoch 3 Iter 0 subLoss 40534.5 multi 1.00 import weight 1.00
Epoch 3 Iter 1 subLoss 39988.1 multi 1.00 import weight 1.00
Epoch 3 Iter 2 subLoss 39601.3 multi 1.00 import weight 1.00
Epoch 3 Iter 3 subLoss 38956.9 multi 1.00 import weight 1.00
Epoch 3 Iter 4 subLoss 38872.1 multi 1.00 import weight 1.00
Epoch 3 Iter 5 subLoss 38447.8 multi 1.00 import weight 1.00
Epoch 3 Iter 6 subLoss 37980.1 multi 1.00 import weight 1.00
Epoch 3 Iter 7 subLoss 38778.0 multi 1.00 import weight 1.00
Epoch 3 Iter 8 subLoss 38871.6 multi 3.99 import weight 1.00
Epoch 3 Iter 9 subLoss 77300.1 multi 1.00 import weight 0.00
Epoch 3 Iter 10 subLoss 46620.0 multi 1.00 import weight 0.00
Epoch 3 Iter 11 subLoss 46256.0 multi 1.00 import weight 0.00
Epoch 3 Acc: 40.42 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4625 train Loss: 46138.3 test Loss: 7460.8
Epoch 4 Iter 0 subLoss 45248.9 multi 1.00 import weight 0.00
Epoch 4 Iter 1 subLoss 44408.7 multi 1.00 import weight 0.00
Epoch 4 Iter 2 subLoss 43450.7 multi 1.00 import weight 0.00
Epoch 4 Iter 3 subLoss 41930.0 multi 1.00 import weight 0.00
Epoch 4 Iter 4 subLoss 40756.5 multi 1.00 import weight 0.00
Epoch 4 Iter 5 subLoss 39386.6 multi 1.00 import weight 0.00
Epoch 4 Iter 6 subLoss 38696.7 multi 1.00 import weight 0.00
Epoch 4 Iter 7 subLoss 37202.5 multi 1.00 import weight 0.00
Epoch 4 Iter 8 subLoss 36820.4 multi 1.00 import weight 0.00
Epoch 4 Iter 9 subLoss 36814.6 multi 1.00 import weight 0.00
Epoch 4 Iter 10 subLoss 37686.4 multi 1.00 import weight 0.00
Epoch 4 Iter 11 subLoss 39573.1 multi 1.00 import weight 0.00
Epoch 4 Acc: 38.92 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3957 train Loss: 42662.5 test Loss: 6431.2
Epoch 5 Iter 0 subLoss 41965.7 multi 1.00 import weight 0.00
Epoch 5 Iter 1 subLoss 36807.9 multi 1.00 import weight 0.00
Epoch 5 Iter 2 subLoss 34347.7 multi 1.00 import weight 0.00
Epoch 5 Iter 3 subLoss 32921.4 multi 1.00 import weight 0.00
Epoch 5 Iter 4 subLoss 32400.0 multi 1.00 import weight 0.00
Epoch 5 Iter 5 subLoss 31295.0 multi 1.00 import weight 0.00
Epoch 5 Iter 6 subLoss 30681.8 multi 1.00 import weight 0.00
Epoch 5 Iter 7 subLoss 30093.5 multi 1.00 import weight 0.00
Epoch 5 Iter 8 subLoss 33378.5 multi 1.00 import weight 0.00
Epoch 5 Iter 9 subLoss 43441.8 multi 1.00 import weight 0.00
Epoch 5 Iter 10 subLoss 42310.1 multi 1.00 import weight 0.00
Epoch 5 Iter 11 subLoss 32678.2 multi 1.00 import weight 0.00
Epoch 5 Acc: 75.44 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3267 train Loss: 30799.0 test Loss: 4326.0
Epoch 6 Iter 0 subLoss 30051.7 multi 1.00 import weight 0.00
Epoch 6 Iter 1 subLoss 28840.7 multi 1.00 import weight 0.00
Epoch 6 Iter 2 subLoss 27252.1 multi 1.00 import weight 0.00
Epoch 6 Iter 3 subLoss 26056.5 multi 1.00 import weight 0.00
Epoch 6 Iter 4 subLoss 25653.2 multi 1.00 import weight 0.00
Epoch 6 Iter 5 subLoss 26109.7 multi 1.00 import weight 0.00
Epoch 6 Iter 6 subLoss 29808.4 multi 1.00 import weight 0.00
Epoch 6 Iter 7 subLoss 40162.4 multi 1.00 import weight 0.00
Epoch 6 Iter 8 subLoss 33885.0 multi 1.00 import weight 0.00
Epoch 6 Iter 9 subLoss 27430.4 multi 1.00 import weight 0.00
Epoch 6 Iter 10 subLoss 25165.3 multi 1.00 import weight 0.00
Epoch 6 Iter 11 subLoss 23848.5 multi 1.00 import weight 0.00
Epoch 6 Acc: 88.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2384 train Loss: 22997.8 test Loss: 2809.4
Epoch 7 Iter 0 subLoss 21931.4 multi 1.00 import weight 0.00
Epoch 7 Iter 1 subLoss 21784.1 multi 1.00 import weight 0.00
Epoch 7 Iter 2 subLoss 21254.4 multi 1.00 import weight 0.00
Epoch 7 Iter 3 subLoss 20006.2 multi 1.00 import weight 0.00
Epoch 7 Iter 4 subLoss 20087.6 multi 1.00 import weight 0.00
Epoch 7 Iter 5 subLoss 22153.4 multi 1.00 import weight 0.00
Epoch 7 Iter 6 subLoss 40086.0 multi 1.00 import weight 0.00
Epoch 7 Iter 7 subLoss 47087.8 multi 1.00 import weight 0.00
Epoch 7 Iter 8 subLoss 31205.0 multi 1.00 import weight 0.00
Epoch 7 Iter 9 subLoss 27649.9 multi 1.00 import weight 0.00
Epoch 7 Iter 10 subLoss 24903.7 multi 1.00 import weight 0.00
Epoch 7 Iter 11 subLoss 23392.6 multi 1.00 import weight 0.00
Epoch 7 Acc: 89.43 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2339 train Loss: 22781.9 test Loss: 2667.2
Epoch 8 Iter 0 subLoss 22368.9 multi 1.00 import weight 0.00
Epoch 8 Iter 1 subLoss 21481.2 multi 1.00 import weight 0.00
Epoch 8 Iter 2 subLoss 21064.7 multi 1.00 import weight 0.00
Epoch 8 Iter 3 subLoss 22676.0 multi 1.00 import weight 0.00
Epoch 8 Iter 4 subLoss 30694.2 multi -1.99 import weight 0.00
Epoch 8 Iter 5 subLoss 466642.0 multi 1.00 import weight 0.00
Epoch 8 Iter 6 subLoss 48822.0 multi 1.00 import weight 0.00
Epoch 8 Iter 7 subLoss 48997.8 multi 1.00 import weight 0.00
Epoch 8 Iter 8 subLoss 48815.9 multi 1.00 import weight 0.00
Epoch 8 Iter 9 subLoss 48679.6 multi 1.00 import weight 0.00
Epoch 8 Iter 10 subLoss 48710.8 multi 1.00 import weight 0.00
Epoch 8 Iter 11 subLoss 48318.6 multi 1.00 import weight 0.00
Epoch 8 Acc: 22.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4831 train Loss: 49119.7 test Loss: 8212.0
Epoch 9 Iter 0 subLoss 47855.2 multi 1.00 import weight 0.00
Epoch 9 Iter 1 subLoss 47867.3 multi -1.99 import weight 0.00
Epoch 9 Iter 2 subLoss 48663.6 multi 1.00 import weight 0.00
Epoch 9 Iter 3 subLoss 47894.3 multi 1.00 import weight 0.00
Epoch 9 Iter 4 subLoss 48191.1 multi 1.00 import weight 0.00
Epoch 9 Iter 5 subLoss 47845.4 multi -1.99 import weight 0.00
Epoch 9 Iter 6 subLoss 48125.3 multi 1.00 import weight 0.00
Epoch 9 Iter 7 subLoss 47978.7 multi -1.99 import weight 0.00
Epoch 9 Iter 8 subLoss 48582.9 multi 1.00 import weight 0.00
Epoch 9 Iter 9 subLoss 48372.1 multi 1.00 import weight 0.00
Epoch 9 Iter 10 subLoss 48046.9 multi 1.00 import weight 0.00
Epoch 9 Iter 11 subLoss 47947.4 multi 1.00 import weight 0.00
Epoch 9 Acc: 22.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4794 train Loss: 48686.6 test Loss: 8101.2
Epoch 10 Iter 0 subLoss 47398.9 multi 1.00 import weight 0.00
Epoch 10 Iter 1 subLoss 47658.8 multi -1.99 import weight 0.00
Epoch 10 Iter 2 subLoss 47889.5 multi 1.00 import weight 0.00
Epoch 10 Iter 3 subLoss 47968.8 multi 3.99 import weight 1.00
Epoch 10 Iter 4 subLoss 47241.3 multi 1.00 import weight 0.00
Epoch 10 Iter 5 subLoss 47032.0 multi 1.00 import weight 0.00
Epoch 10 Iter 6 subLoss 46335.6 multi 1.00 import weight 0.00
Epoch 10 Iter 7 subLoss 46260.7 multi -1.99 import weight 0.00
Epoch 10 Iter 8 subLoss 46770.2 multi -1.99 import weight 0.00
Epoch 10 Iter 9 subLoss 47332.4 multi 1.00 import weight 0.00
Epoch 10 Iter 10 subLoss 47104.3 multi 1.00 import weight 0.00
Epoch 10 Iter 11 subLoss 46707.5 multi 1.00 import weight 0.00
Epoch 10 Acc: 34.68 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4670 train Loss: 47508.1 test Loss: 7780.2
Epoch 11 Iter 0 subLoss 46687.9 multi 1.00 import weight 0.00
Epoch 11 Iter 1 subLoss 46479.7 multi 1.00 import weight 0.00
Epoch 11 Iter 2 subLoss 46249.6 multi 1.00 import weight 0.00
Epoch 11 Iter 3 subLoss 46052.9 multi 1.00 import weight 0.00
Epoch 11 Iter 4 subLoss 45025.7 multi 1.00 import weight 0.00
Epoch 11 Iter 5 subLoss 44804.9 multi 1.00 import weight 0.00
Epoch 11 Iter 6 subLoss 44713.8 multi 1.00 import weight 0.00
Epoch 11 Iter 7 subLoss 44459.1 multi 1.00 import weight 0.00
Epoch 11 Iter 8 subLoss 44422.4 multi 1.00 import weight 0.00
Epoch 11 Iter 9 subLoss 43460.9 multi -1.99 import weight 0.00
Epoch 11 Iter 10 subLoss 44516.8 multi 1.00 import weight 0.00
Epoch 11 Iter 11 subLoss 43823.5 multi 1.00 import weight 0.00
Epoch 11 Acc: 56.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4382 train Loss: 44451.4 test Loss: 7017.5
Epoch 12 Iter 0 subLoss 43785.2 multi 1.00 import weight 0.00
Epoch 12 Iter 1 subLoss 43179.6 multi 1.00 import weight 0.00
Epoch 12 Iter 2 subLoss 43136.4 multi 1.00 import weight 0.00
Epoch 12 Iter 3 subLoss 42064.7 multi 1.00 import weight 0.00
Epoch 12 Iter 4 subLoss 41426.6 multi 1.00 import weight 0.00
Epoch 12 Iter 5 subLoss 40935.7 multi 1.00 import weight 0.00
Epoch 12 Iter 6 subLoss 40941.1 multi -1.99 import weight 0.00
Epoch 12 Iter 7 subLoss 42441.2 multi 3.99 import weight 1.00
Epoch 12 Iter 8 subLoss 42085.7 multi 1.00 import weight 0.00
Epoch 12 Iter 9 subLoss 39787.8 multi 1.00 import weight 0.00
Epoch 12 Iter 10 subLoss 39014.1 multi 1.00 import weight 0.00
Epoch 12 Iter 11 subLoss 37894.0 multi 1.00 import weight 0.00
Epoch 12 Acc: 65.64 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3789 train Loss: 38328.8 test Loss: 5657.4
Epoch 13 Iter 0 subLoss 37310.7 multi 1.00 import weight 0.00
Epoch 13 Iter 1 subLoss 36343.2 multi 1.00 import weight 0.00
Epoch 13 Iter 2 subLoss 36690.7 multi 1.00 import weight 0.00
Epoch 13 Iter 3 subLoss 35629.1 multi 1.00 import weight 0.00
Epoch 13 Iter 4 subLoss 34640.6 multi 1.00 import weight 0.00
Epoch 13 Iter 5 subLoss 34151.2 multi 1.00 import weight 0.00
Epoch 13 Iter 6 subLoss 33431.2 multi 1.00 import weight 0.00
Epoch 13 Iter 7 subLoss 33043.6 multi 1.00 import weight 0.00
Epoch 13 Iter 8 subLoss 32683.4 multi -1.99 import weight 0.00
Epoch 13 Iter 9 subLoss 41673.1 multi 1.00 import weight 0.00
Epoch 13 Iter 10 subLoss 38870.2 multi 6.97 import weight 1.00
Epoch 13 Iter 11 subLoss 83650.8 multi 1.00 import weight 0.00
Epoch 13 Acc: 70.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 8365 train Loss: 39013.7 test Loss: 5501.6
Epoch 14 Iter 0 subLoss 38525.1 multi 1.00 import weight 0.00
Epoch 14 Iter 1 subLoss 34431.2 multi 1.00 import weight 0.00
Epoch 14 Iter 2 subLoss 32130.3 multi 1.00 import weight 0.00
Epoch 14 Iter 3 subLoss 31281.0 multi 1.00 import weight 0.00
Epoch 14 Iter 4 subLoss 29592.7 multi 1.00 import weight 0.00
Epoch 14 Iter 5 subLoss 29546.0 multi 1.00 import weight 0.00
Epoch 14 Iter 6 subLoss 27769.7 multi 1.00 import weight 0.00
Epoch 14 Iter 7 subLoss 26876.9 multi 1.00 import weight 0.00
Epoch 14 Iter 8 subLoss 25904.0 multi 1.00 import weight 0.00
Epoch 14 Iter 9 subLoss 25547.4 multi 1.00 import weight 0.00
Epoch 14 Iter 10 subLoss 25129.4 multi 1.00 import weight 0.00
Epoch 14 Iter 11 subLoss 25272.9 multi 1.00 import weight 0.00
Epoch 14 Acc: 77.89 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2527 train Loss: 29125.4 test Loss: 3492.2
Epoch 15 Iter 0 subLoss 28936.1 multi 1.00 import weight 0.00
Epoch 15 Iter 1 subLoss 26347.4 multi 1.00 import weight 0.00
Epoch 15 Iter 2 subLoss 24850.1 multi 1.00 import weight 0.00
Epoch 15 Iter 3 subLoss 24668.2 multi 1.00 import weight 0.00
Epoch 15 Iter 4 subLoss 22473.9 multi 1.00 import weight 0.00
Epoch 15 Iter 5 subLoss 22468.4 multi 1.00 import weight 0.00
Epoch 15 Iter 6 subLoss 22861.0 multi 1.00 import weight 0.00
Epoch 15 Iter 7 subLoss 26422.7 multi 1.00 import weight 0.00
Epoch 15 Iter 8 subLoss 19363.9 multi 1.00 import weight 0.00
Epoch 15 Iter 9 subLoss 19346.0 multi 1.00 import weight 0.00
Epoch 15 Iter 10 subLoss 19770.3 multi 1.00 import weight 0.00
Epoch 15 Iter 11 subLoss 22245.0 multi 1.00 import weight 0.00
Epoch 15 Acc: 88.66 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2224 train Loss: 19349.7 test Loss: 1854.9
Epoch 16 Iter 0 subLoss 19348.9 multi 3.99 import weight 0.00
Epoch 16 Iter 1 subLoss 70243.5 multi 1.00 import weight 0.00
Epoch 16 Iter 2 subLoss 49245.9 multi 1.00 import weight 0.00
Epoch 16 Iter 3 subLoss 34451.1 multi 1.00 import weight 0.00
Epoch 16 Iter 4 subLoss 29965.7 multi 1.00 import weight 0.00
Epoch 16 Iter 5 subLoss 26774.5 multi 1.00 import weight 0.00
Epoch 16 Iter 6 subLoss 24989.1 multi 1.00 import weight 0.00
Epoch 16 Iter 7 subLoss 22671.4 multi 3.99 import weight 0.00
Epoch 16 Iter 8 subLoss 20125.6 multi 1.00 import weight 0.00
Epoch 16 Iter 9 subLoss 18945.7 multi 1.00 import weight 0.00
Epoch 16 Iter 10 subLoss 18150.8 multi 1.00 import weight 0.00
Epoch 16 Iter 11 subLoss 17440.8 multi 1.00 import weight 0.00
Epoch 16 Acc: 87.43 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1744 train Loss: 17633.5 test Loss: 2112.3
Epoch 17 Iter 0 subLoss 17278.0 multi 1.00 import weight 0.00
Epoch 17 Iter 1 subLoss 17400.8 multi 1.00 import weight 0.00
Epoch 17 Iter 2 subLoss 16234.8 multi 1.00 import weight 0.00
Epoch 17 Iter 3 subLoss 15977.3 multi 1.00 import weight 0.00
Epoch 17 Iter 4 subLoss 16184.5 multi 1.00 import weight 0.00
Epoch 17 Iter 5 subLoss 16449.3 multi 1.00 import weight 0.00
Epoch 17 Iter 6 subLoss 17177.2 multi 1.00 import weight 0.00
Epoch 17 Iter 7 subLoss 21950.6 multi 1.00 import weight 0.00
Epoch 17 Iter 8 subLoss 37779.4 multi 1.00 import weight 0.00
Epoch 17 Iter 9 subLoss 30490.7 multi 1.00 import weight 0.00
Epoch 17 Iter 10 subLoss 19729.9 multi 1.00 import weight 0.00
Epoch 17 Iter 11 subLoss 16580.6 multi 1.00 import weight 0.00
Epoch 17 Acc: 90.83 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1658 train Loss: 15366.4 test Loss: 1665.0
Epoch 18 Iter 0 subLoss 15640.9 multi 1.00 import weight 0.00
Epoch 18 Iter 1 subLoss 13878.6 multi 1.00 import weight 0.00
Epoch 18 Iter 2 subLoss 13292.1 multi 1.00 import weight 0.00
Epoch 18 Iter 3 subLoss 13152.5 multi 1.00 import weight 0.00
Epoch 18 Iter 4 subLoss 13576.9 multi 1.00 import weight 0.00
Epoch 18 Iter 5 subLoss 15315.2 multi 1.00 import weight 0.00
Epoch 18 Iter 6 subLoss 19941.4 multi 1.00 import weight 0.00
Epoch 18 Iter 7 subLoss 29690.4 multi 1.00 import weight 0.00
Epoch 18 Iter 8 subLoss 29979.2 multi -1.99 import weight 0.00
Epoch 18 Iter 9 subLoss 289820.3 multi 1.00 import weight 0.00
Epoch 18 Iter 10 subLoss 56042.8 multi 1.00 import weight 0.00
Epoch 18 Iter 11 subLoss 42552.3 multi 1.00 import weight 0.00
Epoch 18 Acc: 36.49 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4255 train Loss: 41224.8 test Loss: 6783.8
Epoch 19 Iter 0 subLoss 40542.0 multi -1.99 import weight 0.00
Epoch 19 Iter 1 subLoss 43852.9 multi 1.00 import weight 0.00
Epoch 19 Iter 2 subLoss 42416.3 multi 1.00 import weight 0.00
Epoch 19 Iter 3 subLoss 40622.2 multi 1.00 import weight 0.00
Epoch 19 Iter 4 subLoss 39037.1 multi 1.00 import weight 0.00
Epoch 19 Iter 5 subLoss 37622.4 multi 1.00 import weight 0.00
Epoch 19 Iter 6 subLoss 36703.8 multi -1.99 import weight 0.00
Epoch 19 Iter 7 subLoss 39028.1 multi -1.99 import weight 0.00
Epoch 19 Iter 8 subLoss 42562.0 multi -1.99 import weight 0.00
Epoch 19 Iter 9 subLoss 53867.6 multi 1.00 import weight 0.00
Epoch 19 Iter 10 subLoss 43572.3 multi 1.00 import weight 0.00
Epoch 19 Iter 11 subLoss 41875.1 multi 1.00 import weight 0.00
Epoch 19 Acc: 36.78 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4187 train Loss: 41622.6 test Loss: 6849.9
Epoch 20 Iter 0 subLoss 41180.0 multi 1.00 import weight 0.00
Epoch 20 Iter 1 subLoss 39388.1 multi 3.99 import weight 0.00
Epoch 20 Iter 2 subLoss 35666.2 multi 1.00 import weight 0.00
Epoch 20 Iter 3 subLoss 34743.1 multi 1.00 import weight 0.00
Epoch 20 Iter 4 subLoss 33785.6 multi 1.00 import weight 0.00
Epoch 20 Iter 5 subLoss 33148.3 multi 1.00 import weight 0.00
Epoch 20 Iter 6 subLoss 32869.9 multi 1.00 import weight 0.00
Epoch 20 Iter 7 subLoss 32402.4 multi 3.99 import weight 0.00
Epoch 20 Iter 8 subLoss 30283.0 multi 1.00 import weight 0.00
Epoch 20 Iter 9 subLoss 29315.7 multi 1.00 import weight 0.00
Epoch 20 Iter 10 subLoss 28954.9 multi 1.00 import weight 0.00
Epoch 20 Iter 11 subLoss 28044.6 multi 1.00 import weight 0.00
Epoch 20 Acc: 85.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2804 train Loss: 28314.1 test Loss: 3814.6
Epoch 21 Iter 0 subLoss 27706.5 multi 1.00 import weight 0.00
Epoch 21 Iter 1 subLoss 26767.0 multi 1.00 import weight 0.00
Epoch 21 Iter 2 subLoss 26207.3 multi 1.00 import weight 0.00
Epoch 21 Iter 3 subLoss 26935.3 multi 1.00 import weight 0.00
Epoch 21 Iter 4 subLoss 25702.6 multi 1.00 import weight 0.00
Epoch 21 Iter 5 subLoss 25345.1 multi 1.00 import weight 0.00
Epoch 21 Iter 6 subLoss 24854.3 multi 3.99 import weight 0.00
Epoch 21 Iter 7 subLoss 45279.3 multi 1.00 import weight 0.00
Epoch 21 Iter 8 subLoss 34380.3 multi 1.00 import weight 0.00
Epoch 21 Iter 9 subLoss 28241.3 multi 1.00 import weight 0.00
Epoch 21 Iter 10 subLoss 25706.2 multi 3.99 import weight 0.00
Epoch 21 Iter 11 subLoss 25465.2 multi 1.00 import weight 0.00
Epoch 21 Acc: 81.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2546 train Loss: 25386.9 test Loss: 3235.7
Epoch 22 Iter 0 subLoss 24937.0 multi 1.00 import weight 0.00
Epoch 22 Iter 1 subLoss 25345.9 multi 3.99 import weight 0.00
Epoch 22 Iter 2 subLoss 118603.7 multi 1.00 import weight 0.00
Epoch 22 Iter 3 subLoss 42759.1 multi 1.00 import weight 0.00
Epoch 22 Iter 4 subLoss 40968.9 multi 1.00 import weight 0.00
Epoch 22 Iter 5 subLoss 39375.0 multi 1.00 import weight 0.00
Epoch 22 Iter 6 subLoss 37740.3 multi 1.00 import weight 0.00
Epoch 22 Iter 7 subLoss 35859.5 multi 1.00 import weight 0.00
Epoch 22 Iter 8 subLoss 34763.7 multi 1.00 import weight 0.00
Epoch 22 Iter 9 subLoss 31847.0 multi 1.00 import weight 0.00
Epoch 22 Iter 10 subLoss 29654.6 multi 1.00 import weight 0.00
Epoch 22 Iter 11 subLoss 27361.2 multi 1.00 import weight 0.00
Epoch 22 Acc: 91.59 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2736 train Loss: 25076.0 test Loss: 2956.8
Epoch 23 Iter 0 subLoss 24763.9 multi 1.00 import weight 0.00
Epoch 23 Iter 1 subLoss 23325.7 multi 1.00 import weight 0.00
Epoch 23 Iter 2 subLoss 22533.4 multi 1.00 import weight 0.00
Epoch 23 Iter 3 subLoss 23654.6 multi 1.00 import weight 0.00
Epoch 23 Iter 4 subLoss 22014.4 multi 1.00 import weight 0.00
Epoch 23 Iter 5 subLoss 22769.3 multi 1.00 import weight 0.00
Epoch 23 Iter 6 subLoss 18995.0 multi 1.00 import weight 0.00
Epoch 23 Iter 7 subLoss 17632.6 multi 1.00 import weight 0.00
Epoch 23 Iter 8 subLoss 17636.6 multi 3.99 import weight 0.00
Epoch 23 Iter 9 subLoss 66411.0 multi 1.00 import weight 0.00
Epoch 23 Iter 10 subLoss 32646.7 multi 1.00 import weight 0.00
Epoch 23 Iter 11 subLoss 24186.6 multi 1.00 import weight 0.00
Epoch 23 Acc: 82.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2418 train Loss: 22598.8 test Loss: 2835.0
Epoch 24 Iter 0 subLoss 22259.8 multi -1.99 import weight 0.00
Epoch 24 Iter 1 subLoss 25155.8 multi 1.00 import weight 0.00
Epoch 24 Iter 2 subLoss 23351.4 multi 1.00 import weight 0.00
Epoch 24 Iter 3 subLoss 22159.4 multi 3.99 import weight 0.00
Epoch 24 Iter 4 subLoss 18222.7 multi 1.00 import weight 0.00
Epoch 24 Iter 5 subLoss 16614.4 multi 1.00 import weight 0.00
Epoch 24 Iter 6 subLoss 15817.7 multi 1.00 import weight 0.00
Epoch 24 Iter 7 subLoss 15515.2 multi 1.00 import weight 0.00
Epoch 24 Iter 8 subLoss 15339.5 multi 1.00 import weight 0.00
Epoch 24 Iter 9 subLoss 14296.4 multi 1.00 import weight 0.00
Epoch 24 Iter 10 subLoss 13565.3 multi 1.00 import weight 0.00
Epoch 24 Iter 11 subLoss 14931.9 multi 1.00 import weight 0.00
Epoch 24 Acc: 92.57 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1493 train Loss: 13679.4 test Loss: 1339.7
Epoch 25 Iter 0 subLoss 13627.4 multi 1.00 import weight 0.00
Epoch 25 Iter 1 subLoss 12440.4 multi 1.00 import weight 0.00
Epoch 25 Iter 2 subLoss 12736.9 multi 1.00 import weight 0.00
Epoch 25 Iter 3 subLoss 12459.5 multi -1.99 import weight 0.00
Epoch 25 Iter 4 subLoss 13452.6 multi 1.00 import weight 0.00
Epoch 25 Iter 5 subLoss 12271.7 multi 1.00 import weight 0.00
Epoch 25 Iter 6 subLoss 12420.5 multi 1.00 import weight 0.00
Epoch 25 Iter 7 subLoss 11753.4 multi 1.00 import weight 0.00
Epoch 25 Iter 8 subLoss 11688.3 multi 1.00 import weight 0.00
Epoch 25 Iter 9 subLoss 11330.1 multi 1.00 import weight 0.00
Epoch 25 Iter 10 subLoss 10823.5 multi 1.00 import weight 0.00
Epoch 25 Iter 11 subLoss 10873.2 multi 1.00 import weight 0.00
Epoch 25 Acc: 91.54 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1087 train Loss: 11170.9 test Loss: 1299.8
Epoch 26 Iter 0 subLoss 10741.7 multi 1.00 import weight 0.00
Epoch 26 Iter 1 subLoss 12099.5 multi 1.00 import weight 0.00
Epoch 26 Iter 2 subLoss 13965.7 multi 1.00 import weight 0.00
Epoch 26 Iter 3 subLoss 21080.3 multi 1.00 import weight 0.00
Epoch 26 Iter 4 subLoss 35070.0 multi 1.00 import weight 0.00
Epoch 26 Iter 5 subLoss 40138.8 multi 1.00 import weight 0.00
Epoch 26 Iter 6 subLoss 27890.9 multi 1.00 import weight 0.00
Epoch 26 Iter 7 subLoss 18003.4 multi 1.00 import weight 0.00
Epoch 26 Iter 8 subLoss 14071.4 multi 1.00 import weight 0.00
Epoch 26 Iter 9 subLoss 11758.4 multi 3.99 import weight 0.00
Epoch 26 Iter 10 subLoss 11618.5 multi 1.00 import weight 0.00
Epoch 26 Iter 11 subLoss 10209.7 multi 1.00 import weight 0.00
Epoch 26 Acc: 93.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1020 train Loss: 10177.0 test Loss: 1087.8
Epoch 27 Iter 0 subLoss 9769.2 multi 1.00 import weight 0.00
Epoch 27 Iter 1 subLoss 9967.6 multi 1.00 import weight 0.00
Epoch 27 Iter 2 subLoss 9510.6 multi 1.00 import weight 0.00
Epoch 27 Iter 3 subLoss 10246.7 multi 1.00 import weight 0.00
Epoch 27 Iter 4 subLoss 11700.1 multi 1.00 import weight 0.00
Epoch 27 Iter 5 subLoss 12928.1 multi 1.00 import weight 0.00
Epoch 27 Iter 6 subLoss 15691.9 multi 1.00 import weight 0.00
Epoch 27 Iter 7 subLoss 13173.7 multi 1.00 import weight 0.00
Epoch 27 Iter 8 subLoss 9674.8 multi 1.00 import weight 0.00
Epoch 27 Iter 9 subLoss 9235.2 multi 1.00 import weight 0.00
Epoch 27 Iter 10 subLoss 8738.4 multi 1.00 import weight 0.00
Epoch 27 Iter 11 subLoss 8699.4 multi 1.00 import weight 0.00
Epoch 27 Acc: 93.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 869 train Loss: 8828.1 test Loss: 995.3
Epoch 28 Iter 0 subLoss 7941.8 multi 1.00 import weight 0.00
Epoch 28 Iter 1 subLoss 8722.0 multi 1.00 import weight 0.00
Epoch 28 Iter 2 subLoss 8932.7 multi 1.00 import weight 0.00
Epoch 28 Iter 3 subLoss 8542.5 multi 1.00 import weight 0.00
Epoch 28 Iter 4 subLoss 8051.6 multi 1.00 import weight 0.00
Epoch 28 Iter 5 subLoss 7867.3 multi 1.00 import weight 0.00
Epoch 28 Iter 6 subLoss 8028.3 multi 1.00 import weight 0.00
Epoch 28 Iter 7 subLoss 7894.7 multi 1.00 import weight 0.00
Epoch 28 Iter 8 subLoss 8275.4 multi 1.00 import weight 0.00
Epoch 28 Iter 9 subLoss 9057.2 multi 1.00 import weight 0.00
Epoch 28 Iter 10 subLoss 8403.6 multi 1.00 import weight 0.00
Epoch 28 Iter 11 subLoss 8801.6 multi 1.00 import weight 0.00
Epoch 28 Acc: 92.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 880 train Loss: 9583.9 test Loss: 1142.5
Epoch 29 Iter 0 subLoss 9559.2 multi 1.00 import weight 0.00
Epoch 29 Iter 1 subLoss 8812.3 multi -1.99 import weight 0.00
Epoch 29 Iter 2 subLoss 84840.0 multi 1.00 import weight 0.00
Epoch 29 Iter 3 subLoss 58668.3 multi 1.00 import weight 0.00
Epoch 29 Iter 4 subLoss 45729.4 multi 1.00 import weight 0.00
Epoch 29 Iter 5 subLoss 33972.8 multi 1.00 import weight 0.00
Epoch 29 Iter 6 subLoss 29857.0 multi 1.00 import weight 0.00
Epoch 29 Iter 7 subLoss 25887.6 multi 1.00 import weight 0.00
Epoch 29 Iter 8 subLoss 23882.2 multi 1.00 import weight 0.00
Epoch 29 Iter 9 subLoss 22143.4 multi 1.00 import weight 0.00
Epoch 29 Iter 10 subLoss 20051.0 multi 1.00 import weight 0.00
Epoch 29 Iter 11 subLoss 19203.8 multi 1.00 import weight 0.00
Epoch 29 Acc: 81.16 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1920 train Loss: 17889.1 test Loss: 2296.0
Epoch 30 Iter 0 subLoss 17278.4 multi 3.99 import weight 0.00
Epoch 30 Iter 1 subLoss 13830.7 multi 1.00 import weight 0.00
Epoch 30 Iter 2 subLoss 13182.0 multi -1.99 import weight 0.00
Epoch 30 Iter 3 subLoss 26592.9 multi 1.00 import weight 0.00
Epoch 30 Iter 4 subLoss 14734.4 multi 1.00 import weight 0.00
Epoch 30 Iter 5 subLoss 12649.9 multi 1.00 import weight 0.00
Epoch 30 Iter 6 subLoss 11927.5 multi 1.00 import weight 0.00
Epoch 30 Iter 7 subLoss 11042.0 multi 1.00 import weight 0.00
Epoch 30 Iter 8 subLoss 10900.3 multi 1.00 import weight 0.00
Epoch 30 Iter 9 subLoss 10649.1 multi 1.00 import weight 0.00
Epoch 30 Iter 10 subLoss 10047.7 multi 1.00 import weight 0.00
Epoch 30 Iter 11 subLoss 9414.3 multi 1.00 import weight 0.00
Epoch 30 Acc: 94.01 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 941 train Loss: 9199.1 test Loss: 989.7
Epoch 31 Iter 0 subLoss 8567.2 multi 1.00 import weight 0.00
Epoch 31 Iter 1 subLoss 9152.7 multi 1.00 import weight 0.00
Epoch 31 Iter 2 subLoss 8929.4 multi 1.00 import weight 0.00
Epoch 31 Iter 3 subLoss 8635.6 multi 1.00 import weight 0.00
Epoch 31 Iter 4 subLoss 8099.6 multi 1.00 import weight 0.00
Epoch 31 Iter 5 subLoss 8710.6 multi 1.00 import weight 0.00
Epoch 31 Iter 6 subLoss 8292.0 multi 1.00 import weight 0.00
Epoch 31 Iter 7 subLoss 6452.1 multi 1.00 import weight 0.00
Epoch 31 Iter 8 subLoss 7591.7 multi 1.00 import weight 0.00
Epoch 31 Iter 9 subLoss 8433.9 multi 1.00 import weight 0.00
Epoch 31 Iter 10 subLoss 6792.9 multi 1.00 import weight 0.00
Epoch 31 Iter 11 subLoss 7748.3 multi 1.00 import weight 0.00
Epoch 31 Acc: 95.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 774 train Loss: 7287.3 test Loss: 793.9
Epoch 32 Iter 0 subLoss 6650.4 multi 1.00 import weight 0.00
Epoch 32 Iter 1 subLoss 7425.6 multi 1.00 import weight 0.00
Epoch 32 Iter 2 subLoss 7239.5 multi 1.00 import weight 0.00
Epoch 32 Iter 3 subLoss 7528.1 multi 1.00 import weight 0.00
Epoch 32 Iter 4 subLoss 6886.9 multi 1.00 import weight 0.00
Epoch 32 Iter 5 subLoss 7522.1 multi 3.99 import weight 0.00
Epoch 32 Iter 6 subLoss 15471.0 multi 1.00 import weight 0.00
Epoch 32 Iter 7 subLoss 15907.9 multi 1.00 import weight 0.00
Epoch 32 Iter 8 subLoss 15490.2 multi 1.00 import weight 0.00
Epoch 32 Iter 9 subLoss 10311.7 multi 1.00 import weight 0.00
Epoch 32 Iter 10 subLoss 7453.7 multi 1.00 import weight 0.00
Epoch 32 Iter 11 subLoss 7158.3 multi 1.00 import weight 0.00
Epoch 32 Acc: 94.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 715 train Loss: 7105.0 test Loss: 833.9
Epoch 33 Iter 0 subLoss 6157.0 multi 1.00 import weight 0.00
Epoch 33 Iter 1 subLoss 6853.5 multi 1.00 import weight 0.00
Epoch 33 Iter 2 subLoss 6758.7 multi 1.00 import weight 0.00
Epoch 33 Iter 3 subLoss 5963.7 multi 1.00 import weight 0.00
Epoch 33 Iter 4 subLoss 6269.5 multi 1.00 import weight 0.00
Epoch 33 Iter 5 subLoss 6444.2 multi 1.00 import weight 0.00
Epoch 33 Iter 6 subLoss 7030.4 multi 1.00 import weight 0.00
Epoch 33 Iter 7 subLoss 6379.3 multi 1.00 import weight 0.00
Epoch 33 Iter 8 subLoss 6174.8 multi 1.00 import weight 0.00
Epoch 33 Iter 9 subLoss 5838.9 multi 1.00 import weight 0.00
Epoch 33 Iter 10 subLoss 6576.6 multi 1.00 import weight 0.00
Epoch 33 Iter 11 subLoss 6777.8 multi 1.00 import weight 0.00
Epoch 33 Acc: 95.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 677 train Loss: 6120.4 test Loss: 676.0
Epoch 34 Iter 0 subLoss 6128.1 multi 1.00 import weight 0.00
Epoch 34 Iter 1 subLoss 6252.8 multi 1.00 import weight 0.00
Epoch 34 Iter 2 subLoss 6134.3 multi -1.99 import weight 0.00
Epoch 34 Iter 3 subLoss 6583.7 multi -1.99 import weight 0.00
Epoch 34 Iter 4 subLoss 12829.8 multi 1.00 import weight 0.00
Epoch 34 Iter 5 subLoss 10209.6 multi 3.99 import weight 0.00
Epoch 34 Iter 6 subLoss 77657.1 multi 1.00 import weight 0.00
Epoch 34 Iter 7 subLoss 43375.1 multi 1.00 import weight 0.00
Epoch 34 Iter 8 subLoss 34996.7 multi 1.00 import weight 0.00
Epoch 34 Iter 9 subLoss 19569.9 multi 1.00 import weight 0.00
Epoch 34 Iter 10 subLoss 15438.1 multi 1.00 import weight 0.00
Epoch 34 Iter 11 subLoss 13155.5 multi 3.99 import weight 0.00
Epoch 34 Acc: 91.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1315 train Loss: 10342.8 test Loss: 1331.0
Epoch 35 Iter 0 subLoss 10019.1 multi 1.00 import weight 0.00
Epoch 35 Iter 1 subLoss 9377.3 multi 1.00 import weight 0.00
Epoch 35 Iter 2 subLoss 8256.9 multi 1.00 import weight 0.00
Epoch 35 Iter 3 subLoss 7669.3 multi 1.00 import weight 0.00
Epoch 35 Iter 4 subLoss 7610.5 multi 1.00 import weight 0.00
Epoch 35 Iter 5 subLoss 8429.4 multi 1.00 import weight 0.00
Epoch 35 Iter 6 subLoss 6917.8 multi 1.00 import weight 0.00
Epoch 35 Iter 7 subLoss 6938.5 multi 1.00 import weight 0.00
Epoch 35 Iter 8 subLoss 7207.9 multi 1.00 import weight 0.00
Epoch 35 Iter 9 subLoss 7287.9 multi 1.00 import weight 0.00
Epoch 35 Iter 10 subLoss 6431.0 multi 1.00 import weight 0.00
Epoch 35 Iter 11 subLoss 7110.5 multi 1.00 import weight 0.00
Epoch 35 Acc: 95.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 711 train Loss: 6618.3 test Loss: 806.8
Epoch 36 Iter 0 subLoss 7009.0 multi 1.00 import weight 0.00
Epoch 36 Iter 1 subLoss 6051.4 multi 1.00 import weight 0.00
Epoch 36 Iter 2 subLoss 5943.6 multi 1.00 import weight 0.00
Epoch 36 Iter 3 subLoss 6420.2 multi 1.00 import weight 0.00
Epoch 36 Iter 4 subLoss 5891.3 multi 1.00 import weight 0.00
Epoch 36 Iter 5 subLoss 6022.6 multi 1.00 import weight 0.00
Epoch 36 Iter 6 subLoss 6008.1 multi 1.00 import weight 0.00
Epoch 36 Iter 7 subLoss 6107.8 multi 1.00 import weight 0.00
Epoch 36 Iter 8 subLoss 6284.0 multi 1.00 import weight 0.00
Epoch 36 Iter 9 subLoss 6399.2 multi 1.00 import weight 0.00
Epoch 36 Iter 10 subLoss 6088.7 multi 1.00 import weight 0.00
Epoch 36 Iter 11 subLoss 5565.7 multi 1.00 import weight 0.00
Epoch 36 Acc: 95.74 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 556 train Loss: 5741.2 test Loss: 682.5
Epoch 37 Iter 0 subLoss 5845.6 multi -1.99 import weight 0.00
Epoch 37 Iter 1 subLoss 6530.2 multi 1.00 import weight 0.00
Epoch 37 Iter 2 subLoss 6419.6 multi 1.00 import weight 0.00
Epoch 37 Iter 3 subLoss 6379.4 multi 3.99 import weight 0.00
Epoch 37 Iter 4 subLoss 11613.1 multi 3.99 import weight 0.00
Epoch 37 Iter 5 subLoss 153393.4 multi 1.00 import weight 0.00
Epoch 37 Iter 6 subLoss 57649.8 multi 1.00 import weight 0.00
Epoch 37 Iter 7 subLoss 30346.8 multi 1.00 import weight 0.00
Epoch 37 Iter 8 subLoss 24727.4 multi 1.00 import weight 0.00
Epoch 37 Iter 9 subLoss 22156.3 multi 3.98 import weight 1.00
Epoch 37 Iter 10 subLoss 16057.9 multi 1.00 import weight 0.00
Epoch 37 Iter 11 subLoss 13653.2 multi 1.00 import weight 0.00
Epoch 37 Acc: 90.70 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1365 train Loss: 12029.2 test Loss: 1506.4
Epoch 38 Iter 0 subLoss 11597.4 multi 1.00 import weight 0.00
Epoch 38 Iter 1 subLoss 10659.1 multi -1.99 import weight 0.00
Epoch 38 Iter 2 subLoss 13433.5 multi 1.00 import weight 0.00
Epoch 38 Iter 3 subLoss 12096.4 multi 3.99 import weight 0.00
Epoch 38 Iter 4 subLoss 8846.6 multi 1.00 import weight 0.00
Epoch 38 Iter 5 subLoss 8496.6 multi 1.00 import weight 0.00
Epoch 38 Iter 6 subLoss 7761.9 multi 1.00 import weight 0.00
Epoch 38 Iter 7 subLoss 7518.8 multi 1.00 import weight 0.00
Epoch 38 Iter 8 subLoss 7525.3 multi 3.98 import weight 1.00
Epoch 38 Iter 9 subLoss 6795.4 multi 3.99 import weight 0.00
Epoch 38 Iter 10 subLoss 7899.2 multi 3.99 import weight 0.00
Epoch 38 Iter 11 subLoss 25268.7 multi 1.00 import weight 0.00
Epoch 38 Acc: 81.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2526 train Loss: 15442.8 test Loss: 2884.9
Epoch 39 Iter 0 subLoss 15501.8 multi -1.99 import weight 0.00
Epoch 39 Iter 1 subLoss 190159.9 multi 1.00 import weight 0.00
Epoch 39 Iter 2 subLoss 46537.5 multi 1.00 import weight 0.00
Epoch 39 Iter 3 subLoss 32688.6 multi 1.00 import weight 0.00
Epoch 39 Iter 4 subLoss 30053.2 multi 3.99 import weight 0.00
Epoch 39 Iter 5 subLoss 22415.4 multi 1.00 import weight 0.00
Epoch 39 Iter 6 subLoss 21134.3 multi 1.00 import weight 0.00
Epoch 39 Iter 7 subLoss 19523.6 multi 1.00 import weight 0.00
Epoch 39 Iter 8 subLoss 18504.0 multi 1.00 import weight 0.00
Epoch 39 Iter 9 subLoss 17539.9 multi 1.00 import weight 0.00
Epoch 39 Iter 10 subLoss 16405.3 multi 1.00 import weight 0.00
Epoch 39 Iter 11 subLoss 14487.5 multi 1.00 import weight 0.00
Epoch 39 Acc: 93.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1448 train Loss: 14145.8 test Loss: 1626.8
Epoch 40 Iter 0 subLoss 13741.9 multi 1.00 import weight 0.00
Epoch 40 Iter 1 subLoss 13024.5 multi 1.00 import weight 0.00
Epoch 40 Iter 2 subLoss 11674.2 multi 1.00 import weight 0.00
Epoch 40 Iter 3 subLoss 11285.8 multi 1.00 import weight 0.00
Epoch 40 Iter 4 subLoss 10284.6 multi 1.00 import weight 0.00
Epoch 40 Iter 5 subLoss 9813.0 multi 1.00 import weight 0.00
Epoch 40 Iter 6 subLoss 8925.3 multi 3.99 import weight 0.00
Epoch 40 Iter 7 subLoss 8256.4 multi 3.99 import weight 0.00
Epoch 40 Iter 8 subLoss 8704.7 multi -1.99 import weight 0.00
Epoch 40 Iter 9 subLoss 82669.1 multi 1.00 import weight 0.00
Epoch 40 Iter 10 subLoss 41044.4 multi 1.00 import weight 0.00
Epoch 40 Iter 11 subLoss 33279.1 multi 1.00 import weight 0.00
Epoch 40 Acc: 72.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3327 train Loss: 30538.7 test Loss: 4579.6
Epoch 41 Iter 0 subLoss 31278.1 multi 1.00 import weight 0.00
Epoch 41 Iter 1 subLoss 28998.3 multi 1.00 import weight 0.00
Epoch 41 Iter 2 subLoss 28479.1 multi 1.00 import weight 0.00
Epoch 41 Iter 3 subLoss 25881.6 multi 3.99 import weight 0.00
Epoch 41 Iter 4 subLoss 22813.1 multi 1.00 import weight 0.00
Epoch 41 Iter 5 subLoss 20136.8 multi -1.99 import weight 0.00
Epoch 41 Iter 6 subLoss 23616.9 multi 1.00 import weight 0.00
Epoch 41 Iter 7 subLoss 21771.0 multi 1.00 import weight 0.00
Epoch 41 Iter 8 subLoss 20629.2 multi 1.00 import weight 0.00
Epoch 41 Iter 9 subLoss 19874.9 multi 1.00 import weight 0.00
Epoch 41 Iter 10 subLoss 18798.9 multi 1.00 import weight 0.00
Epoch 41 Iter 11 subLoss 18154.5 multi 3.99 import weight 0.00
Epoch 41 Acc: 84.34 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1815 train Loss: 15963.6 test Loss: 2366.6
Epoch 42 Iter 0 subLoss 15023.9 multi 1.00 import weight 0.00
Epoch 42 Iter 1 subLoss 14804.5 multi 1.00 import weight 0.00
Epoch 42 Iter 2 subLoss 12141.4 multi 1.00 import weight 0.00
Epoch 42 Iter 3 subLoss 11566.9 multi 1.00 import weight 0.00
Epoch 42 Iter 4 subLoss 10016.6 multi 3.99 import weight 0.00
Epoch 42 Iter 5 subLoss 15730.0 multi 1.00 import weight 0.00
Epoch 42 Iter 6 subLoss 13194.6 multi -1.99 import weight 0.00
Epoch 42 Iter 7 subLoss 102574.5 multi 1.00 import weight 0.00
Epoch 42 Iter 8 subLoss 45422.5 multi 1.00 import weight 0.00
Epoch 42 Iter 9 subLoss 25512.7 multi 1.00 import weight 0.00
Epoch 42 Iter 10 subLoss 19585.1 multi 1.00 import weight 0.00
Epoch 42 Iter 11 subLoss 16844.1 multi 1.00 import weight 0.00
Epoch 42 Acc: 88.87 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1684 train Loss: 15690.9 test Loss: 2119.6
Epoch 43 Iter 0 subLoss 15875.9 multi 1.00 import weight 0.00
Epoch 43 Iter 1 subLoss 13521.1 multi 1.00 import weight 0.00
Epoch 43 Iter 2 subLoss 12850.4 multi 1.00 import weight 0.00
Epoch 43 Iter 3 subLoss 11913.0 multi 1.00 import weight 0.00
Epoch 43 Iter 4 subLoss 10827.8 multi 3.99 import weight 0.00
Epoch 43 Iter 5 subLoss 9145.7 multi 1.00 import weight 0.00
Epoch 43 Iter 6 subLoss 8232.0 multi 1.00 import weight 0.00
Epoch 43 Iter 7 subLoss 8863.4 multi 1.00 import weight 0.00
Epoch 43 Iter 8 subLoss 7897.2 multi 6.97 import weight 1.00
Epoch 43 Iter 9 subLoss 7371.3 multi 1.00 import weight 0.00
Epoch 43 Iter 10 subLoss 7409.6 multi 1.00 import weight 0.00
Epoch 43 Iter 11 subLoss 6737.0 multi 1.00 import weight 0.00
Epoch 43 Acc: 95.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 673 train Loss: 6880.1 test Loss: 766.3
Epoch 44 Iter 0 subLoss 6867.8 multi -1.99 import weight 0.00
Epoch 44 Iter 1 subLoss 7518.1 multi 3.99 import weight 0.00
Epoch 44 Iter 2 subLoss 6598.4 multi -1.99 import weight 0.00
Epoch 44 Iter 3 subLoss 8628.3 multi 1.00 import weight 0.00
Epoch 44 Iter 4 subLoss 6315.9 multi 1.00 import weight 0.00
Epoch 44 Iter 5 subLoss 6703.1 multi 1.00 import weight 0.00
Epoch 44 Iter 6 subLoss 6147.9 multi -1.99 import weight 0.00
Epoch 44 Iter 7 subLoss 6454.0 multi 1.00 import weight 0.00
Epoch 44 Iter 8 subLoss 6771.8 multi 3.99 import weight 0.00
Epoch 44 Iter 9 subLoss 5905.9 multi -1.99 import weight 0.00
Epoch 44 Iter 10 subLoss 8130.6 multi 1.00 import weight 0.00
Epoch 44 Iter 11 subLoss 6535.6 multi 3.99 import weight 0.00
Epoch 44 Acc: 93.79 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 653 train Loss: 7332.3 test Loss: 958.6
Epoch 45 Iter 0 subLoss 6727.5 multi 1.00 import weight 0.00
Epoch 45 Iter 1 subLoss 6068.6 multi -1.99 import weight 0.00
Epoch 45 Iter 2 subLoss 7569.5 multi 1.00 import weight 0.00
Epoch 45 Iter 3 subLoss 6439.9 multi 1.00 import weight 0.00
Epoch 45 Iter 4 subLoss 5777.0 multi 1.00 import weight 0.00
Epoch 45 Iter 5 subLoss 6174.7 multi 3.99 import weight 0.00
Epoch 45 Iter 6 subLoss 5927.1 multi 1.00 import weight 0.00
Epoch 45 Iter 7 subLoss 5423.0 multi 1.00 import weight 0.00
Epoch 45 Iter 8 subLoss 5355.3 multi 1.00 import weight 0.00
Epoch 45 Iter 9 subLoss 5486.0 multi 1.00 import weight 0.00
Epoch 45 Iter 10 subLoss 6118.6 multi -1.99 import weight 0.00
Epoch 45 Iter 11 subLoss 6039.2 multi -1.99 import weight 0.00
Epoch 45 Acc: 94.92 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 603 train Loss: 7836.3 test Loss: 804.4
Epoch 46 Iter 0 subLoss 8134.7 multi 3.99 import weight 0.00
Epoch 46 Iter 1 subLoss 31491.9 multi 1.00 import weight 0.00
Epoch 46 Iter 2 subLoss 18370.8 multi 1.00 import weight 0.00
Epoch 46 Iter 3 subLoss 7964.7 multi 1.00 import weight 0.00
Epoch 46 Iter 4 subLoss 6990.8 multi 1.00 import weight 0.00
Epoch 46 Iter 5 subLoss 6171.6 multi 6.97 import weight 1.00
Epoch 46 Iter 6 subLoss 7680.1 multi 1.00 import weight 0.00
Epoch 46 Iter 7 subLoss 5273.2 multi 1.00 import weight 0.00
Epoch 46 Iter 8 subLoss 4481.1 multi 1.00 import weight 0.00
Epoch 46 Iter 9 subLoss 6107.4 multi 3.99 import weight 0.00
Epoch 46 Iter 10 subLoss 5191.1 multi 1.00 import weight 0.00
Epoch 46 Iter 11 subLoss 5250.7 multi 1.00 import weight 0.00
Epoch 46 Acc: 96.32 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 525 train Loss: 5293.9 test Loss: 578.0
Epoch 47 Iter 0 subLoss 5815.3 multi 1.00 import weight 0.00
Epoch 47 Iter 1 subLoss 4697.1 multi 1.00 import weight 0.00
Epoch 47 Iter 2 subLoss 5060.3 multi 1.00 import weight 0.00
Epoch 47 Iter 3 subLoss 5144.3 multi 1.00 import weight 0.00
Epoch 47 Iter 4 subLoss 5256.3 multi 3.99 import weight 0.00
Epoch 47 Iter 5 subLoss 5094.5 multi 1.00 import weight 0.00
Epoch 47 Iter 6 subLoss 5658.8 multi 1.00 import weight 0.00
Epoch 47 Iter 7 subLoss 5198.9 multi 3.99 import weight 0.00
Epoch 47 Iter 8 subLoss 4405.3 multi 1.00 import weight 0.00
Epoch 47 Iter 9 subLoss 4875.1 multi 1.00 import weight 0.00
Epoch 47 Iter 10 subLoss 4578.4 multi 1.00 import weight 0.00
Epoch 47 Iter 11 subLoss 4817.0 multi 1.00 import weight 0.00
Epoch 47 Acc: 96.67 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 481 train Loss: 4721.7 test Loss: 526.5
Epoch 48 Iter 0 subLoss 4661.3 multi 1.00 import weight 0.00
Epoch 48 Iter 1 subLoss 4861.4 multi 1.00 import weight 0.00
Epoch 48 Iter 2 subLoss 4485.9 multi 3.99 import weight 0.00
Epoch 48 Iter 3 subLoss 5598.5 multi 1.00 import weight 0.00
Epoch 48 Iter 4 subLoss 4659.8 multi 1.00 import weight 0.00
Epoch 48 Iter 5 subLoss 4544.6 multi 1.00 import weight 0.00
Epoch 48 Iter 6 subLoss 5111.2 multi 1.00 import weight 0.00
Epoch 48 Iter 7 subLoss 4157.8 multi 1.00 import weight 0.00
Epoch 48 Iter 8 subLoss 5454.3 multi 1.00 import weight 0.00
Epoch 48 Iter 9 subLoss 4758.4 multi 1.00 import weight 0.00
Epoch 48 Iter 10 subLoss 4479.6 multi 1.00 import weight 0.00
Epoch 48 Iter 11 subLoss 3946.2 multi 1.00 import weight 0.00
Epoch 48 Acc: 96.91 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 394 train Loss: 4732.3 test Loss: 484.6
Epoch 49 Iter 0 subLoss 4763.6 multi -1.99 import weight 0.00
Epoch 49 Iter 1 subLoss 4976.5 multi 1.00 import weight 0.00
Epoch 49 Iter 2 subLoss 4210.8 multi 1.00 import weight 0.00
Epoch 49 Iter 3 subLoss 4373.7 multi 1.00 import weight 0.00
Epoch 49 Iter 4 subLoss 4536.2 multi 1.00 import weight 0.00
Epoch 49 Iter 5 subLoss 4786.8 multi 1.00 import weight 0.00
Epoch 49 Iter 6 subLoss 4383.0 multi -1.99 import weight 0.00
Epoch 49 Iter 7 subLoss 4372.9 multi 3.99 import weight 0.00
Epoch 49 Iter 8 subLoss 4871.5 multi 1.00 import weight 0.00
Epoch 49 Iter 9 subLoss 4336.8 multi 1.00 import weight 0.00
Epoch 49 Iter 10 subLoss 4195.7 multi 1.00 import weight 0.00
Epoch 49 Iter 11 subLoss 4679.5 multi -1.99 import weight 0.00
Epoch 49 Acc: 96.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 467 train Loss: 4587.7 test Loss: 459.2
Epoch 50 Iter 0 subLoss 4853.4 multi 1.00 import weight 0.00
Epoch 50 Iter 1 subLoss 4900.5 multi 1.00 import weight 0.00
Epoch 50 Iter 2 subLoss 4710.0 multi -1.99 import weight 0.00
Epoch 50 Iter 3 subLoss 4691.0 multi 3.99 import weight 0.00
Epoch 50 Iter 4 subLoss 9178.8 multi 1.00 import weight 0.00
Epoch 50 Iter 5 subLoss 5842.4 multi 1.00 import weight 0.00
Epoch 50 Iter 6 subLoss 4610.6 multi 1.00 import weight 0.00
Epoch 50 Iter 7 subLoss 4083.1 multi 1.00 import weight 0.00
Epoch 50 Iter 8 subLoss 4654.2 multi 3.99 import weight 0.00
Epoch 50 Iter 9 subLoss 4763.4 multi 1.00 import weight 0.00
Epoch 50 Iter 10 subLoss 4102.9 multi 1.00 import weight 0.00
Epoch 50 Iter 11 subLoss 3961.1 multi 1.00 import weight 0.00
Epoch 50 Acc: 97.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 396 train Loss: 4391.9 test Loss: 435.5
Epoch 51 Iter 0 subLoss 4242.3 multi 1.00 import weight 0.00
Epoch 51 Iter 1 subLoss 3696.5 multi 1.00 import weight 0.00
Epoch 51 Iter 2 subLoss 4058.4 multi 1.00 import weight 0.00
Epoch 51 Iter 3 subLoss 4702.2 multi -1.98 import weight 0.00
Epoch 51 Iter 4 subLoss 3593.0 multi 1.00 import weight 0.00
Epoch 51 Iter 5 subLoss 4282.2 multi 1.00 import weight 0.00
Epoch 51 Iter 6 subLoss 4413.1 multi -1.99 import weight 0.00
Epoch 51 Iter 7 subLoss 5154.4 multi -1.99 import weight 0.00
Epoch 51 Iter 8 subLoss 6186.3 multi -7.96 import weight 0.00
Epoch 51 Iter 9 subLoss 135488.2 multi 1.00 import weight 0.00
Epoch 51 Iter 10 subLoss 25011.0 multi 1.00 import weight 0.00
Epoch 51 Iter 11 subLoss 15212.5 multi 1.00 import weight 0.00
Epoch 51 Acc: 91.71 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1521 train Loss: 13242.8 test Loss: 1791.6
Epoch 52 Iter 0 subLoss 12675.8 multi 1.00 import weight 0.00
Epoch 52 Iter 1 subLoss 11317.1 multi 1.00 import weight 0.00
Epoch 52 Iter 2 subLoss 9606.3 multi 1.00 import weight 0.00
Epoch 52 Iter 3 subLoss 8724.9 multi 1.00 import weight 0.00
Epoch 52 Iter 4 subLoss 7727.0 multi 1.00 import weight 0.00
Epoch 52 Iter 5 subLoss 6954.9 multi 1.00 import weight 0.00
Epoch 52 Iter 6 subLoss 6255.4 multi 3.99 import weight 0.00
Epoch 52 Iter 7 subLoss 5395.9 multi 1.00 import weight 0.00
Epoch 52 Iter 8 subLoss 5310.8 multi 1.00 import weight 0.00
Epoch 52 Iter 9 subLoss 4930.6 multi 1.00 import weight 0.00
Epoch 52 Iter 10 subLoss 5257.9 multi 6.97 import weight 1.00
Epoch 52 Iter 11 subLoss 5008.2 multi 1.00 import weight 0.00
Epoch 52 Acc: 96.77 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 500 train Loss: 4658.0 test Loss: 518.8
Epoch 53 Iter 0 subLoss 4571.2 multi 3.99 import weight 0.00
Epoch 53 Iter 1 subLoss 4830.9 multi 1.00 import weight 0.00
Epoch 53 Iter 2 subLoss 4247.0 multi 3.99 import weight 0.00
Epoch 53 Iter 3 subLoss 4333.6 multi 3.99 import weight 0.00
Epoch 53 Iter 4 subLoss 4218.9 multi 3.99 import weight 0.00
Epoch 53 Iter 5 subLoss 7288.9 multi 3.99 import weight 0.00
Epoch 53 Iter 6 subLoss 28162.9 multi 1.00 import weight 0.00
Epoch 53 Iter 7 subLoss 6493.1 multi 1.00 import weight 0.00
Epoch 53 Iter 8 subLoss 4707.0 multi 1.00 import weight 1.00
Epoch 53 Iter 9 subLoss 4850.6 multi 3.99 import weight 0.00
Epoch 53 Iter 10 subLoss 4901.5 multi 3.99 import weight 0.00
Epoch 53 Iter 11 subLoss 6139.2 multi 1.00 import weight 0.00
Epoch 53 Acc: 96.21 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 613 train Loss: 4664.5 test Loss: 578.0
Epoch 54 Iter 0 subLoss 4881.5 multi -4.97 import weight 0.00
Epoch 54 Iter 1 subLoss 8087.8 multi 1.00 import weight 0.00
Epoch 54 Iter 2 subLoss 5856.6 multi -4.97 import weight 0.00
Epoch 54 Iter 3 subLoss 14678.7 multi 1.00 import weight 0.00
Epoch 54 Iter 4 subLoss 9063.0 multi -1.99 import weight 0.00
Epoch 54 Iter 5 subLoss 18945.6 multi 3.99 import weight 0.00
Epoch 54 Iter 6 subLoss 12879.9 multi 1.00 import weight 0.00
Epoch 54 Iter 7 subLoss 9811.6 multi 3.99 import weight 0.00
Epoch 54 Iter 8 subLoss 5737.8 multi 1.00 import weight 0.00
Epoch 54 Iter 9 subLoss 5253.3 multi 9.96 import weight 1.00
Epoch 54 Iter 10 subLoss 5955.9 multi -1.99 import weight 0.00
Epoch 54 Iter 11 subLoss 17477.5 multi 1.00 import weight 0.00
Epoch 54 Acc: 97.28 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1747 train Loss: 4976.2 test Loss: 495.7
Epoch 55 Iter 0 subLoss 5430.3 multi -1.99 import weight 0.00
Epoch 55 Iter 1 subLoss 5717.4 multi 1.00 import weight 0.00
Epoch 55 Iter 2 subLoss 4241.9 multi 6.97 import weight 0.00
Epoch 55 Iter 3 subLoss 5112.6 multi 3.99 import weight 0.00
Epoch 55 Iter 4 subLoss 9353.2 multi 1.00 import weight 0.00
Epoch 55 Iter 5 subLoss 4617.6 multi 3.99 import weight 0.00
Epoch 55 Iter 6 subLoss 4388.8 multi -1.98 import weight 0.00
Epoch 55 Iter 7 subLoss 6670.9 multi 1.00 import weight 0.00
Epoch 55 Iter 8 subLoss 4402.3 multi 3.99 import weight 0.00
Epoch 55 Iter 9 subLoss 4221.2 multi -4.97 import weight 0.00
Epoch 55 Iter 10 subLoss 6711.0 multi -1.99 import weight 0.00
Epoch 55 Iter 11 subLoss 43069.5 multi 1.00 import weight 0.00
Epoch 55 Acc: 75.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4306 train Loss: 26761.5 test Loss: 3869.8
Epoch 56 Iter 0 subLoss 25578.9 multi 1.00 import weight 0.00
Epoch 56 Iter 1 subLoss 8478.4 multi 1.00 import weight 0.00
Epoch 56 Iter 2 subLoss 6808.7 multi -4.97 import weight 0.00
Epoch 56 Iter 3 subLoss 14377.1 multi 1.00 import weight 0.00
Epoch 56 Iter 4 subLoss 9736.4 multi 1.00 import weight 0.00
Epoch 56 Iter 5 subLoss 8763.6 multi 1.00 import weight 0.00
Epoch 56 Iter 6 subLoss 7269.1 multi 1.00 import weight 0.00
Epoch 56 Iter 7 subLoss 6320.2 multi -1.99 import weight 0.00
Epoch 56 Iter 8 subLoss 8021.3 multi 3.99 import weight 0.00
Epoch 56 Iter 9 subLoss 6116.4 multi -1.98 import weight 0.00
Epoch 56 Iter 10 subLoss 6491.6 multi 3.99 import weight 0.00
Epoch 56 Iter 11 subLoss 5288.3 multi -1.99 import weight 0.00
Epoch 56 Acc: 95.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 528 train Loss: 6256.9 test Loss: 853.6
Epoch 57 Iter 0 subLoss 6873.4 multi -1.99 import weight 0.00
Epoch 57 Iter 1 subLoss 9958.8 multi 1.00 import weight 0.00
Epoch 57 Iter 2 subLoss 6027.0 multi 3.99 import weight 0.00
Epoch 57 Iter 3 subLoss 6084.5 multi 3.99 import weight 0.00
Epoch 57 Iter 4 subLoss 5157.1 multi 1.00 import weight 0.00
Epoch 57 Iter 5 subLoss 4436.2 multi 1.00 import weight 0.00
Epoch 57 Iter 6 subLoss 4944.5 multi -1.99 import weight 0.00
Epoch 57 Iter 7 subLoss 4988.6 multi -1.99 import weight 0.00
Epoch 57 Iter 8 subLoss 4429.0 multi -1.99 import weight 0.00
Epoch 57 Iter 9 subLoss 5661.2 multi -1.99 import weight 0.00
Epoch 57 Iter 10 subLoss 11893.7 multi 1.00 import weight 0.00
Epoch 57 Iter 11 subLoss 5434.9 multi 1.00 import weight 0.00
Epoch 57 Acc: 96.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 543 train Loss: 5022.7 test Loss: 589.2
Epoch 58 Iter 0 subLoss 5348.7 multi 1.00 import weight 0.00
Epoch 58 Iter 1 subLoss 5110.8 multi 6.97 import weight 0.00
Epoch 58 Iter 2 subLoss 5165.0 multi -4.97 import weight 0.00
Epoch 58 Iter 3 subLoss 17224.1 multi 1.00 import weight 0.00
Epoch 58 Iter 4 subLoss 5543.8 multi 1.00 import weight 0.00
Epoch 58 Iter 5 subLoss 4638.0 multi 1.00 import weight 0.00
Epoch 58 Iter 6 subLoss 4221.9 multi -1.98 import weight 0.00
Epoch 58 Iter 7 subLoss 4791.3 multi -1.99 import weight 0.00
Epoch 58 Iter 8 subLoss 5188.4 multi 1.00 import weight 0.00
Epoch 58 Iter 9 subLoss 4795.1 multi 1.00 import weight 0.00
Epoch 58 Iter 10 subLoss 5047.5 multi 1.00 import weight 0.00
Epoch 58 Iter 11 subLoss 4504.9 multi 1.00 import weight 0.00
Epoch 58 Acc: 96.91 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 450 train Loss: 4647.2 test Loss: 542.4
Epoch 59 Iter 0 subLoss 4597.8 multi 1.00 import weight 0.00
Epoch 59 Iter 1 subLoss 4220.0 multi 1.00 import weight 0.00
Epoch 59 Iter 2 subLoss 4579.9 multi 6.97 import weight 0.00
Epoch 59 Iter 3 subLoss 5219.8 multi 1.00 import weight 0.00
Epoch 59 Iter 4 subLoss 4446.3 multi -1.99 import weight 0.00
Epoch 59 Iter 5 subLoss 4725.2 multi 1.00 import weight 0.00
Epoch 59 Iter 6 subLoss 4026.7 multi 1.00 import weight 0.00
Epoch 59 Iter 7 subLoss 4162.5 multi -1.99 import weight 0.00
Epoch 59 Iter 8 subLoss 4529.5 multi 1.00 import weight 0.00
Epoch 59 Iter 9 subLoss 4175.3 multi -1.99 import weight 0.00
Epoch 59 Iter 10 subLoss 5326.2 multi -1.99 import weight 0.00
Epoch 59 Iter 11 subLoss 4587.5 multi -7.96 import weight 0.00
Epoch 59 Acc: 83.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 458 train Loss: 14759.1 test Loss: 2689.6
Epoch 60 Iter 0 subLoss 15371.9 multi 1.00 import weight 0.00
Epoch 60 Iter 1 subLoss 4823.5 multi -1.99 import weight 0.00
Epoch 60 Iter 2 subLoss 7682.7 multi 3.99 import weight 0.00
Epoch 60 Iter 3 subLoss 7537.6 multi -7.96 import weight 0.00
Epoch 60 Iter 4 subLoss 29862.9 multi -1.99 import weight 0.00
Epoch 60 Iter 5 subLoss 136399.7 multi 1.00 import weight 0.00
Epoch 60 Iter 6 subLoss 35153.7 multi 1.00 import weight 0.00
Epoch 60 Iter 7 subLoss 21629.6 multi 1.00 import weight 0.00
Epoch 60 Iter 8 subLoss 19875.5 multi 3.99 import weight 0.00
Epoch 60 Iter 9 subLoss 13729.1 multi 1.00 import weight 0.00
Epoch 60 Iter 10 subLoss 12025.0 multi 1.00 import weight 0.00
Epoch 60 Iter 11 subLoss 11644.0 multi 1.00 import weight 0.00
Epoch 60 Acc: 91.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1164 train Loss: 10733.7 test Loss: 1338.7
Epoch 61 Iter 0 subLoss 10400.9 multi 1.00 import weight 0.00
Epoch 61 Iter 1 subLoss 10098.0 multi 1.00 import weight 0.00
Epoch 61 Iter 2 subLoss 9160.1 multi -1.99 import weight 0.00
Epoch 61 Iter 3 subLoss 10680.8 multi 1.00 import weight 0.00
Epoch 61 Iter 4 subLoss 10098.5 multi 3.99 import weight 0.00
Epoch 61 Iter 5 subLoss 8124.9 multi 1.00 import weight 0.00
Epoch 61 Iter 6 subLoss 6945.4 multi -1.99 import weight 0.00
Epoch 61 Iter 7 subLoss 7703.8 multi 1.00 import weight 0.00
Epoch 61 Iter 8 subLoss 7746.2 multi 3.99 import weight 0.00
Epoch 61 Iter 9 subLoss 6766.7 multi -1.99 import weight 0.00
Epoch 61 Iter 10 subLoss 7147.4 multi 1.00 import weight 0.00
Epoch 61 Iter 11 subLoss 6785.7 multi -4.97 import weight 0.00
Epoch 61 Acc: 91.92 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 678 train Loss: 9369.6 test Loss: 1202.8
Epoch 62 Iter 0 subLoss 9325.0 multi 1.00 import weight 0.00
Epoch 62 Iter 1 subLoss 7764.4 multi 3.99 import weight 0.00
Epoch 62 Iter 2 subLoss 7321.0 multi 1.00 import weight 0.00
Epoch 62 Iter 3 subLoss 6748.5 multi -1.99 import weight 0.00
Epoch 62 Iter 4 subLoss 6599.1 multi 1.00 import weight 0.00
Epoch 62 Iter 5 subLoss 7039.6 multi 3.99 import weight 0.00
Epoch 62 Iter 6 subLoss 5709.3 multi 1.00 import weight 0.00
Epoch 62 Iter 7 subLoss 6153.8 multi 1.00 import weight 0.00
Epoch 62 Iter 8 subLoss 6051.8 multi 3.99 import weight 0.00
Epoch 62 Iter 9 subLoss 5168.9 multi -1.98 import weight 0.00
Epoch 62 Iter 10 subLoss 5689.5 multi 1.00 import weight 0.00
Epoch 62 Iter 11 subLoss 5675.7 multi -1.99 import weight 0.00
Epoch 62 Acc: 95.41 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 567 train Loss: 5893.3 test Loss: 765.4
Epoch 63 Iter 0 subLoss 6022.6 multi 6.97 import weight 0.00
Epoch 63 Iter 1 subLoss 6890.3 multi -1.99 import weight 0.00
Epoch 63 Iter 2 subLoss 19064.0 multi 1.00 import weight 0.00
Epoch 63 Iter 3 subLoss 6232.5 multi 1.00 import weight 0.00
Epoch 63 Iter 4 subLoss 5388.5 multi 1.00 import weight 0.00
Epoch 63 Iter 5 subLoss 5001.9 multi 3.99 import weight 0.00
Epoch 63 Iter 6 subLoss 4809.1 multi -4.97 import weight 0.00
Epoch 63 Iter 7 subLoss 4813.9 multi 1.00 import weight 0.00
Epoch 63 Iter 8 subLoss 5331.0 multi -1.99 import weight 0.00
Epoch 63 Iter 9 subLoss 4638.0 multi 3.99 import weight 0.00
Epoch 63 Iter 10 subLoss 4768.0 multi 3.98 import weight 0.00
Epoch 63 Iter 11 subLoss 4700.1 multi 3.99 import weight 1.00
Epoch 63 Acc: 96.77 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 3.99 Pidx 470 train Loss: 4599.6 test Loss: 535.7
Epoch 64 Iter 0 subLoss 4260.4 multi 1.00 import weight 0.00
Epoch 64 Iter 1 subLoss 5200.5 multi -4.97 import weight 0.00
Epoch 64 Iter 2 subLoss 5621.3 multi 1.00 import weight 0.00
Epoch 64 Iter 3 subLoss 4840.0 multi 1.00 import weight 0.00
Epoch 64 Iter 4 subLoss 3665.1 multi 1.00 import weight 0.00
Epoch 64 Iter 5 subLoss 4606.6 multi -1.99 import weight 0.00
Epoch 64 Iter 6 subLoss 4077.9 multi 1.00 import weight 0.00
Epoch 64 Iter 7 subLoss 4720.5 multi 3.99 import weight 0.00
Epoch 64 Iter 8 subLoss 4702.7 multi 6.97 import weight 1.00
Epoch 64 Iter 9 subLoss 5764.7 multi 1.00 import weight 0.00
Epoch 64 Iter 10 subLoss 4591.7 multi 1.00 import weight 0.00
Epoch 64 Iter 11 subLoss 4629.9 multi -4.97 import weight 0.00
Epoch 64 Acc: 93.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 462 train Loss: 6163.3 test Loss: 937.9
Epoch 65 Iter 0 subLoss 5944.1 multi 3.99 import weight 0.00
Epoch 65 Iter 1 subLoss 8553.8 multi -1.99 import weight 0.00
Epoch 65 Iter 2 subLoss 40650.4 multi 1.00 import weight 0.00
Epoch 65 Iter 3 subLoss 11095.4 multi 1.00 import weight 0.00
Epoch 65 Iter 4 subLoss 4890.6 multi -1.99 import weight 0.00
Epoch 65 Iter 5 subLoss 6245.9 multi -1.99 import weight 0.00
Epoch 65 Iter 6 subLoss 10396.2 multi 1.00 import weight 0.00
Epoch 65 Iter 7 subLoss 6167.8 multi -4.97 import weight 0.00
Epoch 65 Iter 8 subLoss 11873.1 multi 1.00 import weight 0.00
Epoch 65 Iter 9 subLoss 6154.8 multi 3.98 import weight 0.00
Epoch 65 Iter 10 subLoss 5205.7 multi -1.98 import weight 0.00
Epoch 65 Iter 11 subLoss 6579.2 multi 3.99 import weight 0.00
Epoch 65 Acc: 96.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 657 train Loss: 5250.8 test Loss: 550.1
Epoch 66 Iter 0 subLoss 4975.1 multi 3.99 import weight 0.00
Epoch 66 Iter 1 subLoss 4378.4 multi 6.97 import weight 0.00
Epoch 66 Iter 2 subLoss 4667.6 multi -1.98 import weight 0.00
Epoch 66 Iter 3 subLoss 6267.7 multi -1.98 import weight 0.00
Epoch 66 Iter 4 subLoss 20355.8 multi 1.00 import weight 0.00
Epoch 66 Iter 5 subLoss 5160.2 multi 1.00 import weight 0.00
Epoch 66 Iter 6 subLoss 4481.5 multi 3.98 import weight 0.00
Epoch 66 Iter 7 subLoss 4333.0 multi 6.97 import weight 0.00
Epoch 66 Iter 8 subLoss 5276.5 multi 3.99 import weight 0.00
Epoch 66 Iter 9 subLoss 5029.2 multi 1.00 import weight 0.00
Epoch 66 Iter 10 subLoss 4160.4 multi 1.00 import weight 0.00
Epoch 66 Iter 11 subLoss 3831.8 multi 1.00 import weight 0.00
Epoch 66 Acc: 97.45 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 383 train Loss: 4026.5 test Loss: 441.9
Epoch 67 Iter 0 subLoss 3110.5 multi 1.00 import weight 0.00
Epoch 67 Iter 1 subLoss 4421.4 multi 1.00 import weight 0.00
Epoch 67 Iter 2 subLoss 3755.8 multi 1.00 import weight 0.00
Epoch 67 Iter 3 subLoss 3833.5 multi 3.99 import weight 0.00
Epoch 67 Iter 4 subLoss 4094.5 multi -1.99 import weight 0.00
Epoch 67 Iter 5 subLoss 4662.8 multi 1.00 import weight 0.00
Epoch 67 Iter 6 subLoss 4512.9 multi -1.99 import weight 0.00
Epoch 67 Iter 7 subLoss 3816.2 multi 1.00 import weight 0.00
Epoch 67 Iter 8 subLoss 4051.6 multi 3.99 import weight 0.00
Epoch 67 Iter 9 subLoss 3953.0 multi -1.99 import weight 0.00
Epoch 67 Iter 10 subLoss 4383.9 multi -1.99 import weight 0.00
Epoch 67 Iter 11 subLoss 6118.6 multi 1.00 import weight 0.00
Epoch 67 Acc: 96.32 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 611 train Loss: 4523.9 test Loss: 528.2
Epoch 68 Iter 0 subLoss 4711.2 multi -13.93 import weight 0.00
Epoch 68 Iter 1 subLoss 27083.2 multi 1.00 import weight 0.00
Epoch 68 Iter 2 subLoss 12078.6 multi 1.00 import weight 0.00
Epoch 68 Iter 3 subLoss 8383.4 multi 1.00 import weight 0.00
Epoch 68 Iter 4 subLoss 7712.3 multi -1.99 import weight 0.00
Epoch 68 Iter 5 subLoss 9773.0 multi -1.99 import weight 0.00
Epoch 68 Iter 6 subLoss 15743.7 multi -1.99 import weight 0.00
Epoch 68 Iter 7 subLoss 31180.4 multi 1.00 import weight 0.00
Epoch 68 Iter 8 subLoss 19172.2 multi 1.00 import weight 0.00
Epoch 68 Iter 9 subLoss 14160.6 multi 1.00 import weight 0.00
Epoch 68 Iter 10 subLoss 10488.9 multi 1.00 import weight 0.00
Epoch 68 Iter 11 subLoss 8193.0 multi 1.00 import weight 0.00
Epoch 68 Acc: 91.40 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 819 train Loss: 7432.6 test Loss: 1125.7
Epoch 69 Iter 0 subLoss 7414.1 multi -1.99 import weight 0.00
Epoch 69 Iter 1 subLoss 8574.5 multi -1.99 import weight 0.00
Epoch 69 Iter 2 subLoss 13210.8 multi 1.00 import weight 0.00
Epoch 69 Iter 3 subLoss 11401.8 multi 1.00 import weight 0.00
Epoch 69 Iter 4 subLoss 9278.4 multi 1.00 import weight 0.00
Epoch 69 Iter 5 subLoss 6967.1 multi -1.99 import weight 0.00
Epoch 69 Iter 6 subLoss 9485.5 multi 1.00 import weight 0.00
Epoch 69 Iter 7 subLoss 7671.6 multi -1.99 import weight 0.00
Epoch 69 Iter 8 subLoss 10055.4 multi -1.99 import weight 0.00
Epoch 69 Iter 9 subLoss 13385.3 multi 1.00 import weight 0.00
Epoch 69 Iter 10 subLoss 12038.5 multi -1.99 import weight 0.00
Epoch 69 Iter 11 subLoss 16725.8 multi 1.00 import weight 0.00
Epoch 69 Acc: 83.65 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1672 train Loss: 13339.1 test Loss: 2372.6
Epoch 70 Iter 0 subLoss 12214.8 multi 1.00 import weight 0.00
Epoch 70 Iter 1 subLoss 11837.1 multi 1.00 import weight 0.00
Epoch 70 Iter 2 subLoss 8870.9 multi -1.99 import weight 0.00
Epoch 70 Iter 3 subLoss 12335.8 multi 1.00 import weight 0.00
Epoch 70 Iter 4 subLoss 10944.4 multi 1.00 import weight 0.00
Epoch 70 Iter 5 subLoss 9414.0 multi 3.99 import weight 0.00
Epoch 70 Iter 6 subLoss 5279.6 multi 6.97 import weight 0.00
Epoch 70 Iter 7 subLoss 4325.5 multi 1.00 import weight 0.00
Epoch 70 Iter 8 subLoss 4253.3 multi -7.96 import weight 0.00
Epoch 70 Iter 9 subLoss 4981.0 multi -1.98 import weight 0.00
Epoch 70 Iter 10 subLoss 6432.4 multi 3.98 import weight 0.00
Epoch 70 Iter 11 subLoss 4498.2 multi -7.96 import weight 0.00
Epoch 70 Acc: 89.78 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 449 train Loss: 8691.5 test Loss: 1484.7
Epoch 71 Iter 0 subLoss 8438.0 multi 1.00 import weight 0.00
Epoch 71 Iter 1 subLoss 6355.1 multi 1.00 import weight 0.00
Epoch 71 Iter 2 subLoss 5301.9 multi 1.00 import weight 0.00
Epoch 71 Iter 3 subLoss 4863.9 multi -1.98 import weight 0.00
Epoch 71 Iter 4 subLoss 5570.1 multi -1.99 import weight 0.00
Epoch 71 Iter 5 subLoss 7582.9 multi 1.00 import weight 0.00
Epoch 71 Iter 6 subLoss 5796.4 multi 1.00 import weight 0.00
Epoch 71 Iter 7 subLoss 5941.2 multi 6.97 import weight 0.00
Epoch 71 Iter 8 subLoss 5409.2 multi -1.99 import weight 0.00
Epoch 71 Iter 9 subLoss 7484.1 multi 1.00 import weight 0.00
Epoch 71 Iter 10 subLoss 5810.4 multi 3.99 import weight 0.00
Epoch 71 Iter 11 subLoss 4329.8 multi 3.99 import weight 0.00
Epoch 71 Acc: 97.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 432 train Loss: 4416.3 test Loss: 446.9
Epoch 72 Iter 0 subLoss 4366.8 multi 1.00 import weight 0.00
Epoch 72 Iter 1 subLoss 3921.7 multi 1.00 import weight 0.00
Epoch 72 Iter 2 subLoss 3978.1 multi -1.99 import weight 0.00
Epoch 72 Iter 3 subLoss 4401.4 multi 6.97 import weight 0.00
Epoch 72 Iter 4 subLoss 3839.7 multi 6.97 import weight 0.00
Epoch 72 Iter 5 subLoss 4343.2 multi -7.96 import weight 0.00
Epoch 72 Iter 6 subLoss 40032.4 multi 1.00 import weight 0.00
Epoch 72 Iter 7 subLoss 5378.8 multi 1.00 import weight 0.00
Epoch 72 Iter 8 subLoss 5202.3 multi 1.00 import weight 0.00
Epoch 72 Iter 9 subLoss 4725.4 multi 3.98 import weight 0.00
Epoch 72 Iter 10 subLoss 4940.2 multi 1.00 import weight 0.00
Epoch 72 Iter 11 subLoss 4469.5 multi 1.00 import weight 0.00
Epoch 72 Acc: 97.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 446 train Loss: 4314.1 test Loss: 483.3
Epoch 73 Iter 0 subLoss 4201.2 multi -1.99 import weight 0.00
Epoch 73 Iter 1 subLoss 4966.3 multi 1.00 import weight 0.00
Epoch 73 Iter 2 subLoss 4438.1 multi -1.98 import weight 0.00
Epoch 73 Iter 3 subLoss 4154.5 multi 3.99 import weight 0.00
Epoch 73 Iter 4 subLoss 4564.7 multi 1.00 import weight 0.00
Epoch 73 Iter 5 subLoss 4055.2 multi 6.97 import weight 0.00
Epoch 73 Iter 6 subLoss 3971.7 multi 1.00 import weight 0.00
Epoch 73 Iter 7 subLoss 3718.6 multi 1.00 import weight 0.00
Epoch 73 Iter 8 subLoss 3608.2 multi -1.99 import weight 0.00
Epoch 73 Iter 9 subLoss 4092.2 multi 1.00 import weight 0.00
Epoch 73 Iter 10 subLoss 3613.4 multi -1.99 import weight 0.00
Epoch 73 Iter 11 subLoss 3514.1 multi 1.00 import weight 0.00
Epoch 73 Acc: 97.26 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 351 train Loss: 3968.0 test Loss: 442.0
Epoch 74 Iter 0 subLoss 4223.2 multi 3.99 import weight 0.00
Epoch 74 Iter 1 subLoss 3628.8 multi -1.99 import weight 0.00
Epoch 74 Iter 2 subLoss 3968.9 multi 1.00 import weight 0.00
Epoch 74 Iter 3 subLoss 4341.2 multi -4.97 import weight 0.00
Epoch 74 Iter 4 subLoss 3842.9 multi -7.96 import weight 0.00
Epoch 74 Iter 5 subLoss 6417.6 multi 3.99 import weight 0.00
Epoch 74 Iter 6 subLoss 5654.4 multi 3.99 import weight 0.00
Epoch 74 Iter 7 subLoss 4142.9 multi 1.00 import weight 0.00
Epoch 74 Iter 8 subLoss 3986.0 multi -4.97 import weight 0.00
Epoch 74 Iter 9 subLoss 4613.0 multi 3.98 import weight 0.00
Epoch 74 Iter 10 subLoss 4730.7 multi -7.96 import weight 0.00
Epoch 74 Iter 11 subLoss 32597.4 multi 1.00 import weight 0.00
Epoch 74 Acc: 95.68 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3259 train Loss: 6592.8 test Loss: 719.8
Epoch 75 Iter 0 subLoss 6279.9 multi -4.97 import weight 0.00
Epoch 75 Iter 1 subLoss 18779.6 multi 1.00 import weight 0.00
Epoch 75 Iter 2 subLoss 8994.4 multi 1.00 import weight 0.00
Epoch 75 Iter 3 subLoss 6777.9 multi 3.98 import weight 0.00
Epoch 75 Iter 4 subLoss 4942.0 multi 3.98 import weight 0.00
Epoch 75 Iter 5 subLoss 5155.1 multi 3.98 import weight 0.00
Epoch 75 Iter 6 subLoss 4686.2 multi -1.99 import weight 0.00
Epoch 75 Iter 7 subLoss 3769.9 multi -1.99 import weight 0.00
Epoch 75 Iter 8 subLoss 4279.7 multi -1.99 import weight 0.00
Epoch 75 Iter 9 subLoss 4582.7 multi -4.97 import weight 0.00
Epoch 75 Iter 10 subLoss 5985.9 multi 1.00 import weight 0.00
Epoch 75 Iter 11 subLoss 5461.4 multi -1.99 import weight 0.00
Epoch 75 Acc: 96.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 546 train Loss: 6315.2 test Loss: 654.1
Epoch 76 Iter 0 subLoss 6634.8 multi 1.00 import weight 0.00
Epoch 76 Iter 1 subLoss 5676.8 multi 1.00 import weight 0.00
Epoch 76 Iter 2 subLoss 5472.5 multi -1.99 import weight 0.00
Epoch 76 Iter 3 subLoss 5831.0 multi 3.99 import weight 0.00
Epoch 76 Iter 4 subLoss 4760.1 multi 6.97 import weight 0.00
Epoch 76 Iter 5 subLoss 4710.8 multi -10.94 import weight 0.00
Epoch 76 Iter 6 subLoss 6224.2 multi 1.00 import weight 0.00
Epoch 76 Iter 7 subLoss 5575.5 multi 1.00 import weight 0.00
Epoch 76 Iter 8 subLoss 4662.1 multi 3.99 import weight 0.00
Epoch 76 Iter 9 subLoss 4401.7 multi 9.96 import weight 0.00
Epoch 76 Iter 10 subLoss 4989.6 multi 1.00 import weight 0.00
Epoch 76 Iter 11 subLoss 4430.9 multi 1.00 import weight 0.00
Epoch 76 Acc: 97.43 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 443 train Loss: 4165.4 test Loss: 438.5
Epoch 77 Iter 0 subLoss 3777.8 multi -1.99 import weight 0.00
Epoch 77 Iter 1 subLoss 4777.9 multi -10.94 import weight 0.00
Epoch 77 Iter 2 subLoss 13870.9 multi 3.99 import weight 0.00
Epoch 77 Iter 3 subLoss 13725.5 multi 3.99 import weight 0.00
Epoch 77 Iter 4 subLoss 18707.3 multi 1.00 import weight 0.00
Epoch 77 Iter 5 subLoss 10181.9 multi 1.00 import weight 0.00
Epoch 77 Iter 6 subLoss 8861.3 multi 3.99 import weight 0.00
Epoch 77 Iter 7 subLoss 5548.7 multi 3.99 import weight 0.00
Epoch 77 Iter 8 subLoss 5050.9 multi -1.99 import weight 0.00
Epoch 77 Iter 9 subLoss 5646.7 multi 1.00 import weight 0.00
Epoch 77 Iter 10 subLoss 4943.1 multi 6.97 import weight 0.00
Epoch 77 Iter 11 subLoss 4168.7 multi 1.00 import weight 0.00
Epoch 77 Acc: 97.28 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 416 train Loss: 4485.6 test Loss: 468.4
Epoch 78 Iter 0 subLoss 4461.2 multi 3.99 import weight 0.00
Epoch 78 Iter 1 subLoss 4106.4 multi -1.98 import weight 0.00
Epoch 78 Iter 2 subLoss 4558.6 multi -1.99 import weight 0.00
Epoch 78 Iter 3 subLoss 4268.6 multi 1.00 import weight 0.00
Epoch 78 Iter 4 subLoss 4599.5 multi 1.00 import weight 0.00
Epoch 78 Iter 5 subLoss 4935.2 multi 3.99 import weight 0.00
Epoch 78 Iter 6 subLoss 4175.7 multi -4.97 import weight 0.00
Epoch 78 Iter 7 subLoss 4194.3 multi 3.99 import weight 0.00
Epoch 78 Iter 8 subLoss 3841.0 multi -4.97 import weight 0.00
Epoch 78 Iter 9 subLoss 4608.5 multi -4.97 import weight 0.00
Epoch 78 Iter 10 subLoss 13288.2 multi 1.00 import weight 0.00
Epoch 78 Iter 11 subLoss 4364.7 multi 3.99 import weight 0.00
Epoch 78 Acc: 97.26 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 436 train Loss: 4469.7 test Loss: 486.9
Epoch 79 Iter 0 subLoss 4380.2 multi 1.00 import weight 0.00
Epoch 79 Iter 1 subLoss 4490.9 multi -4.97 import weight 0.00
Epoch 79 Iter 2 subLoss 4989.4 multi 3.99 import weight 0.00
Epoch 79 Iter 3 subLoss 4728.7 multi 3.99 import weight 0.00
Epoch 79 Iter 4 subLoss 4545.5 multi 1.00 import weight 0.00
Epoch 79 Iter 5 subLoss 4149.1 multi 3.99 import weight 0.00
Epoch 79 Iter 6 subLoss 4080.7 multi 1.00 import weight 0.00
Epoch 79 Iter 7 subLoss 4219.6 multi 3.98 import weight 0.00
Epoch 79 Iter 8 subLoss 3806.8 multi 1.00 import weight 0.00
Epoch 79 Iter 9 subLoss 3525.0 multi -1.99 import weight 0.00
Epoch 79 Iter 10 subLoss 3813.8 multi 1.00 import weight 0.00
Epoch 79 Iter 11 subLoss 3881.5 multi 1.00 import weight 0.00
Epoch 79 Acc: 97.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 388 train Loss: 4072.8 test Loss: 422.7
Epoch 80 Iter 0 subLoss 3676.6 multi -1.99 import weight 0.00
Epoch 80 Iter 1 subLoss 4249.6 multi 9.96 import weight 0.00
Epoch 80 Iter 2 subLoss 4341.0 multi -1.99 import weight 0.00
Epoch 80 Iter 3 subLoss 3854.4 multi -4.97 import weight 0.00
Epoch 80 Iter 4 subLoss 6528.5 multi 1.00 import weight 0.00
Epoch 80 Iter 5 subLoss 4791.0 multi 3.98 import weight 0.00
Epoch 80 Iter 6 subLoss 4141.8 multi 6.97 import weight 0.00
Epoch 80 Iter 7 subLoss 3974.2 multi 1.00 import weight 0.00
Epoch 80 Iter 8 subLoss 3605.4 multi 1.00 import weight 0.00
Epoch 80 Iter 9 subLoss 4000.4 multi 1.00 import weight 0.00
Epoch 80 Iter 10 subLoss 3617.6 multi -1.98 import weight 0.00
Epoch 80 Iter 11 subLoss 3877.0 multi 1.00 import weight 0.00
Epoch 80 Acc: 97.39 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 387 train Loss: 3781.7 test Loss: 418.1
Epoch 81 Iter 0 subLoss 3625.2 multi -1.98 import weight 0.00
Epoch 81 Iter 1 subLoss 3794.4 multi 1.00 import weight 0.00
Epoch 81 Iter 2 subLoss 4127.9 multi 1.00 import weight 0.00
Epoch 81 Iter 3 subLoss 3616.8 multi 1.00 import weight 0.00
Epoch 81 Iter 4 subLoss 3497.4 multi 1.00 import weight 0.00
Epoch 81 Iter 5 subLoss 3786.7 multi -1.99 import weight 0.00
Epoch 81 Iter 6 subLoss 3954.0 multi 1.00 import weight 0.00
Epoch 81 Iter 7 subLoss 4477.5 multi -1.98 import weight 0.00
Epoch 81 Iter 8 subLoss 4057.2 multi 9.96 import weight 0.00
Epoch 81 Iter 9 subLoss 3343.8 multi 1.00 import weight 0.00
Epoch 81 Iter 10 subLoss 3810.1 multi 3.98 import weight 0.00
Epoch 81 Iter 11 subLoss 3454.6 multi 1.00 import weight 0.00
Epoch 81 Acc: 97.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 345 train Loss: 3676.9 test Loss: 391.2
Epoch 82 Iter 0 subLoss 3541.5 multi 1.00 import weight 0.00
Epoch 82 Iter 1 subLoss 3548.3 multi 3.99 import weight 0.00
Epoch 82 Iter 2 subLoss 3951.8 multi 3.98 import weight 0.00
Epoch 82 Iter 3 subLoss 3610.3 multi 3.99 import weight 0.00
Epoch 82 Iter 4 subLoss 3266.9 multi 1.00 import weight 0.00
Epoch 82 Iter 5 subLoss 3567.0 multi 1.00 import weight 0.00
Epoch 82 Iter 6 subLoss 3350.6 multi -1.99 import weight 0.00
Epoch 82 Iter 7 subLoss 3540.4 multi 6.97 import weight 0.00
Epoch 82 Iter 8 subLoss 3261.3 multi 3.99 import weight 0.00
Epoch 82 Iter 9 subLoss 3570.3 multi -1.99 import weight 0.00
Epoch 82 Iter 10 subLoss 3822.3 multi -7.96 import weight 0.00
Epoch 82 Iter 11 subLoss 9913.1 multi 1.00 import weight 0.00
Epoch 82 Acc: 93.97 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 991 train Loss: 4922.2 test Loss: 773.3
Epoch 83 Iter 0 subLoss 4727.6 multi 6.97 import weight 1.00
Epoch 83 Iter 1 subLoss 6989.6 multi 1.00 import weight 0.00
Epoch 83 Iter 2 subLoss 4827.2 multi -1.98 import weight 0.00
Epoch 83 Iter 3 subLoss 6835.9 multi 1.00 import weight 0.00
Epoch 83 Iter 4 subLoss 4054.1 multi 12.94 import weight 1.00
Epoch 83 Iter 5 subLoss 14397.4 multi 1.00 import weight 0.00
Epoch 83 Iter 6 subLoss 5165.6 multi 1.00 import weight 0.00
Epoch 83 Iter 7 subLoss 3605.6 multi 3.98 import weight 0.00
Epoch 83 Iter 8 subLoss 3240.0 multi 1.00 import weight 0.00
Epoch 83 Iter 9 subLoss 3732.5 multi 1.00 import weight 0.00
Epoch 83 Iter 10 subLoss 3819.7 multi 6.97 import weight 0.00
Epoch 83 Iter 11 subLoss 3548.9 multi 9.96 import weight 0.00
Epoch 83 Acc: 97.76 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 354 train Loss: 3392.8 test Loss: 357.2
Epoch 84 Iter 0 subLoss 3254.3 multi -1.99 import weight 0.00
Epoch 84 Iter 1 subLoss 3590.1 multi 3.99 import weight 0.00
Epoch 84 Iter 2 subLoss 2930.9 multi 1.00 import weight 0.00
Epoch 84 Iter 3 subLoss 2859.4 multi 1.00 import weight 0.00
Epoch 84 Iter 4 subLoss 3443.6 multi 1.00 import weight 0.00
Epoch 84 Iter 5 subLoss 3254.4 multi 1.00 import weight 0.00
Epoch 84 Iter 6 subLoss 3427.2 multi 1.00 import weight 0.00
Epoch 84 Iter 7 subLoss 3277.4 multi -4.97 import weight 0.00
Epoch 84 Iter 8 subLoss 3256.2 multi 3.98 import weight 0.00
Epoch 84 Iter 9 subLoss 3108.4 multi 1.00 import weight 0.00
Epoch 84 Iter 10 subLoss 3452.4 multi 1.00 import weight 0.00
Epoch 84 Iter 11 subLoss 3188.1 multi 1.00 import weight 0.00
Epoch 84 Acc: 98.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 318 train Loss: 3295.5 test Loss: 334.7
Epoch 85 Iter 0 subLoss 3121.8 multi -1.99 import weight 0.00
Epoch 85 Iter 1 subLoss 3053.7 multi 1.00 import weight 0.00
Epoch 85 Iter 2 subLoss 3384.9 multi 1.00 import weight 0.00
Epoch 85 Iter 3 subLoss 3094.8 multi 1.00 import weight 0.00
Epoch 85 Iter 4 subLoss 3089.8 multi 1.00 import weight 0.00
Epoch 85 Iter 5 subLoss 2986.6 multi 1.00 import weight 0.00
Epoch 85 Iter 6 subLoss 3441.0 multi 3.99 import weight 0.00
Epoch 85 Iter 7 subLoss 3173.1 multi 1.00 import weight 0.00
Epoch 85 Iter 8 subLoss 2643.5 multi 1.00 import weight 0.00
Epoch 85 Iter 9 subLoss 3492.5 multi 3.99 import weight 0.00
Epoch 85 Iter 10 subLoss 2945.2 multi -1.99 import weight 0.00
Epoch 85 Iter 11 subLoss 3189.9 multi 1.00 import weight 0.00
Epoch 85 Acc: 97.90 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 318 train Loss: 3207.3 test Loss: 333.0
Epoch 86 Iter 0 subLoss 3106.2 multi 1.00 import weight 0.00
Epoch 86 Iter 1 subLoss 3462.0 multi -4.97 import weight 0.00
Epoch 86 Iter 2 subLoss 4169.1 multi 3.99 import weight 0.00
Epoch 86 Iter 3 subLoss 3590.5 multi 6.97 import weight 0.00
Epoch 86 Iter 4 subLoss 4976.5 multi 3.98 import weight 0.00
Epoch 86 Iter 5 subLoss 3888.0 multi 1.00 import weight 0.00
Epoch 86 Iter 6 subLoss 3616.8 multi 3.99 import weight 1.00
Epoch 86 Iter 7 subLoss 3553.4 multi -10.94 import weight 0.00
Epoch 86 Iter 8 subLoss 6385.6 multi -4.97 import weight 0.00
Epoch 86 Iter 9 subLoss 48457.6 multi 1.00 import weight 0.00
Epoch 86 Iter 10 subLoss 10381.1 multi 1.00 import weight 0.00
Epoch 86 Iter 11 subLoss 7154.0 multi 1.00 import weight 0.00
Epoch 86 Acc: 96.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 715 train Loss: 5809.9 test Loss: 593.4
Epoch 87 Iter 0 subLoss 5314.1 multi 1.00 import weight 0.00
Epoch 87 Iter 1 subLoss 4899.8 multi 1.00 import weight 0.00
Epoch 87 Iter 2 subLoss 4522.7 multi 1.00 import weight 0.00
Epoch 87 Iter 3 subLoss 4140.3 multi 9.96 import weight 0.00
Epoch 87 Iter 4 subLoss 4195.2 multi 6.97 import weight 0.00
Epoch 87 Iter 5 subLoss 5493.0 multi -1.99 import weight 0.00
Epoch 87 Iter 6 subLoss 8887.3 multi -1.99 import weight 0.00
Epoch 87 Iter 7 subLoss 19392.6 multi 1.00 import weight 0.00
Epoch 87 Iter 8 subLoss 6896.4 multi 1.00 import weight 0.00
Epoch 87 Iter 9 subLoss 5239.3 multi 1.00 import weight 0.00
Epoch 87 Iter 10 subLoss 4763.1 multi 9.96 import weight 1.00
Epoch 87 Iter 11 subLoss 9492.6 multi -1.99 import weight 0.00
Epoch 87 Acc: 40.28 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 949 train Loss: 93210.8 test Loss: 18903.9
Epoch 88 Iter 0 subLoss 88324.1 multi 1.00 import weight 0.00
Epoch 88 Iter 1 subLoss 9883.1 multi 1.00 import weight 0.00
Epoch 88 Iter 2 subLoss 6288.9 multi 1.00 import weight 0.00
Epoch 88 Iter 3 subLoss 5791.2 multi 3.99 import weight 0.00
Epoch 88 Iter 4 subLoss 4727.2 multi 9.96 import weight 1.00
Epoch 88 Iter 5 subLoss 3876.5 multi 3.99 import weight 0.00
Epoch 88 Iter 6 subLoss 3646.0 multi 1.00 import weight 0.00
Epoch 88 Iter 7 subLoss 3635.9 multi -4.97 import weight 0.00
Epoch 88 Iter 8 subLoss 3634.4 multi -1.98 import weight 0.00
Epoch 88 Iter 9 subLoss 3581.1 multi -1.99 import weight 0.00
Epoch 88 Iter 10 subLoss 4271.1 multi -1.98 import weight 0.00
Epoch 88 Iter 11 subLoss 4808.8 multi -4.97 import weight 0.00
Epoch 88 Acc: 80.93 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 480 train Loss: 18402.0 test Loss: 2931.6
Epoch 89 Iter 0 subLoss 18687.9 multi 1.00 import weight 0.00
Epoch 89 Iter 1 subLoss 4016.9 multi -1.99 import weight 0.00
Epoch 89 Iter 2 subLoss 4637.8 multi 3.98 import weight 0.00
Epoch 89 Iter 3 subLoss 4048.5 multi 1.00 import weight 0.00
Epoch 89 Iter 4 subLoss 4559.7 multi -1.98 import weight 0.00
Epoch 89 Iter 5 subLoss 4017.1 multi 1.00 import weight 0.00
Epoch 89 Iter 6 subLoss 4059.5 multi 12.94 import weight 1.00
Epoch 89 Iter 7 subLoss 3440.0 multi 6.97 import weight 0.00
Epoch 89 Iter 8 subLoss 3652.7 multi -1.99 import weight 0.00
Epoch 89 Iter 9 subLoss 4583.9 multi -1.99 import weight 0.00
Epoch 89 Iter 10 subLoss 8091.0 multi 1.00 import weight 0.00
Epoch 89 Iter 11 subLoss 3197.6 multi -4.97 import weight 0.00
Epoch 89 Acc: 92.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 319 train Loss: 7812.2 test Loss: 1100.4
Epoch 90 Iter 0 subLoss 6634.4 multi 3.99 import weight 0.00
Epoch 90 Iter 1 subLoss 5935.2 multi -1.99 import weight 0.00
Epoch 90 Iter 2 subLoss 13514.1 multi 1.00 import weight 0.00
Epoch 90 Iter 3 subLoss 4443.7 multi -4.97 import weight 0.00
Epoch 90 Iter 4 subLoss 13051.8 multi 1.00 import weight 0.00
Epoch 90 Iter 5 subLoss 5997.8 multi -1.99 import weight 0.00
Epoch 90 Iter 6 subLoss 10590.9 multi 1.00 import weight 0.00
Epoch 90 Iter 7 subLoss 6135.0 multi 3.98 import weight 0.00
Epoch 90 Iter 8 subLoss 4000.2 multi 3.99 import weight 0.00
Epoch 90 Iter 9 subLoss 3620.0 multi -7.96 import weight 0.00
Epoch 90 Iter 10 subLoss 4164.6 multi 6.97 import weight 0.00
Epoch 90 Iter 11 subLoss 3566.0 multi 1.00 import weight 0.00
Epoch 90 Acc: 97.41 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 356 train Loss: 3937.0 test Loss: 420.8
Epoch 91 Iter 0 subLoss 3475.3 multi -1.99 import weight 0.00
Epoch 91 Iter 1 subLoss 4107.8 multi 1.00 import weight 0.00
Epoch 91 Iter 2 subLoss 4434.7 multi 3.99 import weight 0.00
Epoch 91 Iter 3 subLoss 3034.9 multi 1.00 import weight 0.00
Epoch 91 Iter 4 subLoss 4277.2 multi 1.00 import weight 0.00
Epoch 91 Iter 5 subLoss 3380.0 multi 1.00 import weight 0.00
Epoch 91 Iter 6 subLoss 3150.4 multi 1.00 import weight 0.00
Epoch 91 Iter 7 subLoss 3079.7 multi 1.00 import weight 0.00
Epoch 91 Iter 8 subLoss 4236.8 multi -10.94 import weight 0.00
Epoch 91 Iter 9 subLoss 4399.8 multi -10.94 import weight 0.00
Epoch 91 Iter 10 subLoss 9092.1 multi 1.00 import weight 0.00
Epoch 91 Iter 11 subLoss 4850.3 multi 6.97 import weight 0.00
Epoch 91 Acc: 96.67 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 485 train Loss: 4684.3 test Loss: 513.5
Epoch 92 Iter 0 subLoss 4843.3 multi -4.97 import weight 0.00
Epoch 92 Iter 1 subLoss 9908.6 multi 1.00 import weight 0.00
Epoch 92 Iter 2 subLoss 6320.4 multi 1.00 import weight 0.00
Epoch 92 Iter 3 subLoss 4465.0 multi 6.97 import weight 0.00
Epoch 92 Iter 4 subLoss 4261.9 multi 3.98 import weight 0.00
Epoch 92 Iter 5 subLoss 3339.2 multi 1.00 import weight 0.00
Epoch 92 Iter 6 subLoss 3714.2 multi 3.99 import weight 0.00
Epoch 92 Iter 7 subLoss 3300.1 multi 1.00 import weight 0.00
Epoch 92 Iter 8 subLoss 3666.4 multi 1.00 import weight 0.00
Epoch 92 Iter 9 subLoss 3371.9 multi 3.99 import weight 0.00
Epoch 92 Iter 10 subLoss 3545.7 multi 12.94 import weight 0.00
Epoch 92 Iter 11 subLoss 4127.4 multi 3.99 import weight 0.00
Epoch 92 Acc: 97.66 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 412 train Loss: 3472.9 test Loss: 359.6
Epoch 93 Iter 0 subLoss 3441.3 multi 9.96 import weight 0.00
Epoch 93 Iter 1 subLoss 3795.7 multi 1.00 import weight 0.00
Epoch 93 Iter 2 subLoss 3065.6 multi -1.99 import weight 0.00
Epoch 93 Iter 3 subLoss 4280.5 multi -4.97 import weight 0.00
Epoch 93 Iter 4 subLoss 9367.0 multi -1.99 import weight 0.00
Epoch 93 Iter 5 subLoss 44982.7 multi 1.00 import weight 0.00
Epoch 93 Iter 6 subLoss 6494.5 multi 6.97 import weight 0.00
Epoch 93 Iter 7 subLoss 10835.5 multi -4.97 import weight 0.00
Epoch 93 Iter 8 subLoss 145866.0 multi 1.00 import weight 0.00
Epoch 93 Iter 9 subLoss 38297.8 multi 1.00 import weight 0.00
Epoch 93 Iter 10 subLoss 20229.7 multi 1.00 import weight 0.00
Epoch 93 Iter 11 subLoss 17141.3 multi 1.00 import weight 0.00
Epoch 93 Acc: 76.36 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1714 train Loss: 14542.6 test Loss: 2526.5
Epoch 94 Iter 0 subLoss 14385.0 multi -1.99 import weight 0.00
Epoch 94 Iter 1 subLoss 18473.6 multi 1.00 import weight 0.00
Epoch 94 Iter 2 subLoss 16285.4 multi 1.00 import weight 0.00
Epoch 94 Iter 3 subLoss 13718.7 multi 1.00 import weight 0.00
Epoch 94 Iter 4 subLoss 12175.7 multi 1.00 import weight 0.00
Epoch 94 Iter 5 subLoss 10482.7 multi 3.99 import weight 0.00
Epoch 94 Iter 6 subLoss 8021.3 multi 6.97 import weight 0.00
Epoch 94 Iter 7 subLoss 4798.1 multi 6.97 import weight 0.00
Epoch 94 Iter 8 subLoss 4546.0 multi 3.98 import weight 0.00
Epoch 94 Iter 9 subLoss 3447.6 multi 12.94 import weight 0.00
Epoch 94 Iter 10 subLoss 3916.8 multi 1.00 import weight 0.00
Epoch 94 Iter 11 subLoss 3849.9 multi -1.99 import weight 0.00
Epoch 94 Acc: 96.44 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 384 train Loss: 4276.7 test Loss: 557.2
Epoch 95 Iter 0 subLoss 4471.3 multi -1.99 import weight 0.00
Epoch 95 Iter 1 subLoss 7376.3 multi 3.99 import weight 0.00
Epoch 95 Iter 2 subLoss 12925.5 multi 3.99 import weight 0.00
Epoch 95 Iter 3 subLoss 5845.0 multi 1.00 import weight 0.00
Epoch 95 Iter 4 subLoss 4206.3 multi -4.97 import weight 0.00
Epoch 95 Iter 5 subLoss 10585.2 multi 1.00 import weight 0.00
Epoch 95 Iter 6 subLoss 4932.0 multi 6.97 import weight 0.00
Epoch 95 Iter 7 subLoss 3937.1 multi -1.99 import weight 0.00
Epoch 95 Iter 8 subLoss 4638.7 multi 6.97 import weight 0.00
Epoch 95 Iter 9 subLoss 4354.6 multi -7.96 import weight 0.00
Epoch 95 Iter 10 subLoss 38285.5 multi 1.00 import weight 0.00
Epoch 95 Iter 11 subLoss 7113.8 multi 3.99 import weight 0.00
Epoch 95 Acc: 97.16 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 711 train Loss: 4037.6 test Loss: 504.1
Epoch 96 Iter 0 subLoss 3938.2 multi 1.00 import weight 0.00
Epoch 96 Iter 1 subLoss 3933.5 multi 3.98 import weight 0.00
Epoch 96 Iter 2 subLoss 3248.3 multi 3.99 import weight 0.00
Epoch 96 Iter 3 subLoss 2885.3 multi 1.00 import weight 0.00
Epoch 96 Iter 4 subLoss 3252.3 multi 3.99 import weight 0.00
Epoch 96 Iter 5 subLoss 3181.5 multi 3.98 import weight 0.00
Epoch 96 Iter 6 subLoss 3358.3 multi 1.00 import weight 0.00
Epoch 96 Iter 7 subLoss 3593.2 multi 6.97 import weight 0.00
Epoch 96 Iter 8 subLoss 3372.4 multi 6.97 import weight 0.00
Epoch 96 Iter 9 subLoss 2925.9 multi 1.00 import weight 0.00
Epoch 96 Iter 10 subLoss 3109.5 multi 3.98 import weight 0.00
Epoch 96 Iter 11 subLoss 3120.2 multi 1.00 import weight 0.00
Epoch 96 Acc: 98.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 312 train Loss: 3022.8 test Loss: 316.3
Epoch 97 Iter 0 subLoss 2633.8 multi 1.00 import weight 0.00
Epoch 97 Iter 1 subLoss 2784.0 multi 1.00 import weight 0.00
Epoch 97 Iter 2 subLoss 3120.4 multi 3.98 import weight 0.00
Epoch 97 Iter 3 subLoss 3365.9 multi -4.97 import weight 0.00
Epoch 97 Iter 4 subLoss 3843.9 multi 1.00 import weight 0.00
Epoch 97 Iter 5 subLoss 4085.5 multi 3.98 import weight 0.00
Epoch 97 Iter 6 subLoss 3507.6 multi -4.97 import weight 0.00
Epoch 97 Iter 7 subLoss 3000.7 multi 1.00 import weight 0.00
Epoch 97 Iter 8 subLoss 2767.0 multi 1.00 import weight 0.00
Epoch 97 Iter 9 subLoss 3115.7 multi -4.97 import weight 0.00
Epoch 97 Iter 10 subLoss 2602.0 multi 1.00 import weight 0.00
Epoch 97 Iter 11 subLoss 2898.1 multi -1.99 import weight 0.00
Epoch 97 Acc: 97.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 289 train Loss: 3328.7 test Loss: 398.9
Epoch 98 Iter 0 subLoss 3677.4 multi -1.98 import weight 0.00
Epoch 98 Iter 1 subLoss 4035.3 multi -1.99 import weight 0.00
Epoch 98 Iter 2 subLoss 5357.2 multi 1.00 import weight 0.00
Epoch 98 Iter 3 subLoss 4058.9 multi 15.93 import weight 1.00
Epoch 98 Iter 4 subLoss 18850.6 multi 1.00 import weight 0.00
Epoch 98 Iter 5 subLoss 5370.1 multi 3.99 import weight 0.00
Epoch 98 Iter 6 subLoss 3079.4 multi 1.00 import weight 0.00
Epoch 98 Iter 7 subLoss 3050.0 multi -1.99 import weight 0.00
Epoch 98 Iter 8 subLoss 3355.0 multi 3.98 import weight 0.00
Epoch 98 Iter 9 subLoss 3059.1 multi 1.00 import weight 0.00
Epoch 98 Iter 10 subLoss 2861.4 multi -1.99 import weight 0.00
Epoch 98 Iter 11 subLoss 2893.9 multi 1.00 import weight 0.00
Epoch 98 Acc: 98.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 289 train Loss: 3068.6 test Loss: 337.1
Epoch 99 Iter 0 subLoss 2925.9 multi 3.99 import weight 0.00
Epoch 99 Iter 1 subLoss 3413.9 multi 1.00 import weight 0.00
Epoch 99 Iter 2 subLoss 3005.9 multi 3.99 import weight 0.00
Epoch 99 Iter 3 subLoss 2947.0 multi 1.00 import weight 0.00
Epoch 99 Iter 4 subLoss 2958.6 multi -4.97 import weight 0.00
Epoch 99 Iter 5 subLoss 3335.9 multi 3.99 import weight 0.00
Epoch 99 Iter 6 subLoss 3108.2 multi 6.97 import weight 0.00
Epoch 99 Iter 7 subLoss 2841.6 multi 1.00 import weight 0.00
Epoch 99 Iter 8 subLoss 2799.7 multi -1.99 import weight 0.00
Epoch 99 Iter 9 subLoss 3317.2 multi -1.99 import weight 0.00
Epoch 99 Iter 10 subLoss 2725.7 multi 1.00 import weight 0.00
Epoch 99 Iter 11 subLoss 3060.4 multi -1.98 import weight 0.00
Epoch 99 Acc: 98.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 306 train Loss: 3145.7 test Loss: 330.0
Epoch 100 Iter 0 subLoss 3653.9 multi 1.00 import weight 0.00
Epoch 100 Iter 1 subLoss 2694.5 multi 1.00 import weight 0.00
Epoch 100 Iter 2 subLoss 3207.6 multi -1.99 import weight 0.00
Epoch 100 Iter 3 subLoss 3434.2 multi -1.99 import weight 0.00
Epoch 100 Iter 4 subLoss 2767.9 multi 3.99 import weight 0.00
Epoch 100 Iter 5 subLoss 3086.7 multi -1.98 import weight 0.00
Epoch 100 Iter 6 subLoss 2956.5 multi -1.98 import weight 0.00
Epoch 100 Iter 7 subLoss 3717.5 multi 6.97 import weight 0.00
Epoch 100 Iter 8 subLoss 2882.0 multi 3.99 import weight 0.00
Epoch 100 Iter 9 subLoss 3203.7 multi 1.00 import weight 0.00
Epoch 100 Iter 10 subLoss 3278.2 multi -1.98 import weight 0.00
Epoch 100 Iter 11 subLoss 2587.1 multi 1.00 import weight 0.00
Epoch 100 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 258 train Loss: 2930.2 test Loss: 320.0
Epoch 101 Iter 0 subLoss 2818.5 multi 1.00 import weight 0.00
Epoch 101 Iter 1 subLoss 2951.8 multi 1.00 import weight 0.00
Epoch 101 Iter 2 subLoss 2676.4 multi 1.00 import weight 0.00
Epoch 101 Iter 3 subLoss 2858.4 multi 1.00 import weight 0.00
Epoch 101 Iter 4 subLoss 2661.8 multi 1.00 import weight 0.00
Epoch 101 Iter 5 subLoss 3318.8 multi 1.00 import weight 0.00
Epoch 101 Iter 6 subLoss 3007.6 multi 6.97 import weight 0.00
Epoch 101 Iter 7 subLoss 2478.2 multi 1.00 import weight 0.00
Epoch 101 Iter 8 subLoss 3193.5 multi -4.97 import weight 0.00
Epoch 101 Iter 9 subLoss 3128.6 multi 3.99 import weight 0.00
Epoch 101 Iter 10 subLoss 2819.3 multi 3.99 import weight 0.00
Epoch 101 Iter 11 subLoss 2666.2 multi 3.99 import weight 0.00
Epoch 101 Acc: 98.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 266 train Loss: 2799.3 test Loss: 288.3
Epoch 102 Iter 0 subLoss 3006.1 multi 9.96 import weight 0.00
Epoch 102 Iter 1 subLoss 2797.0 multi 1.00 import weight 0.00
Epoch 102 Iter 2 subLoss 2937.2 multi -1.98 import weight 0.00
Epoch 102 Iter 3 subLoss 2658.4 multi -1.99 import weight 0.00
Epoch 102 Iter 4 subLoss 2899.4 multi 1.00 import weight 0.00
Epoch 102 Iter 5 subLoss 3281.2 multi -4.97 import weight 0.00
Epoch 102 Iter 6 subLoss 3830.6 multi 6.97 import weight 0.00
Epoch 102 Iter 7 subLoss 3840.8 multi 1.00 import weight 0.00
Epoch 102 Iter 8 subLoss 3393.7 multi -1.99 import weight 0.00
Epoch 102 Iter 9 subLoss 4029.2 multi -1.98 import weight 0.00
Epoch 102 Iter 10 subLoss 7393.2 multi 1.00 import weight 0.00
Epoch 102 Iter 11 subLoss 4021.4 multi 1.00 import weight 0.00
Epoch 102 Acc: 97.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 402 train Loss: 3335.2 test Loss: 354.4
Epoch 103 Iter 0 subLoss 3783.9 multi 1.00 import weight 0.00
Epoch 103 Iter 1 subLoss 2786.5 multi 3.99 import weight 0.00
Epoch 103 Iter 2 subLoss 2744.3 multi 1.00 import weight 0.00
Epoch 103 Iter 3 subLoss 2977.8 multi 1.00 import weight 0.00
Epoch 103 Iter 4 subLoss 2721.5 multi 3.99 import weight 0.00
Epoch 103 Iter 5 subLoss 2517.0 multi 1.00 import weight 0.00
Epoch 103 Iter 6 subLoss 2396.7 multi 1.00 import weight 0.00
Epoch 103 Iter 7 subLoss 2853.4 multi 3.98 import weight 0.00
Epoch 103 Iter 8 subLoss 2724.9 multi 6.97 import weight 0.00
Epoch 103 Iter 9 subLoss 2946.0 multi 1.00 import weight 0.00
Epoch 103 Iter 10 subLoss 2568.7 multi 1.00 import weight 0.00
Epoch 103 Iter 11 subLoss 2702.6 multi -1.99 import weight 0.00
Epoch 103 Acc: 98.21 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 270 train Loss: 2819.2 test Loss: 292.3
Epoch 104 Iter 0 subLoss 2771.5 multi -4.97 import weight 0.00
Epoch 104 Iter 1 subLoss 2973.0 multi 3.99 import weight 0.00
Epoch 104 Iter 2 subLoss 2490.7 multi 1.00 import weight 0.00
Epoch 104 Iter 3 subLoss 3080.2 multi 1.00 import weight 0.00
Epoch 104 Iter 4 subLoss 2627.5 multi 1.00 import weight 0.00
Epoch 104 Iter 5 subLoss 2669.8 multi 3.98 import weight 0.00
Epoch 104 Iter 6 subLoss 2712.5 multi -1.99 import weight 0.00
Epoch 104 Iter 7 subLoss 2684.5 multi -1.99 import weight 0.00
Epoch 104 Iter 8 subLoss 2799.6 multi 1.00 import weight 0.00
Epoch 104 Iter 9 subLoss 2566.7 multi 3.99 import weight 0.00
Epoch 104 Iter 10 subLoss 2479.6 multi 3.99 import weight 0.00
Epoch 104 Iter 11 subLoss 2484.6 multi -4.97 import weight 0.00
Epoch 104 Acc: 97.74 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 248 train Loss: 3128.4 test Loss: 358.5
Epoch 105 Iter 0 subLoss 2716.8 multi 1.00 import weight 0.00
Epoch 105 Iter 1 subLoss 2499.3 multi 1.00 import weight 0.00
Epoch 105 Iter 2 subLoss 3114.4 multi -4.97 import weight 0.00
Epoch 105 Iter 3 subLoss 3550.7 multi -10.94 import weight 0.00
Epoch 105 Iter 4 subLoss 32074.7 multi 1.00 import weight 0.00
Epoch 105 Iter 5 subLoss 4476.3 multi 1.00 import weight 0.00
Epoch 105 Iter 6 subLoss 3973.6 multi 3.99 import weight 0.00
Epoch 105 Iter 7 subLoss 2969.3 multi -7.96 import weight 0.00
Epoch 105 Iter 8 subLoss 3971.7 multi 6.97 import weight 0.00
Epoch 105 Iter 9 subLoss 3733.8 multi 3.99 import weight 0.00
Epoch 105 Iter 10 subLoss 2742.4 multi 3.99 import weight 0.00
Epoch 105 Iter 11 subLoss 2748.7 multi 6.97 import weight 0.00
Epoch 105 Acc: 98.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 274 train Loss: 2859.0 test Loss: 331.7
Epoch 106 Iter 0 subLoss 2916.1 multi 1.00 import weight 0.00
Epoch 106 Iter 1 subLoss 3336.9 multi 6.97 import weight 0.00
Epoch 106 Iter 2 subLoss 2818.2 multi 6.97 import weight 0.00
Epoch 106 Iter 3 subLoss 2439.1 multi 1.00 import weight 0.00
Epoch 106 Iter 4 subLoss 2673.3 multi -4.97 import weight 0.00
Epoch 106 Iter 5 subLoss 3092.6 multi -4.97 import weight 0.00
Epoch 106 Iter 6 subLoss 5239.5 multi 3.99 import weight 0.00
Epoch 106 Iter 7 subLoss 3666.0 multi 1.00 import weight 0.00
Epoch 106 Iter 8 subLoss 2842.1 multi 3.99 import weight 0.00
Epoch 106 Iter 9 subLoss 2599.9 multi -1.99 import weight 0.00
Epoch 106 Iter 10 subLoss 2714.3 multi 3.98 import weight 0.00
Epoch 106 Iter 11 subLoss 2463.2 multi 1.00 import weight 0.00
Epoch 106 Acc: 98.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 246 train Loss: 2767.8 test Loss: 290.6
Epoch 107 Iter 0 subLoss 3092.5 multi -1.99 import weight 0.00
Epoch 107 Iter 1 subLoss 2439.8 multi 3.99 import weight 0.00
Epoch 107 Iter 2 subLoss 2783.8 multi 3.98 import weight 0.00
Epoch 107 Iter 3 subLoss 2063.6 multi 1.00 import weight 0.00
Epoch 107 Iter 4 subLoss 2696.4 multi 1.00 import weight 0.00
Epoch 107 Iter 5 subLoss 2544.5 multi 1.00 import weight 0.00
Epoch 107 Iter 6 subLoss 2988.1 multi -1.98 import weight 0.00
Epoch 107 Iter 7 subLoss 2707.3 multi -1.98 import weight 0.00
Epoch 107 Iter 8 subLoss 2943.0 multi 3.99 import weight 0.00
Epoch 107 Iter 9 subLoss 2592.9 multi 1.00 import weight 0.00
Epoch 107 Iter 10 subLoss 2399.6 multi 3.99 import weight 0.00
Epoch 107 Iter 11 subLoss 3216.7 multi -4.97 import weight 0.00
Epoch 107 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 321 train Loss: 3016.0 test Loss: 317.9
Epoch 108 Iter 0 subLoss 2736.0 multi -7.96 import weight 0.00
Epoch 108 Iter 1 subLoss 6259.1 multi 3.98 import weight 0.00
Epoch 108 Iter 2 subLoss 5397.0 multi 1.00 import weight 0.00
Epoch 108 Iter 3 subLoss 4336.7 multi 3.99 import weight 0.00
Epoch 108 Iter 4 subLoss 3004.8 multi 12.94 import weight 0.00
Epoch 108 Iter 5 subLoss 2734.0 multi -4.97 import weight 0.00
Epoch 108 Iter 6 subLoss 3469.3 multi -1.98 import weight 0.00
Epoch 108 Iter 7 subLoss 7069.4 multi 1.00 import weight 0.00
Epoch 108 Iter 8 subLoss 3037.5 multi 3.99 import weight 0.00
Epoch 108 Iter 9 subLoss 2233.2 multi 1.00 import weight 0.00
Epoch 108 Iter 10 subLoss 2860.7 multi -4.97 import weight 0.00
Epoch 108 Iter 11 subLoss 2912.8 multi 3.99 import weight 0.00
Epoch 108 Acc: 98.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 291 train Loss: 2859.6 test Loss: 308.7
Epoch 109 Iter 0 subLoss 2556.7 multi -1.99 import weight 0.00
Epoch 109 Iter 1 subLoss 2890.3 multi 3.99 import weight 0.00
Epoch 109 Iter 2 subLoss 3017.0 multi -13.93 import weight 0.00
Epoch 109 Iter 3 subLoss 3503.1 multi -1.98 import weight 0.00
Epoch 109 Iter 4 subLoss 4751.8 multi 3.99 import weight 0.00
Epoch 109 Iter 5 subLoss 3305.7 multi 3.99 import weight 0.00
Epoch 109 Iter 6 subLoss 2448.5 multi -4.97 import weight 0.00
Epoch 109 Iter 7 subLoss 2460.3 multi 3.99 import weight 0.00
Epoch 109 Iter 8 subLoss 2829.1 multi -7.96 import weight 0.00
Epoch 109 Iter 9 subLoss 3045.6 multi -1.98 import weight 0.00
Epoch 109 Iter 10 subLoss 3272.5 multi 1.00 import weight 0.00
Epoch 109 Iter 11 subLoss 3377.4 multi 6.97 import weight 0.00
Epoch 109 Acc: 97.49 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 337 train Loss: 3136.4 test Loss: 400.0
Epoch 110 Iter 0 subLoss 3249.5 multi 6.97 import weight 0.00
Epoch 110 Iter 1 subLoss 2985.5 multi 1.00 import weight 0.00
Epoch 110 Iter 2 subLoss 3561.9 multi 1.00 import weight 0.00
Epoch 110 Iter 3 subLoss 2582.9 multi 3.99 import weight 0.00
Epoch 110 Iter 4 subLoss 2616.6 multi -1.99 import weight 0.00
Epoch 110 Iter 5 subLoss 2519.4 multi 3.99 import weight 0.00
Epoch 110 Iter 6 subLoss 3318.7 multi 1.00 import weight 0.00
Epoch 110 Iter 7 subLoss 2529.7 multi -4.97 import weight 0.00
Epoch 110 Iter 8 subLoss 2479.2 multi 1.00 import weight 0.00
Epoch 110 Iter 9 subLoss 2807.0 multi -7.96 import weight 0.00
Epoch 110 Iter 10 subLoss 2640.8 multi 1.00 import weight 0.00
Epoch 110 Iter 11 subLoss 3161.5 multi -1.99 import weight 0.00
Epoch 110 Acc: 97.28 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 316 train Loss: 3353.0 test Loss: 403.4
Epoch 111 Iter 0 subLoss 3309.0 multi 6.97 import weight 0.00
Epoch 111 Iter 1 subLoss 3086.4 multi 3.99 import weight 0.00
Epoch 111 Iter 2 subLoss 2804.1 multi -4.97 import weight 0.00
Epoch 111 Iter 3 subLoss 2704.7 multi 1.00 import weight 0.00
Epoch 111 Iter 4 subLoss 2496.8 multi 3.98 import weight 0.00
Epoch 111 Iter 5 subLoss 2394.0 multi 6.97 import weight 0.00
Epoch 111 Iter 6 subLoss 3758.8 multi 3.99 import weight 0.00
Epoch 111 Iter 7 subLoss 2681.5 multi -1.98 import weight 0.00
Epoch 111 Iter 8 subLoss 3231.9 multi 1.00 import weight 0.00
Epoch 111 Iter 9 subLoss 2695.3 multi 1.00 import weight 0.00
Epoch 111 Iter 10 subLoss 2589.6 multi 6.97 import weight 0.00
Epoch 111 Iter 11 subLoss 2672.0 multi -1.99 import weight 0.00
Epoch 111 Acc: 98.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 267 train Loss: 2884.6 test Loss: 303.9
Epoch 112 Iter 0 subLoss 2902.1 multi -10.94 import weight 0.00
Epoch 112 Iter 1 subLoss 7347.6 multi 1.00 import weight 0.00
Epoch 112 Iter 2 subLoss 4066.9 multi -19.90 import weight 0.00
Epoch 112 Iter 3 subLoss 126219.4 multi 1.00 import weight 0.00
Epoch 112 Iter 4 subLoss 24306.7 multi 1.00 import weight 0.00
Epoch 112 Iter 5 subLoss 12954.1 multi 1.00 import weight 0.00
Epoch 112 Iter 6 subLoss 11694.2 multi -1.99 import weight 0.00
Epoch 112 Iter 7 subLoss 12673.5 multi 3.99 import weight 0.00
Epoch 112 Iter 8 subLoss 9702.5 multi 1.00 import weight 0.00
Epoch 112 Iter 9 subLoss 8742.5 multi -1.99 import weight 0.00
Epoch 112 Iter 10 subLoss 10370.8 multi 1.00 import weight 0.00
Epoch 112 Iter 11 subLoss 10161.7 multi 1.00 import weight 0.00
Epoch 112 Acc: 93.60 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1016 train Loss: 9135.6 test Loss: 1134.3
Epoch 113 Iter 0 subLoss 8747.8 multi 1.00 import weight 0.00
Epoch 113 Iter 1 subLoss 8674.8 multi 1.00 import weight 0.00
Epoch 113 Iter 2 subLoss 7517.2 multi 6.97 import weight 0.00
Epoch 113 Iter 3 subLoss 5260.6 multi -10.94 import weight 0.00
Epoch 113 Iter 4 subLoss 8461.4 multi 1.00 import weight 0.00
Epoch 113 Iter 5 subLoss 7302.8 multi 1.00 import weight 0.00
Epoch 113 Iter 6 subLoss 6709.9 multi 3.99 import weight 0.00
Epoch 113 Iter 7 subLoss 4731.0 multi -13.93 import weight 0.00
Epoch 113 Iter 8 subLoss 9834.7 multi 1.00 import weight 0.00
Epoch 113 Iter 9 subLoss 8479.8 multi 1.00 import weight 0.00
Epoch 113 Iter 10 subLoss 7818.5 multi 1.00 import weight 0.00
Epoch 113 Iter 11 subLoss 7397.8 multi 3.99 import weight 0.00
Epoch 113 Acc: 96.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 739 train Loss: 5991.7 test Loss: 649.6
Epoch 114 Iter 0 subLoss 5437.3 multi 3.98 import weight 0.00
Epoch 114 Iter 1 subLoss 5122.6 multi -7.96 import weight 0.00
Epoch 114 Iter 2 subLoss 6831.2 multi 3.99 import weight 0.00
Epoch 114 Iter 3 subLoss 5525.0 multi 1.00 import weight 0.00
Epoch 114 Iter 4 subLoss 5950.2 multi -4.97 import weight 0.00
Epoch 114 Iter 5 subLoss 7095.5 multi 1.00 import weight 0.00
Epoch 114 Iter 6 subLoss 5695.9 multi -1.99 import weight 0.00
Epoch 114 Iter 7 subLoss 7380.9 multi -4.97 import weight 0.00
Epoch 114 Iter 8 subLoss 11710.0 multi -1.99 import weight 0.00
Epoch 114 Iter 9 subLoss 24563.9 multi 1.00 import weight 0.00
Epoch 114 Iter 10 subLoss 12144.8 multi 3.99 import weight 0.00
Epoch 114 Iter 11 subLoss 7500.8 multi 1.00 import weight 0.00
Epoch 114 Acc: 96.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 750 train Loss: 6953.2 test Loss: 677.3
Epoch 115 Iter 0 subLoss 6664.9 multi -1.99 import weight 0.00
Epoch 115 Iter 1 subLoss 7665.3 multi 3.99 import weight 0.00
Epoch 115 Iter 2 subLoss 6761.5 multi 1.00 import weight 0.00
Epoch 115 Iter 3 subLoss 5580.6 multi -4.97 import weight 0.00
Epoch 115 Iter 4 subLoss 6959.4 multi 1.00 import weight 0.00
Epoch 115 Iter 5 subLoss 6639.4 multi 6.97 import weight 0.00
Epoch 115 Iter 6 subLoss 5451.6 multi 3.99 import weight 0.00
Epoch 115 Iter 7 subLoss 4647.5 multi -10.94 import weight 0.00
Epoch 115 Iter 8 subLoss 14203.6 multi 1.00 import weight 0.00
Epoch 115 Iter 9 subLoss 6571.7 multi 6.97 import weight 0.00
Epoch 115 Iter 10 subLoss 6369.4 multi -1.99 import weight 0.00
Epoch 115 Iter 11 subLoss 9159.7 multi 1.00 import weight 0.00
Epoch 115 Acc: 92.53 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 915 train Loss: 6444.5 test Loss: 1030.8
Epoch 116 Iter 0 subLoss 6705.5 multi 6.97 import weight 0.00
Epoch 116 Iter 1 subLoss 6359.1 multi 3.99 import weight 0.00
Epoch 116 Iter 2 subLoss 5055.2 multi 1.00 import weight 0.00
Epoch 116 Iter 3 subLoss 4462.5 multi 9.96 import weight 0.00
Epoch 116 Iter 4 subLoss 4923.1 multi 1.00 import weight 0.00
Epoch 116 Iter 5 subLoss 3411.1 multi 3.99 import weight 0.00
Epoch 116 Iter 6 subLoss 3209.1 multi 1.00 import weight 0.00
Epoch 116 Iter 7 subLoss 3440.9 multi 12.94 import weight 0.00
Epoch 116 Iter 8 subLoss 3699.9 multi 3.99 import weight 0.00
Epoch 116 Iter 9 subLoss 3949.3 multi -4.97 import weight 0.00
Epoch 116 Iter 10 subLoss 5631.7 multi -1.99 import weight 0.00
Epoch 116 Iter 11 subLoss 14930.8 multi 3.99 import weight 0.00
Epoch 116 Acc: 66.18 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1493 train Loss: 26173.4 test Loss: 5105.9
Epoch 117 Iter 0 subLoss 25432.1 multi 1.00 import weight 0.00
Epoch 117 Iter 1 subLoss 5311.7 multi 3.98 import weight 0.00
Epoch 117 Iter 2 subLoss 3715.7 multi 9.96 import weight 0.00
Epoch 117 Iter 3 subLoss 3218.6 multi -4.97 import weight 0.00
Epoch 117 Iter 4 subLoss 3498.0 multi 6.97 import weight 0.00
Epoch 117 Iter 5 subLoss 3195.4 multi -1.99 import weight 0.00
Epoch 117 Iter 6 subLoss 3212.0 multi -1.99 import weight 0.00
Epoch 117 Iter 7 subLoss 3377.6 multi 9.96 import weight 0.00
Epoch 117 Iter 8 subLoss 3785.6 multi 3.98 import weight 0.00
Epoch 117 Iter 9 subLoss 2262.8 multi 1.00 import weight 0.00
Epoch 117 Iter 10 subLoss 3565.2 multi 3.99 import weight 0.00
Epoch 117 Iter 11 subLoss 2971.7 multi 3.98 import weight 0.00
Epoch 117 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 297 train Loss: 2995.0 test Loss: 312.7
Epoch 118 Iter 0 subLoss 3087.9 multi 6.97 import weight 0.00
Epoch 118 Iter 1 subLoss 3103.0 multi 3.99 import weight 0.00
Epoch 118 Iter 2 subLoss 2667.6 multi 6.97 import weight 0.00
Epoch 118 Iter 3 subLoss 2810.9 multi 3.99 import weight 0.00
Epoch 118 Iter 4 subLoss 2682.2 multi -1.99 import weight 0.00
Epoch 118 Iter 5 subLoss 2578.8 multi -4.97 import weight 0.00
Epoch 118 Iter 6 subLoss 2935.5 multi 1.00 import weight 0.00
Epoch 118 Iter 7 subLoss 3113.9 multi -4.97 import weight 0.00
Epoch 118 Iter 8 subLoss 3224.6 multi -7.96 import weight 0.00
Epoch 118 Iter 9 subLoss 12433.8 multi -1.99 import weight 0.00
Epoch 118 Iter 10 subLoss 58794.5 multi 1.00 import weight 0.00
Epoch 118 Iter 11 subLoss 7163.7 multi -4.97 import weight 0.00
Epoch 118 Acc: 78.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 716 train Loss: 25927.7 test Loss: 3714.5
Epoch 119 Iter 0 subLoss 24951.1 multi 1.00 import weight 0.00
Epoch 119 Iter 1 subLoss 10192.2 multi -1.99 import weight 0.00
Epoch 119 Iter 2 subLoss 19301.5 multi 1.00 import weight 0.00
Epoch 119 Iter 3 subLoss 12010.6 multi 1.00 import weight 0.00
Epoch 119 Iter 4 subLoss 7910.9 multi 1.00 import weight 0.00
Epoch 119 Iter 5 subLoss 7332.1 multi -1.99 import weight 0.00
Epoch 119 Iter 6 subLoss 10405.8 multi 1.00 import weight 0.00
Epoch 119 Iter 7 subLoss 7747.9 multi 6.97 import weight 0.00
Epoch 119 Iter 8 subLoss 4021.2 multi 3.99 import weight 0.00
Epoch 119 Iter 9 subLoss 3059.4 multi 1.00 import weight 0.00
Epoch 119 Iter 10 subLoss 3440.1 multi 15.93 import weight 1.00
Epoch 119 Iter 11 subLoss 3069.7 multi -1.99 import weight 0.00
Epoch 119 Acc: 97.66 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 306 train Loss: 3255.1 test Loss: 390.1
Epoch 120 Iter 0 subLoss 3076.4 multi -1.99 import weight 0.00
Epoch 120 Iter 1 subLoss 3445.1 multi 18.91 import weight 1.00
Epoch 120 Iter 2 subLoss 7240.5 multi -1.99 import weight 0.00
Epoch 120 Iter 3 subLoss 28331.2 multi 1.00 import weight 0.00
Epoch 120 Iter 4 subLoss 3573.1 multi -7.96 import weight 0.00
Epoch 120 Iter 5 subLoss 9004.1 multi -1.99 import weight 0.00
Epoch 120 Iter 6 subLoss 39253.3 multi 1.00 import weight 0.00
Epoch 120 Iter 7 subLoss 5148.8 multi 3.99 import weight 0.00
Epoch 120 Iter 8 subLoss 3237.6 multi 1.00 import weight 0.00
Epoch 120 Iter 9 subLoss 3411.6 multi 6.97 import weight 0.00
Epoch 120 Iter 10 subLoss 3629.2 multi -4.97 import weight 0.00
Epoch 120 Iter 11 subLoss 3257.0 multi 3.99 import weight 0.00
Epoch 120 Acc: 97.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 325 train Loss: 3154.2 test Loss: 366.1
Epoch 121 Iter 0 subLoss 3483.2 multi -1.99 import weight 0.00
Epoch 121 Iter 1 subLoss 2863.3 multi -1.99 import weight 0.00
Epoch 121 Iter 2 subLoss 2951.0 multi -1.99 import weight 0.00
Epoch 121 Iter 3 subLoss 2926.9 multi 1.00 import weight 0.00
Epoch 121 Iter 4 subLoss 3228.0 multi -4.97 import weight 0.00
Epoch 121 Iter 5 subLoss 3660.9 multi 3.99 import weight 0.00
Epoch 121 Iter 6 subLoss 3369.7 multi -4.97 import weight 0.00
Epoch 121 Iter 7 subLoss 3226.4 multi -1.99 import weight 0.00
Epoch 121 Iter 8 subLoss 4017.1 multi 1.00 import weight 0.00
Epoch 121 Iter 9 subLoss 3674.2 multi -4.97 import weight 0.00
Epoch 121 Iter 10 subLoss 4584.7 multi 1.00 import weight 0.00
Epoch 121 Iter 11 subLoss 4586.2 multi 3.99 import weight 0.00
Epoch 121 Acc: 98.00 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 458 train Loss: 3643.3 test Loss: 426.9
Epoch 122 Iter 0 subLoss 3708.6 multi -4.97 import weight 0.00
Epoch 122 Iter 1 subLoss 3882.1 multi 1.00 import weight 0.00
Epoch 122 Iter 2 subLoss 3737.5 multi 6.97 import weight 0.00
Epoch 122 Iter 3 subLoss 3617.7 multi 6.97 import weight 0.00
Epoch 122 Iter 4 subLoss 2898.2 multi 6.97 import weight 0.00
Epoch 122 Iter 5 subLoss 2791.1 multi 1.00 import weight 0.00
Epoch 122 Iter 6 subLoss 2547.0 multi 3.99 import weight 0.00
Epoch 122 Iter 7 subLoss 3226.7 multi 1.00 import weight 0.00
Epoch 122 Iter 8 subLoss 2885.1 multi 6.97 import weight 0.00
Epoch 122 Iter 9 subLoss 2295.5 multi 1.00 import weight 0.00
Epoch 122 Iter 10 subLoss 2565.6 multi 3.98 import weight 0.00
Epoch 122 Iter 11 subLoss 2744.9 multi 3.99 import weight 0.00
Epoch 122 Acc: 98.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 274 train Loss: 2753.7 test Loss: 300.5
Epoch 123 Iter 0 subLoss 2638.5 multi 1.00 import weight 0.00
Epoch 123 Iter 1 subLoss 2778.9 multi -1.98 import weight 0.00
Epoch 123 Iter 2 subLoss 2721.4 multi 1.00 import weight 0.00
Epoch 123 Iter 3 subLoss 2511.7 multi 6.97 import weight 0.00
Epoch 123 Iter 4 subLoss 3142.6 multi 1.00 import weight 0.00
Epoch 123 Iter 5 subLoss 2936.7 multi 1.00 import weight 0.00
Epoch 123 Iter 6 subLoss 2476.4 multi 3.99 import weight 0.00
Epoch 123 Iter 7 subLoss 2902.5 multi -10.94 import weight 0.00
Epoch 123 Iter 8 subLoss 2713.2 multi 1.00 import weight 0.00
Epoch 123 Iter 9 subLoss 2447.1 multi -1.98 import weight 0.00
Epoch 123 Iter 10 subLoss 2974.8 multi 6.97 import weight 0.00
Epoch 123 Iter 11 subLoss 2989.2 multi -1.99 import weight 0.00
Epoch 123 Acc: 98.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 298 train Loss: 2842.4 test Loss: 304.8
Epoch 124 Iter 0 subLoss 2744.6 multi 6.97 import weight 0.00
Epoch 124 Iter 1 subLoss 2850.9 multi 3.99 import weight 0.00
Epoch 124 Iter 2 subLoss 2591.0 multi -1.99 import weight 0.00
Epoch 124 Iter 3 subLoss 2921.1 multi 3.99 import weight 0.00
Epoch 124 Iter 4 subLoss 2521.2 multi -4.97 import weight 0.00
Epoch 124 Iter 5 subLoss 2391.4 multi 9.96 import weight 0.00
Epoch 124 Iter 6 subLoss 2478.0 multi 6.97 import weight 0.00
Epoch 124 Iter 7 subLoss 2583.0 multi 6.97 import weight 0.00
Epoch 124 Iter 8 subLoss 2396.4 multi 12.94 import weight 0.00
Epoch 124 Iter 9 subLoss 2794.7 multi 3.99 import weight 0.00
Epoch 124 Iter 10 subLoss 2517.1 multi 9.96 import weight 0.00
Epoch 124 Iter 11 subLoss 2831.4 multi -1.99 import weight 0.00
Epoch 124 Acc: 98.38 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 283 train Loss: 2515.7 test Loss: 269.5
Epoch 125 Iter 0 subLoss 2227.2 multi 1.00 import weight 0.00
Epoch 125 Iter 1 subLoss 2078.0 multi -1.99 import weight 0.00
Epoch 125 Iter 2 subLoss 2498.4 multi 6.97 import weight 0.00
Epoch 125 Iter 3 subLoss 2413.6 multi 1.00 import weight 0.00
Epoch 125 Iter 4 subLoss 2408.6 multi -13.93 import weight 0.00
Epoch 125 Iter 5 subLoss 2803.5 multi -7.96 import weight 0.00
Epoch 125 Iter 6 subLoss 3561.3 multi 6.97 import weight 0.00
Epoch 125 Iter 7 subLoss 2769.2 multi 6.97 import weight 0.00
Epoch 125 Iter 8 subLoss 2833.6 multi 1.00 import weight 0.00
Epoch 125 Iter 9 subLoss 2390.0 multi 15.93 import weight 0.00
Epoch 125 Iter 10 subLoss 4070.0 multi 1.00 import weight 0.00
Epoch 125 Iter 11 subLoss 2836.7 multi 3.98 import weight 0.00
Epoch 125 Acc: 98.29 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 283 train Loss: 2606.1 test Loss: 277.2
Epoch 126 Iter 0 subLoss 3028.0 multi -1.99 import weight 0.00
Epoch 126 Iter 1 subLoss 2495.2 multi 9.96 import weight 0.00
Epoch 126 Iter 2 subLoss 2814.1 multi 3.99 import weight 0.00
Epoch 126 Iter 3 subLoss 2213.8 multi 1.00 import weight 0.00
Epoch 126 Iter 4 subLoss 2225.9 multi 1.00 import weight 0.00
Epoch 126 Iter 5 subLoss 1919.4 multi 1.00 import weight 0.00
Epoch 126 Iter 6 subLoss 2314.4 multi 1.00 import weight 0.00
Epoch 126 Iter 7 subLoss 2623.1 multi 1.00 import weight 0.00
Epoch 126 Iter 8 subLoss 1936.9 multi 1.00 import weight 0.00
Epoch 126 Iter 9 subLoss 3127.4 multi 1.00 import weight 0.00
Epoch 126 Iter 10 subLoss 2341.2 multi 1.00 import weight 0.00
Epoch 126 Iter 11 subLoss 1994.3 multi 1.00 import weight 0.00
Epoch 126 Acc: 98.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 199 train Loss: 2334.1 test Loss: 265.6
Epoch 127 Iter 0 subLoss 2337.1 multi 1.00 import weight 0.00
Epoch 127 Iter 1 subLoss 2354.5 multi -1.99 import weight 0.00
Epoch 127 Iter 2 subLoss 2583.1 multi 9.96 import weight 0.00
Epoch 127 Iter 3 subLoss 2384.5 multi 1.00 import weight 0.00
Epoch 127 Iter 4 subLoss 2470.4 multi 9.96 import weight 0.00
Epoch 127 Iter 5 subLoss 2064.6 multi 3.99 import weight 0.00
Epoch 127 Iter 6 subLoss 2352.7 multi 1.00 import weight 0.00
Epoch 127 Iter 7 subLoss 2500.4 multi -13.93 import weight 0.00
Epoch 127 Iter 8 subLoss 2841.9 multi -1.99 import weight 0.00
Epoch 127 Iter 9 subLoss 3878.8 multi 6.97 import weight 0.00
Epoch 127 Iter 10 subLoss 3529.4 multi 1.00 import weight 0.00
Epoch 127 Iter 11 subLoss 3154.1 multi 1.00 import weight 0.00
Epoch 127 Acc: 98.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 315 train Loss: 2717.4 test Loss: 291.2
Epoch 128 Iter 0 subLoss 2274.4 multi -1.99 import weight 0.00
Epoch 128 Iter 1 subLoss 3017.4 multi -10.94 import weight 0.00
Epoch 128 Iter 2 subLoss 12406.2 multi 1.00 import weight 0.00
Epoch 128 Iter 3 subLoss 4809.0 multi -4.97 import weight 0.00
Epoch 128 Iter 4 subLoss 19096.0 multi 1.00 import weight 0.00
Epoch 128 Iter 5 subLoss 6817.3 multi -1.99 import weight 0.00
Epoch 128 Iter 6 subLoss 12625.1 multi 1.00 import weight 0.00
Epoch 128 Iter 7 subLoss 7108.1 multi -1.99 import weight 0.00
Epoch 128 Iter 8 subLoss 10617.1 multi 1.00 import weight 0.00
Epoch 128 Iter 9 subLoss 8055.0 multi 3.99 import weight 0.00
Epoch 128 Iter 10 subLoss 3312.9 multi 1.00 import weight 0.00
Epoch 128 Iter 11 subLoss 2579.9 multi -4.97 import weight 0.00
Epoch 128 Acc: 97.65 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 257 train Loss: 3391.9 test Loss: 383.8
Epoch 129 Iter 0 subLoss 3640.8 multi -1.98 import weight 0.00
Epoch 129 Iter 1 subLoss 3972.0 multi 9.96 import weight 0.00
Epoch 129 Iter 2 subLoss 3725.0 multi -10.94 import weight 0.00
Epoch 129 Iter 3 subLoss 20161.1 multi 1.00 import weight 0.00
Epoch 129 Iter 4 subLoss 3593.0 multi 9.96 import weight 0.00
Epoch 129 Iter 5 subLoss 3733.9 multi 6.97 import weight 0.00
Epoch 129 Iter 6 subLoss 2979.8 multi 9.96 import weight 0.00
Epoch 129 Iter 7 subLoss 5265.0 multi -7.96 import weight 0.00
Epoch 129 Iter 8 subLoss 58127.2 multi 1.00 import weight 0.00
Epoch 129 Iter 9 subLoss 9583.8 multi 1.00 import weight 0.00
Epoch 129 Iter 10 subLoss 5817.7 multi 6.97 import weight 0.00
Epoch 129 Iter 11 subLoss 2660.8 multi 9.96 import weight 0.00
Epoch 129 Acc: 97.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 266 train Loss: 2864.5 test Loss: 343.3
Epoch 130 Iter 0 subLoss 2650.5 multi -1.98 import weight 0.00
Epoch 130 Iter 1 subLoss 2811.0 multi 6.97 import weight 0.00
Epoch 130 Iter 2 subLoss 2512.4 multi 9.96 import weight 0.00
Epoch 130 Iter 3 subLoss 2437.2 multi 6.97 import weight 0.00
Epoch 130 Iter 4 subLoss 2413.5 multi 1.00 import weight 0.00
Epoch 130 Iter 5 subLoss 2604.7 multi -4.97 import weight 0.00
Epoch 130 Iter 6 subLoss 2476.1 multi 12.94 import weight 0.00
Epoch 130 Iter 7 subLoss 2789.7 multi 3.99 import weight 0.00
Epoch 130 Iter 8 subLoss 2510.4 multi 12.94 import weight 0.00
Epoch 130 Iter 9 subLoss 2473.9 multi 15.93 import weight 1.00
Epoch 130 Iter 10 subLoss 2746.0 multi 9.96 import weight 0.00
Epoch 130 Iter 11 subLoss 2709.8 multi 1.00 import weight 0.00
Epoch 130 Acc: 98.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 270 train Loss: 2530.6 test Loss: 294.8
Epoch 131 Iter 0 subLoss 2580.2 multi 9.96 import weight 0.00
Epoch 131 Iter 1 subLoss 3099.7 multi -4.97 import weight 0.00
Epoch 131 Iter 2 subLoss 4097.9 multi -1.99 import weight 0.00
Epoch 131 Iter 3 subLoss 7244.1 multi 1.00 import weight 0.00
Epoch 131 Iter 4 subLoss 3820.5 multi -7.96 import weight 0.00
Epoch 131 Iter 5 subLoss 42218.6 multi 1.00 import weight 0.00
Epoch 131 Iter 6 subLoss 4190.4 multi 9.96 import weight 0.00
Epoch 131 Iter 7 subLoss 5230.9 multi 6.97 import weight 0.00
Epoch 131 Iter 8 subLoss 4074.0 multi 3.98 import weight 0.00
Epoch 131 Iter 9 subLoss 2333.7 multi 3.99 import weight 0.00
Epoch 131 Iter 10 subLoss 2517.0 multi 15.93 import weight 0.00
Epoch 131 Iter 11 subLoss 2315.2 multi 3.99 import weight 0.00
Epoch 131 Acc: 98.52 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 231 train Loss: 2403.8 test Loss: 257.6
Epoch 132 Iter 0 subLoss 2482.6 multi -19.90 import weight 0.00
Epoch 132 Iter 1 subLoss 5495.9 multi 1.00 import weight 0.00
Epoch 132 Iter 2 subLoss 3704.7 multi -1.98 import weight 0.00
Epoch 132 Iter 3 subLoss 5889.4 multi 1.00 import weight 0.00
Epoch 132 Iter 4 subLoss 3824.7 multi -4.97 import weight 0.00
Epoch 132 Iter 5 subLoss 13473.6 multi 1.00 import weight 0.00
Epoch 132 Iter 6 subLoss 7255.7 multi -4.97 import weight 0.00
Epoch 132 Iter 7 subLoss 30448.9 multi 1.00 import weight 0.00
Epoch 132 Iter 8 subLoss 12088.7 multi -1.99 import weight 0.00
Epoch 132 Iter 9 subLoss 22098.8 multi 1.00 import weight 0.00
Epoch 132 Iter 10 subLoss 13250.8 multi 1.00 import weight 0.00
Epoch 132 Iter 11 subLoss 7336.3 multi 1.00 import weight 0.00
Epoch 132 Acc: 95.64 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 733 train Loss: 6851.6 test Loss: 706.1
Epoch 133 Iter 0 subLoss 7507.6 multi 3.99 import weight 0.00
Epoch 133 Iter 1 subLoss 3029.2 multi -1.98 import weight 0.00
Epoch 133 Iter 2 subLoss 3800.2 multi -1.98 import weight 0.00
Epoch 133 Iter 3 subLoss 4142.1 multi 12.94 import weight 0.00
Epoch 133 Iter 4 subLoss 3117.4 multi -1.98 import weight 0.00
Epoch 133 Iter 5 subLoss 4224.4 multi 3.99 import weight 0.00
Epoch 133 Iter 6 subLoss 2584.4 multi 12.94 import weight 0.00
Epoch 133 Iter 7 subLoss 2561.0 multi 6.97 import weight 0.00
Epoch 133 Iter 8 subLoss 2949.7 multi 1.00 import weight 0.00
Epoch 133 Iter 9 subLoss 2410.9 multi 3.98 import weight 0.00
Epoch 133 Iter 10 subLoss 2251.6 multi 1.00 import weight 0.00
Epoch 133 Iter 11 subLoss 2408.7 multi -13.93 import weight 0.00
Epoch 133 Acc: 98.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 240 train Loss: 2709.1 test Loss: 309.1
Epoch 134 Iter 0 subLoss 2446.9 multi -1.99 import weight 0.00
Epoch 134 Iter 1 subLoss 3149.6 multi 3.99 import weight 0.00
Epoch 134 Iter 2 subLoss 3080.6 multi 6.97 import weight 0.00
Epoch 134 Iter 3 subLoss 2324.8 multi -4.97 import weight 0.00
Epoch 134 Iter 4 subLoss 2046.7 multi 1.00 import weight 0.00
Epoch 134 Iter 5 subLoss 2152.6 multi 1.00 import weight 0.00
Epoch 134 Iter 6 subLoss 2270.2 multi 1.00 import weight 0.00
Epoch 134 Iter 7 subLoss 2294.6 multi 3.99 import weight 0.00
Epoch 134 Iter 8 subLoss 2920.6 multi 6.97 import weight 0.00
Epoch 134 Iter 9 subLoss 2284.9 multi -4.97 import weight 0.00
Epoch 134 Iter 10 subLoss 2438.1 multi 9.96 import weight 0.00
Epoch 134 Iter 11 subLoss 1923.6 multi -1.99 import weight 0.00
Epoch 134 Acc: 98.37 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 192 train Loss: 2355.3 test Loss: 270.9
Epoch 135 Iter 0 subLoss 2031.5 multi 1.00 import weight 0.00
Epoch 135 Iter 1 subLoss 2260.2 multi 1.00 import weight 0.00
Epoch 135 Iter 2 subLoss 2220.5 multi 3.98 import weight 0.00
Epoch 135 Iter 3 subLoss 2414.5 multi 3.99 import weight 0.00
Epoch 135 Iter 4 subLoss 2392.9 multi 15.93 import weight 0.00
Epoch 135 Iter 5 subLoss 2698.5 multi 1.00 import weight 0.00
Epoch 135 Iter 6 subLoss 2789.1 multi 6.97 import weight 0.00
Epoch 135 Iter 7 subLoss 2407.9 multi -13.93 import weight 0.00
Epoch 135 Iter 8 subLoss 2685.3 multi 1.00 import weight 0.00
Epoch 135 Iter 9 subLoss 2488.7 multi -16.91 import weight 0.00
Epoch 135 Iter 10 subLoss 12948.5 multi 1.00 import weight 0.00
Epoch 135 Iter 11 subLoss 4436.0 multi 6.97 import weight 0.00
Epoch 135 Acc: 97.78 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 443 train Loss: 3075.9 test Loss: 374.4
Epoch 136 Iter 0 subLoss 2499.5 multi 6.97 import weight 0.00
Epoch 136 Iter 1 subLoss 2617.6 multi -1.98 import weight 0.00
Epoch 136 Iter 2 subLoss 2895.3 multi 6.97 import weight 0.00
Epoch 136 Iter 3 subLoss 2457.5 multi -7.96 import weight 0.00
Epoch 136 Iter 4 subLoss 2510.9 multi 18.91 import weight 1.00
Epoch 136 Iter 5 subLoss 3543.3 multi 15.93 import weight 0.00
Epoch 136 Iter 6 subLoss 16260.5 multi 1.00 import weight 0.00
Epoch 136 Iter 7 subLoss 4211.8 multi 3.99 import weight 0.00
Epoch 136 Iter 8 subLoss 2795.9 multi 1.00 import weight 0.00
Epoch 136 Iter 9 subLoss 2599.0 multi -10.94 import weight 0.00
Epoch 136 Iter 10 subLoss 4020.9 multi 3.99 import weight 0.00
Epoch 136 Iter 11 subLoss 2850.4 multi 3.99 import weight 0.00
Epoch 136 Acc: 98.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 285 train Loss: 2708.4 test Loss: 316.5
Epoch 137 Iter 0 subLoss 2819.5 multi 9.96 import weight 0.00
Epoch 137 Iter 1 subLoss 2305.5 multi -4.97 import weight 0.00
Epoch 137 Iter 2 subLoss 2345.1 multi -1.98 import weight 0.00
Epoch 137 Iter 3 subLoss 2856.5 multi 6.97 import weight 0.00
Epoch 137 Iter 4 subLoss 2542.3 multi 6.97 import weight 0.00
Epoch 137 Iter 5 subLoss 2686.7 multi 3.99 import weight 0.00
Epoch 137 Iter 6 subLoss 2337.6 multi 3.98 import weight 0.00
Epoch 137 Iter 7 subLoss 2256.8 multi 3.99 import weight 0.00
Epoch 137 Iter 8 subLoss 1816.3 multi 1.00 import weight 0.00
Epoch 137 Iter 9 subLoss 2105.0 multi 1.00 import weight 0.00
Epoch 137 Iter 10 subLoss 2006.1 multi -1.99 import weight 0.00
Epoch 137 Iter 11 subLoss 1899.8 multi 1.00 import weight 0.00
Epoch 137 Acc: 98.46 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 189 train Loss: 2320.5 test Loss: 265.3
Epoch 138 Iter 0 subLoss 2313.2 multi 3.98 import weight 0.00
Epoch 138 Iter 1 subLoss 2277.0 multi 1.00 import weight 0.00
Epoch 138 Iter 2 subLoss 2713.5 multi 1.00 import weight 0.00
Epoch 138 Iter 3 subLoss 2092.4 multi 1.00 import weight 0.00
Epoch 138 Iter 4 subLoss 2266.7 multi 1.00 import weight 0.00
Epoch 138 Iter 5 subLoss 2197.1 multi 1.00 import weight 0.00
Epoch 138 Iter 6 subLoss 2242.9 multi -1.99 import weight 0.00
Epoch 138 Iter 7 subLoss 1801.4 multi 1.00 import weight 0.00
Epoch 138 Iter 8 subLoss 2074.4 multi -1.98 import weight 0.00
Epoch 138 Iter 9 subLoss 2038.7 multi 3.99 import weight 0.00
Epoch 138 Iter 10 subLoss 1942.4 multi -1.99 import weight 0.00
Epoch 138 Iter 11 subLoss 2392.9 multi 18.91 import weight 1.00
Epoch 138 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 18.91 Pidx 239 train Loss: 2640.3 test Loss: 304.9
Epoch 139 Iter 0 subLoss 2063.6 multi 6.97 import weight 0.00
Epoch 139 Iter 1 subLoss 2229.4 multi 6.97 import weight 0.00
Epoch 139 Iter 2 subLoss 2241.4 multi 1.00 import weight 0.00
Epoch 139 Iter 3 subLoss 2307.8 multi -1.98 import weight 0.00
Epoch 139 Iter 4 subLoss 2339.5 multi 6.97 import weight 0.00
Epoch 139 Iter 5 subLoss 1816.6 multi 1.00 import weight 0.00
Epoch 139 Iter 6 subLoss 2137.6 multi 1.00 import weight 0.00
Epoch 139 Iter 7 subLoss 2443.8 multi -1.99 import weight 0.00
Epoch 139 Iter 8 subLoss 1837.9 multi 1.00 import weight 0.00
Epoch 139 Iter 9 subLoss 1840.2 multi -1.99 import weight 0.00
Epoch 139 Iter 10 subLoss 2041.8 multi -1.98 import weight 0.00
Epoch 139 Iter 11 subLoss 2366.0 multi -4.97 import weight 0.00
Epoch 139 Acc: 98.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 236 train Loss: 2258.1 test Loss: 300.9
Epoch 140 Iter 0 subLoss 2232.0 multi -7.96 import weight 0.00
Epoch 140 Iter 1 subLoss 3501.9 multi -1.99 import weight 0.00
Epoch 140 Iter 2 subLoss 7694.7 multi -4.97 import weight 0.00
Epoch 140 Iter 3 subLoss 319402.1 multi 1.00 import weight 0.00
Epoch 140 Iter 4 subLoss 26735.3 multi 1.00 import weight 0.00
Epoch 140 Iter 5 subLoss 14973.9 multi 1.00 import weight 0.00
Epoch 140 Iter 6 subLoss 10000.0 multi 1.00 import weight 0.00
Epoch 140 Iter 7 subLoss 7653.8 multi 1.00 import weight 0.00
Epoch 140 Iter 8 subLoss 5995.9 multi 1.00 import weight 0.00
Epoch 140 Iter 9 subLoss 5295.9 multi -1.99 import weight 0.00
Epoch 140 Iter 10 subLoss 6478.9 multi 1.00 import weight 0.00
Epoch 140 Iter 11 subLoss 5601.9 multi -1.99 import weight 0.00
Epoch 140 Acc: 93.73 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 560 train Loss: 7412.2 test Loss: 1267.6
Epoch 141 Iter 0 subLoss 7256.4 multi -1.98 import weight 0.00
Epoch 141 Iter 1 subLoss 10341.5 multi 1.00 import weight 0.00
Epoch 141 Iter 2 subLoss 7970.4 multi -1.99 import weight 0.00
Epoch 141 Iter 3 subLoss 11946.2 multi 1.00 import weight 0.00
Epoch 141 Iter 4 subLoss 8846.0 multi 3.99 import weight 0.00
Epoch 141 Iter 5 subLoss 4628.6 multi -4.97 import weight 0.00
Epoch 141 Iter 6 subLoss 6022.9 multi 9.96 import weight 0.00
Epoch 141 Iter 7 subLoss 3244.4 multi 3.99 import weight 0.00
Epoch 141 Iter 8 subLoss 3186.3 multi 6.97 import weight 0.00
Epoch 141 Iter 9 subLoss 2817.5 multi 12.94 import weight 1.00
Epoch 141 Iter 10 subLoss 2611.8 multi 1.00 import weight 0.00
Epoch 141 Iter 11 subLoss 2953.7 multi -1.98 import weight 0.00
Epoch 141 Acc: 98.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 295 train Loss: 2927.4 test Loss: 316.0
Epoch 142 Iter 0 subLoss 2562.9 multi 9.96 import weight 0.00
Epoch 142 Iter 1 subLoss 3269.7 multi -7.96 import weight 0.00
Epoch 142 Iter 2 subLoss 7783.8 multi 1.00 import weight 0.00
Epoch 142 Iter 3 subLoss 4544.4 multi 6.97 import weight 0.00
Epoch 142 Iter 4 subLoss 3367.4 multi -1.99 import weight 0.00
Epoch 142 Iter 5 subLoss 3734.3 multi 9.96 import weight 0.00
Epoch 142 Iter 6 subLoss 4858.2 multi 6.97 import weight 0.00
Epoch 142 Iter 7 subLoss 5702.4 multi 1.00 import weight 0.00
Epoch 142 Iter 8 subLoss 5067.2 multi -1.98 import weight 0.00
Epoch 142 Iter 9 subLoss 6160.4 multi -4.97 import weight 0.00
Epoch 142 Iter 10 subLoss 31543.5 multi 1.00 import weight 0.00
Epoch 142 Iter 11 subLoss 8917.6 multi 1.00 import weight 0.00
Epoch 142 Acc: 95.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 891 train Loss: 7011.7 test Loss: 779.4
Epoch 143 Iter 0 subLoss 7614.1 multi 3.99 import weight 0.00
Epoch 143 Iter 1 subLoss 3172.1 multi 1.00 import weight 0.00
Epoch 143 Iter 2 subLoss 2932.8 multi -1.98 import weight 0.00
Epoch 143 Iter 3 subLoss 3280.5 multi -4.97 import weight 0.00
Epoch 143 Iter 4 subLoss 4422.5 multi 3.98 import weight 0.00
Epoch 143 Iter 5 subLoss 2851.1 multi 9.96 import weight 0.00
Epoch 143 Iter 6 subLoss 2673.1 multi -4.97 import weight 0.00
Epoch 143 Iter 7 subLoss 2671.8 multi -1.98 import weight 0.00
Epoch 143 Iter 8 subLoss 2722.7 multi -1.98 import weight 0.00
Epoch 143 Iter 9 subLoss 3249.0 multi 6.97 import weight 0.00
Epoch 143 Iter 10 subLoss 2878.3 multi -7.96 import weight 0.00
Epoch 143 Iter 11 subLoss 3639.1 multi -4.97 import weight 0.00
Epoch 143 Acc: 94.18 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 363 train Loss: 7428.5 test Loss: 972.8
Epoch 144 Iter 0 subLoss 7706.8 multi 1.00 import weight 0.00
Epoch 144 Iter 1 subLoss 4623.7 multi -1.99 import weight 0.00
Epoch 144 Iter 2 subLoss 7373.5 multi 6.97 import weight 0.00
Epoch 144 Iter 3 subLoss 4225.1 multi 3.98 import weight 0.00
Epoch 144 Iter 4 subLoss 2530.1 multi -4.97 import weight 0.00
Epoch 144 Iter 5 subLoss 3167.0 multi -1.98 import weight 0.00
Epoch 144 Iter 6 subLoss 3554.7 multi -10.94 import weight 0.00
Epoch 144 Iter 7 subLoss 7059.5 multi 1.00 import weight 0.00
Epoch 144 Iter 8 subLoss 5029.9 multi 3.99 import weight 0.00
Epoch 144 Iter 9 subLoss 4520.2 multi 3.98 import weight 0.00
Epoch 144 Iter 10 subLoss 3375.7 multi 6.97 import weight 0.00
Epoch 144 Iter 11 subLoss 3388.3 multi -13.93 import weight 0.00
Epoch 144 Acc: 97.94 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 338 train Loss: 3538.2 test Loss: 397.6
Epoch 145 Iter 0 subLoss 3381.1 multi -10.94 import weight 0.00
Epoch 145 Iter 1 subLoss 6089.7 multi 6.97 import weight 0.00
Epoch 145 Iter 2 subLoss 3986.9 multi -13.93 import weight 0.00
Epoch 145 Iter 3 subLoss 23659.9 multi 3.99 import weight 0.00
Epoch 145 Iter 4 subLoss 9749.4 multi -1.99 import weight 0.00
Epoch 145 Iter 5 subLoss 15440.4 multi -1.99 import weight 0.00
Epoch 145 Iter 6 subLoss 29425.5 multi 1.00 import weight 0.00
Epoch 145 Iter 7 subLoss 15039.2 multi -1.99 import weight 0.00
Epoch 145 Iter 8 subLoss 21495.5 multi -1.99 import weight 0.00
Epoch 145 Iter 9 subLoss 31739.9 multi 1.00 import weight 0.00
Epoch 145 Iter 10 subLoss 22459.5 multi 1.00 import weight 0.00
Epoch 145 Iter 11 subLoss 20144.8 multi -1.99 import weight 0.00
Epoch 145 Acc: 61.20 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 2014 train Loss: 25312.7 test Loss: 4974.8
Epoch 146 Iter 0 subLoss 24563.8 multi 3.99 import weight 0.00
Epoch 146 Iter 1 subLoss 17444.6 multi 3.99 import weight 0.00
Epoch 146 Iter 2 subLoss 9534.1 multi 1.00 import weight 0.00
Epoch 146 Iter 3 subLoss 9512.3 multi 3.99 import weight 0.00
Epoch 146 Iter 4 subLoss 6108.2 multi 6.97 import weight 0.00
Epoch 146 Iter 5 subLoss 3767.2 multi -1.98 import weight 0.00
Epoch 146 Iter 6 subLoss 3663.4 multi 6.97 import weight 0.00
Epoch 146 Iter 7 subLoss 3286.5 multi -1.99 import weight 0.00
Epoch 146 Iter 8 subLoss 3437.4 multi 1.00 import weight 0.00
Epoch 146 Iter 9 subLoss 3082.8 multi 9.96 import weight 0.00
Epoch 146 Iter 10 subLoss 2891.5 multi 9.96 import weight 0.00
Epoch 146 Iter 11 subLoss 3059.4 multi 3.99 import weight 0.00
Epoch 146 Acc: 98.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 305 train Loss: 2782.1 test Loss: 318.5
Epoch 147 Iter 0 subLoss 2579.4 multi -7.96 import weight 0.00
Epoch 147 Iter 1 subLoss 2719.3 multi 3.98 import weight 0.00
Epoch 147 Iter 2 subLoss 2780.0 multi -1.99 import weight 0.00
Epoch 147 Iter 3 subLoss 2336.7 multi 9.96 import weight 0.00
Epoch 147 Iter 4 subLoss 3146.7 multi 6.97 import weight 0.00
Epoch 147 Iter 5 subLoss 2865.1 multi -10.94 import weight 0.00
Epoch 147 Iter 6 subLoss 3245.9 multi 9.96 import weight 0.00
Epoch 147 Iter 7 subLoss 2653.7 multi 1.00 import weight 0.00
Epoch 147 Iter 8 subLoss 2700.6 multi 1.00 import weight 0.00
Epoch 147 Iter 9 subLoss 3020.6 multi 1.00 import weight 0.00
Epoch 147 Iter 10 subLoss 3116.5 multi 1.00 import weight 0.00
Epoch 147 Iter 11 subLoss 2557.6 multi -4.97 import weight 0.00
Epoch 147 Acc: 97.57 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 255 train Loss: 3053.2 test Loss: 413.7
Epoch 148 Iter 0 subLoss 2604.4 multi -4.97 import weight 0.00
Epoch 148 Iter 1 subLoss 3231.7 multi -4.97 import weight 0.00
Epoch 148 Iter 2 subLoss 5939.2 multi 1.00 import weight 0.00
Epoch 148 Iter 3 subLoss 5021.5 multi 6.97 import weight 0.00
Epoch 148 Iter 4 subLoss 3697.8 multi 6.97 import weight 0.00
Epoch 148 Iter 5 subLoss 2540.1 multi 6.97 import weight 0.00
Epoch 148 Iter 6 subLoss 2745.1 multi 12.94 import weight 0.00
Epoch 148 Iter 7 subLoss 2541.5 multi 9.96 import weight 0.00
Epoch 148 Iter 8 subLoss 2746.2 multi 15.93 import weight 1.00
Epoch 148 Iter 9 subLoss 3315.9 multi 3.99 import weight 0.00
Epoch 148 Iter 10 subLoss 2000.1 multi 1.00 import weight 0.00
Epoch 148 Iter 11 subLoss 2373.0 multi -1.99 import weight 0.00
Epoch 148 Acc: 98.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 237 train Loss: 2434.0 test Loss: 250.5
Epoch 149 Iter 0 subLoss 2535.4 multi -1.98 import weight 0.00
Epoch 149 Iter 1 subLoss 2546.7 multi 9.96 import weight 0.00
Epoch 149 Iter 2 subLoss 2563.6 multi 9.96 import weight 0.00
Epoch 149 Iter 3 subLoss 1722.9 multi 1.00 import weight 0.00
Epoch 149 Iter 4 subLoss 2384.4 multi 1.00 import weight 0.00
Epoch 149 Iter 5 subLoss 2572.3 multi -7.96 import weight 0.00
Epoch 149 Iter 6 subLoss 2616.6 multi 1.00 import weight 0.00
Epoch 149 Iter 7 subLoss 3012.5 multi -7.96 import weight 0.00
Epoch 149 Iter 8 subLoss 3184.8 multi 6.97 import weight 0.00
Epoch 149 Iter 9 subLoss 2545.1 multi 12.94 import weight 0.00
Epoch 149 Iter 10 subLoss 2666.6 multi 6.97 import weight 0.00
Epoch 149 Iter 11 subLoss 2299.1 multi 3.98 import weight 0.00
Epoch 149 Acc: 98.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 229 train Loss: 2260.3 test Loss: 266.0
Epoch 150 Iter 0 subLoss 2486.9 multi -13.93 import weight 0.00
Epoch 150 Iter 1 subLoss 2341.0 multi -7.96 import weight 0.00
Epoch 150 Iter 2 subLoss 3136.1 multi -13.93 import weight 0.00
Epoch 150 Iter 3 subLoss 6138.1 multi 6.97 import weight 0.00
Epoch 150 Iter 4 subLoss 3145.6 multi 6.97 import weight 0.00
Epoch 150 Iter 5 subLoss 2407.7 multi -13.93 import weight 0.00
Epoch 150 Iter 6 subLoss 6004.1 multi -1.98 import weight 0.00
Epoch 150 Iter 7 subLoss 8541.4 multi 3.99 import weight 0.00
Epoch 150 Iter 8 subLoss 2914.1 multi 1.00 import weight 0.00
Epoch 150 Iter 9 subLoss 2929.2 multi 6.97 import weight 0.00
Epoch 150 Iter 10 subLoss 2414.8 multi 1.00 import weight 0.00
Epoch 150 Iter 11 subLoss 2395.5 multi 18.91 import weight 1.00
Epoch 150 Acc: 98.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 18.91 Pidx 239 train Loss: 2779.0 test Loss: 317.2
Epoch 151 Iter 0 subLoss 2378.0 multi 1.00 import weight 0.00
Epoch 151 Iter 1 subLoss 2525.7 multi -16.91 import weight 0.00
Epoch 151 Iter 2 subLoss 13114.0 multi 1.00 import weight 0.00
Epoch 151 Iter 3 subLoss 4621.6 multi 1.00 import weight 0.00
Epoch 151 Iter 4 subLoss 3303.7 multi 9.96 import weight 0.00
Epoch 151 Iter 5 subLoss 3344.2 multi -4.97 import weight 0.00
Epoch 151 Iter 6 subLoss 5851.7 multi -4.97 import weight 0.00
Epoch 151 Iter 7 subLoss 32019.0 multi 1.00 import weight 0.00
Epoch 151 Iter 8 subLoss 6435.4 multi 6.97 import weight 0.00
Epoch 151 Iter 9 subLoss 2966.9 multi -10.94 import weight 0.00
Epoch 151 Iter 10 subLoss 3908.4 multi 1.00 import weight 0.00
Epoch 151 Iter 11 subLoss 4033.1 multi -10.94 import weight 0.00
Epoch 151 Acc: 83.89 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 403 train Loss: 12694.8 test Loss: 2294.0
Epoch 152 Iter 0 subLoss 13529.8 multi 1.00 import weight 0.00
Epoch 152 Iter 1 subLoss 5990.5 multi 3.98 import weight 0.00
Epoch 152 Iter 2 subLoss 3050.9 multi 6.97 import weight 0.00
Epoch 152 Iter 3 subLoss 2562.8 multi 12.94 import weight 0.00
Epoch 152 Iter 4 subLoss 2609.5 multi -1.99 import weight 0.00
Epoch 152 Iter 5 subLoss 2756.5 multi -22.88 import weight 0.00
Epoch 152 Iter 6 subLoss 20413.8 multi 1.00 import weight 0.00
Epoch 152 Iter 7 subLoss 5757.8 multi 1.00 import weight 0.00
Epoch 152 Iter 8 subLoss 5911.9 multi -1.99 import weight 0.00
Epoch 152 Iter 9 subLoss 7352.2 multi -1.99 import weight 0.00
Epoch 152 Iter 10 subLoss 13033.4 multi -1.99 import weight 0.00
Epoch 152 Iter 11 subLoss 38438.1 multi 1.00 import weight 0.00
Epoch 152 Acc: 91.03 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 3843 train Loss: 10125.0 test Loss: 1542.2
Epoch 153 Iter 0 subLoss 8692.2 multi 3.99 import weight 0.00
Epoch 153 Iter 1 subLoss 4288.8 multi -1.99 import weight 0.00
Epoch 153 Iter 2 subLoss 5417.7 multi -1.99 import weight 0.00
Epoch 153 Iter 3 subLoss 6122.8 multi -4.97 import weight 0.00
Epoch 153 Iter 4 subLoss 12130.3 multi 1.00 import weight 0.00
Epoch 153 Iter 5 subLoss 8483.9 multi -4.97 import weight 0.00
Epoch 153 Iter 6 subLoss 22493.6 multi 1.00 import weight 0.00
Epoch 153 Iter 7 subLoss 14193.3 multi 1.00 import weight 0.00
Epoch 153 Iter 8 subLoss 10680.9 multi 3.99 import weight 0.00
Epoch 153 Iter 9 subLoss 6551.2 multi 1.00 import weight 0.00
Epoch 153 Iter 10 subLoss 6462.4 multi -4.97 import weight 0.00
Epoch 153 Iter 11 subLoss 9858.1 multi 1.00 import weight 0.00
Epoch 153 Acc: 94.40 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 985 train Loss: 8528.7 test Loss: 1225.0
Epoch 154 Iter 0 subLoss 7961.2 multi 3.99 import weight 0.00
Epoch 154 Iter 1 subLoss 5905.6 multi 1.00 import weight 0.00
Epoch 154 Iter 2 subLoss 6453.9 multi 3.98 import weight 0.00
Epoch 154 Iter 3 subLoss 4591.9 multi -4.97 import weight 0.00
Epoch 154 Iter 4 subLoss 5856.4 multi -1.99 import weight 0.00
Epoch 154 Iter 5 subLoss 6345.6 multi 1.00 import weight 0.00
Epoch 154 Iter 6 subLoss 6135.6 multi 6.97 import weight 0.00
Epoch 154 Iter 7 subLoss 4396.4 multi -7.96 import weight 0.00
Epoch 154 Iter 8 subLoss 5871.1 multi 1.00 import weight 0.00
Epoch 154 Iter 9 subLoss 5330.0 multi -4.97 import weight 0.00
Epoch 154 Iter 10 subLoss 7328.7 multi 3.99 import weight 0.00
Epoch 154 Iter 11 subLoss 4978.0 multi 6.97 import weight 0.00
Epoch 154 Acc: 97.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 497 train Loss: 3950.9 test Loss: 488.4
Epoch 155 Iter 0 subLoss 3488.4 multi 1.00 import weight 0.00
Epoch 155 Iter 1 subLoss 3784.8 multi 6.97 import weight 0.00
Epoch 155 Iter 2 subLoss 3268.7 multi -4.97 import weight 0.00
Epoch 155 Iter 3 subLoss 3540.7 multi 18.91 import weight 0.00
Epoch 155 Iter 4 subLoss 3749.6 multi -13.93 import weight 0.00
Epoch 155 Iter 5 subLoss 35066.2 multi 1.00 import weight 0.00
Epoch 155 Iter 6 subLoss 5260.5 multi -4.97 import weight 0.00
Epoch 155 Iter 7 subLoss 12058.9 multi 1.00 import weight 0.00
Epoch 155 Iter 8 subLoss 8213.3 multi 1.00 import weight 0.00
Epoch 155 Iter 9 subLoss 5815.9 multi 9.96 import weight 0.00
Epoch 155 Iter 10 subLoss 3857.7 multi -10.94 import weight 0.00
Epoch 155 Iter 11 subLoss 11288.7 multi 3.99 import weight 0.00
Epoch 155 Acc: 96.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1128 train Loss: 4458.1 test Loss: 694.7
Epoch 156 Iter 0 subLoss 4304.8 multi 1.00 import weight 0.00
Epoch 156 Iter 1 subLoss 4254.2 multi -7.96 import weight 0.00
Epoch 156 Iter 2 subLoss 7375.0 multi 9.96 import weight 0.00
Epoch 156 Iter 3 subLoss 6594.0 multi 3.98 import weight 0.00
Epoch 156 Iter 4 subLoss 3707.6 multi -1.99 import weight 0.00
Epoch 156 Iter 5 subLoss 3722.8 multi -7.96 import weight 0.00
Epoch 156 Iter 6 subLoss 6161.8 multi -1.99 import weight 0.00
Epoch 156 Iter 7 subLoss 7439.6 multi -1.99 import weight 0.00
Epoch 156 Iter 8 subLoss 9966.0 multi 1.00 import weight 0.00
Epoch 156 Iter 9 subLoss 8536.2 multi 1.00 import weight 0.00
Epoch 156 Iter 10 subLoss 7692.8 multi -1.98 import weight 0.00
Epoch 156 Iter 11 subLoss 10038.9 multi 1.00 import weight 0.00
Epoch 156 Acc: 96.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1003 train Loss: 8618.3 test Loss: 939.5
Epoch 157 Iter 0 subLoss 8746.3 multi 3.98 import weight 0.00
Epoch 157 Iter 1 subLoss 4951.8 multi -10.94 import weight 0.00
Epoch 157 Iter 2 subLoss 10119.7 multi 1.00 import weight 0.00
Epoch 157 Iter 3 subLoss 9079.8 multi -1.99 import weight 0.00
Epoch 157 Iter 4 subLoss 10523.6 multi 1.00 import weight 0.00
Epoch 157 Iter 5 subLoss 9374.2 multi 1.00 import weight 0.00
Epoch 157 Iter 6 subLoss 8922.7 multi 3.98 import weight 0.00
Epoch 157 Iter 7 subLoss 6502.1 multi -7.96 import weight 0.00
Epoch 157 Iter 8 subLoss 9877.1 multi 1.00 import weight 0.00
Epoch 157 Iter 9 subLoss 9884.7 multi 1.00 import weight 0.00
Epoch 157 Iter 10 subLoss 9307.4 multi 1.00 import weight 0.00
Epoch 157 Iter 11 subLoss 8651.8 multi 1.00 import weight 0.00
Epoch 157 Acc: 96.71 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 865 train Loss: 8237.8 test Loss: 777.4
Epoch 158 Iter 0 subLoss 7560.7 multi 3.99 import weight 0.00
Epoch 158 Iter 1 subLoss 6643.2 multi -7.96 import weight 0.00
Epoch 158 Iter 2 subLoss 9027.8 multi 1.00 import weight 0.00
Epoch 158 Iter 3 subLoss 9405.9 multi 1.00 import weight 0.00
Epoch 158 Iter 4 subLoss 8295.9 multi 3.99 import weight 0.00
Epoch 158 Iter 5 subLoss 6564.5 multi -1.99 import weight 0.00
Epoch 158 Iter 6 subLoss 7657.9 multi 3.99 import weight 0.00
Epoch 158 Iter 7 subLoss 6636.7 multi 9.96 import weight 0.00
Epoch 158 Iter 8 subLoss 4736.3 multi -10.94 import weight 0.00
Epoch 158 Iter 9 subLoss 6221.7 multi 3.99 import weight 0.00
Epoch 158 Iter 10 subLoss 5231.3 multi 9.96 import weight 0.00
Epoch 158 Iter 11 subLoss 3820.3 multi -1.99 import weight 0.00
Epoch 158 Acc: 97.61 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 382 train Loss: 4254.6 test Loss: 432.3
Epoch 159 Iter 0 subLoss 4194.9 multi 12.94 import weight 0.00
Epoch 159 Iter 1 subLoss 3989.3 multi -10.94 import weight 0.00
Epoch 159 Iter 2 subLoss 5639.3 multi 1.00 import weight 0.00
Epoch 159 Iter 3 subLoss 4906.2 multi 1.00 import weight 0.00
Epoch 159 Iter 4 subLoss 4699.2 multi 3.98 import weight 0.00
Epoch 159 Iter 5 subLoss 3442.8 multi 18.91 import weight 1.00
Epoch 159 Iter 6 subLoss 3072.6 multi 1.00 import weight 0.00
Epoch 159 Iter 7 subLoss 3293.4 multi -7.96 import weight 0.00
Epoch 159 Iter 8 subLoss 5804.9 multi -4.97 import weight 0.00
Epoch 159 Iter 9 subLoss 50996.4 multi 1.00 import weight 0.00
Epoch 159 Iter 10 subLoss 4911.7 multi -7.96 import weight 0.00
Epoch 159 Iter 11 subLoss 12345.3 multi -1.99 import weight 0.00
Epoch 159 Acc: 58.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1234 train Loss: 37822.5 test Loss: 7441.2
Epoch 160 Iter 0 subLoss 36600.5 multi 1.00 import weight 0.00
Epoch 160 Iter 1 subLoss 9403.7 multi 3.99 import weight 0.00
Epoch 160 Iter 2 subLoss 4863.4 multi -4.97 import weight 0.00
Epoch 160 Iter 3 subLoss 6075.2 multi -1.99 import weight 0.00
Epoch 160 Iter 4 subLoss 7404.9 multi -1.98 import weight 0.00
Epoch 160 Iter 5 subLoss 10258.3 multi -1.99 import weight 0.00
Epoch 160 Iter 6 subLoss 18459.1 multi 1.00 import weight 0.00
Epoch 160 Iter 7 subLoss 10941.7 multi 3.99 import weight 0.00
Epoch 160 Iter 8 subLoss 5657.7 multi 3.98 import weight 0.00
Epoch 160 Iter 9 subLoss 4480.3 multi -1.99 import weight 0.00
Epoch 160 Iter 10 subLoss 4764.6 multi 9.96 import weight 0.00
Epoch 160 Iter 11 subLoss 3365.6 multi 1.00 import weight 0.00
Epoch 160 Acc: 97.45 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 336 train Loss: 3435.7 test Loss: 442.0
Epoch 161 Iter 0 subLoss 3531.1 multi -4.97 import weight 0.00
Epoch 161 Iter 1 subLoss 3510.3 multi -4.97 import weight 0.00
Epoch 161 Iter 2 subLoss 4554.7 multi -4.97 import weight 0.00
Epoch 161 Iter 3 subLoss 5995.0 multi 6.97 import weight 0.00
Epoch 161 Iter 4 subLoss 3467.5 multi 1.00 import weight 0.00
Epoch 161 Iter 5 subLoss 3592.3 multi 12.94 import weight 0.00
Epoch 161 Iter 6 subLoss 3747.2 multi -10.94 import weight 0.00
Epoch 161 Iter 7 subLoss 6315.6 multi 3.99 import weight 0.00
Epoch 161 Iter 8 subLoss 3647.5 multi -1.99 import weight 0.00
Epoch 161 Iter 9 subLoss 3303.8 multi 9.96 import weight 0.00
Epoch 161 Iter 10 subLoss 2559.1 multi -13.93 import weight 0.00
Epoch 161 Iter 11 subLoss 3580.9 multi -1.98 import weight 0.00
Epoch 161 Acc: 97.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 358 train Loss: 3767.2 test Loss: 479.7
Epoch 162 Iter 0 subLoss 3511.0 multi -1.99 import weight 0.00
Epoch 162 Iter 1 subLoss 3473.6 multi -4.97 import weight 0.00
Epoch 162 Iter 2 subLoss 5898.3 multi 1.00 import weight 0.00
Epoch 162 Iter 3 subLoss 4481.4 multi 1.00 import weight 0.00
Epoch 162 Iter 4 subLoss 4248.8 multi 9.96 import weight 0.00
Epoch 162 Iter 5 subLoss 3062.6 multi -4.97 import weight 0.00
Epoch 162 Iter 6 subLoss 3980.1 multi -7.96 import weight 0.00
Epoch 162 Iter 7 subLoss 5042.5 multi 3.99 import weight 0.00
Epoch 162 Iter 8 subLoss 3990.7 multi -10.94 import weight 0.00
Epoch 162 Iter 9 subLoss 7838.1 multi 1.00 import weight 0.00
Epoch 162 Iter 10 subLoss 6635.5 multi 12.94 import weight 0.00
Epoch 162 Iter 11 subLoss 4636.0 multi 1.00 import weight 0.00
Epoch 162 Acc: 96.73 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 463 train Loss: 4017.0 test Loss: 540.1
Epoch 163 Iter 0 subLoss 3788.2 multi 9.96 import weight 0.00
Epoch 163 Iter 1 subLoss 3527.4 multi -1.99 import weight 0.00
Epoch 163 Iter 2 subLoss 3408.3 multi -1.99 import weight 0.00
Epoch 163 Iter 3 subLoss 3425.4 multi -4.97 import weight 0.00
Epoch 163 Iter 4 subLoss 4546.2 multi 9.96 import weight 0.00
Epoch 163 Iter 5 subLoss 3061.2 multi -1.98 import weight 0.00
Epoch 163 Iter 6 subLoss 3975.3 multi 12.94 import weight 0.00
Epoch 163 Iter 7 subLoss 3518.4 multi 1.00 import weight 0.00
Epoch 163 Iter 8 subLoss 3374.7 multi 6.97 import weight 0.00
Epoch 163 Iter 9 subLoss 2781.3 multi 6.97 import weight 0.00
Epoch 163 Iter 10 subLoss 2664.6 multi 9.96 import weight 0.00
Epoch 163 Iter 11 subLoss 2868.0 multi -7.96 import weight 0.00
Epoch 163 Acc: 97.98 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 286 train Loss: 2961.7 test Loss: 342.4
Epoch 164 Iter 0 subLoss 2776.6 multi 1.00 import weight 0.00
Epoch 164 Iter 1 subLoss 2834.2 multi 6.97 import weight 0.00
Epoch 164 Iter 2 subLoss 2447.2 multi 1.00 import weight 0.00
Epoch 164 Iter 3 subLoss 2667.2 multi 12.94 import weight 0.00
Epoch 164 Iter 4 subLoss 2520.2 multi -13.93 import weight 0.00
Epoch 164 Iter 5 subLoss 3093.1 multi -7.96 import weight 0.00
Epoch 164 Iter 6 subLoss 4218.2 multi 6.97 import weight 0.00
Epoch 164 Iter 7 subLoss 2373.2 multi 3.98 import weight 0.00
Epoch 164 Iter 8 subLoss 2469.3 multi 3.98 import weight 0.00
Epoch 164 Iter 9 subLoss 2731.2 multi -7.96 import weight 0.00
Epoch 164 Iter 10 subLoss 2498.4 multi 6.97 import weight 0.00
Epoch 164 Iter 11 subLoss 2816.2 multi 15.93 import weight 1.00
Epoch 164 Acc: 97.80 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 15.93 Pidx 281 train Loss: 2945.6 test Loss: 361.3
Epoch 165 Iter 0 subLoss 2592.9 multi -7.96 import weight 0.00
Epoch 165 Iter 1 subLoss 5273.1 multi 1.00 import weight 0.00
Epoch 165 Iter 2 subLoss 3488.3 multi 1.00 import weight 0.00
Epoch 165 Iter 3 subLoss 3231.3 multi -1.99 import weight 0.00
Epoch 165 Iter 4 subLoss 3819.2 multi 6.97 import weight 0.00
Epoch 165 Iter 5 subLoss 2674.6 multi -7.96 import weight 0.00
Epoch 165 Iter 6 subLoss 3155.4 multi -4.97 import weight 0.00
Epoch 165 Iter 7 subLoss 3593.6 multi 12.94 import weight 0.00
Epoch 165 Iter 8 subLoss 3576.5 multi -7.96 import weight 0.00
Epoch 165 Iter 9 subLoss 11173.0 multi 1.00 import weight 0.00
Epoch 165 Iter 10 subLoss 3481.1 multi 3.99 import weight 0.00
Epoch 165 Iter 11 subLoss 2858.8 multi 12.94 import weight 0.00
Epoch 165 Acc: 97.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 285 train Loss: 2750.8 test Loss: 324.6
Epoch 166 Iter 0 subLoss 2543.0 multi 15.93 import weight 0.00
Epoch 166 Iter 1 subLoss 2456.2 multi -10.94 import weight 0.00
Epoch 166 Iter 2 subLoss 3476.3 multi -1.99 import weight 0.00
Epoch 166 Iter 3 subLoss 4383.0 multi 3.99 import weight 0.00
Epoch 166 Iter 4 subLoss 2680.0 multi -4.97 import weight 0.00
Epoch 166 Iter 5 subLoss 3002.9 multi 15.93 import weight 0.00
Epoch 166 Iter 6 subLoss 2752.7 multi -19.90 import weight 0.00
Epoch 166 Iter 7 subLoss 8066.4 multi -4.97 import weight 0.00
Epoch 166 Iter 8 subLoss 30067.0 multi -4.97 import weight 0.00
Epoch 166 Iter 9 subLoss 1197915.8 multi 1.00 import weight 0.00
Epoch 166 Iter 10 subLoss 57545.8 multi 1.00 import weight 0.00
Epoch 166 Iter 11 subLoss 55209.4 multi 1.00 import weight 0.00
Epoch 166 Acc: 22.32 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 5520 train Loss: 55288.8 test Loss: 9599.2
Epoch 167 Iter 0 subLoss 53416.7 multi 1.00 import weight 0.00
Epoch 167 Iter 1 subLoss 53416.8 multi 3.99 import weight 0.00
Epoch 167 Iter 2 subLoss 49553.4 multi 1.00 import weight 0.00
Epoch 167 Iter 3 subLoss 47631.3 multi 1.00 import weight 0.00
Epoch 167 Iter 4 subLoss 44833.4 multi 1.00 import weight 0.00
Epoch 167 Iter 5 subLoss 42812.4 multi 1.00 import weight 0.00
Epoch 167 Iter 6 subLoss 41351.1 multi 1.00 import weight 0.00
Epoch 167 Iter 7 subLoss 39603.6 multi 3.99 import weight 0.00
Epoch 167 Iter 8 subLoss 32166.5 multi 1.00 import weight 0.00
Epoch 167 Iter 9 subLoss 30773.4 multi 1.00 import weight 0.00
Epoch 167 Iter 10 subLoss 28912.9 multi 1.00 import weight 0.00
Epoch 167 Iter 11 subLoss 28400.4 multi 1.00 import weight 0.00
Epoch 167 Acc: 74.47 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2840 train Loss: 28115.4 test Loss: 4310.2
Epoch 168 Iter 0 subLoss 28109.3 multi 1.00 import weight 0.00
Epoch 168 Iter 1 subLoss 26908.9 multi 1.00 import weight 0.00
Epoch 168 Iter 2 subLoss 25282.5 multi -1.99 import weight 0.00
Epoch 168 Iter 3 subLoss 27441.4 multi -1.99 import weight 0.00
Epoch 168 Iter 4 subLoss 29124.1 multi 1.00 import weight 0.00
Epoch 168 Iter 5 subLoss 28236.2 multi 1.00 import weight 0.00
Epoch 168 Iter 6 subLoss 27817.3 multi 1.00 import weight 0.00
Epoch 168 Iter 7 subLoss 27198.8 multi 1.00 import weight 0.00
Epoch 168 Iter 8 subLoss 25471.7 multi -1.99 import weight 0.00
Epoch 168 Iter 9 subLoss 27456.5 multi -1.99 import weight 0.00
Epoch 168 Iter 10 subLoss 30152.9 multi 1.00 import weight 0.00
Epoch 168 Iter 11 subLoss 28164.8 multi 3.99 import weight 0.00
Epoch 168 Acc: 76.34 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 2816 train Loss: 25586.9 test Loss: 3877.7
Epoch 169 Iter 0 subLoss 25890.7 multi -4.97 import weight 0.00
Epoch 169 Iter 1 subLoss 29566.9 multi 1.00 import weight 0.00
Epoch 169 Iter 2 subLoss 28808.5 multi 1.00 import weight 0.00
Epoch 169 Iter 3 subLoss 27472.3 multi 1.00 import weight 0.00
Epoch 169 Iter 4 subLoss 25666.5 multi -1.99 import weight 0.00
Epoch 169 Iter 5 subLoss 27842.3 multi 1.00 import weight 0.00
Epoch 169 Iter 6 subLoss 26640.9 multi 1.00 import weight 0.00
Epoch 169 Iter 7 subLoss 26068.7 multi -1.99 import weight 0.00
Epoch 169 Iter 8 subLoss 29270.6 multi 1.00 import weight 0.00
Epoch 169 Iter 9 subLoss 27801.4 multi 1.00 import weight 0.00
Epoch 169 Iter 10 subLoss 27452.6 multi 1.00 import weight 0.00
Epoch 169 Iter 11 subLoss 25549.7 multi 3.99 import weight 0.00
Epoch 169 Acc: 77.27 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 2554 train Loss: 24096.0 test Loss: 3626.1
Epoch 170 Iter 0 subLoss 23098.4 multi 1.00 import weight 0.00
Epoch 170 Iter 1 subLoss 22987.6 multi 1.00 import weight 0.00
Epoch 170 Iter 2 subLoss 22312.3 multi 1.00 import weight 0.00
Epoch 170 Iter 3 subLoss 22141.6 multi 3.99 import weight 0.00
Epoch 170 Iter 4 subLoss 19226.6 multi 1.00 import weight 0.00
Epoch 170 Iter 5 subLoss 18361.2 multi 1.00 import weight 0.00
Epoch 170 Iter 6 subLoss 17548.9 multi -1.99 import weight 0.00
Epoch 170 Iter 7 subLoss 19138.3 multi 1.00 import weight 0.00
Epoch 170 Iter 8 subLoss 18325.3 multi 1.00 import weight 0.00
Epoch 170 Iter 9 subLoss 18236.0 multi -1.99 import weight 0.00
Epoch 170 Iter 10 subLoss 18381.9 multi -1.99 import weight 0.00
Epoch 170 Iter 11 subLoss 20272.7 multi 1.00 import weight 0.00
Epoch 170 Acc: 77.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2027 train Loss: 20050.2 test Loss: 2989.2
Epoch 171 Iter 0 subLoss 20520.2 multi 1.00 import weight 0.00
Epoch 171 Iter 1 subLoss 19310.2 multi -1.99 import weight 0.00
Epoch 171 Iter 2 subLoss 20076.9 multi 1.00 import weight 0.00
Epoch 171 Iter 3 subLoss 19567.9 multi 3.99 import weight 0.00
Epoch 171 Iter 4 subLoss 17669.8 multi 1.00 import weight 0.00
Epoch 171 Iter 5 subLoss 16558.4 multi 1.00 import weight 0.00
Epoch 171 Iter 6 subLoss 15872.4 multi 3.99 import weight 0.00
Epoch 171 Iter 7 subLoss 13808.4 multi 1.00 import weight 0.00
Epoch 171 Iter 8 subLoss 14694.6 multi 1.00 import weight 0.00
Epoch 171 Iter 9 subLoss 13123.5 multi -1.99 import weight 0.00
Epoch 171 Iter 10 subLoss 14685.9 multi -1.99 import weight 0.00
Epoch 171 Iter 11 subLoss 17324.2 multi 1.00 import weight 0.00
Epoch 171 Acc: 80.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1732 train Loss: 15139.0 test Loss: 2177.0
Epoch 172 Iter 0 subLoss 14984.4 multi -1.99 import weight 0.00
Epoch 172 Iter 1 subLoss 16215.4 multi 1.00 import weight 0.00
Epoch 172 Iter 2 subLoss 15644.8 multi 3.99 import weight 0.00
Epoch 172 Iter 3 subLoss 13867.4 multi 1.00 import weight 0.00
Epoch 172 Iter 4 subLoss 13036.9 multi 1.00 import weight 0.00
Epoch 172 Iter 5 subLoss 11796.2 multi 1.00 import weight 0.00
Epoch 172 Iter 6 subLoss 12050.4 multi 3.99 import weight 0.00
Epoch 172 Iter 7 subLoss 11835.6 multi 3.99 import weight 0.00
Epoch 172 Iter 8 subLoss 18815.7 multi 1.00 import weight 0.00
Epoch 172 Iter 9 subLoss 10923.8 multi 1.00 import weight 0.00
Epoch 172 Iter 10 subLoss 9664.0 multi 1.00 import weight 0.00
Epoch 172 Iter 11 subLoss 8841.0 multi 6.97 import weight 0.00
Epoch 172 Acc: 93.87 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 884 train Loss: 7784.6 test Loss: 916.0
Epoch 173 Iter 0 subLoss 7737.9 multi -1.99 import weight 0.00
Epoch 173 Iter 1 subLoss 8442.7 multi -4.97 import weight 0.00
Epoch 173 Iter 2 subLoss 34724.6 multi 1.00 import weight 0.00
Epoch 173 Iter 3 subLoss 18460.2 multi -1.99 import weight 0.00
Epoch 173 Iter 4 subLoss 145603.6 multi 1.00 import weight 0.00
Epoch 173 Iter 5 subLoss 23376.8 multi 1.00 import weight 0.00
Epoch 173 Iter 6 subLoss 20654.6 multi 1.00 import weight 0.00
Epoch 173 Iter 7 subLoss 16711.9 multi 1.00 import weight 0.00
Epoch 173 Iter 8 subLoss 15469.4 multi 1.00 import weight 0.00
Epoch 173 Iter 9 subLoss 14181.6 multi 1.00 import weight 0.00
Epoch 173 Iter 10 subLoss 12609.4 multi 1.00 import weight 0.00
Epoch 173 Iter 11 subLoss 11913.7 multi 3.99 import weight 0.00
Epoch 173 Acc: 92.47 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1191 train Loss: 9930.4 test Loss: 1219.5
Epoch 174 Iter 0 subLoss 9909.5 multi 3.99 import weight 0.00
Epoch 174 Iter 1 subLoss 8769.4 multi 3.99 import weight 0.00
Epoch 174 Iter 2 subLoss 7812.6 multi 3.99 import weight 0.00
Epoch 174 Iter 3 subLoss 10007.9 multi -1.99 import weight 0.00
Epoch 174 Iter 4 subLoss 57438.4 multi 1.00 import weight 0.00
Epoch 174 Iter 5 subLoss 11474.5 multi 1.00 import weight 0.00
Epoch 174 Iter 6 subLoss 9681.3 multi -1.99 import weight 0.00
Epoch 174 Iter 7 subLoss 11928.7 multi -1.98 import weight 0.00
Epoch 174 Iter 8 subLoss 16308.7 multi 1.00 import weight 0.00
Epoch 174 Iter 9 subLoss 12887.8 multi -1.99 import weight 0.00
Epoch 174 Iter 10 subLoss 17422.5 multi 1.00 import weight 0.00
Epoch 174 Iter 11 subLoss 14625.0 multi 1.00 import weight 0.00
Epoch 174 Acc: 90.64 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1462 train Loss: 12654.5 test Loss: 1763.7
Epoch 175 Iter 0 subLoss 12730.8 multi 3.99 import weight 0.00
Epoch 175 Iter 1 subLoss 9205.4 multi 1.00 import weight 0.00
Epoch 175 Iter 2 subLoss 8196.2 multi 3.99 import weight 0.00
Epoch 175 Iter 3 subLoss 7069.3 multi 1.00 import weight 0.00
Epoch 175 Iter 4 subLoss 6715.5 multi -4.97 import weight 0.00
Epoch 175 Iter 5 subLoss 8172.1 multi 1.00 import weight 0.00
Epoch 175 Iter 6 subLoss 7774.4 multi -4.97 import weight 0.00
Epoch 175 Iter 7 subLoss 8637.4 multi 1.00 import weight 0.00
Epoch 175 Iter 8 subLoss 8339.4 multi 1.00 import weight 0.00
Epoch 175 Iter 9 subLoss 7429.2 multi 1.00 import weight 0.00
Epoch 175 Iter 10 subLoss 7648.3 multi 1.00 import weight 0.00
Epoch 175 Iter 11 subLoss 7547.1 multi -1.99 import weight 0.00
Epoch 175 Acc: 95.21 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 754 train Loss: 7985.5 test Loss: 959.7
Epoch 176 Iter 0 subLoss 7855.5 multi 1.00 import weight 0.00
Epoch 176 Iter 1 subLoss 7762.9 multi 6.97 import weight 0.00
Epoch 176 Iter 2 subLoss 6747.8 multi 1.00 import weight 0.00
Epoch 176 Iter 3 subLoss 6817.5 multi 1.00 import weight 0.00
Epoch 176 Iter 4 subLoss 6463.4 multi -4.97 import weight 0.00
Epoch 176 Iter 5 subLoss 7243.5 multi 3.98 import weight 0.00
Epoch 176 Iter 6 subLoss 5805.4 multi -1.98 import weight 0.00
Epoch 176 Iter 7 subLoss 6727.8 multi -1.98 import weight 0.00
Epoch 176 Iter 8 subLoss 9269.1 multi 1.00 import weight 0.00
Epoch 176 Iter 9 subLoss 6763.2 multi 3.98 import weight 0.00
Epoch 176 Iter 10 subLoss 6448.5 multi -7.96 import weight 0.00
Epoch 176 Iter 11 subLoss 40126.8 multi 1.00 import weight 0.00
Epoch 176 Acc: 91.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4012 train Loss: 9924.7 test Loss: 1446.6
Epoch 177 Iter 0 subLoss 9893.3 multi -4.97 import weight 0.00
Epoch 177 Iter 1 subLoss 27906.2 multi -1.99 import weight 0.00
Epoch 177 Iter 2 subLoss 96396.2 multi 1.00 import weight 0.00
Epoch 177 Iter 3 subLoss 23867.6 multi 1.00 import weight 0.00
Epoch 177 Iter 4 subLoss 19484.8 multi 1.00 import weight 0.00
Epoch 177 Iter 5 subLoss 16015.4 multi 1.00 import weight 0.00
Epoch 177 Iter 6 subLoss 12860.9 multi -1.99 import weight 0.00
Epoch 177 Iter 7 subLoss 17823.1 multi 1.00 import weight 0.00
Epoch 177 Iter 8 subLoss 14769.5 multi 1.00 import weight 0.00
Epoch 177 Iter 9 subLoss 12796.5 multi 1.00 import weight 0.00
Epoch 177 Iter 10 subLoss 10799.0 multi 1.00 import weight 0.00
Epoch 177 Iter 11 subLoss 10381.0 multi 1.00 import weight 0.00
Epoch 177 Acc: 93.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1038 train Loss: 9896.5 test Loss: 1373.5
Epoch 178 Iter 0 subLoss 9414.5 multi 1.00 import weight 0.00
Epoch 178 Iter 1 subLoss 9023.3 multi 3.99 import weight 0.00
Epoch 178 Iter 2 subLoss 8198.8 multi 6.97 import weight 0.00
Epoch 178 Iter 3 subLoss 6271.7 multi -1.98 import weight 0.00
Epoch 178 Iter 4 subLoss 5783.1 multi -1.99 import weight 0.00
Epoch 178 Iter 5 subLoss 7557.9 multi -1.99 import weight 0.00
Epoch 178 Iter 6 subLoss 9689.4 multi 1.00 import weight 0.00
Epoch 178 Iter 7 subLoss 7110.5 multi 3.98 import weight 0.00
Epoch 178 Iter 8 subLoss 6787.2 multi -4.97 import weight 0.00
Epoch 178 Iter 9 subLoss 10460.7 multi 1.00 import weight 0.00
Epoch 178 Iter 10 subLoss 7668.7 multi 1.00 import weight 0.00
Epoch 178 Iter 11 subLoss 6617.3 multi 1.00 import weight 0.00
Epoch 178 Acc: 95.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 661 train Loss: 6840.9 test Loss: 765.0
Epoch 179 Iter 0 subLoss 6461.1 multi -1.99 import weight 0.00
Epoch 179 Iter 1 subLoss 7293.6 multi -4.97 import weight 0.00
Epoch 179 Iter 2 subLoss 10412.8 multi -4.97 import weight 0.00
Epoch 179 Iter 3 subLoss 60040.4 multi 1.00 import weight 0.00
Epoch 179 Iter 4 subLoss 13106.6 multi 1.00 import weight 0.00
Epoch 179 Iter 5 subLoss 11568.2 multi 3.99 import weight 0.00
Epoch 179 Iter 6 subLoss 8429.4 multi 3.99 import weight 0.00
Epoch 179 Iter 7 subLoss 7366.1 multi -1.99 import weight 0.00
Epoch 179 Iter 8 subLoss 8650.1 multi 3.99 import weight 0.00
Epoch 179 Iter 9 subLoss 7147.8 multi 3.99 import weight 0.00
Epoch 179 Iter 10 subLoss 6849.1 multi -4.97 import weight 0.00
Epoch 179 Iter 11 subLoss 7612.5 multi 6.97 import weight 0.00
Epoch 179 Acc: 96.75 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 761 train Loss: 6498.6 test Loss: 596.9
Epoch 180 Iter 0 subLoss 6324.6 multi 1.00 import weight 0.00
Epoch 180 Iter 1 subLoss 6171.0 multi 1.00 import weight 0.00
Epoch 180 Iter 2 subLoss 5998.6 multi 9.96 import weight 0.00
Epoch 180 Iter 3 subLoss 6172.1 multi 3.99 import weight 0.00
Epoch 180 Iter 4 subLoss 5760.0 multi 1.00 import weight 0.00
Epoch 180 Iter 5 subLoss 5027.8 multi 9.96 import weight 0.00
Epoch 180 Iter 6 subLoss 8011.5 multi 1.00 import weight 0.00
Epoch 180 Iter 7 subLoss 4964.5 multi 1.00 import weight 0.00
Epoch 180 Iter 8 subLoss 4548.3 multi 12.94 import weight 0.00
Epoch 180 Iter 9 subLoss 4414.2 multi -7.96 import weight 0.00
Epoch 180 Iter 10 subLoss 9691.1 multi -4.97 import weight 0.00
Epoch 180 Iter 11 subLoss 145948.4 multi 1.00 import weight 0.00
Epoch 180 Acc: 87.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 14594 train Loss: 14216.3 test Loss: 2049.5
Epoch 181 Iter 0 subLoss 13510.6 multi 3.99 import weight 0.00
Epoch 181 Iter 1 subLoss 7827.9 multi -4.97 import weight 0.00
Epoch 181 Iter 2 subLoss 22712.5 multi 1.00 import weight 0.00
Epoch 181 Iter 3 subLoss 9843.8 multi -1.99 import weight 0.00
Epoch 181 Iter 4 subLoss 11466.7 multi 1.00 import weight 0.00
Epoch 181 Iter 5 subLoss 10655.0 multi 1.00 import weight 0.00
Epoch 181 Iter 6 subLoss 9698.0 multi -1.98 import weight 0.00
Epoch 181 Iter 7 subLoss 12206.6 multi 1.00 import weight 0.00
Epoch 181 Iter 8 subLoss 10454.8 multi 1.00 import weight 0.00
Epoch 181 Iter 9 subLoss 9385.8 multi -4.97 import weight 0.00
Epoch 181 Iter 10 subLoss 15775.3 multi 1.00 import weight 0.00
Epoch 181 Iter 11 subLoss 12798.2 multi 3.99 import weight 0.00
Epoch 181 Acc: 95.78 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1279 train Loss: 8327.0 test Loss: 990.4
Epoch 182 Iter 0 subLoss 7871.4 multi -1.99 import weight 0.00
Epoch 182 Iter 1 subLoss 9519.5 multi 6.97 import weight 0.00
Epoch 182 Iter 2 subLoss 5853.0 multi 1.00 import weight 0.00
Epoch 182 Iter 3 subLoss 5874.0 multi 3.99 import weight 0.00
Epoch 182 Iter 4 subLoss 5665.4 multi -4.97 import weight 0.00
Epoch 182 Iter 5 subLoss 6136.9 multi 9.96 import weight 0.00
Epoch 182 Iter 6 subLoss 5458.7 multi 6.97 import weight 0.00
Epoch 182 Iter 7 subLoss 6282.7 multi 1.00 import weight 0.00
Epoch 182 Iter 8 subLoss 4641.4 multi -10.94 import weight 0.00
Epoch 182 Iter 9 subLoss 21895.2 multi 1.00 import weight 0.00
Epoch 182 Iter 10 subLoss 8233.4 multi 3.99 import weight 0.00
Epoch 182 Iter 11 subLoss 5000.8 multi 6.97 import weight 0.00
Epoch 182 Acc: 97.30 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 500 train Loss: 4775.1 test Loss: 507.2
Epoch 183 Iter 0 subLoss 4755.8 multi 6.97 import weight 0.00
Epoch 183 Iter 1 subLoss 5002.9 multi 9.96 import weight 0.00
Epoch 183 Iter 2 subLoss 4445.8 multi -7.96 import weight 0.00
Epoch 183 Iter 3 subLoss 9793.2 multi 1.00 import weight 0.00
Epoch 183 Iter 4 subLoss 6973.4 multi -1.99 import weight 0.00
Epoch 183 Iter 5 subLoss 10278.3 multi 1.00 import weight 0.00
Epoch 183 Iter 6 subLoss 6874.0 multi 1.00 import weight 0.00
Epoch 183 Iter 7 subLoss 6359.0 multi 3.98 import weight 0.00
Epoch 183 Iter 8 subLoss 3848.7 multi 3.98 import weight 0.00
Epoch 183 Iter 9 subLoss 3666.6 multi 9.96 import weight 0.00
Epoch 183 Iter 10 subLoss 3741.5 multi -7.96 import weight 0.00
Epoch 183 Iter 11 subLoss 4457.7 multi -7.96 import weight 0.00
Epoch 183 Acc: 62.54 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 445 train Loss: 30603.1 test Loss: 6680.5
Epoch 184 Iter 0 subLoss 29090.5 multi 1.00 import weight 0.00
Epoch 184 Iter 1 subLoss 4850.2 multi 9.96 import weight 0.00
Epoch 184 Iter 2 subLoss 4988.8 multi 1.00 import weight 0.00
Epoch 184 Iter 3 subLoss 4131.8 multi -4.97 import weight 0.00
Epoch 184 Iter 4 subLoss 7320.0 multi -1.99 import weight 0.00
Epoch 184 Iter 5 subLoss 19398.3 multi 3.99 import weight 0.00
Epoch 184 Iter 6 subLoss 33268.6 multi 1.00 import weight 0.00
Epoch 184 Iter 7 subLoss 15626.4 multi 1.00 import weight 0.00
Epoch 184 Iter 8 subLoss 12941.2 multi 3.99 import weight 0.00
Epoch 184 Iter 9 subLoss 8565.9 multi 1.00 import weight 0.00
Epoch 184 Iter 10 subLoss 8069.7 multi -1.98 import weight 0.00
Epoch 184 Iter 11 subLoss 9432.0 multi 1.00 import weight 0.00
Epoch 184 Acc: 96.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 943 train Loss: 8791.1 test Loss: 1090.2
Epoch 185 Iter 0 subLoss 8668.3 multi -4.97 import weight 0.00
Epoch 185 Iter 1 subLoss 10770.2 multi 1.00 import weight 0.00
Epoch 185 Iter 2 subLoss 9893.2 multi -1.98 import weight 0.00
Epoch 185 Iter 3 subLoss 11491.5 multi 1.00 import weight 0.00
Epoch 185 Iter 4 subLoss 10303.1 multi 1.00 import weight 0.00
Epoch 185 Iter 5 subLoss 10972.7 multi 1.00 import weight 0.00
Epoch 185 Iter 6 subLoss 9150.1 multi 3.98 import weight 0.00
Epoch 185 Iter 7 subLoss 7563.1 multi 3.98 import weight 0.00
Epoch 185 Iter 8 subLoss 6829.4 multi -4.97 import weight 0.00
Epoch 185 Iter 9 subLoss 8514.6 multi 1.00 import weight 0.00
Epoch 185 Iter 10 subLoss 7851.4 multi 3.99 import weight 0.00
Epoch 185 Iter 11 subLoss 6637.8 multi 15.93 import weight 0.00
Epoch 185 Acc: 97.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 663 train Loss: 4721.1 test Loss: 549.3
Epoch 186 Iter 0 subLoss 5188.4 multi 3.99 import weight 0.00
Epoch 186 Iter 1 subLoss 3631.3 multi -1.99 import weight 0.00
Epoch 186 Iter 2 subLoss 4746.3 multi -7.96 import weight 0.00
Epoch 186 Iter 3 subLoss 5540.7 multi 6.97 import weight 0.00
Epoch 186 Iter 4 subLoss 4262.7 multi 3.99 import weight 0.00
Epoch 186 Iter 5 subLoss 4399.1 multi -7.96 import weight 0.00
Epoch 186 Iter 6 subLoss 4628.6 multi 3.99 import weight 0.00
Epoch 186 Iter 7 subLoss 4029.5 multi 6.97 import weight 0.00
Epoch 186 Iter 8 subLoss 4124.4 multi 6.97 import weight 0.00
Epoch 186 Iter 9 subLoss 3625.2 multi -4.97 import weight 0.00
Epoch 186 Iter 10 subLoss 3617.3 multi 9.96 import weight 0.00
Epoch 186 Iter 11 subLoss 3689.9 multi -7.96 import weight 0.00
Epoch 186 Acc: 92.78 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 368 train Loss: 8821.6 test Loss: 1083.7
Epoch 187 Iter 0 subLoss 7954.7 multi -1.99 import weight 0.00
Epoch 187 Iter 1 subLoss 38105.8 multi 1.00 import weight 0.00
Epoch 187 Iter 2 subLoss 4467.6 multi 9.96 import weight 0.00
Epoch 187 Iter 3 subLoss 3743.8 multi -4.97 import weight 0.00
Epoch 187 Iter 4 subLoss 3623.9 multi -4.97 import weight 0.00
Epoch 187 Iter 5 subLoss 4805.2 multi -1.99 import weight 0.00
Epoch 187 Iter 6 subLoss 5687.2 multi -1.98 import weight 0.00
Epoch 187 Iter 7 subLoss 7663.5 multi 3.99 import weight 0.00
Epoch 187 Iter 8 subLoss 4529.0 multi 6.97 import weight 0.00
Epoch 187 Iter 9 subLoss 3470.2 multi 1.00 import weight 0.00
Epoch 187 Iter 10 subLoss 3635.1 multi -4.97 import weight 0.00
Epoch 187 Iter 11 subLoss 3815.7 multi 9.96 import weight 0.00
Epoch 187 Acc: 97.76 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 381 train Loss: 3833.9 test Loss: 383.9
Epoch 188 Iter 0 subLoss 4199.8 multi 15.93 import weight 0.00
Epoch 188 Iter 1 subLoss 3368.5 multi 3.99 import weight 0.00
Epoch 188 Iter 2 subLoss 3226.6 multi 3.99 import weight 0.00
Epoch 188 Iter 3 subLoss 3912.5 multi 1.00 import weight 0.00
Epoch 188 Iter 4 subLoss 3189.8 multi 9.96 import weight 0.00
Epoch 188 Iter 5 subLoss 3716.5 multi 3.99 import weight 0.00
Epoch 188 Iter 6 subLoss 3212.7 multi 1.00 import weight 0.00
Epoch 188 Iter 7 subLoss 3322.4 multi -13.93 import weight 0.00
Epoch 188 Iter 8 subLoss 3504.4 multi 1.00 import weight 0.00
Epoch 188 Iter 9 subLoss 3304.4 multi 12.94 import weight 0.00
Epoch 188 Iter 10 subLoss 3223.5 multi 3.98 import weight 0.00
Epoch 188 Iter 11 subLoss 3554.4 multi -10.94 import weight 0.00
Epoch 188 Acc: 97.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 355 train Loss: 3945.4 test Loss: 354.9
Epoch 189 Iter 0 subLoss 3871.9 multi 9.96 import weight 0.00
Epoch 189 Iter 1 subLoss 3854.2 multi -10.94 import weight 0.00
Epoch 189 Iter 2 subLoss 9897.4 multi 1.00 import weight 0.00
Epoch 189 Iter 3 subLoss 6205.5 multi 1.00 import weight 0.00
Epoch 189 Iter 4 subLoss 5184.7 multi 6.97 import weight 0.00
Epoch 189 Iter 5 subLoss 3564.0 multi 3.98 import weight 0.00
Epoch 189 Iter 6 subLoss 3091.7 multi -4.97 import weight 0.00
Epoch 189 Iter 7 subLoss 3328.3 multi -10.94 import weight 0.00
Epoch 189 Iter 8 subLoss 3922.5 multi -1.98 import weight 0.00
Epoch 189 Iter 9 subLoss 3975.7 multi 15.93 import weight 0.00
Epoch 189 Iter 10 subLoss 5572.9 multi 3.98 import weight 0.00
Epoch 189 Iter 11 subLoss 3530.9 multi -4.97 import weight 0.00
Epoch 189 Acc: 97.88 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 353 train Loss: 4030.0 test Loss: 371.2
Epoch 190 Iter 0 subLoss 4214.7 multi 9.96 import weight 0.00
Epoch 190 Iter 1 subLoss 2944.3 multi 1.00 import weight 0.00
Epoch 190 Iter 2 subLoss 3026.1 multi 1.00 import weight 0.00
Epoch 190 Iter 3 subLoss 3348.6 multi -1.99 import weight 0.00
Epoch 190 Iter 4 subLoss 3192.7 multi -7.96 import weight 0.00
Epoch 190 Iter 5 subLoss 3940.2 multi -1.99 import weight 0.00
Epoch 190 Iter 6 subLoss 4103.6 multi 1.00 import weight 0.00
Epoch 190 Iter 7 subLoss 3951.2 multi 1.00 import weight 0.00
Epoch 190 Iter 8 subLoss 4199.1 multi 18.91 import weight 0.00
Epoch 190 Iter 9 subLoss 5929.2 multi 1.00 import weight 0.00
Epoch 190 Iter 10 subLoss 4123.1 multi 9.96 import weight 0.00
Epoch 190 Iter 11 subLoss 3791.8 multi -7.96 import weight 0.00
Epoch 190 Acc: 93.17 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 379 train Loss: 6160.8 test Loss: 965.9
Epoch 191 Iter 0 subLoss 5692.9 multi -1.98 import weight 0.00
Epoch 191 Iter 1 subLoss 8916.6 multi 3.99 import weight 0.00
Epoch 191 Iter 2 subLoss 3560.1 multi 6.97 import weight 0.00
Epoch 191 Iter 3 subLoss 3230.3 multi -4.97 import weight 0.00
Epoch 191 Iter 4 subLoss 3787.4 multi 12.94 import weight 0.00
Epoch 191 Iter 5 subLoss 3810.4 multi 12.94 import weight 0.00
Epoch 191 Iter 6 subLoss 4785.3 multi 1.00 import weight 0.00
Epoch 191 Iter 7 subLoss 4111.5 multi -10.94 import weight 0.00
Epoch 191 Iter 8 subLoss 12323.3 multi 1.00 import weight 0.00
Epoch 191 Iter 9 subLoss 7763.5 multi 9.96 import weight 0.00
Epoch 191 Iter 10 subLoss 6082.3 multi 6.97 import weight 0.00
Epoch 191 Iter 11 subLoss 3903.1 multi 3.99 import weight 0.00
Epoch 191 Acc: 97.63 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 390 train Loss: 3413.0 test Loss: 391.1
Epoch 192 Iter 0 subLoss 3040.7 multi 1.00 import weight 0.00
Epoch 192 Iter 1 subLoss 3133.5 multi -10.94 import weight 0.00
Epoch 192 Iter 2 subLoss 3985.5 multi -10.94 import weight 0.00
Epoch 192 Iter 3 subLoss 11587.1 multi 1.00 import weight 0.00
Epoch 192 Iter 4 subLoss 6970.4 multi 1.00 import weight 0.00
Epoch 192 Iter 5 subLoss 6502.5 multi -4.97 import weight 0.00
Epoch 192 Iter 6 subLoss 15137.7 multi 1.00 import weight 0.00
Epoch 192 Iter 7 subLoss 9188.2 multi -1.99 import weight 0.00
Epoch 192 Iter 8 subLoss 13898.2 multi 1.00 import weight 0.00
Epoch 192 Iter 9 subLoss 8877.5 multi -1.98 import weight 0.00
Epoch 192 Iter 10 subLoss 16231.2 multi 3.99 import weight 0.00
Epoch 192 Iter 11 subLoss 5868.8 multi -10.94 import weight 0.00
Epoch 192 Acc: 93.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 586 train Loss: 10646.3 test Loss: 1540.1
Epoch 193 Iter 0 subLoss 10458.3 multi 3.99 import weight 0.00
Epoch 193 Iter 1 subLoss 6128.0 multi -1.99 import weight 0.00
Epoch 193 Iter 2 subLoss 7112.9 multi 6.97 import weight 0.00
Epoch 193 Iter 3 subLoss 4932.2 multi 6.97 import weight 0.00
Epoch 193 Iter 4 subLoss 3925.2 multi 1.00 import weight 0.00
Epoch 193 Iter 5 subLoss 3624.0 multi -1.99 import weight 0.00
Epoch 193 Iter 6 subLoss 3840.5 multi 6.97 import weight 0.00
Epoch 193 Iter 7 subLoss 3838.6 multi 1.00 import weight 0.00
Epoch 193 Iter 8 subLoss 3350.3 multi 1.00 import weight 0.00
Epoch 193 Iter 9 subLoss 3267.0 multi -1.98 import weight 0.00
Epoch 193 Iter 10 subLoss 2994.1 multi -10.94 import weight 0.00
Epoch 193 Iter 11 subLoss 4086.1 multi 1.00 import weight 0.00
Epoch 193 Acc: 97.80 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 408 train Loss: 3884.6 test Loss: 432.7
Epoch 194 Iter 0 subLoss 3490.3 multi -1.99 import weight 0.00
Epoch 194 Iter 1 subLoss 4432.3 multi 6.97 import weight 0.00
Epoch 194 Iter 2 subLoss 3250.7 multi -1.99 import weight 0.00
Epoch 194 Iter 3 subLoss 3342.6 multi 1.00 import weight 0.00
Epoch 194 Iter 4 subLoss 3296.9 multi -4.97 import weight 0.00
Epoch 194 Iter 5 subLoss 3670.6 multi -7.96 import weight 0.00
Epoch 194 Iter 6 subLoss 4484.4 multi 3.98 import weight 0.00
Epoch 194 Iter 7 subLoss 3774.2 multi -1.98 import weight 0.00
Epoch 194 Iter 8 subLoss 4177.5 multi -7.96 import weight 0.00
Epoch 194 Iter 9 subLoss 5146.0 multi 6.97 import weight 0.00
Epoch 194 Iter 10 subLoss 4076.4 multi 6.97 import weight 0.00
Epoch 194 Iter 11 subLoss 3440.0 multi 1.00 import weight 0.00
Epoch 194 Acc: 97.96 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 343 train Loss: 3620.7 test Loss: 379.7
Epoch 195 Iter 0 subLoss 3591.6 multi 15.93 import weight 0.00
Epoch 195 Iter 1 subLoss 3236.0 multi -1.99 import weight 0.00
Epoch 195 Iter 2 subLoss 3087.3 multi 9.96 import weight 0.00
Epoch 195 Iter 3 subLoss 2876.5 multi -10.94 import weight 0.00
Epoch 195 Iter 4 subLoss 3674.1 multi -4.97 import weight 0.00
Epoch 195 Iter 5 subLoss 4360.8 multi 3.98 import weight 0.00
Epoch 195 Iter 6 subLoss 3251.3 multi 1.00 import weight 0.00
Epoch 195 Iter 7 subLoss 3300.6 multi 12.94 import weight 0.00
Epoch 195 Iter 8 subLoss 3074.8 multi -1.98 import weight 0.00
Epoch 195 Iter 9 subLoss 3635.5 multi -4.97 import weight 0.00
Epoch 195 Iter 10 subLoss 4292.5 multi -7.96 import weight 0.00
Epoch 195 Iter 11 subLoss 5856.1 multi 3.99 import weight 0.00
Epoch 195 Acc: 97.88 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 585 train Loss: 3467.2 test Loss: 371.3
Epoch 196 Iter 0 subLoss 2968.4 multi -7.96 import weight 0.00
Epoch 196 Iter 1 subLoss 3784.6 multi 12.94 import weight 0.00
Epoch 196 Iter 2 subLoss 3490.3 multi 1.00 import weight 0.00
Epoch 196 Iter 3 subLoss 2964.1 multi -4.97 import weight 0.00
Epoch 196 Iter 4 subLoss 4096.8 multi -1.99 import weight 0.00
Epoch 196 Iter 5 subLoss 4747.4 multi -4.97 import weight 0.00
Epoch 196 Iter 6 subLoss 11166.5 multi 1.00 import weight 0.00
Epoch 196 Iter 7 subLoss 5931.6 multi 1.00 import weight 0.00
Epoch 196 Iter 8 subLoss 5662.1 multi -1.99 import weight 0.00
Epoch 196 Iter 9 subLoss 8038.6 multi -7.96 import weight 0.00
Epoch 196 Iter 10 subLoss 88087.9 multi 1.00 import weight 0.00
Epoch 196 Iter 11 subLoss 12947.9 multi 6.97 import weight 0.00
Epoch 196 Acc: 95.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 1294 train Loss: 5475.2 test Loss: 724.0
Epoch 197 Iter 0 subLoss 5779.3 multi -1.98 import weight 0.00
Epoch 197 Iter 1 subLoss 6008.6 multi -7.96 import weight 0.00
Epoch 197 Iter 2 subLoss 25317.8 multi 1.00 import weight 0.00
Epoch 197 Iter 3 subLoss 11178.1 multi 1.00 import weight 0.00
Epoch 197 Iter 4 subLoss 8777.2 multi -4.97 import weight 0.00
Epoch 197 Iter 5 subLoss 19369.4 multi 3.99 import weight 0.00
Epoch 197 Iter 6 subLoss 7655.6 multi 3.98 import weight 0.00
Epoch 197 Iter 7 subLoss 5696.7 multi 1.00 import weight 0.00
Epoch 197 Iter 8 subLoss 4889.5 multi -1.98 import weight 0.00
Epoch 197 Iter 9 subLoss 5686.3 multi 1.00 import weight 0.00
Epoch 197 Iter 10 subLoss 5376.8 multi 6.97 import weight 0.00
Epoch 197 Iter 11 subLoss 4953.5 multi -7.96 import weight 0.00
Epoch 197 Acc: 96.44 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 495 train Loss: 5326.5 test Loss: 601.5
Epoch 198 Iter 0 subLoss 5215.4 multi -4.97 import weight 0.00
Epoch 198 Iter 1 subLoss 6604.1 multi -7.96 import weight 0.00
Epoch 198 Iter 2 subLoss 18126.7 multi 1.00 import weight 0.00
Epoch 198 Iter 3 subLoss 10077.2 multi 1.00 import weight 0.00
Epoch 198 Iter 4 subLoss 8764.4 multi 6.97 import weight 0.00
Epoch 198 Iter 5 subLoss 5669.8 multi 1.00 import weight 0.00
Epoch 198 Iter 6 subLoss 5340.3 multi 1.00 import weight 0.00
Epoch 198 Iter 7 subLoss 4790.9 multi 6.97 import weight 0.00
Epoch 198 Iter 8 subLoss 4619.6 multi 3.99 import weight 0.00
Epoch 198 Iter 9 subLoss 3968.5 multi -4.97 import weight 0.00
Epoch 198 Iter 10 subLoss 4167.9 multi 9.96 import weight 0.00
Epoch 198 Iter 11 subLoss 3684.8 multi -10.94 import weight 0.00
Epoch 198 Acc: 97.14 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 368 train Loss: 4600.2 test Loss: 499.1
Epoch 199 Iter 0 subLoss 4300.0 multi -4.97 import weight 0.00
Epoch 199 Iter 1 subLoss 5312.1 multi 6.97 import weight 0.00
Epoch 199 Iter 2 subLoss 3810.8 multi 15.93 import weight 0.00
Epoch 199 Iter 3 subLoss 4021.2 multi 9.96 import weight 0.00
Epoch 199 Iter 4 subLoss 3611.1 multi 12.94 import weight 0.00
Epoch 199 Iter 5 subLoss 3247.7 multi 1.00 import weight 0.00
Epoch 199 Iter 6 subLoss 3641.3 multi -7.96 import weight 0.00
Epoch 199 Iter 7 subLoss 4004.7 multi 3.98 import weight 0.00
Epoch 199 Iter 8 subLoss 3288.7 multi 1.00 import weight 0.00
Epoch 199 Iter 9 subLoss 3303.5 multi 15.93 import weight 0.00
Epoch 199 Iter 10 subLoss 3882.6 multi -1.99 import weight 0.00
Epoch 199 Iter 11 subLoss 3717.5 multi 6.97 import weight 0.00
Epoch 199 Acc: 98.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 371 train Loss: 3213.5 test Loss: 319.9
Epoch 200 Iter 0 subLoss 3742.4 multi -1.98 import weight 0.00
Epoch 200 Iter 1 subLoss 3180.5 multi 12.94 import weight 0.00
Epoch 200 Iter 2 subLoss 3260.9 multi -4.97 import weight 0.00
Epoch 200 Iter 3 subLoss 4714.2 multi -7.96 import weight 0.00
Epoch 200 Iter 4 subLoss 15231.9 multi 1.00 import weight 0.00
Epoch 200 Iter 5 subLoss 5506.7 multi -4.97 import weight 0.00
Epoch 200 Iter 6 subLoss 17942.7 multi 1.00 import weight 0.00
Epoch 200 Iter 7 subLoss 6187.8 multi -10.94 import weight 0.00
Epoch 200 Iter 8 subLoss 62182.1 multi 1.00 import weight 0.00
Epoch 200 Iter 9 subLoss 16219.0 multi 3.99 import weight 0.00
Epoch 200 Iter 10 subLoss 7464.5 multi -1.99 import weight 0.00
Epoch 200 Iter 11 subLoss 9731.9 multi 3.99 import weight 0.00
Epoch 200 Acc: 91.77 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 973 train Loss: 5744.9 test Loss: 1104.1
Epoch 201 Iter 0 subLoss 5736.4 multi 3.99 import weight 0.00
Epoch 201 Iter 1 subLoss 4573.5 multi 6.97 import weight 0.00
Epoch 201 Iter 2 subLoss 3487.7 multi 1.00 import weight 0.00
Epoch 201 Iter 3 subLoss 3440.0 multi 18.91 import weight 1.00
Epoch 201 Iter 4 subLoss 4443.3 multi -7.96 import weight 0.00
Epoch 201 Iter 5 subLoss 4621.4 multi 3.98 import weight 0.00
Epoch 201 Iter 6 subLoss 3028.2 multi 3.99 import weight 0.00
Epoch 201 Iter 7 subLoss 3291.1 multi -4.97 import weight 0.00
Epoch 201 Iter 8 subLoss 3363.7 multi 3.98 import weight 0.00
Epoch 201 Iter 9 subLoss 3433.5 multi 3.99 import weight 0.00
Epoch 201 Iter 10 subLoss 3257.5 multi 1.00 import weight 0.00
Epoch 201 Iter 11 subLoss 3009.4 multi 15.93 import weight 0.00
Epoch 201 Acc: 97.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 300 train Loss: 3259.1 test Loss: 335.8
Epoch 202 Iter 0 subLoss 3447.6 multi 18.91 import weight 1.00
Epoch 202 Iter 1 subLoss 3806.6 multi -1.99 import weight 0.00
Epoch 202 Iter 2 subLoss 4152.8 multi -7.96 import weight 0.00
Epoch 202 Iter 3 subLoss 8172.6 multi 3.99 import weight 0.00
Epoch 202 Iter 4 subLoss 3909.6 multi 6.97 import weight 0.00
Epoch 202 Iter 5 subLoss 3193.6 multi -7.96 import weight 0.00
Epoch 202 Iter 6 subLoss 3268.1 multi -4.97 import weight 0.00
Epoch 202 Iter 7 subLoss 3287.5 multi 3.99 import weight 0.00
Epoch 202 Iter 8 subLoss 3948.0 multi 1.00 import weight 0.00
Epoch 202 Iter 9 subLoss 4039.9 multi -13.93 import weight 0.00
Epoch 202 Iter 10 subLoss 4574.8 multi 9.96 import weight 0.00
Epoch 202 Iter 11 subLoss 3225.1 multi 6.97 import weight 0.00
Epoch 202 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 322 train Loss: 3148.1 test Loss: 336.7
Epoch 203 Iter 0 subLoss 3403.0 multi 1.00 import weight 0.00
Epoch 203 Iter 1 subLoss 3074.0 multi 1.00 import weight 0.00
Epoch 203 Iter 2 subLoss 3128.9 multi -1.99 import weight 0.00
Epoch 203 Iter 3 subLoss 2720.8 multi -1.99 import weight 0.00
Epoch 203 Iter 4 subLoss 3333.8 multi 3.99 import weight 0.00
Epoch 203 Iter 5 subLoss 2769.7 multi 3.99 import weight 0.00
Epoch 203 Iter 6 subLoss 3197.2 multi -4.97 import weight 0.00
Epoch 203 Iter 7 subLoss 3541.5 multi 15.93 import weight 0.00
Epoch 203 Iter 8 subLoss 2936.3 multi -1.99 import weight 0.00
Epoch 203 Iter 9 subLoss 2912.2 multi 3.99 import weight 0.00
Epoch 203 Iter 10 subLoss 3136.9 multi -10.94 import weight 0.00
Epoch 203 Iter 11 subLoss 3549.2 multi 18.91 import weight 0.00
Epoch 203 Acc: 98.15 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 354 train Loss: 3547.7 test Loss: 334.2
Epoch 204 Iter 0 subLoss 4274.6 multi -1.99 import weight 0.00
Epoch 204 Iter 1 subLoss 4003.7 multi 6.97 import weight 0.00
Epoch 204 Iter 2 subLoss 2722.9 multi 1.00 import weight 0.00
Epoch 204 Iter 3 subLoss 3041.0 multi 3.99 import weight 0.00
Epoch 204 Iter 4 subLoss 2534.9 multi -4.97 import weight 0.00
Epoch 204 Iter 5 subLoss 2993.4 multi -7.96 import weight 0.00
Epoch 204 Iter 6 subLoss 3312.8 multi -7.96 import weight 0.00
Epoch 204 Iter 7 subLoss 3500.2 multi -1.98 import weight 0.00
Epoch 204 Iter 8 subLoss 3738.3 multi 9.96 import weight 0.00
Epoch 204 Iter 9 subLoss 3174.9 multi 1.00 import weight 0.00
Epoch 204 Iter 10 subLoss 3044.4 multi 6.97 import weight 0.00
Epoch 204 Iter 11 subLoss 2591.8 multi -4.97 import weight 0.00
Epoch 204 Acc: 98.11 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 259 train Loss: 2999.0 test Loss: 313.3
Epoch 205 Iter 0 subLoss 3244.6 multi 3.99 import weight 0.00
Epoch 205 Iter 1 subLoss 3280.9 multi 6.97 import weight 0.00
Epoch 205 Iter 2 subLoss 3002.4 multi 15.93 import weight 0.00
Epoch 205 Iter 3 subLoss 3131.8 multi -7.96 import weight 0.00
Epoch 205 Iter 4 subLoss 2870.2 multi -7.96 import weight 0.00
Epoch 205 Iter 5 subLoss 4396.7 multi -4.97 import weight 0.00
Epoch 205 Iter 6 subLoss 17913.0 multi 1.00 import weight 0.00
Epoch 205 Iter 7 subLoss 3802.1 multi 1.00 import weight 0.00
Epoch 205 Iter 8 subLoss 3736.2 multi 12.94 import weight 0.00
Epoch 205 Iter 9 subLoss 2967.1 multi -1.98 import weight 0.00
Epoch 205 Iter 10 subLoss 3247.7 multi 6.97 import weight 0.00
Epoch 205 Iter 11 subLoss 2713.1 multi 3.99 import weight 0.00
Epoch 205 Acc: 98.07 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 271 train Loss: 2887.3 test Loss: 312.8
Epoch 206 Iter 0 subLoss 3039.6 multi -7.96 import weight 0.00
Epoch 206 Iter 1 subLoss 3261.6 multi -1.98 import weight 0.00
Epoch 206 Iter 2 subLoss 2757.7 multi -16.91 import weight 0.00
Epoch 206 Iter 3 subLoss 3563.0 multi 9.96 import weight 0.00
Epoch 206 Iter 4 subLoss 3065.5 multi 1.00 import weight 0.00
Epoch 206 Iter 5 subLoss 2703.4 multi 3.98 import weight 0.00
Epoch 206 Iter 6 subLoss 3282.0 multi 9.96 import weight 0.00
Epoch 206 Iter 7 subLoss 2894.7 multi 12.94 import weight 0.00
Epoch 206 Iter 8 subLoss 2924.9 multi 6.97 import weight 0.00
Epoch 206 Iter 9 subLoss 2917.4 multi 6.97 import weight 0.00
Epoch 206 Iter 10 subLoss 2573.5 multi -7.96 import weight 0.00
Epoch 206 Iter 11 subLoss 3041.3 multi 6.97 import weight 0.00
Epoch 206 Acc: 98.13 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 304 train Loss: 2830.3 test Loss: 291.9
Epoch 207 Iter 0 subLoss 2698.3 multi -1.98 import weight 0.00
Epoch 207 Iter 1 subLoss 2781.3 multi 6.97 import weight 0.00
Epoch 207 Iter 2 subLoss 2606.2 multi -4.97 import weight 0.00
Epoch 207 Iter 3 subLoss 2222.1 multi 9.96 import weight 0.00
Epoch 207 Iter 4 subLoss 2446.9 multi 3.98 import weight 0.00
Epoch 207 Iter 5 subLoss 2643.4 multi 1.00 import weight 0.00
Epoch 207 Iter 6 subLoss 2463.5 multi 3.99 import weight 0.00
Epoch 207 Iter 7 subLoss 2477.4 multi 12.94 import weight 0.00
Epoch 207 Iter 8 subLoss 2667.1 multi 15.93 import weight 0.00
Epoch 207 Iter 9 subLoss 3609.6 multi -13.93 import weight 0.00
Epoch 207 Iter 10 subLoss 9308.6 multi 3.99 import weight 0.00
Epoch 207 Iter 11 subLoss 2618.1 multi -1.98 import weight 0.00
Epoch 207 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 261 train Loss: 3215.5 test Loss: 320.5
Epoch 208 Iter 0 subLoss 2714.7 multi 3.99 import weight 0.00
Epoch 208 Iter 1 subLoss 2399.8 multi 21.90 import weight 0.00
Epoch 208 Iter 2 subLoss 3231.2 multi -1.99 import weight 0.00
Epoch 208 Iter 3 subLoss 3572.1 multi -13.93 import weight 0.00
Epoch 208 Iter 4 subLoss 135922.6 multi 1.00 import weight 0.00
Epoch 208 Iter 5 subLoss 5863.3 multi -10.94 import weight 0.00
Epoch 208 Iter 6 subLoss 21843.0 multi 1.00 import weight 0.00
Epoch 208 Iter 7 subLoss 13206.5 multi -1.99 import weight 0.00
Epoch 208 Iter 8 subLoss 22465.1 multi 1.00 import weight 0.00
Epoch 208 Iter 9 subLoss 13823.7 multi 1.00 import weight 0.00
Epoch 208 Iter 10 subLoss 11084.5 multi 1.00 import weight 0.00
Epoch 208 Iter 11 subLoss 9247.6 multi -1.99 import weight 0.00
Epoch 208 Acc: 83.69 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 924 train Loss: 11905.8 test Loss: 2137.9
Epoch 209 Iter 0 subLoss 12614.7 multi -1.99 import weight 0.00
Epoch 209 Iter 1 subLoss 16999.6 multi 1.00 import weight 0.00
Epoch 209 Iter 2 subLoss 12522.8 multi 1.00 import weight 0.00
Epoch 209 Iter 3 subLoss 10503.7 multi 1.00 import weight 0.00
Epoch 209 Iter 4 subLoss 9625.3 multi 1.00 import weight 0.00
Epoch 209 Iter 5 subLoss 8529.8 multi -1.99 import weight 0.00
Epoch 209 Iter 6 subLoss 10642.5 multi 3.99 import weight 0.00
Epoch 209 Iter 7 subLoss 6803.7 multi -1.98 import weight 0.00
Epoch 209 Iter 8 subLoss 7694.7 multi 1.00 import weight 0.00
Epoch 209 Iter 9 subLoss 7028.9 multi 1.00 import weight 0.00
Epoch 209 Iter 10 subLoss 7002.5 multi 1.00 import weight 0.00
Epoch 209 Iter 11 subLoss 6175.8 multi 6.97 import weight 0.00
Epoch 209 Acc: 97.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 617 train Loss: 4739.2 test Loss: 639.9
Epoch 210 Iter 0 subLoss 4214.1 multi 12.94 import weight 0.00
Epoch 210 Iter 1 subLoss 3495.2 multi 1.00 import weight 0.00
Epoch 210 Iter 2 subLoss 2898.1 multi 15.93 import weight 0.00
Epoch 210 Iter 3 subLoss 2743.3 multi 15.93 import weight 0.00
Epoch 210 Iter 4 subLoss 3173.5 multi 3.99 import weight 0.00
Epoch 210 Iter 5 subLoss 3197.6 multi -1.99 import weight 0.00
Epoch 210 Iter 6 subLoss 3175.9 multi 6.97 import weight 0.00
Epoch 210 Iter 7 subLoss 2799.7 multi -1.99 import weight 0.00
Epoch 210 Iter 8 subLoss 2891.6 multi 18.91 import weight 0.00
Epoch 210 Iter 9 subLoss 3088.4 multi 6.97 import weight 0.00
Epoch 210 Iter 10 subLoss 2476.6 multi 15.93 import weight 0.00
Epoch 210 Iter 11 subLoss 2708.3 multi 3.99 import weight 0.00
Epoch 210 Acc: 98.50 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 270 train Loss: 2644.7 test Loss: 259.8
Epoch 211 Iter 0 subLoss 2239.2 multi -7.96 import weight 0.00
Epoch 211 Iter 1 subLoss 2688.8 multi -4.97 import weight 0.00
Epoch 211 Iter 2 subLoss 3129.0 multi 1.00 import weight 0.00
Epoch 211 Iter 3 subLoss 3479.5 multi 3.99 import weight 0.00
Epoch 211 Iter 4 subLoss 2234.0 multi -4.97 import weight 0.00
Epoch 211 Iter 5 subLoss 2684.6 multi -1.99 import weight 0.00
Epoch 211 Iter 6 subLoss 2760.8 multi 3.99 import weight 0.00
Epoch 211 Iter 7 subLoss 3401.4 multi 3.98 import weight 0.00
Epoch 211 Iter 8 subLoss 2491.1 multi 9.96 import weight 0.00
Epoch 211 Iter 9 subLoss 2738.0 multi -10.94 import weight 0.00
Epoch 211 Iter 10 subLoss 2475.2 multi 18.91 import weight 1.00
Epoch 211 Iter 11 subLoss 2569.7 multi 12.94 import weight 0.00
Epoch 211 Acc: 98.56 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 256 train Loss: 2729.0 test Loss: 261.9
Epoch 212 Iter 0 subLoss 3131.8 multi -7.96 import weight 0.00
Epoch 212 Iter 1 subLoss 3652.3 multi -4.97 import weight 0.00
Epoch 212 Iter 2 subLoss 5041.4 multi 6.97 import weight 0.00
Epoch 212 Iter 3 subLoss 3094.0 multi -7.96 import weight 0.00
Epoch 212 Iter 4 subLoss 3998.4 multi -10.94 import weight 0.00
Epoch 212 Iter 5 subLoss 9983.9 multi 1.00 import weight 0.00
Epoch 212 Iter 6 subLoss 8105.2 multi -4.97 import weight 0.00
Epoch 212 Iter 7 subLoss 26421.1 multi 3.99 import weight 0.00
Epoch 212 Iter 8 subLoss 5114.3 multi 9.96 import weight 0.00
Epoch 212 Iter 9 subLoss 2764.5 multi 6.97 import weight 0.00
Epoch 212 Iter 10 subLoss 2470.6 multi 21.90 import weight 1.00
Epoch 212 Iter 11 subLoss 3443.3 multi 21.90 import weight 1.00
Epoch 212 Acc: 98.29 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 21.90 Pidx 344 train Loss: 2777.2 test Loss: 308.0
Epoch 213 Iter 0 subLoss 2528.5 multi -10.94 import weight 0.00
Epoch 213 Iter 1 subLoss 3427.5 multi -1.99 import weight 0.00
Epoch 213 Iter 2 subLoss 3948.6 multi 3.99 import weight 0.00
Epoch 213 Iter 3 subLoss 3289.8 multi 12.94 import weight 0.00
Epoch 213 Iter 4 subLoss 3155.6 multi -1.99 import weight 0.00
Epoch 213 Iter 5 subLoss 2733.8 multi -7.96 import weight 0.00
Epoch 213 Iter 6 subLoss 3738.6 multi 15.93 import weight 0.00
Epoch 213 Iter 7 subLoss 3309.9 multi 15.93 import weight 0.00
Epoch 213 Iter 8 subLoss 3214.4 multi 3.99 import weight 0.00
Epoch 213 Iter 9 subLoss 2145.3 multi -1.99 import weight 0.00
Epoch 213 Iter 10 subLoss 2780.1 multi 9.96 import weight 0.00
Epoch 213 Iter 11 subLoss 2585.2 multi 6.97 import weight 0.00
Epoch 213 Acc: 98.33 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 258 train Loss: 2484.5 test Loss: 272.5
Epoch 214 Iter 0 subLoss 2436.4 multi 12.94 import weight 0.00
Epoch 214 Iter 1 subLoss 2589.6 multi 9.96 import weight 0.00
Epoch 214 Iter 2 subLoss 2498.8 multi 12.94 import weight 0.00
Epoch 214 Iter 3 subLoss 2253.9 multi 1.00 import weight 0.00
Epoch 214 Iter 4 subLoss 2299.5 multi 6.97 import weight 0.00
Epoch 214 Iter 5 subLoss 2335.2 multi 12.94 import weight 0.00
Epoch 214 Iter 6 subLoss 2597.1 multi -7.96 import weight 0.00
Epoch 214 Iter 7 subLoss 3230.3 multi 1.00 import weight 0.00
Epoch 214 Iter 8 subLoss 2227.2 multi 12.94 import weight 0.00
Epoch 214 Iter 9 subLoss 2492.4 multi 15.93 import weight 0.00
Epoch 214 Iter 10 subLoss 2902.1 multi -22.88 import weight 0.00
Epoch 214 Iter 11 subLoss 18514.4 multi -1.99 import weight 0.00
Epoch 214 Acc: 51.04 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1851 train Loss: 155593.8 test Loss: 22366.1
Epoch 215 Iter 0 subLoss 151466.8 multi 1.00 import weight 0.00
Epoch 215 Iter 1 subLoss 11846.8 multi -4.97 import weight 0.00
Epoch 215 Iter 2 subLoss 138535.7 multi 1.00 import weight 0.00
Epoch 215 Iter 3 subLoss 15819.8 multi 3.99 import weight 0.00
Epoch 215 Iter 4 subLoss 8712.0 multi 1.00 import weight 0.00
Epoch 215 Iter 5 subLoss 7758.4 multi -7.96 import weight 0.00
Epoch 215 Iter 6 subLoss 17026.6 multi 1.00 import weight 0.00
Epoch 215 Iter 7 subLoss 15439.0 multi 3.99 import weight 0.00
Epoch 215 Iter 8 subLoss 8693.1 multi 6.97 import weight 0.00
Epoch 215 Iter 9 subLoss 4935.9 multi 9.96 import weight 0.00
Epoch 215 Iter 10 subLoss 3210.4 multi 6.97 import weight 0.00
Epoch 215 Iter 11 subLoss 2972.1 multi 1.00 import weight 0.00
Epoch 215 Acc: 97.92 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 297 train Loss: 3112.4 test Loss: 367.9
Epoch 216 Iter 0 subLoss 2807.8 multi -10.94 import weight 0.00
Epoch 216 Iter 1 subLoss 3700.8 multi 1.00 import weight 0.00
Epoch 216 Iter 2 subLoss 3125.5 multi 3.99 import weight 0.00
Epoch 216 Iter 3 subLoss 2916.4 multi 6.97 import weight 0.00
Epoch 216 Iter 4 subLoss 2803.1 multi -7.96 import weight 0.00
Epoch 216 Iter 5 subLoss 3216.4 multi 9.96 import weight 0.00
Epoch 216 Iter 6 subLoss 3043.8 multi 9.96 import weight 0.00
Epoch 216 Iter 7 subLoss 2457.6 multi -10.94 import weight 0.00
Epoch 216 Iter 8 subLoss 2939.3 multi -1.99 import weight 0.00
Epoch 216 Iter 9 subLoss 3255.2 multi -1.99 import weight 0.00
Epoch 216 Iter 10 subLoss 4109.1 multi 1.00 import weight 0.00
Epoch 216 Iter 11 subLoss 3110.3 multi 3.99 import weight 0.00
Epoch 216 Acc: 97.82 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 311 train Loss: 3123.3 test Loss: 375.8
Epoch 217 Iter 0 subLoss 3304.8 multi 18.91 import weight 0.00
Epoch 217 Iter 1 subLoss 2937.5 multi 1.00 import weight 0.00
Epoch 217 Iter 2 subLoss 2591.9 multi -4.97 import weight 0.00
Epoch 217 Iter 3 subLoss 3408.9 multi 6.97 import weight 0.00
Epoch 217 Iter 4 subLoss 2692.7 multi -4.97 import weight 0.00
Epoch 217 Iter 5 subLoss 3066.1 multi 3.99 import weight 0.00
Epoch 217 Iter 6 subLoss 2771.2 multi -4.97 import weight 0.00
Epoch 217 Iter 7 subLoss 2922.6 multi 3.99 import weight 0.00
Epoch 217 Iter 8 subLoss 2915.3 multi 9.96 import weight 0.00
Epoch 217 Iter 9 subLoss 2907.7 multi -19.90 import weight 0.00
Epoch 217 Iter 10 subLoss 3465.9 multi 3.99 import weight 0.00
Epoch 217 Iter 11 subLoss 2463.4 multi 3.99 import weight 0.00
Epoch 217 Acc: 97.92 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 246 train Loss: 2839.7 test Loss: 347.3
Epoch 218 Iter 0 subLoss 2813.9 multi 12.94 import weight 0.00
Epoch 218 Iter 1 subLoss 2541.4 multi 15.93 import weight 0.00
Epoch 218 Iter 2 subLoss 2346.6 multi -7.96 import weight 0.00
Epoch 218 Iter 3 subLoss 2991.9 multi -4.97 import weight 0.00
Epoch 218 Iter 4 subLoss 3295.4 multi -13.93 import weight 0.00
Epoch 218 Iter 5 subLoss 7574.5 multi -7.96 import weight 0.00
Epoch 218 Iter 6 subLoss 69330.0 multi 1.00 import weight 0.00
Epoch 218 Iter 7 subLoss 14821.1 multi 1.00 import weight 0.00
Epoch 218 Iter 8 subLoss 10077.4 multi 3.99 import weight 0.00
Epoch 218 Iter 9 subLoss 5529.4 multi 3.99 import weight 0.00
Epoch 218 Iter 10 subLoss 4382.3 multi 6.97 import weight 0.00
Epoch 218 Iter 11 subLoss 3219.5 multi 12.94 import weight 0.00
Epoch 218 Acc: 98.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 321 train Loss: 2858.1 test Loss: 319.0
Epoch 219 Iter 0 subLoss 2679.4 multi -4.97 import weight 0.00
Epoch 219 Iter 1 subLoss 2619.4 multi 1.00 import weight 0.00
Epoch 219 Iter 2 subLoss 2740.3 multi 12.94 import weight 0.00
Epoch 219 Iter 3 subLoss 2504.2 multi -25.87 import weight 0.00
Epoch 219 Iter 4 subLoss 3552.5 multi -13.93 import weight 0.00
Epoch 219 Iter 5 subLoss 16885.4 multi 1.00 import weight 0.00
Epoch 219 Iter 6 subLoss 7006.9 multi 3.98 import weight 0.00
Epoch 219 Iter 7 subLoss 3880.0 multi 1.00 import weight 0.00
Epoch 219 Iter 8 subLoss 3598.2 multi 18.91 import weight 0.00
Epoch 219 Iter 9 subLoss 2835.1 multi 9.96 import weight 0.00
Epoch 219 Iter 10 subLoss 2925.3 multi 3.98 import weight 0.00
Epoch 219 Iter 11 subLoss 2608.6 multi -7.96 import weight 0.00
Epoch 219 Acc: 98.19 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 260 train Loss: 2816.8 test Loss: 327.1
Epoch 220 Iter 0 subLoss 2652.0 multi 1.00 import weight 0.00
Epoch 220 Iter 1 subLoss 2361.5 multi -1.98 import weight 0.00
Epoch 220 Iter 2 subLoss 2478.1 multi 21.90 import weight 1.00
Epoch 220 Iter 3 subLoss 2662.4 multi 15.93 import weight 0.00
Epoch 220 Iter 4 subLoss 2866.9 multi -7.96 import weight 0.00
Epoch 220 Iter 5 subLoss 2953.3 multi -1.99 import weight 0.00
Epoch 220 Iter 6 subLoss 3064.0 multi 6.97 import weight 0.00
Epoch 220 Iter 7 subLoss 2659.4 multi 3.99 import weight 0.00
Epoch 220 Iter 8 subLoss 2638.5 multi 1.00 import weight 0.00
Epoch 220 Iter 9 subLoss 2903.4 multi -16.91 import weight 0.00
Epoch 220 Iter 10 subLoss 3028.5 multi 6.97 import weight 0.00
Epoch 220 Iter 11 subLoss 2204.4 multi -1.99 import weight 0.00
Epoch 220 Acc: 98.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 220 train Loss: 2784.4 test Loss: 302.0
Epoch 221 Iter 0 subLoss 2450.4 multi -7.96 import weight 0.00
Epoch 221 Iter 1 subLoss 3036.9 multi -7.96 import weight 0.00
Epoch 221 Iter 2 subLoss 3275.4 multi -13.93 import weight 0.00
Epoch 221 Iter 3 subLoss 23723.9 multi 1.00 import weight 0.00
Epoch 221 Iter 4 subLoss 3606.5 multi -13.93 import weight 0.00
Epoch 221 Iter 5 subLoss 9950.7 multi 3.99 import weight 0.00
Epoch 221 Iter 6 subLoss 3447.2 multi 24.88 import weight 1.00
Epoch 221 Iter 7 subLoss 2991.7 multi -1.99 import weight 0.00
Epoch 221 Iter 8 subLoss 3647.4 multi -4.97 import weight 0.00
Epoch 221 Iter 9 subLoss 6289.3 multi 3.99 import weight 0.00
Epoch 221 Iter 10 subLoss 3125.2 multi 3.98 import weight 0.00
Epoch 221 Iter 11 subLoss 3081.9 multi 9.96 import weight 0.00
Epoch 221 Acc: 97.84 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 308 train Loss: 2851.0 test Loss: 361.1
Epoch 222 Iter 0 subLoss 3186.1 multi 6.97 import weight 0.00
Epoch 222 Iter 1 subLoss 2705.2 multi 3.99 import weight 0.00
Epoch 222 Iter 2 subLoss 2860.0 multi 15.93 import weight 0.00
Epoch 222 Iter 3 subLoss 2944.2 multi -4.97 import weight 0.00
Epoch 222 Iter 4 subLoss 3170.1 multi 9.96 import weight 0.00
Epoch 222 Iter 5 subLoss 3094.4 multi -7.96 import weight 0.00
Epoch 222 Iter 6 subLoss 3112.4 multi 6.97 import weight 0.00
Epoch 222 Iter 7 subLoss 2693.5 multi -1.99 import weight 0.00
Epoch 222 Iter 8 subLoss 2217.9 multi 1.00 import weight 0.00
Epoch 222 Iter 9 subLoss 2209.1 multi 1.00 import weight 0.00
Epoch 222 Iter 10 subLoss 2949.3 multi -1.98 import weight 0.00
Epoch 222 Iter 11 subLoss 2732.1 multi -4.97 import weight 0.00
Epoch 222 Acc: 98.05 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 273 train Loss: 2720.2 test Loss: 319.9
Epoch 223 Iter 0 subLoss 2748.8 multi 12.94 import weight 0.00
Epoch 223 Iter 1 subLoss 2537.7 multi -4.97 import weight 0.00
Epoch 223 Iter 2 subLoss 3076.6 multi -4.97 import weight 0.00
Epoch 223 Iter 3 subLoss 2893.6 multi 21.90 import weight 0.00
Epoch 223 Iter 4 subLoss 3551.7 multi -10.94 import weight 0.00
Epoch 223 Iter 5 subLoss 11800.8 multi -1.99 import weight 0.00
Epoch 223 Iter 6 subLoss 49078.5 multi 1.00 import weight 0.00
Epoch 223 Iter 7 subLoss 4569.7 multi -4.97 import weight 0.00
Epoch 223 Iter 8 subLoss 6345.6 multi 3.99 import weight 0.00
Epoch 223 Iter 9 subLoss 3549.9 multi 21.90 import weight 0.00
Epoch 223 Iter 10 subLoss 3694.4 multi 3.99 import weight 0.00
Epoch 223 Iter 11 subLoss 2726.1 multi -1.98 import weight 0.00
Epoch 223 Acc: 98.38 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 272 train Loss: 2987.5 test Loss: 282.2
Epoch 224 Iter 0 subLoss 3093.2 multi -4.97 import weight 0.00
Epoch 224 Iter 1 subLoss 3603.0 multi -10.94 import weight 0.00
Epoch 224 Iter 2 subLoss 12559.9 multi 1.00 import weight 0.00
Epoch 224 Iter 3 subLoss 6147.5 multi -13.93 import weight 0.00
Epoch 224 Iter 4 subLoss 133782.9 multi 1.00 import weight 0.00
Epoch 224 Iter 5 subLoss 21968.5 multi -1.99 import weight 0.00
Epoch 224 Iter 6 subLoss 38508.0 multi 1.00 import weight 0.00
Epoch 224 Iter 7 subLoss 23417.6 multi 1.00 import weight 0.00
Epoch 224 Iter 8 subLoss 20786.3 multi 1.00 import weight 0.00
Epoch 224 Iter 9 subLoss 18045.3 multi 1.00 import weight 0.00
Epoch 224 Iter 10 subLoss 14783.7 multi 1.00 import weight 0.00
Epoch 224 Iter 11 subLoss 12663.1 multi 1.00 import weight 0.00
Epoch 224 Acc: 82.18 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1266 train Loss: 10821.3 test Loss: 2051.2
Epoch 225 Iter 0 subLoss 11381.5 multi 1.00 import weight 0.00
Epoch 225 Iter 1 subLoss 8834.3 multi 1.00 import weight 0.00
Epoch 225 Iter 2 subLoss 7072.4 multi -4.97 import weight 0.00
Epoch 225 Iter 3 subLoss 14098.5 multi 1.00 import weight 0.00
Epoch 225 Iter 4 subLoss 12251.9 multi 1.00 import weight 0.00
Epoch 225 Iter 5 subLoss 11109.0 multi -1.99 import weight 0.00
Epoch 225 Iter 6 subLoss 13989.5 multi 1.00 import weight 0.00
Epoch 225 Iter 7 subLoss 11282.3 multi 6.97 import weight 0.00
Epoch 225 Iter 8 subLoss 5207.3 multi 3.99 import weight 0.00
Epoch 225 Iter 9 subLoss 3598.4 multi 21.90 import weight 0.00
Epoch 225 Iter 10 subLoss 2639.1 multi 3.99 import weight 0.00
Epoch 225 Iter 11 subLoss 2891.6 multi 24.88 import weight 0.00
Epoch 225 Acc: 98.25 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 24.88 Pidx 289 train Loss: 2878.9 test Loss: 308.3
Epoch 226 Iter 0 subLoss 2696.0 multi 1.00 import weight 0.00
Epoch 226 Iter 1 subLoss 2878.1 multi -7.96 import weight 0.00
Epoch 226 Iter 2 subLoss 3233.8 multi 3.98 import weight 0.00
Epoch 226 Iter 3 subLoss 2563.0 multi 15.93 import weight 0.00
Epoch 226 Iter 4 subLoss 2477.2 multi 24.88 import weight 1.00
Epoch 226 Iter 5 subLoss 2719.1 multi 1.00 import weight 0.00
Epoch 226 Iter 6 subLoss 2443.2 multi 3.99 import weight 0.00
Epoch 226 Iter 7 subLoss 2372.1 multi 3.99 import weight 0.00
Epoch 226 Iter 8 subLoss 2655.7 multi 6.97 import weight 0.00
Epoch 226 Iter 9 subLoss 2511.6 multi 18.91 import weight 0.00
Epoch 226 Iter 10 subLoss 2786.3 multi 9.96 import weight 0.00
Epoch 226 Iter 11 subLoss 2374.7 multi 6.97 import weight 0.00
Epoch 226 Acc: 98.50 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 237 train Loss: 2396.9 test Loss: 282.4
Epoch 227 Iter 0 subLoss 2899.5 multi 27.87 import weight 0.00
Epoch 227 Iter 1 subLoss 3014.4 multi -13.93 import weight 0.00
Epoch 227 Iter 2 subLoss 19741.6 multi 1.00 import weight 0.00
Epoch 227 Iter 3 subLoss 3925.6 multi 3.99 import weight 0.00
Epoch 227 Iter 4 subLoss 2242.4 multi -4.97 import weight 0.00
Epoch 227 Iter 5 subLoss 2679.8 multi -4.97 import weight 0.00
Epoch 227 Iter 6 subLoss 3622.5 multi -1.98 import weight 0.00
Epoch 227 Iter 7 subLoss 4469.1 multi 12.94 import weight 0.00
Epoch 227 Iter 8 subLoss 5112.6 multi 12.94 import weight 0.00
Epoch 227 Iter 9 subLoss 3930.7 multi -1.99 import weight 0.00
Epoch 227 Iter 10 subLoss 4724.8 multi 9.96 import weight 0.00
Epoch 227 Iter 11 subLoss 3882.2 multi 3.98 import weight 0.00
Epoch 227 Acc: 97.90 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 388 train Loss: 2912.9 test Loss: 345.9
Epoch 228 Iter 0 subLoss 2868.8 multi -7.96 import weight 0.00
Epoch 228 Iter 1 subLoss 3420.5 multi 1.00 import weight 0.00
Epoch 228 Iter 2 subLoss 3036.9 multi -4.97 import weight 0.00
Epoch 228 Iter 3 subLoss 5194.9 multi -1.99 import weight 0.00
Epoch 228 Iter 4 subLoss 6725.0 multi 1.00 import weight 0.00
Epoch 228 Iter 5 subLoss 5725.8 multi -1.99 import weight 0.00
Epoch 228 Iter 6 subLoss 6572.5 multi 6.97 import weight 0.00
Epoch 228 Iter 7 subLoss 2585.1 multi 12.94 import weight 0.00
Epoch 228 Iter 8 subLoss 2316.7 multi 3.99 import weight 0.00
Epoch 228 Iter 9 subLoss 2933.5 multi -1.99 import weight 0.00
Epoch 228 Iter 10 subLoss 2313.3 multi 6.97 import weight 0.00
Epoch 228 Iter 11 subLoss 2393.8 multi 24.88 import weight 0.00
Epoch 228 Acc: 98.50 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 24.88 Pidx 239 train Loss: 2608.8 test Loss: 256.4
Epoch 229 Iter 0 subLoss 2155.2 multi 1.00 import weight 0.00
Epoch 229 Iter 1 subLoss 2427.7 multi -13.93 import weight 0.00
Epoch 229 Iter 2 subLoss 3539.5 multi -1.99 import weight 0.00
Epoch 229 Iter 3 subLoss 4496.9 multi -10.94 import weight 0.00
Epoch 229 Iter 4 subLoss 46290.5 multi 1.00 import weight 0.00
Epoch 229 Iter 5 subLoss 6357.0 multi 3.99 import weight 0.00
Epoch 229 Iter 6 subLoss 3688.2 multi -7.96 import weight 0.00
Epoch 229 Iter 7 subLoss 5110.9 multi 15.93 import weight 0.00
Epoch 229 Iter 8 subLoss 3405.9 multi 9.96 import weight 0.00
Epoch 229 Iter 9 subLoss 2506.9 multi -22.88 import weight 0.00
Epoch 229 Iter 10 subLoss 3064.0 multi 9.96 import weight 0.00
Epoch 229 Iter 11 subLoss 2205.8 multi 3.98 import weight 0.00
Epoch 229 Acc: 98.23 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 220 train Loss: 2558.1 test Loss: 304.8
Epoch 230 Iter 0 subLoss 2421.9 multi -10.94 import weight 0.00
Epoch 230 Iter 1 subLoss 2738.8 multi -4.97 import weight 0.00
Epoch 230 Iter 2 subLoss 3016.2 multi -10.94 import weight 0.00
Epoch 230 Iter 3 subLoss 3936.6 multi 1.00 import weight 0.00
Epoch 230 Iter 4 subLoss 4109.2 multi 3.98 import weight 0.00
Epoch 230 Iter 5 subLoss 2800.4 multi -4.97 import weight 0.00
Epoch 230 Iter 6 subLoss 3304.5 multi 18.91 import weight 0.00
Epoch 230 Iter 7 subLoss 2859.1 multi 18.91 import weight 0.00
Epoch 230 Iter 8 subLoss 3447.1 multi 27.87 import weight 1.00
Epoch 230 Iter 9 subLoss 9358.4 multi 3.99 import weight 0.00
Epoch 230 Iter 10 subLoss 2280.1 multi -4.97 import weight 0.00
Epoch 230 Iter 11 subLoss 2383.7 multi -7.96 import weight 0.00
Epoch 230 Acc: 97.57 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 238 train Loss: 3271.1 test Loss: 425.4
Epoch 231 Iter 0 subLoss 3508.2 multi -1.99 import weight 0.00
Epoch 231 Iter 1 subLoss 3659.1 multi -4.97 import weight 0.00
Epoch 231 Iter 2 subLoss 4443.5 multi -4.97 import weight 0.00
Epoch 231 Iter 3 subLoss 9996.1 multi 1.00 import weight 0.00
Epoch 231 Iter 4 subLoss 6828.7 multi -1.98 import weight 0.00
Epoch 231 Iter 5 subLoss 10905.9 multi 3.99 import weight 0.00
Epoch 231 Iter 6 subLoss 3351.4 multi 1.00 import weight 0.00
Epoch 231 Iter 7 subLoss 3201.8 multi -10.94 import weight 0.00
Epoch 231 Iter 8 subLoss 4788.0 multi 3.98 import weight 0.00
Epoch 231 Iter 9 subLoss 3759.3 multi -7.96 import weight 0.00
Epoch 231 Iter 10 subLoss 5673.3 multi -4.97 import weight 0.00
Epoch 231 Iter 11 subLoss 8531.9 multi 1.00 import weight 0.00
Epoch 231 Acc: 94.43 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 853 train Loss: 7448.6 test Loss: 1058.1
Epoch 232 Iter 0 subLoss 7536.1 multi -4.97 import weight 0.00
Epoch 232 Iter 1 subLoss 15783.5 multi -1.99 import weight 0.00
Epoch 232 Iter 2 subLoss 33254.0 multi 1.00 import weight 0.00
Epoch 232 Iter 3 subLoss 19429.7 multi 1.00 import weight 0.00
Epoch 232 Iter 4 subLoss 13293.8 multi 1.00 import weight 0.00
Epoch 232 Iter 5 subLoss 10031.0 multi 3.99 import weight 0.00
Epoch 232 Iter 6 subLoss 5178.7 multi -10.94 import weight 0.00
Epoch 232 Iter 7 subLoss 11577.6 multi -4.97 import weight 0.00
Epoch 232 Iter 8 subLoss 23047.4 multi 1.00 import weight 0.00
Epoch 232 Iter 9 subLoss 17974.7 multi 1.00 import weight 0.00
Epoch 232 Iter 10 subLoss 16352.1 multi 1.00 import weight 0.00
Epoch 232 Iter 11 subLoss 13372.4 multi 1.00 import weight 0.00
Epoch 232 Acc: 90.60 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1337 train Loss: 11178.4 test Loss: 1771.7
Epoch 233 Iter 0 subLoss 11022.7 multi 1.00 import weight 0.00
Epoch 233 Iter 1 subLoss 9032.0 multi -4.97 import weight 0.00
Epoch 233 Iter 2 subLoss 16027.4 multi -1.99 import weight 0.00
Epoch 233 Iter 3 subLoss 21812.0 multi 1.00 import weight 0.00
Epoch 233 Iter 4 subLoss 18720.6 multi 1.00 import weight 0.00
Epoch 233 Iter 5 subLoss 16801.3 multi 1.00 import weight 0.00
Epoch 233 Iter 6 subLoss 13530.1 multi -4.97 import weight 0.00
Epoch 233 Iter 7 subLoss 24718.2 multi 1.00 import weight 0.00
Epoch 233 Iter 8 subLoss 21692.3 multi 1.00 import weight 0.00
Epoch 233 Iter 9 subLoss 19682.8 multi 1.00 import weight 0.00
Epoch 233 Iter 10 subLoss 16831.6 multi 1.00 import weight 0.00
Epoch 233 Iter 11 subLoss 13954.0 multi 1.00 import weight 0.00
Epoch 233 Acc: 89.12 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1395 train Loss: 12961.7 test Loss: 2096.9
Epoch 234 Iter 0 subLoss 12750.2 multi 1.00 import weight 0.00
Epoch 234 Iter 1 subLoss 11851.5 multi -1.99 import weight 0.00
Epoch 234 Iter 2 subLoss 13869.3 multi 3.99 import weight 0.00
Epoch 234 Iter 3 subLoss 8737.0 multi -1.98 import weight 0.00
Epoch 234 Iter 4 subLoss 10045.7 multi -1.98 import weight 0.00
Epoch 234 Iter 5 subLoss 12936.6 multi -4.97 import weight 0.00
Epoch 234 Iter 6 subLoss 20701.4 multi 1.00 import weight 0.00
Epoch 234 Iter 7 subLoss 18475.5 multi 1.00 import weight 0.00
Epoch 234 Iter 8 subLoss 16284.7 multi 3.99 import weight 0.00
Epoch 234 Iter 9 subLoss 10550.7 multi 1.00 import weight 0.00
Epoch 234 Iter 10 subLoss 9597.1 multi -1.99 import weight 0.00
Epoch 234 Iter 11 subLoss 11823.4 multi 1.00 import weight 0.00
Epoch 234 Acc: 92.22 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1182 train Loss: 10885.4 test Loss: 1605.4
Epoch 235 Iter 0 subLoss 10549.0 multi 1.00 import weight 0.00
Epoch 235 Iter 1 subLoss 9718.1 multi -1.99 import weight 0.00
Epoch 235 Iter 2 subLoss 10452.0 multi 6.97 import weight 0.00
Epoch 235 Iter 3 subLoss 6676.8 multi 1.00 import weight 0.00
Epoch 235 Iter 4 subLoss 6125.1 multi 1.00 import weight 0.00
Epoch 235 Iter 5 subLoss 5688.5 multi 1.00 import weight 0.00
Epoch 235 Iter 6 subLoss 5013.8 multi -10.94 import weight 0.00
Epoch 235 Iter 7 subLoss 9637.1 multi -1.99 import weight 0.00
Epoch 235 Iter 8 subLoss 11201.5 multi 1.00 import weight 0.00
Epoch 235 Iter 9 subLoss 9861.8 multi -1.99 import weight 0.00
Epoch 235 Iter 10 subLoss 11551.6 multi 1.00 import weight 0.00
Epoch 235 Iter 11 subLoss 10337.7 multi 1.00 import weight 0.00
Epoch 235 Acc: 93.79 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1033 train Loss: 9817.2 test Loss: 1324.8
Epoch 236 Iter 0 subLoss 9355.8 multi 6.97 import weight 0.00
Epoch 236 Iter 1 subLoss 5421.1 multi 1.00 import weight 0.00
Epoch 236 Iter 2 subLoss 5605.3 multi 1.00 import weight 0.00
Epoch 236 Iter 3 subLoss 5087.8 multi 1.00 import weight 0.00
Epoch 236 Iter 4 subLoss 5036.6 multi -10.94 import weight 0.00
Epoch 236 Iter 5 subLoss 7926.5 multi -1.99 import weight 0.00
Epoch 236 Iter 6 subLoss 8491.3 multi 1.00 import weight 0.00
Epoch 236 Iter 7 subLoss 8534.1 multi 3.98 import weight 0.00
Epoch 236 Iter 8 subLoss 6928.5 multi -1.99 import weight 0.00
Epoch 236 Iter 9 subLoss 7021.2 multi 3.99 import weight 0.00
Epoch 236 Iter 10 subLoss 5666.1 multi 3.99 import weight 0.00
Epoch 236 Iter 11 subLoss 5101.1 multi -1.99 import weight 0.00
Epoch 236 Acc: 96.24 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 510 train Loss: 5390.2 test Loss: 720.6
Epoch 237 Iter 0 subLoss 5088.9 multi 3.99 import weight 0.00
Epoch 237 Iter 1 subLoss 4564.2 multi -1.99 import weight 0.00
Epoch 237 Iter 2 subLoss 4925.2 multi 1.00 import weight 0.00
Epoch 237 Iter 3 subLoss 4794.1 multi 6.97 import weight 0.00
Epoch 237 Iter 4 subLoss 3846.1 multi 6.97 import weight 0.00
Epoch 237 Iter 5 subLoss 2877.7 multi -7.96 import weight 0.00
Epoch 237 Iter 6 subLoss 3829.3 multi -10.94 import weight 0.00
Epoch 237 Iter 7 subLoss 4736.3 multi -10.94 import weight 0.00
Epoch 237 Iter 8 subLoss 8080.6 multi 3.99 import weight 0.00
Epoch 237 Iter 9 subLoss 5621.6 multi 3.99 import weight 0.00
Epoch 237 Iter 10 subLoss 5052.0 multi -1.99 import weight 0.00
Epoch 237 Iter 11 subLoss 4694.0 multi 6.97 import weight 0.00
Epoch 237 Acc: 97.31 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 469 train Loss: 4041.8 test Loss: 491.5
Epoch 238 Iter 0 subLoss 3866.4 multi -7.96 import weight 0.00
Epoch 238 Iter 1 subLoss 4644.7 multi -7.96 import weight 0.00
Epoch 238 Iter 2 subLoss 6371.3 multi 3.98 import weight 0.00
Epoch 238 Iter 3 subLoss 5359.2 multi 1.00 import weight 0.00
Epoch 238 Iter 4 subLoss 5012.0 multi -7.96 import weight 0.00
Epoch 238 Iter 5 subLoss 6587.8 multi -7.96 import weight 0.00
Epoch 238 Iter 6 subLoss 9969.0 multi 1.00 import weight 0.00
Epoch 238 Iter 7 subLoss 9251.6 multi -1.99 import weight 0.00
Epoch 238 Iter 8 subLoss 10557.8 multi 1.00 import weight 0.00
Epoch 238 Iter 9 subLoss 9612.2 multi -1.99 import weight 0.00
Epoch 238 Iter 10 subLoss 11768.5 multi -4.97 import weight 0.00
Epoch 238 Iter 11 subLoss 22061.2 multi 1.00 import weight 0.00
Epoch 238 Acc: 81.86 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2206 train Loss: 17876.1 test Loss: 3145.7
Epoch 239 Iter 0 subLoss 17910.6 multi 3.99 import weight 0.00
Epoch 239 Iter 1 subLoss 9814.7 multi 6.97 import weight 0.00
Epoch 239 Iter 2 subLoss 6819.4 multi 1.00 import weight 0.00
Epoch 239 Iter 3 subLoss 6169.9 multi 1.00 import weight 0.00
Epoch 239 Iter 4 subLoss 6319.3 multi 6.97 import weight 0.00
Epoch 239 Iter 5 subLoss 5018.9 multi -4.97 import weight 0.00
Epoch 239 Iter 6 subLoss 5528.2 multi 6.97 import weight 0.00
Epoch 239 Iter 7 subLoss 4843.0 multi -1.98 import weight 0.00
Epoch 239 Iter 8 subLoss 4524.2 multi 9.96 import weight 0.00
Epoch 239 Iter 9 subLoss 3593.9 multi 24.88 import weight 0.00
Epoch 239 Iter 10 subLoss 3528.0 multi -1.99 import weight 0.00
Epoch 239 Iter 11 subLoss 3102.6 multi -10.94 import weight 0.00
Epoch 239 Acc: 95.02 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 310 train Loss: 5688.1 test Loss: 802.0
Epoch 240 Iter 0 subLoss 5567.8 multi 3.99 import weight 0.00
Epoch 240 Iter 1 subLoss 3312.0 multi -13.93 import weight 0.00
Epoch 240 Iter 2 subLoss 4804.8 multi -4.97 import weight 0.00
Epoch 240 Iter 3 subLoss 9837.1 multi 3.99 import weight 0.00
Epoch 240 Iter 4 subLoss 3720.7 multi -10.94 import weight 0.00
Epoch 240 Iter 5 subLoss 3980.3 multi -7.96 import weight 0.00
Epoch 240 Iter 6 subLoss 6466.2 multi 1.00 import weight 0.00
Epoch 240 Iter 7 subLoss 5989.1 multi 3.99 import weight 0.00
Epoch 240 Iter 8 subLoss 4601.9 multi -4.97 import weight 0.00
Epoch 240 Iter 9 subLoss 5157.6 multi 1.00 import weight 0.00
Epoch 240 Iter 10 subLoss 5431.7 multi 3.99 import weight 0.00
Epoch 240 Iter 11 subLoss 3559.8 multi -10.94 import weight 0.00
Epoch 240 Acc: 93.09 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 355 train Loss: 5831.1 test Loss: 1098.3
Epoch 241 Iter 0 subLoss 5878.5 multi 1.00 import weight 0.00
Epoch 241 Iter 1 subLoss 5417.9 multi 1.00 import weight 0.00
Epoch 241 Iter 2 subLoss 5289.6 multi -7.96 import weight 0.00
Epoch 241 Iter 3 subLoss 8915.6 multi 6.97 import weight 0.00
Epoch 241 Iter 4 subLoss 4523.2 multi 12.94 import weight 0.00
Epoch 241 Iter 5 subLoss 3989.7 multi -4.97 import weight 0.00
Epoch 241 Iter 6 subLoss 4639.5 multi -1.99 import weight 0.00
Epoch 241 Iter 7 subLoss 4598.2 multi -1.98 import weight 0.00
Epoch 241 Iter 8 subLoss 5602.5 multi 3.98 import weight 0.00
Epoch 241 Iter 9 subLoss 4337.7 multi 6.97 import weight 0.00
Epoch 241 Iter 10 subLoss 2530.6 multi -1.98 import weight 0.00
Epoch 241 Iter 11 subLoss 3448.8 multi 30.85 import weight 1.00
Epoch 241 Acc: 97.66 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 30.85 Pidx 344 train Loss: 3172.2 test Loss: 405.5
Epoch 242 Iter 0 subLoss 3050.2 multi -4.97 import weight 0.00
Epoch 242 Iter 1 subLoss 3629.2 multi 1.00 import weight 0.00
Epoch 242 Iter 2 subLoss 3329.4 multi -13.93 import weight 0.00
Epoch 242 Iter 3 subLoss 6270.9 multi 1.00 import weight 0.00
Epoch 242 Iter 4 subLoss 4914.5 multi -4.97 import weight 0.00
Epoch 242 Iter 5 subLoss 12015.7 multi 3.99 import weight 0.00
Epoch 242 Iter 6 subLoss 3005.7 multi 12.94 import weight 0.00
Epoch 242 Iter 7 subLoss 3002.0 multi 15.93 import weight 0.00
Epoch 242 Iter 8 subLoss 2451.3 multi -7.96 import weight 0.00
Epoch 242 Iter 9 subLoss 3283.4 multi 12.94 import weight 0.00
Epoch 242 Iter 10 subLoss 2683.7 multi -4.97 import weight 0.00
Epoch 242 Iter 11 subLoss 2771.1 multi -1.99 import weight 0.00
Epoch 242 Acc: 97.51 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 277 train Loss: 2978.8 test Loss: 390.4
Epoch 243 Iter 0 subLoss 2708.6 multi 1.00 import weight 0.00
Epoch 243 Iter 1 subLoss 3163.9 multi -4.97 import weight 0.00
Epoch 243 Iter 2 subLoss 3741.7 multi -7.96 import weight 0.00
Epoch 243 Iter 3 subLoss 3643.7 multi -1.99 import weight 0.00
Epoch 243 Iter 4 subLoss 4718.9 multi -4.97 import weight 0.00
Epoch 243 Iter 5 subLoss 10559.0 multi 3.98 import weight 0.00
Epoch 243 Iter 6 subLoss 3853.8 multi -13.93 import weight 0.00
Epoch 243 Iter 7 subLoss 16075.0 multi 1.00 import weight 0.00
Epoch 243 Iter 8 subLoss 8924.1 multi 1.00 import weight 0.00
Epoch 243 Iter 9 subLoss 6059.2 multi 6.97 import weight 0.00
Epoch 243 Iter 10 subLoss 3101.5 multi -7.96 import weight 0.00
Epoch 243 Iter 11 subLoss 3590.4 multi 27.87 import weight 0.00
Epoch 243 Acc: 97.57 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 27.87 Pidx 359 train Loss: 3337.3 test Loss: 379.7
Epoch 244 Iter 0 subLoss 3379.0 multi 3.99 import weight 0.00
Epoch 244 Iter 1 subLoss 2903.9 multi -22.88 import weight 0.00
Epoch 244 Iter 2 subLoss 4467.9 multi 15.93 import weight 0.00
Epoch 244 Iter 3 subLoss 3271.6 multi -10.94 import weight 0.00
Epoch 244 Iter 4 subLoss 3994.9 multi -13.93 import weight 0.00
Epoch 244 Iter 5 subLoss 13627.2 multi 3.99 import weight 0.00
Epoch 244 Iter 6 subLoss 3786.7 multi 15.93 import weight 0.00
Epoch 244 Iter 7 subLoss 2761.1 multi 9.96 import weight 0.00
Epoch 244 Iter 8 subLoss 2651.0 multi 9.96 import weight 0.00
Epoch 244 Iter 9 subLoss 3287.5 multi 12.94 import weight 0.00
Epoch 244 Iter 10 subLoss 2360.0 multi 1.00 import weight 0.00
Epoch 244 Iter 11 subLoss 2497.5 multi 18.91 import weight 0.00
Epoch 244 Acc: 98.38 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 249 train Loss: 2664.7 test Loss: 283.8
Epoch 245 Iter 0 subLoss 2907.9 multi -19.90 import weight 0.00
Epoch 245 Iter 1 subLoss 3340.9 multi 1.00 import weight 0.00
Epoch 245 Iter 2 subLoss 3435.5 multi 1.00 import weight 0.00
Epoch 245 Iter 3 subLoss 2465.5 multi 1.00 import weight 0.00
Epoch 245 Iter 4 subLoss 2936.4 multi 1.00 import weight 0.00
Epoch 245 Iter 5 subLoss 2987.9 multi -4.97 import weight 0.00
Epoch 245 Iter 6 subLoss 2952.6 multi -4.97 import weight 0.00
Epoch 245 Iter 7 subLoss 3472.2 multi 3.98 import weight 0.00
Epoch 245 Iter 8 subLoss 3015.6 multi -13.93 import weight 0.00
Epoch 245 Iter 9 subLoss 4121.5 multi 9.96 import weight 0.00
Epoch 245 Iter 10 subLoss 2606.1 multi -4.97 import weight 0.00
Epoch 245 Iter 11 subLoss 3107.9 multi -4.97 import weight 0.00
Epoch 245 Acc: 97.08 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 310 train Loss: 3820.6 test Loss: 459.6
Epoch 246 Iter 0 subLoss 3807.4 multi 3.99 import weight 0.00
Epoch 246 Iter 1 subLoss 3151.2 multi 1.00 import weight 0.00
Epoch 246 Iter 2 subLoss 3198.7 multi -1.98 import weight 0.00
Epoch 246 Iter 3 subLoss 3455.3 multi -37.81 import weight 0.00
Epoch 246 Iter 4 subLoss 10112.2 multi 3.99 import weight 0.00
Epoch 246 Iter 5 subLoss 4163.7 multi 9.96 import weight 0.00
Epoch 246 Iter 6 subLoss 3140.3 multi -1.98 import weight 0.00
Epoch 246 Iter 7 subLoss 2782.8 multi 9.96 import weight 0.00
Epoch 246 Iter 8 subLoss 2928.1 multi 6.97 import weight 0.00
Epoch 246 Iter 9 subLoss 3184.5 multi 6.97 import weight 0.00
Epoch 246 Iter 10 subLoss 2355.5 multi -4.97 import weight 0.00
Epoch 246 Iter 11 subLoss 2915.0 multi 1.00 import weight 0.00
Epoch 246 Acc: 98.35 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 291 train Loss: 3031.9 test Loss: 299.6
Epoch 247 Iter 0 subLoss 3095.7 multi -1.99 import weight 0.00
Epoch 247 Iter 1 subLoss 2833.7 multi 12.94 import weight 0.00
Epoch 247 Iter 2 subLoss 2760.4 multi 12.94 import weight 0.00
Epoch 247 Iter 3 subLoss 2642.2 multi -1.99 import weight 0.00
Epoch 247 Iter 4 subLoss 2912.7 multi 3.98 import weight 0.00
Epoch 247 Iter 5 subLoss 2630.9 multi 6.97 import weight 0.00
Epoch 247 Iter 6 subLoss 2435.7 multi 9.96 import weight 0.00
Epoch 247 Iter 7 subLoss 2537.6 multi 1.00 import weight 0.00
Epoch 247 Iter 8 subLoss 3080.3 multi 9.96 import weight 0.00
Epoch 247 Iter 9 subLoss 2895.4 multi 30.85 import weight 0.00
Epoch 247 Iter 10 subLoss 2985.5 multi -1.99 import weight 0.00
Epoch 247 Iter 11 subLoss 2546.6 multi 9.96 import weight 0.00
Epoch 247 Acc: 98.48 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 254 train Loss: 2610.1 test Loss: 279.6
Epoch 248 Iter 0 subLoss 2712.8 multi 1.00 import weight 0.00
Epoch 248 Iter 1 subLoss 2351.6 multi -1.99 import weight 0.00
Epoch 248 Iter 2 subLoss 2362.9 multi -1.99 import weight 0.00
Epoch 248 Iter 3 subLoss 2349.1 multi -4.97 import weight 0.00
Epoch 248 Iter 4 subLoss 2565.0 multi 18.91 import weight 0.00
Epoch 248 Iter 5 subLoss 2555.2 multi -19.90 import weight 0.00
Epoch 248 Iter 6 subLoss 3634.4 multi -7.96 import weight 0.00
Epoch 248 Iter 7 subLoss 6844.9 multi -1.98 import weight 0.00
Epoch 248 Iter 8 subLoss 15003.0 multi 1.00 import weight 0.00
Epoch 248 Iter 9 subLoss 6120.9 multi 3.99 import weight 0.00
Epoch 248 Iter 10 subLoss 3304.5 multi 21.90 import weight 0.00
Epoch 248 Iter 11 subLoss 3604.7 multi -16.91 import weight 0.00
Epoch 248 Acc: 86.55 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 360 train Loss: 13512.1 test Loss: 2324.0
Epoch 249 Iter 0 subLoss 12849.2 multi 1.00 import weight 0.00
Epoch 249 Iter 1 subLoss 7509.5 multi 6.97 import weight 0.00
Epoch 249 Iter 2 subLoss 3134.8 multi -10.94 import weight 0.00
Epoch 249 Iter 3 subLoss 6924.6 multi 1.00 import weight 0.00
Epoch 249 Iter 4 subLoss 5208.5 multi 3.99 import weight 0.00
Epoch 249 Iter 5 subLoss 3143.5 multi -1.99 import weight 0.00
Epoch 249 Iter 6 subLoss 3684.8 multi -4.97 import weight 0.00
Epoch 249 Iter 7 subLoss 4020.0 multi 12.94 import weight 0.00
Epoch 249 Iter 8 subLoss 2805.4 multi -1.99 import weight 0.00
Epoch 249 Iter 9 subLoss 3136.6 multi -7.96 import weight 0.00
Epoch 249 Iter 10 subLoss 3649.7 multi -1.99 import weight 0.00
Epoch 249 Iter 11 subLoss 4514.3 multi 1.00 import weight 0.00
Epoch 249 Acc: 97.72 BMA: 0.00 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 451 train Loss: 3938.6 test Loss: 395.7
Epoch 250 Iter 0 subLoss 3307.4 multi 24.88 import weight 0.00
Epoch 250 Iter 1 subLoss 6569.1 multi 1.00 import weight 0.00
Epoch 250 Iter 2 subLoss 4768.6 multi 9.96 import weight 0.00
Epoch 250 Iter 3 subLoss 2292.2 multi 6.97 import weight 0.00
Epoch 250 Iter 4 subLoss 2680.9 multi -1.99 import weight 0.00
Epoch 250 Iter 5 subLoss 2208.7 multi 6.97 import weight 0.00
Epoch 250 Iter 6 subLoss 2410.4 multi 3.98 import weight 0.00
Epoch 250 Iter 7 subLoss 2400.0 multi 24.88 import weight 0.00
Epoch 250 Iter 8 subLoss 2676.0 multi -1.99 import weight 0.00
Epoch 250 Iter 9 subLoss 2985.6 multi 1.00 import weight 0.00
Epoch 250 Iter 10 subLoss 2546.5 multi 12.94 import weight 0.00
Epoch 250 Iter 11 subLoss 2674.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0049 / 0.14098 / 11.40
Entropy seen (from low to high)
[3926, 390, 172, 102, 75, 52, 44, 43, 37, 21, 26, 20, 20, 15, 15, 17, 17, 13, 15, 18, 20, 13, 7, 10, 9, 3, 2, 5, 9, 3, 2, 4, 3, 2, 2, 0, 1, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 2, 3, 4, 1, 6, 13, 22, 22, 43, 34, 45, 63, 108, 127, 138, 125, 110, 132, 107, 122, 108, 106, 133, 118, 139, 149, 146, 111, 117, 111, 94, 105, 123, 112, 101, 121, 108, 115, 138, 121, 163, 213, 220, 293, 369]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.2, 39.9, 43.8, 48.4, 51.0, 54.2, 57.5, 60.8, 65.0, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 49.9, 49.9, 57.1, 45.4, 59.9, 72.7, 79.9, 69.9, 84.9]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 6, 7, 11, 15, 11, 15, 10, 20]
Epoch 250 Acc: 98.54 BMA: 98.54 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 267 train Loss: 2288.8 test Loss: 263.4
Epoch 251 Iter 0 subLoss 2521.5 multi -10.94 import weight 0.00
Epoch 251 Iter 1 subLoss 2448.6 multi 3.99 import weight 0.00
Epoch 251 Iter 2 subLoss 2498.2 multi 21.90 import weight 0.00
Epoch 251 Iter 3 subLoss 2366.8 multi 1.00 import weight 0.00
Epoch 251 Iter 4 subLoss 2381.2 multi -4.97 import weight 0.00
Epoch 251 Iter 5 subLoss 2700.1 multi 3.99 import weight 0.00
Epoch 251 Iter 6 subLoss 2366.0 multi 3.98 import weight 0.00
Epoch 251 Iter 7 subLoss 2828.7 multi -25.87 import weight 0.00
Epoch 251 Iter 8 subLoss 2803.7 multi 1.00 import weight 0.00
Epoch 251 Iter 9 subLoss 2628.1 multi -10.94 import weight 0.00
Epoch 251 Iter 10 subLoss 3639.2 multi -4.97 import weight 0.00
Epoch 251 Iter 11 subLoss 6051.6 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0065 / 0.13713 / 9.00
Entropy seen (from low to high)
[3665, 339, 171, 114, 86, 84, 63, 44, 48, 32, 38, 27, 38, 34, 25, 29, 18, 30, 27, 38, 33, 18, 26, 19, 10, 15, 13, 3, 7, 6, 7, 4, 8, 4, 5, 1, 2, 3, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 1, 1, 0, 9, 10, 7, 23, 29, 45, 37, 43, 71, 74, 100, 150, 150, 143, 134, 146, 126, 122, 119, 124, 135, 145, 131, 146, 112, 105, 98, 86, 91, 96, 95, 102, 97, 101, 109, 109, 112, 122, 126, 182, 232, 296, 369]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.4, 36.1, 40.6, 43.5, 47.7, 50.5, 54.0, 58.2, 61.5, 64.5, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 49.9, 33.3, 59.9, 62.4, 57.1, 58.8, 49.9, 52.7, 72.2, 81.2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 5, 8, 21, 17, 22, 36, 36, 32]
Epoch 251 Acc: 96.50 BMA: 97.92 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 605 train Loss: 3342.8 test Loss: 518.0
Epoch 252 Iter 0 subLoss 3808.2 multi 6.97 import weight 0.00
Epoch 252 Iter 1 subLoss 3012.0 multi -10.94 import weight 0.00
Epoch 252 Iter 2 subLoss 3574.3 multi -10.94 import weight 0.00
Epoch 252 Iter 3 subLoss 4995.3 multi -13.93 import weight 0.00
Epoch 252 Iter 4 subLoss 32267.1 multi 1.00 import weight 0.00
Epoch 252 Iter 5 subLoss 13637.5 multi -4.97 import weight 0.00
Epoch 252 Iter 6 subLoss 45580.2 multi 1.00 import weight 0.00
Epoch 252 Iter 7 subLoss 20607.0 multi 1.00 import weight 0.00
Epoch 252 Iter 8 subLoss 15486.2 multi -1.99 import weight 0.00
Epoch 252 Iter 9 subLoss 22338.5 multi 1.00 import weight 0.00
Epoch 252 Iter 10 subLoss 18184.1 multi 1.00 import weight 0.00
Epoch 252 Iter 11 subLoss 14339.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0126 / 0.13013 / 22.17
Entropy seen (from low to high)
[3211, 305, 133, 82, 61, 38, 41, 21, 23, 27, 17, 15, 16, 25, 22, 22, 24, 27, 31, 49, 57, 78, 95, 91, 100, 104, 84, 91, 71, 30, 37, 22, 15, 10, 17, 11, 14, 6, 3, 6, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 4, 13, 14, 20, 23, 28, 56, 67, 79, 79, 106, 94, 129, 158, 142, 168, 176, 146, 139, 143, 126, 109, 128, 119, 116, 123, 115, 95, 84, 90, 101, 70, 84, 90, 91, 85, 109, 100, 108, 118, 145, 183, 215, 270, 203]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 31.2, 33.3, 36.8, 39.9, 43.7, 47.4, 50.6, 54.3, 57.7, 61.2, 64.9, 68.4]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 66.6, 42.8, 42.1, 46.4, 68.9, 66.6, 78.8, 83.7, 87.2, 98.0]
[0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 6, 7, 19, 28, 58, 57, 71, 74, 110, 256]
Epoch 252 Acc: 81.40 BMA: 97.06 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1433 train Loss: 13766.1 test Loss: 2391.7
Epoch 253 Iter 0 subLoss 14458.8 multi 1.00 import weight 0.00
Epoch 253 Iter 1 subLoss 13052.8 multi 3.99 import weight 0.00
Epoch 253 Iter 2 subLoss 7684.9 multi 3.98 import weight 0.00
Epoch 253 Iter 3 subLoss 5374.8 multi 9.96 import weight 0.00
Epoch 253 Iter 4 subLoss 3135.7 multi -4.97 import weight 0.00
Epoch 253 Iter 5 subLoss 4150.5 multi -4.97 import weight 0.00
Epoch 253 Iter 6 subLoss 4692.2 multi 9.96 import weight 0.00
Epoch 253 Iter 7 subLoss 3076.2 multi -4.97 import weight 0.00
Epoch 253 Iter 8 subLoss 3297.1 multi -16.91 import weight 0.00
Epoch 253 Iter 9 subLoss 5982.5 multi 6.97 import weight 0.00
Epoch 253 Iter 10 subLoss 3840.3 multi 9.96 import weight 0.00
Epoch 253 Iter 11 subLoss 3235.2 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0104 / 0.12845 / 16.20
Entropy seen (from low to high)
[3113, 334, 167, 83, 68, 45, 34, 29, 31, 25, 24, 31, 19, 33, 36, 28, 41, 63, 75, 93, 140, 109, 98, 86, 62, 50, 38, 29, 30, 12, 16, 24, 9, 9, 9, 16, 6, 8, 4, 4, 3, 1, 1, 3, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 7, 10, 20, 18, 24, 33, 69, 65, 82, 78, 97, 118, 139, 146, 195, 159, 171, 129, 161, 131, 119, 97, 128, 116, 145, 134, 112, 81, 99, 83, 82, 84, 86, 90, 98, 95, 116, 100, 120, 137, 148, 200, 196, 226, 117]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 33.2, 36.6, 40.3, 44.0, 47.8, 50.7, 54.0, 57.5, 61.4, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.9, 33.3, 83.3, 45.4, 27.2, 47.9, 57.8, 62.2, 72.7, 91.9, 79.9, 90.4]
[0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 6, 11, 11, 25, 38, 45, 44, 50, 60, 63]
Epoch 253 Acc: 97.63 BMA: 97.31 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 323 train Loss: 2986.8 test Loss: 400.7
Epoch 254 Iter 0 subLoss 2643.5 multi -1.98 import weight 0.00
Epoch 254 Iter 1 subLoss 3295.4 multi -13.93 import weight 0.00
Epoch 254 Iter 2 subLoss 3454.7 multi -34.82 import weight 0.00
Epoch 254 Iter 3 subLoss 9520.7 multi -7.96 import weight 0.00
Epoch 254 Iter 4 subLoss 54243.0 multi 1.00 import weight 0.00
Epoch 254 Iter 5 subLoss 11824.8 multi 3.99 import weight 0.00
Epoch 254 Iter 6 subLoss 8419.3 multi -1.99 import weight 0.00
Epoch 254 Iter 7 subLoss 9921.5 multi -1.99 import weight 0.00
Epoch 254 Iter 8 subLoss 10088.5 multi -4.97 import weight 0.00
Epoch 254 Iter 9 subLoss 13978.7 multi -1.99 import weight 0.00
Epoch 254 Iter 10 subLoss 15841.1 multi 1.00 import weight 0.00
Epoch 254 Iter 11 subLoss 14671.4 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0134 / 0.11471 / 25.33
Entropy seen (from low to high)
[1668, 601, 261, 162, 101, 99, 119, 113, 144, 120, 111, 105, 97, 73, 56, 52, 49, 39, 26, 46, 47, 45, 45, 59, 55, 113, 91, 108, 89, 92, 87, 64, 59, 32, 27, 14, 19, 14, 15, 6, 6, 6, 0, 2, 2, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 8, 19, 23, 36, 45, 81, 96, 85, 130, 143, 147, 184, 190, 193, 180, 142, 128, 118, 159, 121, 143, 131, 124, 136, 152, 151, 152, 179, 164, 143, 147, 149, 125, 122, 118, 120, 95, 62, 61, 43, 36, 38, 16, 17, 9]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 30.0, 33.8, 37.2, 40.6, 43.8, 47.3, 50.8, 54.2, 57.8, 61.5, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 39.9, 33.3, 42.8, 65.6, 60.4, 69.9, 67.9, 92.1, 89.9, 97.6, 95.4]
[0, 0, 0, 0, 0, 0, 0, 1, 4, 5, 12, 14, 32, 48, 50, 75, 76, 119, 167, 174]
Epoch 254 Acc: 85.25 BMA: 96.93 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1467 train Loss: 11625.7 test Loss: 2157.8
Epoch 255 Iter 0 subLoss 11412.1 multi -1.99 import weight 0.00
Epoch 255 Iter 1 subLoss 12513.3 multi 1.00 import weight 0.00
Epoch 255 Iter 2 subLoss 11973.2 multi 1.00 import weight 0.00
Epoch 255 Iter 3 subLoss 10917.4 multi -4.97 import weight 0.00
Epoch 255 Iter 4 subLoss 14159.2 multi 1.00 import weight 0.00
Epoch 255 Iter 5 subLoss 14124.5 multi 1.00 import weight 0.00
Epoch 255 Iter 6 subLoss 13017.4 multi 1.00 import weight 0.00
Epoch 255 Iter 7 subLoss 12660.2 multi 3.99 import weight 0.00
Epoch 255 Iter 8 subLoss 9687.5 multi 3.98 import weight 0.00
Epoch 255 Iter 9 subLoss 7826.6 multi -1.98 import weight 0.00
Epoch 255 Iter 10 subLoss 8641.7 multi -4.97 import weight 0.00
Epoch 255 Iter 11 subLoss 10984.0 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0165 / 0.10622 / 28.93
Entropy seen (from low to high)
[985, 565, 378, 303, 220, 149, 87, 81, 79, 86, 82, 108, 115, 99, 102, 99, 73, 81, 78, 53, 70, 49, 51, 51, 51, 54, 55, 68, 77, 102, 101, 103, 93, 109, 82, 62, 46, 25, 22, 18, 11, 6, 6, 2, 1, 1, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 12, 23, 39, 46, 90, 106, 128, 129, 166, 188, 184, 191, 190, 164, 167, 144, 162, 168, 186, 159, 157, 159, 181, 175, 186, 167, 141, 120, 134, 112, 118, 103, 94, 77, 65, 41, 39, 31, 30, 28, 25, 15, 12, 6, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.5, 30.9, 33.3, 36.9, 40.4, 43.8, 47.3, 50.7, 54.5, 57.6, 61.0, 64.7, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.9, 43.9, 70.3, 44.6, 76.9, 73.6, 86.4, 93.9, 93.4, 96.2, 97.2]
[0, 0, 0, 0, 0, 0, 0, 2, 1, 8, 25, 27, 47, 78, 91, 140, 164, 152, 132, 108]
Epoch 255 Acc: 84.34 BMA: 96.63 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1098 train Loss: 12408.9 test Loss: 2337.8
Epoch 256 Iter 0 subLoss 11948.6 multi 3.99 import weight 0.00
Epoch 256 Iter 1 subLoss 10125.9 multi -4.97 import weight 0.00
Epoch 256 Iter 2 subLoss 12086.6 multi 1.00 import weight 0.00
Epoch 256 Iter 3 subLoss 11948.0 multi 6.97 import weight 0.00
Epoch 256 Iter 4 subLoss 8414.9 multi 1.00 import weight 0.00
Epoch 256 Iter 5 subLoss 8695.1 multi 9.96 import weight 0.00
Epoch 256 Iter 6 subLoss 5572.5 multi 3.99 import weight 0.00
Epoch 256 Iter 7 subLoss 5223.0 multi -4.97 import weight 0.00
Epoch 256 Iter 8 subLoss 5707.0 multi -1.99 import weight 0.00
Epoch 256 Iter 9 subLoss 5979.2 multi -1.99 import weight 0.00
Epoch 256 Iter 10 subLoss 6180.0 multi 6.97 import weight 0.00
Epoch 256 Iter 11 subLoss 5364.3 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0169 / 0.10640 / 27.48
Entropy seen (from low to high)
[1068, 563, 415, 268, 200, 134, 77, 81, 90, 78, 112, 96, 96, 111, 107, 78, 81, 78, 55, 71, 59, 59, 58, 57, 50, 53, 62, 64, 80, 85, 71, 78, 79, 71, 83, 68, 72, 47, 29, 20, 13, 12, 6, 2, 1, 1, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 12, 25, 38, 51, 90, 122, 129, 142, 174, 160, 183, 191, 195, 155, 149, 168, 160, 166, 166, 153, 145, 155, 184, 166, 176, 173, 133, 135, 128, 128, 97, 113, 101, 79, 72, 56, 43, 24, 28, 35, 23, 15, 11, 9, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 30.9, 33.6, 36.5, 40.3, 43.9, 47.1, 50.9, 54.2, 57.6, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 45.4, 49.9, 46.3, 66.6, 71.9, 83.4, 84.1, 88.6, 88.5, 97.8, 96.3]
[0, 0, 0, 0, 0, 0, 0, 2, 4, 11, 22, 41, 66, 89, 121, 120, 132, 114, 94, 109]
Epoch 256 Acc: 91.55 BMA: 96.36 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 536 train Loss: 6550.9 test Loss: 1167.6
Epoch 257 Iter 0 subLoss 6906.9 multi -4.97 import weight 0.00
Epoch 257 Iter 1 subLoss 7655.6 multi 6.97 import weight 0.00
Epoch 257 Iter 2 subLoss 5458.4 multi 9.96 import weight 0.00
Epoch 257 Iter 3 subLoss 4771.2 multi -16.91 import weight 0.00
Epoch 257 Iter 4 subLoss 6081.9 multi 9.96 import weight 0.00
Epoch 257 Iter 5 subLoss 4910.1 multi -1.99 import weight 0.00
Epoch 257 Iter 6 subLoss 5396.1 multi 3.98 import weight 0.00
Epoch 257 Iter 7 subLoss 4429.6 multi 3.99 import weight 0.00
Epoch 257 Iter 8 subLoss 4166.3 multi 9.96 import weight 0.00
Epoch 257 Iter 9 subLoss 3984.3 multi -1.98 import weight 0.00
Epoch 257 Iter 10 subLoss 3779.3 multi 1.00 import weight 0.00
Epoch 257 Iter 11 subLoss 3699.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0155 / 0.10796 / 25.94
Entropy seen (from low to high)
[1127, 597, 409, 273, 174, 108, 83, 98, 84, 116, 106, 100, 112, 115, 86, 78, 73, 76, 58, 59, 65, 66, 60, 43, 64, 67, 66, 83, 77, 72, 72, 65, 73, 64, 67, 59, 46, 27, 25, 13, 12, 9, 8, 2, 1, 1, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 8, 25, 32, 51, 70, 110, 138, 128, 168, 166, 180, 195, 168, 175, 173, 157, 138, 151, 173, 149, 141, 144, 178, 155, 185, 174, 142, 137, 128, 130, 114, 113, 111, 96, 82, 66, 49, 34, 28, 32, 23, 19, 12, 10, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 29.8, 33.0, 36.8, 40.4, 43.7, 47.5, 50.7, 54.3, 57.6, 61.3, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 79.9, 33.3, 49.9, 45.7, 64.5, 68.9, 78.0, 79.4, 87.7, 88.4, 98.2, 96.1]
[0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 22, 35, 48, 74, 73, 107, 114, 121, 115, 103]
Epoch 257 Acc: 96.52 BMA: 96.58 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 369 train Loss: 3875.5 test Loss: 553.6
Epoch 258 Iter 0 subLoss 3877.1 multi 9.96 import weight 0.00
Epoch 258 Iter 1 subLoss 3002.1 multi 18.91 import weight 0.00
Epoch 258 Iter 2 subLoss 3187.7 multi 9.96 import weight 0.00
Epoch 258 Iter 3 subLoss 2654.0 multi 6.97 import weight 0.00
Epoch 258 Iter 4 subLoss 2711.5 multi 1.00 import weight 0.00
Epoch 258 Iter 5 subLoss 3441.6 multi 30.85 import weight 1.00
Epoch 258 Iter 6 subLoss 3289.2 multi 15.93 import weight 0.00
Epoch 258 Iter 7 subLoss 3419.1 multi -4.97 import weight 0.00
Epoch 258 Iter 8 subLoss 4729.7 multi 9.96 import weight 0.00
Epoch 258 Iter 9 subLoss 3346.7 multi 3.98 import weight 0.00
Epoch 258 Iter 10 subLoss 2545.5 multi 15.93 import weight 0.00
Epoch 258 Iter 11 subLoss 2953.8 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0135 / 0.10811 / 26.13
Entropy seen (from low to high)
[1175, 626, 410, 257, 153, 105, 88, 107, 116, 114, 114, 111, 124, 89, 88, 82, 70, 63, 51, 62, 67, 57, 64, 72, 69, 78, 74, 78, 70, 74, 64, 62, 66, 72, 46, 31, 22, 21, 13, 15, 6, 5, 4, 3, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 1, 8, 18, 30, 42, 64, 86, 119, 139, 158, 189, 180, 189, 202, 192, 162, 171, 159, 151, 166, 159, 136, 161, 130, 139, 177, 167, 151, 144, 133, 127, 121, 115, 96, 108, 82, 69, 60, 31, 36, 29, 28, 13, 12, 8, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 33.4, 36.9, 40.2, 43.7, 47.5, 50.6, 54.2, 57.8, 61.3, 64.8, 68.2]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 49.9, 12.4, 47.0, 49.9, 69.2, 60.3, 64.1, 81.1, 88.0, 90.9, 99.9, 95.4]
[0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 17, 16, 39, 58, 53, 90, 84, 121, 122, 131]
Epoch 258 Acc: 98.09 BMA: 97.26 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 295 train Loss: 2552.9 test Loss: 306.1
Epoch 259 Iter 0 subLoss 2600.0 multi -4.97 import weight 0.00
Epoch 259 Iter 1 subLoss 2216.7 multi -4.97 import weight 0.00
Epoch 259 Iter 2 subLoss 2804.8 multi 3.98 import weight 0.00
Epoch 259 Iter 3 subLoss 2636.4 multi 6.97 import weight 0.00
Epoch 259 Iter 4 subLoss 2795.4 multi -7.96 import weight 0.00
Epoch 259 Iter 5 subLoss 2653.9 multi 9.96 import weight 0.00
Epoch 259 Iter 6 subLoss 2634.2 multi 9.96 import weight 0.00
Epoch 259 Iter 7 subLoss 2314.8 multi 9.96 import weight 0.00
Epoch 259 Iter 8 subLoss 2476.1 multi 24.88 import weight 0.00
Epoch 259 Iter 9 subLoss 2832.2 multi 12.94 import weight 0.00
Epoch 259 Iter 10 subLoss 2406.9 multi -22.88 import weight 0.00
Epoch 259 Iter 11 subLoss 2952.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0120 / 0.10827 / 25.68
Entropy seen (from low to high)
[1178, 648, 425, 246, 141, 101, 122, 106, 126, 122, 126, 131, 97, 94, 86, 75, 60, 66, 68, 62, 53, 65, 72, 84, 80, 79, 72, 82, 73, 57, 67, 69, 50, 40, 31, 17, 19, 13, 15, 5, 4, 5, 3, 3, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 1, 8, 16, 27, 38, 59, 78, 105, 115, 167, 191, 170, 193, 202, 210, 181, 180, 168, 172, 171, 147, 139, 139, 140, 141, 139, 156, 176, 145, 107, 133, 138, 103, 105, 91, 99, 79, 58, 44, 40, 33, 23, 13, 11, 8, 2]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.0, 30.6, 33.3, 36.9, 40.4, 43.7, 47.4, 50.7, 54.2, 57.8, 61.2, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.3, 33.3, 44.4, 61.5, 59.2, 59.9, 69.6, 78.2, 89.1, 93.1, 93.8, 97.7]
[0, 0, 0, 0, 0, 0, 0, 1, 3, 9, 9, 13, 27, 50, 56, 46, 74, 88, 130, 136]
Epoch 259 Acc: 97.51 BMA: 97.47 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 295 train Loss: 2922.6 test Loss: 356.3
Epoch 260 Iter 0 subLoss 2865.1 multi -7.96 import weight 0.00
Epoch 260 Iter 1 subLoss 4872.5 multi -1.99 import weight 0.00
Epoch 260 Iter 2 subLoss 12290.1 multi 1.00 import weight 0.00
Epoch 260 Iter 3 subLoss 3660.7 multi 6.97 import weight 0.00
Epoch 260 Iter 4 subLoss 2994.7 multi -7.96 import weight 0.00
Epoch 260 Iter 5 subLoss 2661.4 multi 3.99 import weight 0.00
Epoch 260 Iter 6 subLoss 2223.7 multi 9.96 import weight 0.00
Epoch 260 Iter 7 subLoss 2598.3 multi -1.99 import weight 0.00
Epoch 260 Iter 8 subLoss 2831.1 multi 15.93 import weight 0.00
Epoch 260 Iter 9 subLoss 2009.1 multi 3.98 import weight 0.00
Epoch 260 Iter 10 subLoss 2028.5 multi 1.00 import weight 0.00
Epoch 260 Iter 11 subLoss 2348.3 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0109 / 0.10972 / 24.82
Entropy seen (from low to high)
[1228, 683, 399, 230, 130, 111, 124, 136, 133, 136, 133, 109, 96, 90, 79, 65, 66, 76, 63, 59, 68, 72, 86, 85, 74, 71, 87, 67, 65, 69, 62, 36, 34, 27, 16, 21, 15, 13, 8, 3, 4, 3, 4, 2, 1, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 2, 3, 14, 24, 22, 59, 79, 84, 110, 136, 192, 178, 173, 232, 196, 196, 205, 157, 154, 186, 131, 158, 141, 132, 127, 135, 156, 139, 147, 150, 111, 137, 132, 113, 96, 93, 102, 61, 65, 39, 37, 21, 12, 13, 8, 3]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3, 33.1, 36.5, 40.5, 43.8, 47.5, 50.8, 54.3, 57.8, 61.2, 64.7, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.9, 39.9, 66.6, 46.1, 64.9, 64.8, 69.7, 81.2, 73.4, 91.9, 94.9, 95.6]
[0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 9, 13, 20, 37, 43, 48, 49, 75, 100, 138]
Epoch 260 Acc: 98.13 BMA: 97.65 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 234 train Loss: 2488.0 test Loss: 291.0
Epoch 261 Iter 0 subLoss 2707.1 multi 6.97 import weight 0.00
Epoch 261 Iter 1 subLoss 2507.4 multi -25.87 import weight 0.00
Epoch 261 Iter 2 subLoss 2485.5 multi -31.84 import weight 0.00
Epoch 261 Iter 3 subLoss 4870.8 multi 1.00 import weight 0.00
Epoch 261 Iter 4 subLoss 4161.8 multi 12.94 import weight 0.00
Epoch 261 Iter 5 subLoss 3422.2 multi 1.00 import weight 0.00
Epoch 261 Iter 6 subLoss 3033.9 multi -1.99 import weight 0.00
Epoch 261 Iter 7 subLoss 3842.0 multi 12.94 import weight 0.00
Epoch 261 Iter 8 subLoss 2765.8 multi 15.93 import weight 0.00
Epoch 261 Iter 9 subLoss 2721.2 multi -7.96 import weight 0.00
Epoch 261 Iter 10 subLoss 2940.6 multi -4.97 import weight 0.00
Epoch 261 Iter 11 subLoss 3441.1 multi 33.84 import weight 1.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 261 Acc: 72.45 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 33.84 Pidx 344 train Loss: 19848.1 test Loss: 4558.8
Epoch 262 Iter 0 subLoss 20082.8 multi 1.00 import weight 0.00
Epoch 262 Iter 1 subLoss 5409.8 multi -4.97 import weight 0.00
Epoch 262 Iter 2 subLoss 13732.8 multi -4.97 import weight 0.00
Epoch 262 Iter 3 subLoss 227357.6 multi 1.00 import weight 0.00
Epoch 262 Iter 4 subLoss 18013.5 multi -1.99 import weight 0.00
Epoch 262 Iter 5 subLoss 27203.4 multi -1.99 import weight 0.00
Epoch 262 Iter 6 subLoss 37012.8 multi 1.00 import weight 0.00
Epoch 262 Iter 7 subLoss 31682.1 multi 1.00 import weight 0.00
Epoch 262 Iter 8 subLoss 27994.2 multi 1.00 import weight 0.00
Epoch 262 Iter 9 subLoss 23302.7 multi 1.00 import weight 0.00
Epoch 262 Iter 10 subLoss 20012.2 multi -1.99 import weight 0.00
Epoch 262 Iter 11 subLoss 27597.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 262 Acc: 58.49 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2759 train Loss: 24865.7 test Loss: 5499.7
Epoch 263 Iter 0 subLoss 24139.3 multi 1.00 import weight 0.00
Epoch 263 Iter 1 subLoss 20669.1 multi -1.99 import weight 0.00
Epoch 263 Iter 2 subLoss 26508.6 multi 1.00 import weight 0.00
Epoch 263 Iter 3 subLoss 24160.6 multi 1.00 import weight 0.00
Epoch 263 Iter 4 subLoss 22134.1 multi 1.00 import weight 0.00
Epoch 263 Iter 5 subLoss 18350.7 multi 1.00 import weight 0.00
Epoch 263 Iter 6 subLoss 15278.7 multi 1.00 import weight 0.00
Epoch 263 Iter 7 subLoss 13285.0 multi 3.99 import weight 0.00
Epoch 263 Iter 8 subLoss 7203.4 multi 3.99 import weight 0.00
Epoch 263 Iter 9 subLoss 5472.4 multi 1.00 import weight 0.00
Epoch 263 Iter 10 subLoss 4565.8 multi 1.00 import weight 0.00
Epoch 263 Iter 11 subLoss 4755.5 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 263 Acc: 96.13 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 475 train Loss: 4023.1 test Loss: 713.1
Epoch 264 Iter 0 subLoss 3921.2 multi 6.97 import weight 0.00
Epoch 264 Iter 1 subLoss 3111.5 multi 1.00 import weight 0.00
Epoch 264 Iter 2 subLoss 3104.5 multi -4.97 import weight 0.00
Epoch 264 Iter 3 subLoss 3582.0 multi -7.96 import weight 0.00
Epoch 264 Iter 4 subLoss 3759.2 multi -7.96 import weight 0.00
Epoch 264 Iter 5 subLoss 5697.3 multi -1.99 import weight 0.00
Epoch 264 Iter 6 subLoss 6621.8 multi -1.99 import weight 0.00
Epoch 264 Iter 7 subLoss 7813.7 multi 6.97 import weight 0.00
Epoch 264 Iter 8 subLoss 3954.1 multi -1.98 import weight 0.00
Epoch 264 Iter 9 subLoss 4440.9 multi -1.99 import weight 0.00
Epoch 264 Iter 10 subLoss 5340.0 multi -1.98 import weight 0.00
Epoch 264 Iter 11 subLoss 5542.5 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 264 Acc: 97.08 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 554 train Loss: 3660.0 test Loss: 561.9
Epoch 265 Iter 0 subLoss 3549.7 multi 21.90 import weight 0.00
Epoch 265 Iter 1 subLoss 2918.2 multi 6.97 import weight 0.00
Epoch 265 Iter 2 subLoss 2512.2 multi 15.93 import weight 0.00
Epoch 265 Iter 3 subLoss 2760.1 multi 18.91 import weight 0.00
Epoch 265 Iter 4 subLoss 2526.7 multi -10.94 import weight 0.00
Epoch 265 Iter 5 subLoss 3313.7 multi -16.91 import weight 0.00
Epoch 265 Iter 6 subLoss 7532.0 multi -1.99 import weight 0.00
Epoch 265 Iter 7 subLoss 12803.2 multi -4.97 import weight 0.00
Epoch 265 Iter 8 subLoss 154926.8 multi 1.00 import weight 0.00
Epoch 265 Iter 9 subLoss 10927.3 multi 1.00 import weight 0.00
Epoch 265 Iter 10 subLoss 8627.8 multi 3.99 import weight 0.00
Epoch 265 Iter 11 subLoss 5329.0 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 265 Acc: 93.75 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 532 train Loss: 7304.2 test Loss: 1039.3
Epoch 266 Iter 0 subLoss 6698.6 multi 1.00 import weight 0.00
Epoch 266 Iter 1 subLoss 6838.6 multi 1.00 import weight 0.00
Epoch 266 Iter 2 subLoss 5673.9 multi -4.97 import weight 0.00
Epoch 266 Iter 3 subLoss 7769.9 multi 9.96 import weight 0.00
Epoch 266 Iter 4 subLoss 4757.4 multi 6.97 import weight 0.00
Epoch 266 Iter 5 subLoss 3776.3 multi 3.99 import weight 0.00
Epoch 266 Iter 6 subLoss 3684.7 multi -1.98 import weight 0.00
Epoch 266 Iter 7 subLoss 3793.7 multi -13.93 import weight 0.00
Epoch 266 Iter 8 subLoss 5039.8 multi -7.96 import weight 0.00
Epoch 266 Iter 9 subLoss 6680.0 multi 3.98 import weight 0.00
Epoch 266 Iter 10 subLoss 5461.3 multi -7.96 import weight 0.00
Epoch 266 Iter 11 subLoss 7492.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 266 Acc: 94.05 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 749 train Loss: 7466.6 test Loss: 1094.2
Epoch 267 Iter 0 subLoss 7551.7 multi 1.00 import weight 0.00
Epoch 267 Iter 1 subLoss 6651.1 multi 1.00 import weight 0.00
Epoch 267 Iter 2 subLoss 7234.1 multi 3.99 import weight 0.00
Epoch 267 Iter 3 subLoss 6195.2 multi -4.97 import weight 0.00
Epoch 267 Iter 4 subLoss 6242.1 multi 1.00 import weight 0.00
Epoch 267 Iter 5 subLoss 6594.3 multi 3.99 import weight 0.00
Epoch 267 Iter 6 subLoss 6050.3 multi 12.94 import weight 0.00
Epoch 267 Iter 7 subLoss 4323.9 multi 6.97 import weight 0.00
Epoch 267 Iter 8 subLoss 3723.7 multi -7.96 import weight 0.00
Epoch 267 Iter 9 subLoss 4201.8 multi -13.93 import weight 0.00
Epoch 267 Iter 10 subLoss 5752.9 multi 3.99 import weight 0.00
Epoch 267 Iter 11 subLoss 5226.0 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 267 Acc: 96.87 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 522 train Loss: 5561.1 test Loss: 739.6
Epoch 268 Iter 0 subLoss 5224.4 multi 1.00 import weight 0.00
Epoch 268 Iter 1 subLoss 5111.2 multi 15.93 import weight 0.00
Epoch 268 Iter 2 subLoss 3700.1 multi -1.98 import weight 0.00
Epoch 268 Iter 3 subLoss 4082.7 multi 1.00 import weight 0.00
Epoch 268 Iter 4 subLoss 3492.0 multi 3.99 import weight 0.00
Epoch 268 Iter 5 subLoss 3715.9 multi 3.99 import weight 0.00
Epoch 268 Iter 6 subLoss 3448.3 multi 36.82 import weight 1.00
Epoch 268 Iter 7 subLoss 3508.7 multi -1.99 import weight 0.00
Epoch 268 Iter 8 subLoss 4462.3 multi 18.91 import weight 0.00
Epoch 268 Iter 9 subLoss 4339.6 multi 6.97 import weight 0.00
Epoch 268 Iter 10 subLoss 3223.3 multi -1.98 import weight 0.00
Epoch 268 Iter 11 subLoss 3311.8 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 268 Acc: 97.16 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 331 train Loss: 5089.6 test Loss: 537.1
Epoch 269 Iter 0 subLoss 5330.6 multi -1.99 import weight 0.00
Epoch 269 Iter 1 subLoss 5334.2 multi 1.00 import weight 0.00
Epoch 269 Iter 2 subLoss 4455.8 multi -13.93 import weight 0.00
Epoch 269 Iter 3 subLoss 14256.4 multi 1.00 import weight 0.00
Epoch 269 Iter 4 subLoss 11131.1 multi 1.00 import weight 0.00
Epoch 269 Iter 5 subLoss 8738.8 multi 1.00 import weight 0.00
Epoch 269 Iter 6 subLoss 7221.5 multi 1.00 import weight 0.00
Epoch 269 Iter 7 subLoss 7103.2 multi 1.00 import weight 0.00
Epoch 269 Iter 8 subLoss 6136.8 multi 3.99 import weight 0.00
Epoch 269 Iter 9 subLoss 4479.1 multi -10.94 import weight 0.00
Epoch 269 Iter 10 subLoss 8248.9 multi -4.97 import weight 0.00
Epoch 269 Iter 11 subLoss 12919.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 269 Acc: 94.98 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1291 train Loss: 11069.2 test Loss: 1136.9
Epoch 270 Iter 0 subLoss 11243.2 multi 1.00 import weight 0.00
Epoch 270 Iter 1 subLoss 8885.7 multi -1.98 import weight 0.00
Epoch 270 Iter 2 subLoss 12001.4 multi 1.00 import weight 0.00
Epoch 270 Iter 3 subLoss 9487.4 multi 3.99 import weight 0.00
Epoch 270 Iter 4 subLoss 7176.8 multi -1.99 import weight 0.00
Epoch 270 Iter 5 subLoss 8149.5 multi -4.97 import weight 0.00
Epoch 270 Iter 6 subLoss 10526.8 multi 3.99 import weight 0.00
Epoch 270 Iter 7 subLoss 7192.7 multi 1.00 import weight 0.00
Epoch 270 Iter 8 subLoss 6712.6 multi -1.99 import weight 0.00
Epoch 270 Iter 9 subLoss 7592.9 multi 1.00 import weight 0.00
Epoch 270 Iter 10 subLoss 6928.0 multi 3.98 import weight 0.00
Epoch 270 Iter 11 subLoss 6162.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 270 Acc: 97.30 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 616 train Loss: 5048.0 test Loss: 547.4
Epoch 271 Iter 0 subLoss 4452.5 multi -10.94 import weight 0.00
Epoch 271 Iter 1 subLoss 6456.0 multi 3.99 import weight 0.00
Epoch 271 Iter 2 subLoss 5741.2 multi -4.97 import weight 0.00
Epoch 271 Iter 3 subLoss 6718.7 multi 1.00 import weight 0.00
Epoch 271 Iter 4 subLoss 7228.8 multi 3.99 import weight 0.00
Epoch 271 Iter 5 subLoss 5886.3 multi -4.97 import weight 0.00
Epoch 271 Iter 6 subLoss 7066.4 multi 3.98 import weight 0.00
Epoch 271 Iter 7 subLoss 5452.7 multi 12.94 import weight 0.00
Epoch 271 Iter 8 subLoss 4122.7 multi 12.94 import weight 0.00
Epoch 271 Iter 9 subLoss 3447.8 multi 39.81 import weight 1.00
Epoch 271 Iter 10 subLoss 2530.7 multi -1.99 import weight 0.00
Epoch 271 Iter 11 subLoss 2843.4 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 271 Acc: 96.81 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 284 train Loss: 3666.2 test Loss: 497.4
Epoch 272 Iter 0 subLoss 3787.4 multi 12.94 import weight 0.00
Epoch 272 Iter 1 subLoss 3006.4 multi 18.91 import weight 0.00
Epoch 272 Iter 2 subLoss 2393.6 multi 24.88 import weight 0.00
Epoch 272 Iter 3 subLoss 2821.3 multi -22.88 import weight 0.00
Epoch 272 Iter 4 subLoss 5645.6 multi -1.98 import weight 0.00
Epoch 272 Iter 5 subLoss 7952.5 multi 1.00 import weight 0.00
Epoch 272 Iter 6 subLoss 6573.1 multi 6.97 import weight 0.00
Epoch 272 Iter 7 subLoss 2429.4 multi -10.94 import weight 0.00
Epoch 272 Iter 8 subLoss 3538.1 multi -1.99 import weight 0.00
Epoch 272 Iter 9 subLoss 3392.7 multi -4.97 import weight 0.00
Epoch 272 Iter 10 subLoss 5060.0 multi 1.00 import weight 0.00
Epoch 272 Iter 11 subLoss 4124.4 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 272 Acc: 98.09 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 412 train Loss: 2807.1 test Loss: 315.3
Epoch 273 Iter 0 subLoss 2635.7 multi 12.94 import weight 0.00
Epoch 273 Iter 1 subLoss 2585.8 multi 15.93 import weight 0.00
Epoch 273 Iter 2 subLoss 2492.0 multi 21.90 import weight 0.00
Epoch 273 Iter 3 subLoss 2565.3 multi 18.91 import weight 0.00
Epoch 273 Iter 4 subLoss 2397.0 multi 27.87 import weight 0.00
Epoch 273 Iter 5 subLoss 2345.3 multi 1.00 import weight 0.00
Epoch 273 Iter 6 subLoss 2516.4 multi 18.91 import weight 0.00
Epoch 273 Iter 7 subLoss 2379.9 multi -1.99 import weight 0.00
Epoch 273 Iter 8 subLoss 2507.3 multi -25.87 import weight 0.00
Epoch 273 Iter 9 subLoss 7237.6 multi 1.00 import weight 0.00
Epoch 273 Iter 10 subLoss 5280.3 multi -4.97 import weight 0.00
Epoch 273 Iter 11 subLoss 16826.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 273 Acc: 89.30 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1682 train Loss: 6951.1 test Loss: 1469.3
Epoch 274 Iter 0 subLoss 6495.1 multi 9.96 import weight 0.00
Epoch 274 Iter 1 subLoss 2361.4 multi 6.97 import weight 0.00
Epoch 274 Iter 2 subLoss 2632.8 multi 15.93 import weight 0.00
Epoch 274 Iter 3 subLoss 2523.6 multi -10.94 import weight 0.00
Epoch 274 Iter 4 subLoss 2996.6 multi -4.97 import weight 0.00
Epoch 274 Iter 5 subLoss 2510.1 multi 18.91 import weight 0.00
Epoch 274 Iter 6 subLoss 2085.2 multi -4.97 import weight 0.00
Epoch 274 Iter 7 subLoss 2483.3 multi -28.85 import weight 0.00
Epoch 274 Iter 8 subLoss 3196.4 multi -4.97 import weight 0.00
Epoch 274 Iter 9 subLoss 3321.1 multi -16.91 import weight 0.00
Epoch 274 Iter 10 subLoss 24026.3 multi 1.00 import weight 0.00
Epoch 274 Iter 11 subLoss 6647.4 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 274 Acc: 41.90 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 664 train Loss: 54128.3 test Loss: 10565.0
Epoch 275 Iter 0 subLoss 53594.2 multi 1.00 import weight 0.00
Epoch 275 Iter 1 subLoss 32632.0 multi 1.00 import weight 0.00
Epoch 275 Iter 2 subLoss 22231.0 multi 1.00 import weight 0.00
Epoch 275 Iter 3 subLoss 16264.3 multi 3.99 import weight 0.00
Epoch 275 Iter 4 subLoss 6855.9 multi -1.98 import weight 0.00
Epoch 275 Iter 5 subLoss 7734.2 multi 1.00 import weight 0.00
Epoch 275 Iter 6 subLoss 7133.9 multi 1.00 import weight 0.00
Epoch 275 Iter 7 subLoss 6344.5 multi 6.97 import weight 0.00
Epoch 275 Iter 8 subLoss 3865.5 multi -7.96 import weight 0.00
Epoch 275 Iter 9 subLoss 5438.9 multi 6.97 import weight 0.00
Epoch 275 Iter 10 subLoss 4246.1 multi 12.94 import weight 0.00
Epoch 275 Iter 11 subLoss 2542.8 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 275 Acc: 98.23 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 254 train Loss: 2712.7 test Loss: 284.5
Epoch 276 Iter 0 subLoss 2604.0 multi -7.96 import weight 0.00
Epoch 276 Iter 1 subLoss 3075.2 multi -1.99 import weight 0.00
Epoch 276 Iter 2 subLoss 2752.4 multi -22.88 import weight 0.00
Epoch 276 Iter 3 subLoss 3401.4 multi 9.96 import weight 0.00
Epoch 276 Iter 4 subLoss 2325.9 multi -13.93 import weight 0.00
Epoch 276 Iter 5 subLoss 3961.0 multi -4.97 import weight 0.00
Epoch 276 Iter 6 subLoss 4367.4 multi 6.97 import weight 0.00
Epoch 276 Iter 7 subLoss 3446.7 multi 42.79 import weight 1.00
Epoch 276 Iter 8 subLoss 2732.6 multi -4.97 import weight 0.00
Epoch 276 Iter 9 subLoss 2980.5 multi 3.99 import weight 0.00
Epoch 276 Iter 10 subLoss 2920.0 multi 1.00 import weight 0.00
Epoch 276 Iter 11 subLoss 3213.4 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 276 Acc: 98.17 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 321 train Loss: 2411.1 test Loss: 292.0
Epoch 277 Iter 0 subLoss 2505.7 multi -22.88 import weight 0.00
Epoch 277 Iter 1 subLoss 3103.0 multi -1.99 import weight 0.00
Epoch 277 Iter 2 subLoss 2663.0 multi 6.97 import weight 0.00
Epoch 277 Iter 3 subLoss 2525.2 multi -10.94 import weight 0.00
Epoch 277 Iter 4 subLoss 3392.2 multi -1.99 import weight 0.00
Epoch 277 Iter 5 subLoss 3372.3 multi 6.97 import weight 0.00
Epoch 277 Iter 6 subLoss 2613.2 multi -4.97 import weight 0.00
Epoch 277 Iter 7 subLoss 2669.5 multi 9.96 import weight 0.00
Epoch 277 Iter 8 subLoss 2126.0 multi 1.00 import weight 0.00
Epoch 277 Iter 9 subLoss 2579.8 multi -16.91 import weight 0.00
Epoch 277 Iter 10 subLoss 2850.3 multi 18.91 import weight 0.00
Epoch 277 Iter 11 subLoss 3085.5 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 277 Acc: 98.19 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 308 train Loss: 2459.7 test Loss: 289.5
Epoch 278 Iter 0 subLoss 2958.8 multi 1.00 import weight 0.00
Epoch 278 Iter 1 subLoss 2565.5 multi 21.90 import weight 0.00
Epoch 278 Iter 2 subLoss 2351.5 multi -7.96 import weight 0.00
Epoch 278 Iter 3 subLoss 2243.3 multi -1.99 import weight 0.00
Epoch 278 Iter 4 subLoss 2524.9 multi -7.96 import weight 0.00
Epoch 278 Iter 5 subLoss 2684.6 multi -4.97 import weight 0.00
Epoch 278 Iter 6 subLoss 2882.6 multi -4.97 import weight 0.00
Epoch 278 Iter 7 subLoss 3449.5 multi 45.78 import weight 1.00
Epoch 278 Iter 8 subLoss 24949.6 multi -1.99 import weight 0.00
Epoch 278 Iter 9 subLoss 140625.0 multi 1.00 import weight 0.00
Epoch 278 Iter 10 subLoss 15746.8 multi 1.00 import weight 0.00
Epoch 278 Iter 11 subLoss 9061.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 278 Acc: 94.57 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 906 train Loss: 6658.3 test Loss: 1063.7
Epoch 279 Iter 0 subLoss 6570.5 multi 9.96 import weight 0.00
Epoch 279 Iter 1 subLoss 5349.1 multi -4.97 import weight 0.00
Epoch 279 Iter 2 subLoss 12122.6 multi 1.00 import weight 0.00
Epoch 279 Iter 3 subLoss 6261.8 multi -1.99 import weight 0.00
Epoch 279 Iter 4 subLoss 10804.7 multi -1.99 import weight 0.00
Epoch 279 Iter 5 subLoss 32561.1 multi 1.00 import weight 0.00
Epoch 279 Iter 6 subLoss 7322.2 multi 3.98 import weight 0.00
Epoch 279 Iter 7 subLoss 3364.6 multi 3.99 import weight 0.00
Epoch 279 Iter 8 subLoss 3879.6 multi 9.96 import weight 0.00
Epoch 279 Iter 9 subLoss 2959.4 multi 3.99 import weight 0.00
Epoch 279 Iter 10 subLoss 3066.0 multi 9.96 import weight 0.00
Epoch 279 Iter 11 subLoss 2556.4 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 279 Acc: 96.85 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 255 train Loss: 3992.1 test Loss: 511.4
Epoch 280 Iter 0 subLoss 3600.3 multi -13.93 import weight 0.00
Epoch 280 Iter 1 subLoss 9159.2 multi 6.97 import weight 0.00
Epoch 280 Iter 2 subLoss 3956.4 multi 1.00 import weight 0.00
Epoch 280 Iter 3 subLoss 3609.5 multi -10.94 import weight 0.00
Epoch 280 Iter 4 subLoss 6639.2 multi 15.93 import weight 0.00
Epoch 280 Iter 5 subLoss 3351.2 multi -1.99 import weight 0.00
Epoch 280 Iter 6 subLoss 3762.8 multi -4.97 import weight 0.00
Epoch 280 Iter 7 subLoss 6267.1 multi 1.00 import weight 0.00
Epoch 280 Iter 8 subLoss 5049.7 multi 3.99 import weight 0.00
Epoch 280 Iter 9 subLoss 3621.9 multi 3.99 import weight 0.00
Epoch 280 Iter 10 subLoss 3034.3 multi 1.00 import weight 0.00
Epoch 280 Iter 11 subLoss 3216.1 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 280 Acc: 97.55 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 321 train Loss: 2738.8 test Loss: 387.1
Epoch 281 Iter 0 subLoss 2770.0 multi -10.94 import weight 0.00
Epoch 281 Iter 1 subLoss 3449.1 multi 48.76 import weight 1.00
Epoch 281 Iter 2 subLoss 7537.8 multi 1.00 import weight 0.00
Epoch 281 Iter 3 subLoss 5832.0 multi 6.97 import weight 0.00
Epoch 281 Iter 4 subLoss 2343.9 multi 3.99 import weight 0.00
Epoch 281 Iter 5 subLoss 2471.4 multi 27.87 import weight 0.00
Epoch 281 Iter 6 subLoss 1949.2 multi 1.00 import weight 0.00
Epoch 281 Iter 7 subLoss 2493.0 multi 21.90 import weight 0.00
Epoch 281 Iter 8 subLoss 2197.0 multi 3.99 import weight 0.00
Epoch 281 Iter 9 subLoss 2203.1 multi 6.97 import weight 0.00
Epoch 281 Iter 10 subLoss 1945.7 multi 3.98 import weight 0.00
Epoch 281 Iter 11 subLoss 2195.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 281 Acc: 98.52 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 219 train Loss: 2171.8 test Loss: 248.7
Epoch 282 Iter 0 subLoss 2009.4 multi 6.97 import weight 0.00
Epoch 282 Iter 1 subLoss 1908.8 multi -1.99 import weight 0.00
Epoch 282 Iter 2 subLoss 1972.0 multi 1.00 import weight 0.00
Epoch 282 Iter 3 subLoss 2324.7 multi -10.94 import weight 0.00
Epoch 282 Iter 4 subLoss 1973.7 multi 3.99 import weight 0.00
Epoch 282 Iter 5 subLoss 1678.1 multi 1.00 import weight 0.00
Epoch 282 Iter 6 subLoss 2200.5 multi 6.97 import weight 0.00
Epoch 282 Iter 7 subLoss 1980.0 multi 6.97 import weight 0.00
Epoch 282 Iter 8 subLoss 2259.8 multi -1.99 import weight 0.00
Epoch 282 Iter 9 subLoss 2236.2 multi -7.96 import weight 0.00
Epoch 282 Iter 10 subLoss 2230.3 multi -4.97 import weight 0.00
Epoch 282 Iter 11 subLoss 2396.8 multi 30.85 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 282 Acc: 98.29 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 30.85 Pidx 239 train Loss: 2561.0 test Loss: 263.9
Epoch 283 Iter 0 subLoss 2238.2 multi -1.99 import weight 0.00
Epoch 283 Iter 1 subLoss 2575.7 multi -16.91 import weight 0.00
Epoch 283 Iter 2 subLoss 6001.1 multi -4.97 import weight 0.00
Epoch 283 Iter 3 subLoss 15927.7 multi 1.00 import weight 0.00
Epoch 283 Iter 4 subLoss 8424.8 multi 1.00 import weight 0.00
Epoch 283 Iter 5 subLoss 6771.2 multi 1.00 import weight 0.00
Epoch 283 Iter 6 subLoss 6442.1 multi -4.97 import weight 0.00
Epoch 283 Iter 7 subLoss 15951.2 multi 1.00 import weight 0.00
Epoch 283 Iter 8 subLoss 9421.9 multi -7.96 import weight 0.00
Epoch 283 Iter 9 subLoss 67939.1 multi 1.00 import weight 0.00
Epoch 283 Iter 10 subLoss 17922.3 multi -4.97 import weight 0.00
Epoch 283 Iter 11 subLoss 95269.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 283 Acc: 81.07 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 9526 train Loss: 26799.1 test Loss: 4467.4
Epoch 284 Iter 0 subLoss 23504.0 multi 1.00 import weight 0.00
Epoch 284 Iter 1 subLoss 20128.8 multi 3.99 import weight 0.00
Epoch 284 Iter 2 subLoss 7894.2 multi 9.96 import weight 0.00
Epoch 284 Iter 3 subLoss 4034.9 multi -13.93 import weight 0.00
Epoch 284 Iter 4 subLoss 6572.8 multi 12.94 import weight 0.00
Epoch 284 Iter 5 subLoss 3810.6 multi 6.97 import weight 0.00
Epoch 284 Iter 6 subLoss 3195.1 multi -1.99 import weight 0.00
Epoch 284 Iter 7 subLoss 3644.0 multi -1.98 import weight 0.00
Epoch 284 Iter 8 subLoss 3633.3 multi -4.97 import weight 0.00
Epoch 284 Iter 9 subLoss 4613.0 multi 3.99 import weight 0.00
Epoch 284 Iter 10 subLoss 3711.0 multi 6.97 import weight 0.00
Epoch 284 Iter 11 subLoss 3154.3 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 284 Acc: 97.90 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 315 train Loss: 3345.5 test Loss: 437.8
Epoch 285 Iter 0 subLoss 3509.5 multi 1.00 import weight 0.00
Epoch 285 Iter 1 subLoss 3003.7 multi 18.91 import weight 0.00
Epoch 285 Iter 2 subLoss 2483.9 multi -28.85 import weight 0.00
Epoch 285 Iter 3 subLoss 3533.1 multi 1.00 import weight 0.00
Epoch 285 Iter 4 subLoss 3432.1 multi 1.00 import weight 0.00
Epoch 285 Iter 5 subLoss 3233.8 multi 6.97 import weight 0.00
Epoch 285 Iter 6 subLoss 2753.6 multi -19.90 import weight 0.00
Epoch 285 Iter 7 subLoss 3674.6 multi -4.97 import weight 0.00
Epoch 285 Iter 8 subLoss 4326.7 multi 9.96 import weight 0.00
Epoch 285 Iter 9 subLoss 2746.8 multi 9.96 import weight 0.00
Epoch 285 Iter 10 subLoss 3155.6 multi 1.00 import weight 0.00
Epoch 285 Iter 11 subLoss 2920.5 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 285 Acc: 98.15 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 292 train Loss: 2711.3 test Loss: 326.5
Epoch 286 Iter 0 subLoss 2693.6 multi -4.97 import weight 0.00
Epoch 286 Iter 1 subLoss 2742.1 multi 12.94 import weight 0.00
Epoch 286 Iter 2 subLoss 2520.4 multi -4.97 import weight 0.00
Epoch 286 Iter 3 subLoss 2336.7 multi 9.96 import weight 0.00
Epoch 286 Iter 4 subLoss 2496.2 multi 21.90 import weight 0.00
Epoch 286 Iter 5 subLoss 2680.6 multi -1.98 import weight 0.00
Epoch 286 Iter 6 subLoss 2386.4 multi -4.97 import weight 0.00
Epoch 286 Iter 7 subLoss 2304.0 multi -7.96 import weight 0.00
Epoch 286 Iter 8 subLoss 2770.1 multi -7.96 import weight 0.00
Epoch 286 Iter 9 subLoss 4550.2 multi -7.96 import weight 0.00
Epoch 286 Iter 10 subLoss 19894.6 multi 1.00 import weight 0.00
Epoch 286 Iter 11 subLoss 6235.1 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 286 Acc: 87.31 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 623 train Loss: 10516.9 test Loss: 1945.2
Epoch 287 Iter 0 subLoss 9832.2 multi 6.97 import weight 0.00
Epoch 287 Iter 1 subLoss 3827.7 multi -10.94 import weight 0.00
Epoch 287 Iter 2 subLoss 13849.2 multi -1.99 import weight 0.00
Epoch 287 Iter 3 subLoss 41897.6 multi 1.00 import weight 0.00
Epoch 287 Iter 4 subLoss 10103.3 multi -4.97 import weight 0.00
Epoch 287 Iter 5 subLoss 46450.2 multi 1.00 import weight 0.00
Epoch 287 Iter 6 subLoss 14331.3 multi 3.99 import weight 0.00
Epoch 287 Iter 7 subLoss 4682.6 multi 1.00 import weight 0.00
Epoch 287 Iter 8 subLoss 4441.1 multi 1.00 import weight 0.00
Epoch 287 Iter 9 subLoss 3629.7 multi 6.97 import weight 0.00
Epoch 287 Iter 10 subLoss 3314.8 multi -10.94 import weight 0.00
Epoch 287 Iter 11 subLoss 4037.9 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 287 Acc: 92.43 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 403 train Loss: 6567.4 test Loss: 1280.6
Epoch 288 Iter 0 subLoss 6694.2 multi 3.99 import weight 0.00
Epoch 288 Iter 1 subLoss 5183.2 multi 6.97 import weight 0.00
Epoch 288 Iter 2 subLoss 3673.0 multi -1.99 import weight 0.00
Epoch 288 Iter 3 subLoss 4109.5 multi 6.97 import weight 0.00
Epoch 288 Iter 4 subLoss 3042.9 multi 1.00 import weight 0.00
Epoch 288 Iter 5 subLoss 2733.3 multi -1.99 import weight 0.00
Epoch 288 Iter 6 subLoss 3148.9 multi -4.97 import weight 0.00
Epoch 288 Iter 7 subLoss 3240.1 multi -4.97 import weight 0.00
Epoch 288 Iter 8 subLoss 4004.0 multi 3.99 import weight 0.00
Epoch 288 Iter 9 subLoss 3442.1 multi 48.76 import weight 1.00
Epoch 288 Iter 10 subLoss 3527.0 multi 1.00 import weight 0.00
Epoch 288 Iter 11 subLoss 3146.6 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 288 Acc: 98.09 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 314 train Loss: 3645.4 test Loss: 329.8
Epoch 289 Iter 0 subLoss 4167.2 multi 15.93 import weight 0.00
Epoch 289 Iter 1 subLoss 2361.4 multi 6.97 import weight 0.00
Epoch 289 Iter 2 subLoss 2320.2 multi -7.96 import weight 0.00
Epoch 289 Iter 3 subLoss 2378.9 multi -4.97 import weight 0.00
Epoch 289 Iter 4 subLoss 2925.9 multi 6.97 import weight 0.00
Epoch 289 Iter 5 subLoss 2421.7 multi -7.96 import weight 0.00
Epoch 289 Iter 6 subLoss 2214.6 multi -7.96 import weight 0.00
Epoch 289 Iter 7 subLoss 2996.8 multi -4.97 import weight 0.00
Epoch 289 Iter 8 subLoss 2804.2 multi 3.99 import weight 0.00
Epoch 289 Iter 9 subLoss 2898.6 multi 30.85 import weight 0.00
Epoch 289 Iter 10 subLoss 2561.9 multi 21.90 import weight 0.00
Epoch 289 Iter 11 subLoss 2326.8 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 289 Acc: 98.44 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 232 train Loss: 2525.4 test Loss: 270.3
Epoch 290 Iter 0 subLoss 1782.1 multi 1.00 import weight 0.00
Epoch 290 Iter 1 subLoss 2292.0 multi 9.96 import weight 0.00
Epoch 290 Iter 2 subLoss 1901.2 multi 1.00 import weight 0.00
Epoch 290 Iter 3 subLoss 2799.2 multi -4.97 import weight 0.00
Epoch 290 Iter 4 subLoss 2705.2 multi 6.97 import weight 0.00
Epoch 290 Iter 5 subLoss 1908.5 multi 3.98 import weight 0.00
Epoch 290 Iter 6 subLoss 1919.5 multi -4.97 import weight 0.00
Epoch 290 Iter 7 subLoss 2239.7 multi 1.00 import weight 0.00
Epoch 290 Iter 8 subLoss 2028.4 multi 3.99 import weight 0.00
Epoch 290 Iter 9 subLoss 2482.5 multi -25.87 import weight 0.00
Epoch 290 Iter 10 subLoss 2677.0 multi -4.97 import weight 0.00
Epoch 290 Iter 11 subLoss 2986.2 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 290 Acc: 98.31 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 298 train Loss: 2543.8 test Loss: 283.5
Epoch 291 Iter 0 subLoss 2471.1 multi 30.85 import weight 0.00
Epoch 291 Iter 1 subLoss 2370.8 multi -1.98 import weight 0.00
Epoch 291 Iter 2 subLoss 2822.9 multi -19.90 import weight 0.00
Epoch 291 Iter 3 subLoss 4233.1 multi -13.93 import weight 0.00
Epoch 291 Iter 4 subLoss 68473.5 multi 1.00 import weight 0.00
Epoch 291 Iter 5 subLoss 9571.6 multi 1.00 import weight 0.00
Epoch 291 Iter 6 subLoss 6427.3 multi -1.98 import weight 0.00
Epoch 291 Iter 7 subLoss 9599.1 multi 1.00 import weight 0.00
Epoch 291 Iter 8 subLoss 6857.9 multi 1.00 import weight 0.00
Epoch 291 Iter 9 subLoss 6383.8 multi -4.97 import weight 0.00
Epoch 291 Iter 10 subLoss 11124.9 multi 1.00 import weight 0.00
Epoch 291 Iter 11 subLoss 8500.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 291 Acc: 83.93 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 850 train Loss: 23886.6 test Loss: 3608.9
Epoch 292 Iter 0 subLoss 25073.4 multi 1.00 import weight 0.00
Epoch 292 Iter 1 subLoss 14162.1 multi 1.00 import weight 0.00
Epoch 292 Iter 2 subLoss 10295.3 multi -1.99 import weight 0.00
Epoch 292 Iter 3 subLoss 15377.9 multi 3.99 import weight 0.00
Epoch 292 Iter 4 subLoss 5263.2 multi -1.99 import weight 0.00
Epoch 292 Iter 5 subLoss 6385.9 multi -1.99 import weight 0.00
Epoch 292 Iter 6 subLoss 7043.0 multi -4.97 import weight 0.00
Epoch 292 Iter 7 subLoss 12904.0 multi 1.00 import weight 0.00
Epoch 292 Iter 8 subLoss 9593.3 multi 3.98 import weight 0.00
Epoch 292 Iter 9 subLoss 5368.1 multi -4.97 import weight 0.00
Epoch 292 Iter 10 subLoss 7295.6 multi -1.98 import weight 0.00
Epoch 292 Iter 11 subLoss 8291.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 292 Acc: 96.58 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 829 train Loss: 4971.5 test Loss: 607.1
Epoch 293 Iter 0 subLoss 4965.2 multi 1.00 import weight 0.00
Epoch 293 Iter 1 subLoss 4651.4 multi -1.99 import weight 0.00
Epoch 293 Iter 2 subLoss 4850.0 multi 9.96 import weight 0.00
Epoch 293 Iter 3 subLoss 3365.1 multi 3.99 import weight 0.00
Epoch 293 Iter 4 subLoss 2968.9 multi -16.91 import weight 0.00
Epoch 293 Iter 5 subLoss 4470.8 multi -7.96 import weight 0.00
Epoch 293 Iter 6 subLoss 5301.6 multi 1.00 import weight 0.00
Epoch 293 Iter 7 subLoss 5268.5 multi 1.00 import weight 0.00
Epoch 293 Iter 8 subLoss 4995.6 multi -10.94 import weight 0.00
Epoch 293 Iter 9 subLoss 8155.6 multi -1.99 import weight 0.00
Epoch 293 Iter 10 subLoss 9758.5 multi -1.99 import weight 0.00
Epoch 293 Iter 11 subLoss 14578.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 293 Acc: 91.40 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1457 train Loss: 11092.6 test Loss: 1518.9
Epoch 294 Iter 0 subLoss 11946.4 multi 9.96 import weight 0.00
Epoch 294 Iter 1 subLoss 7592.4 multi 3.98 import weight 0.00
Epoch 294 Iter 2 subLoss 4269.7 multi 6.97 import weight 0.00
Epoch 294 Iter 3 subLoss 3712.9 multi 9.96 import weight 0.00
Epoch 294 Iter 4 subLoss 3048.9 multi 3.98 import weight 0.00
Epoch 294 Iter 5 subLoss 2885.0 multi -1.98 import weight 0.00
Epoch 294 Iter 6 subLoss 2975.9 multi 1.00 import weight 0.00
Epoch 294 Iter 7 subLoss 2824.4 multi -16.91 import weight 0.00
Epoch 294 Iter 8 subLoss 3827.3 multi -7.96 import weight 0.00
Epoch 294 Iter 9 subLoss 4287.6 multi -1.99 import weight 0.00
Epoch 294 Iter 10 subLoss 5112.3 multi 18.91 import weight 0.00
Epoch 294 Iter 11 subLoss 3396.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 294 Acc: 98.00 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 339 train Loss: 3270.5 test Loss: 403.5
Epoch 295 Iter 0 subLoss 3388.8 multi -16.91 import weight 0.00
Epoch 295 Iter 1 subLoss 4824.8 multi 1.00 import weight 0.00
Epoch 295 Iter 2 subLoss 4710.5 multi -1.98 import weight 0.00
Epoch 295 Iter 3 subLoss 5065.3 multi -4.97 import weight 0.00
Epoch 295 Iter 4 subLoss 12308.4 multi -1.99 import weight 0.00
Epoch 295 Iter 5 subLoss 51819.5 multi 1.00 import weight 0.00
Epoch 295 Iter 6 subLoss 8807.6 multi 3.99 import weight 0.00
Epoch 295 Iter 7 subLoss 4010.0 multi -4.97 import weight 0.00
Epoch 295 Iter 8 subLoss 5188.9 multi 9.96 import weight 0.00
Epoch 295 Iter 9 subLoss 3319.6 multi -7.96 import weight 0.00
Epoch 295 Iter 10 subLoss 3674.9 multi 1.00 import weight 0.00
Epoch 295 Iter 11 subLoss 3610.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 295 Acc: 97.28 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 361 train Loss: 4025.9 test Loss: 541.9
Epoch 296 Iter 0 subLoss 4043.7 multi -10.94 import weight 0.00
Epoch 296 Iter 1 subLoss 5113.9 multi 21.90 import weight 0.00
Epoch 296 Iter 2 subLoss 3260.3 multi -1.99 import weight 0.00
Epoch 296 Iter 3 subLoss 3245.8 multi -1.98 import weight 0.00
Epoch 296 Iter 4 subLoss 3233.7 multi 9.96 import weight 0.00
Epoch 296 Iter 5 subLoss 2843.8 multi -10.94 import weight 0.00
Epoch 296 Iter 6 subLoss 2889.2 multi 1.00 import weight 0.00
Epoch 296 Iter 7 subLoss 3124.9 multi 1.00 import weight 0.00
Epoch 296 Iter 8 subLoss 2794.6 multi -1.99 import weight 0.00
Epoch 296 Iter 9 subLoss 3316.5 multi -4.97 import weight 0.00
Epoch 296 Iter 10 subLoss 3682.0 multi -7.96 import weight 0.00
Epoch 296 Iter 11 subLoss 3558.6 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 296 Acc: 96.75 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 355 train Loss: 5538.1 test Loss: 713.6
Epoch 297 Iter 0 subLoss 5728.7 multi 1.00 import weight 0.00
Epoch 297 Iter 1 subLoss 4850.8 multi 12.94 import weight 0.00
Epoch 297 Iter 2 subLoss 3294.7 multi -13.93 import weight 0.00
Epoch 297 Iter 3 subLoss 4071.0 multi 9.96 import weight 0.00
Epoch 297 Iter 4 subLoss 3323.9 multi -22.88 import weight 0.00
Epoch 297 Iter 5 subLoss 4456.2 multi -10.94 import weight 0.00
Epoch 297 Iter 6 subLoss 5152.4 multi 3.99 import weight 0.00
Epoch 297 Iter 7 subLoss 4772.5 multi -13.93 import weight 0.00
Epoch 297 Iter 8 subLoss 7682.7 multi 6.97 import weight 0.00
Epoch 297 Iter 9 subLoss 4648.0 multi -7.96 import weight 0.00
Epoch 297 Iter 10 subLoss 5752.3 multi 3.98 import weight 0.00
Epoch 297 Iter 11 subLoss 5237.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 297 Acc: 97.35 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 523 train Loss: 5069.3 test Loss: 630.3
Epoch 298 Iter 0 subLoss 4928.6 multi -1.99 import weight 0.00
Epoch 298 Iter 1 subLoss 5042.4 multi 6.97 import weight 0.00
Epoch 298 Iter 2 subLoss 4989.6 multi 3.98 import weight 0.00
Epoch 298 Iter 3 subLoss 4444.7 multi 3.99 import weight 0.00
Epoch 298 Iter 4 subLoss 3850.1 multi -16.91 import weight 0.00
Epoch 298 Iter 5 subLoss 4967.7 multi 3.99 import weight 0.00
Epoch 298 Iter 6 subLoss 4702.8 multi 1.00 import weight 0.00
Epoch 298 Iter 7 subLoss 4974.4 multi 1.00 import weight 0.00
Epoch 298 Iter 8 subLoss 4854.4 multi 15.93 import weight 0.00
Epoch 298 Iter 9 subLoss 3673.6 multi 3.98 import weight 0.00
Epoch 298 Iter 10 subLoss 3406.9 multi 6.97 import weight 0.00
Epoch 298 Iter 11 subLoss 3477.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 298 Acc: 98.13 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 347 train Loss: 3154.1 test Loss: 361.3
Epoch 299 Iter 0 subLoss 3142.9 multi 1.00 import weight 0.00
Epoch 299 Iter 1 subLoss 2930.5 multi -7.96 import weight 0.00
Epoch 299 Iter 2 subLoss 3206.9 multi -16.91 import weight 0.00
Epoch 299 Iter 3 subLoss 3995.3 multi -13.93 import weight 0.00
Epoch 299 Iter 4 subLoss 5341.6 multi -1.99 import weight 0.00
Epoch 299 Iter 5 subLoss 6153.6 multi 3.99 import weight 0.00
Epoch 299 Iter 6 subLoss 4589.7 multi 1.00 import weight 0.00
Epoch 299 Iter 7 subLoss 4729.3 multi 9.96 import weight 0.00
Epoch 299 Iter 8 subLoss 3287.7 multi 18.91 import weight 0.00
Epoch 299 Iter 9 subLoss 3481.5 multi -4.97 import weight 0.00
Epoch 299 Iter 10 subLoss 3354.9 multi 1.00 import weight 0.00
Epoch 299 Iter 11 subLoss 3490.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 299 Acc: 98.13 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 349 train Loss: 3125.6 test Loss: 344.0
Epoch 300 Iter 0 subLoss 3255.4 multi -4.97 import weight 0.00
Epoch 300 Iter 1 subLoss 2871.2 multi -7.96 import weight 0.00
Epoch 300 Iter 2 subLoss 3606.9 multi -7.96 import weight 0.00
Epoch 300 Iter 3 subLoss 3820.9 multi -4.97 import weight 0.00
Epoch 300 Iter 4 subLoss 4639.9 multi 1.00 import weight 0.00
Epoch 300 Iter 5 subLoss 3936.0 multi 1.00 import weight 0.00
Epoch 300 Iter 6 subLoss 4351.5 multi -4.97 import weight 0.00
Epoch 300 Iter 7 subLoss 5159.0 multi 6.97 import weight 0.00
Epoch 300 Iter 8 subLoss 4709.4 multi 3.99 import weight 0.00
Epoch 300 Iter 9 subLoss 4070.2 multi 12.94 import weight 0.00
Epoch 300 Iter 10 subLoss 3238.2 multi 12.94 import weight 0.00
Epoch 300 Iter 11 subLoss 2775.8 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 300 Acc: 98.27 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 277 train Loss: 3045.8 test Loss: 325.5
Epoch 301 Iter 0 subLoss 2783.8 multi 3.99 import weight 0.00
Epoch 301 Iter 1 subLoss 2546.0 multi 18.91 import weight 0.00
Epoch 301 Iter 2 subLoss 2416.9 multi 3.99 import weight 0.00
Epoch 301 Iter 3 subLoss 2375.0 multi 1.00 import weight 0.00
Epoch 301 Iter 4 subLoss 2582.3 multi 12.94 import weight 0.00
Epoch 301 Iter 5 subLoss 2656.8 multi 12.94 import weight 0.00
Epoch 301 Iter 6 subLoss 2434.1 multi 6.97 import weight 0.00
Epoch 301 Iter 7 subLoss 2305.3 multi -7.96 import weight 0.00
Epoch 301 Iter 8 subLoss 2540.3 multi 21.90 import weight 0.00
Epoch 301 Iter 9 subLoss 2720.6 multi -4.97 import weight 0.00
Epoch 301 Iter 10 subLoss 2011.2 multi -10.94 import weight 0.00
Epoch 301 Iter 11 subLoss 2760.6 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 301 Acc: 98.50 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 276 train Loss: 2545.1 test Loss: 263.0
Epoch 302 Iter 0 subLoss 2575.5 multi -16.91 import weight 0.00
Epoch 302 Iter 1 subLoss 2452.8 multi -7.96 import weight 0.00
Epoch 302 Iter 2 subLoss 2933.9 multi -4.97 import weight 0.00
Epoch 302 Iter 3 subLoss 2991.4 multi -4.97 import weight 0.00
Epoch 302 Iter 4 subLoss 5576.4 multi 6.97 import weight 0.00
Epoch 302 Iter 5 subLoss 2432.9 multi 9.96 import weight 0.00
Epoch 302 Iter 6 subLoss 2091.8 multi 1.00 import weight 0.00
Epoch 302 Iter 7 subLoss 2281.1 multi -1.99 import weight 0.00
Epoch 302 Iter 8 subLoss 2774.5 multi -4.97 import weight 0.00
Epoch 302 Iter 9 subLoss 2091.0 multi 3.98 import weight 0.00
Epoch 302 Iter 10 subLoss 2078.5 multi -1.99 import weight 0.00
Epoch 302 Iter 11 subLoss 2449.0 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 302 Acc: 98.44 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 244 train Loss: 2525.8 test Loss: 276.6
Epoch 303 Iter 0 subLoss 2368.0 multi 9.96 import weight 0.00
Epoch 303 Iter 1 subLoss 2596.8 multi -4.97 import weight 0.00
Epoch 303 Iter 2 subLoss 2213.1 multi -4.97 import weight 0.00
Epoch 303 Iter 3 subLoss 2548.5 multi 24.88 import weight 0.00
Epoch 303 Iter 4 subLoss 2483.8 multi -25.87 import weight 0.00
Epoch 303 Iter 5 subLoss 3084.7 multi 9.96 import weight 0.00
Epoch 303 Iter 6 subLoss 2821.9 multi -13.93 import weight 0.00
Epoch 303 Iter 7 subLoss 2473.5 multi 33.84 import weight 0.00
Epoch 303 Iter 8 subLoss 2573.0 multi -13.93 import weight 0.00
Epoch 303 Iter 9 subLoss 3018.3 multi -16.91 import weight 0.00
Epoch 303 Iter 10 subLoss 6415.8 multi 6.97 import weight 0.00
Epoch 303 Iter 11 subLoss 2502.1 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 303 Acc: 97.74 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 250 train Loss: 3603.4 test Loss: 413.9
Epoch 304 Iter 0 subLoss 3375.6 multi 3.99 import weight 0.00
Epoch 304 Iter 1 subLoss 2375.9 multi 1.00 import weight 0.00
Epoch 304 Iter 2 subLoss 2450.6 multi -7.96 import weight 0.00
Epoch 304 Iter 3 subLoss 3027.3 multi -4.97 import weight 0.00
Epoch 304 Iter 4 subLoss 4042.4 multi -7.96 import weight 0.00
Epoch 304 Iter 5 subLoss 9011.5 multi -1.99 import weight 0.00
Epoch 304 Iter 6 subLoss 22627.5 multi 1.00 import weight 0.00
Epoch 304 Iter 7 subLoss 8071.9 multi -4.97 import weight 0.00
Epoch 304 Iter 8 subLoss 32903.1 multi 1.00 import weight 0.00
Epoch 304 Iter 9 subLoss 7322.0 multi 6.97 import weight 0.00
Epoch 304 Iter 10 subLoss 3280.9 multi 21.90 import weight 0.00
Epoch 304 Iter 11 subLoss 3458.2 multi -55.72 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 304 Acc: 88.38 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -55.72 Pidx 345 train Loss: 10563.0 test Loss: 2385.1
Epoch 305 Iter 0 subLoss 10045.3 multi 1.00 import weight 0.00
Epoch 305 Iter 1 subLoss 6875.0 multi 3.98 import weight 0.00
Epoch 305 Iter 2 subLoss 3857.2 multi -13.93 import weight 0.00
Epoch 305 Iter 3 subLoss 6048.3 multi -1.99 import weight 0.00
Epoch 305 Iter 4 subLoss 8807.8 multi 6.97 import weight 0.00
Epoch 305 Iter 5 subLoss 3674.3 multi 6.97 import weight 0.00
Epoch 305 Iter 6 subLoss 4176.8 multi -19.90 import weight 0.00
Epoch 305 Iter 7 subLoss 4788.6 multi 1.00 import weight 0.00
Epoch 305 Iter 8 subLoss 5168.7 multi -4.97 import weight 0.00
Epoch 305 Iter 9 subLoss 5081.1 multi 6.97 import weight 0.00
Epoch 305 Iter 10 subLoss 3764.8 multi -1.99 import weight 0.00
Epoch 305 Iter 11 subLoss 4555.4 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 305 Acc: 96.15 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 455 train Loss: 4962.6 test Loss: 667.2
Epoch 306 Iter 0 subLoss 4926.8 multi 1.00 import weight 0.00
Epoch 306 Iter 1 subLoss 5246.1 multi -13.93 import weight 0.00
Epoch 306 Iter 2 subLoss 6668.8 multi -1.98 import weight 0.00
Epoch 306 Iter 3 subLoss 8425.6 multi 3.99 import weight 0.00
Epoch 306 Iter 4 subLoss 6014.0 multi -10.94 import weight 0.00
Epoch 306 Iter 5 subLoss 8547.8 multi -1.99 import weight 0.00
Epoch 306 Iter 6 subLoss 10779.4 multi 3.99 import weight 0.00
Epoch 306 Iter 7 subLoss 7402.7 multi 1.00 import weight 0.00
Epoch 306 Iter 8 subLoss 6735.8 multi -4.97 import weight 0.00
Epoch 306 Iter 9 subLoss 9316.1 multi -4.97 import weight 0.00
Epoch 306 Iter 10 subLoss 13827.4 multi 3.99 import weight 0.00
Epoch 306 Iter 11 subLoss 8586.8 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 306 Acc: 91.05 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 858 train Loss: 9517.1 test Loss: 1474.3
Epoch 307 Iter 0 subLoss 9484.4 multi 6.97 import weight 0.00
Epoch 307 Iter 1 subLoss 6063.3 multi -10.94 import weight 0.00
Epoch 307 Iter 2 subLoss 9112.3 multi 1.00 import weight 0.00
Epoch 307 Iter 3 subLoss 7914.5 multi 3.99 import weight 0.00
Epoch 307 Iter 4 subLoss 6961.7 multi -1.98 import weight 0.00
Epoch 307 Iter 5 subLoss 7771.1 multi -10.94 import weight 0.00
Epoch 307 Iter 6 subLoss 12191.6 multi 1.00 import weight 0.00
Epoch 307 Iter 7 subLoss 10970.7 multi 3.99 import weight 0.00
Epoch 307 Iter 8 subLoss 9366.5 multi -4.97 import weight 0.00
Epoch 307 Iter 9 subLoss 11788.3 multi 1.00 import weight 0.00
Epoch 307 Iter 10 subLoss 10394.2 multi -1.98 import weight 0.00
Epoch 307 Iter 11 subLoss 12005.3 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 307 Acc: 90.66 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 1200 train Loss: 9189.0 test Loss: 1508.7
Epoch 308 Iter 0 subLoss 8329.8 multi 1.00 import weight 0.00
Epoch 308 Iter 1 subLoss 8792.8 multi 1.00 import weight 0.00
Epoch 308 Iter 2 subLoss 8746.3 multi 1.00 import weight 0.00
Epoch 308 Iter 3 subLoss 7954.3 multi 3.98 import weight 0.00
Epoch 308 Iter 4 subLoss 7393.7 multi 3.98 import weight 0.00
Epoch 308 Iter 5 subLoss 5952.5 multi -1.99 import weight 0.00
Epoch 308 Iter 6 subLoss 6056.6 multi 12.94 import weight 0.00
Epoch 308 Iter 7 subLoss 5163.6 multi -1.99 import weight 0.00
Epoch 308 Iter 8 subLoss 4934.9 multi 3.98 import weight 0.00
Epoch 308 Iter 9 subLoss 5273.1 multi -1.98 import weight 0.00
Epoch 308 Iter 10 subLoss 5171.8 multi -13.93 import weight 0.00
Epoch 308 Iter 11 subLoss 6876.1 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 308 Acc: 95.37 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 687 train Loss: 5599.7 test Loss: 854.1
Epoch 309 Iter 0 subLoss 4789.5 multi 3.99 import weight 0.00
Epoch 309 Iter 1 subLoss 4975.2 multi 3.98 import weight 0.00
Epoch 309 Iter 2 subLoss 4892.3 multi 1.00 import weight 0.00
Epoch 309 Iter 3 subLoss 4593.8 multi -1.99 import weight 0.00
Epoch 309 Iter 4 subLoss 4152.3 multi -1.98 import weight 0.00
Epoch 309 Iter 5 subLoss 4763.7 multi 6.97 import weight 0.00
Epoch 309 Iter 6 subLoss 4543.0 multi 15.93 import weight 0.00
Epoch 309 Iter 7 subLoss 3746.2 multi -4.97 import weight 0.00
Epoch 309 Iter 8 subLoss 3776.2 multi 1.00 import weight 0.00
Epoch 309 Iter 9 subLoss 3880.2 multi 1.00 import weight 0.00
Epoch 309 Iter 10 subLoss 3981.5 multi 1.00 import weight 0.00
Epoch 309 Iter 11 subLoss 3859.7 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 309 Acc: 96.63 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 385 train Loss: 4351.9 test Loss: 582.1
Epoch 310 Iter 0 subLoss 4641.9 multi -7.96 import weight 0.00
Epoch 310 Iter 1 subLoss 4710.4 multi -4.97 import weight 0.00
Epoch 310 Iter 2 subLoss 5898.0 multi 1.00 import weight 0.00
Epoch 310 Iter 3 subLoss 5528.5 multi 9.96 import weight 0.00
Epoch 310 Iter 4 subLoss 3748.3 multi -1.98 import weight 0.00
Epoch 310 Iter 5 subLoss 3569.9 multi 1.00 import weight 0.00
Epoch 310 Iter 6 subLoss 4320.4 multi 12.94 import weight 0.00
Epoch 310 Iter 7 subLoss 3574.5 multi -10.94 import weight 0.00
Epoch 310 Iter 8 subLoss 4082.2 multi -1.99 import weight 0.00
Epoch 310 Iter 9 subLoss 4766.0 multi 9.96 import weight 0.00
Epoch 310 Iter 10 subLoss 3931.7 multi 3.99 import weight 0.00
Epoch 310 Iter 11 subLoss 3180.0 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 310 Acc: 97.22 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 317 train Loss: 3336.2 test Loss: 471.2
Epoch 311 Iter 0 subLoss 3231.6 multi 15.93 import weight 0.00
Epoch 311 Iter 1 subLoss 3310.3 multi -1.99 import weight 0.00
Epoch 311 Iter 2 subLoss 3084.5 multi 12.94 import weight 0.00
Epoch 311 Iter 3 subLoss 3158.6 multi -4.97 import weight 0.00
Epoch 311 Iter 4 subLoss 3221.2 multi -4.97 import weight 0.00
Epoch 311 Iter 5 subLoss 3074.3 multi -1.99 import weight 0.00
Epoch 311 Iter 6 subLoss 3337.7 multi -1.98 import weight 0.00
Epoch 311 Iter 7 subLoss 2949.3 multi -7.96 import weight 0.00
Epoch 311 Iter 8 subLoss 3420.1 multi 3.98 import weight 0.00
Epoch 311 Iter 9 subLoss 3326.4 multi -22.88 import weight 0.00
Epoch 311 Iter 10 subLoss 4525.8 multi 12.94 import weight 0.00
Epoch 311 Iter 11 subLoss 3476.9 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 311 Acc: 97.82 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 347 train Loss: 3218.3 test Loss: 375.2
Epoch 312 Iter 0 subLoss 3034.5 multi 1.00 import weight 0.00
Epoch 312 Iter 1 subLoss 2753.4 multi -22.88 import weight 0.00
Epoch 312 Iter 2 subLoss 3786.8 multi 12.94 import weight 0.00
Epoch 312 Iter 3 subLoss 3601.2 multi -4.97 import weight 0.00
Epoch 312 Iter 4 subLoss 3557.5 multi -7.96 import weight 0.00
Epoch 312 Iter 5 subLoss 3255.8 multi -1.98 import weight 0.00
Epoch 312 Iter 6 subLoss 3732.6 multi 12.94 import weight 0.00
Epoch 312 Iter 7 subLoss 3485.0 multi -4.97 import weight 0.00
Epoch 312 Iter 8 subLoss 4141.3 multi 12.94 import weight 0.00
Epoch 312 Iter 9 subLoss 2736.6 multi -1.99 import weight 0.00
Epoch 312 Iter 10 subLoss 3509.4 multi 1.00 import weight 0.00
Epoch 312 Iter 11 subLoss 2716.1 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 312 Acc: 97.66 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 271 train Loss: 3272.1 test Loss: 410.5
Epoch 313 Iter 0 subLoss 2994.1 multi -1.99 import weight 0.00
Epoch 313 Iter 1 subLoss 3278.5 multi -10.94 import weight 0.00
Epoch 313 Iter 2 subLoss 3607.5 multi -1.99 import weight 0.00
Epoch 313 Iter 3 subLoss 3209.9 multi -13.93 import weight 0.00
Epoch 313 Iter 4 subLoss 4035.0 multi -7.96 import weight 0.00
Epoch 313 Iter 5 subLoss 6355.9 multi 3.99 import weight 0.00
Epoch 313 Iter 6 subLoss 5269.0 multi 3.98 import weight 0.00
Epoch 313 Iter 7 subLoss 4151.8 multi -1.99 import weight 0.00
Epoch 313 Iter 8 subLoss 4288.6 multi 1.00 import weight 0.00
Epoch 313 Iter 9 subLoss 3997.2 multi -13.93 import weight 0.00
Epoch 313 Iter 10 subLoss 5949.8 multi 1.00 import weight 0.00
Epoch 313 Iter 11 subLoss 5017.4 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 313 Acc: 92.88 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 501 train Loss: 5747.7 test Loss: 1028.0
Epoch 314 Iter 0 subLoss 5396.7 multi 6.97 import weight 0.00
Epoch 314 Iter 1 subLoss 4432.6 multi 6.97 import weight 0.00
Epoch 314 Iter 2 subLoss 3265.6 multi -4.97 import weight 0.00
Epoch 314 Iter 3 subLoss 3805.2 multi 6.97 import weight 0.00
Epoch 314 Iter 4 subLoss 3903.0 multi 9.96 import weight 0.00
Epoch 314 Iter 5 subLoss 3811.7 multi 6.97 import weight 0.00
Epoch 314 Iter 6 subLoss 3286.5 multi 21.90 import weight 0.00
Epoch 314 Iter 7 subLoss 2983.1 multi 6.97 import weight 0.00
Epoch 314 Iter 8 subLoss 3192.3 multi 1.00 import weight 0.00
Epoch 314 Iter 9 subLoss 2974.7 multi 3.99 import weight 0.00
Epoch 314 Iter 10 subLoss 3164.4 multi -13.93 import weight 0.00
Epoch 314 Iter 11 subLoss 3116.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 314 Acc: 97.45 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 311 train Loss: 3292.6 test Loss: 412.9
Epoch 315 Iter 0 subLoss 3795.6 multi -16.91 import weight 0.00
Epoch 315 Iter 1 subLoss 3224.7 multi -1.99 import weight 0.00
Epoch 315 Iter 2 subLoss 4633.6 multi 3.99 import weight 0.00
Epoch 315 Iter 3 subLoss 3801.5 multi 6.97 import weight 0.00
Epoch 315 Iter 4 subLoss 3734.2 multi 15.93 import weight 0.00
Epoch 315 Iter 5 subLoss 2396.1 multi 30.85 import weight 0.00
Epoch 315 Iter 6 subLoss 2732.8 multi 1.00 import weight 0.00
Epoch 315 Iter 7 subLoss 3219.6 multi 12.94 import weight 0.00
Epoch 315 Iter 8 subLoss 2794.8 multi -1.98 import weight 0.00
Epoch 315 Iter 9 subLoss 3008.4 multi 12.94 import weight 0.00
Epoch 315 Iter 10 subLoss 2311.9 multi 6.97 import weight 0.00
Epoch 315 Iter 11 subLoss 2790.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 315 Acc: 98.23 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 279 train Loss: 2803.2 test Loss: 322.4
Epoch 316 Iter 0 subLoss 2126.3 multi 3.99 import weight 0.00
Epoch 316 Iter 1 subLoss 3039.2 multi 3.98 import weight 0.00
Epoch 316 Iter 2 subLoss 2277.9 multi 1.00 import weight 0.00
Epoch 316 Iter 3 subLoss 2809.7 multi -4.97 import weight 0.00
Epoch 316 Iter 4 subLoss 2920.5 multi 9.96 import weight 0.00
Epoch 316 Iter 5 subLoss 3246.0 multi -7.96 import weight 0.00
Epoch 316 Iter 6 subLoss 2783.7 multi 3.98 import weight 0.00
Epoch 316 Iter 7 subLoss 2399.1 multi 33.84 import weight 0.00
Epoch 316 Iter 8 subLoss 2744.0 multi 6.97 import weight 0.00
Epoch 316 Iter 9 subLoss 2785.7 multi 6.97 import weight 0.00
Epoch 316 Iter 10 subLoss 2547.1 multi 27.87 import weight 0.00
Epoch 316 Iter 11 subLoss 2451.4 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 316 Acc: 97.94 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 245 train Loss: 2943.2 test Loss: 330.1
Epoch 317 Iter 0 subLoss 2419.1 multi 6.97 import weight 0.00
Epoch 317 Iter 1 subLoss 2715.0 multi 1.00 import weight 0.00
Epoch 317 Iter 2 subLoss 2980.0 multi 6.97 import weight 0.00
Epoch 317 Iter 3 subLoss 2347.0 multi 3.98 import weight 0.00
Epoch 317 Iter 4 subLoss 2655.5 multi 15.93 import weight 0.00
Epoch 317 Iter 5 subLoss 2396.6 multi 36.82 import weight 0.00
Epoch 317 Iter 6 subLoss 1990.7 multi 3.99 import weight 0.00
Epoch 317 Iter 7 subLoss 1979.6 multi 9.96 import weight 0.00
Epoch 317 Iter 8 subLoss 2302.2 multi -4.97 import weight 0.00
Epoch 317 Iter 9 subLoss 1983.4 multi -10.94 import weight 0.00
Epoch 317 Iter 10 subLoss 2072.3 multi 1.00 import weight 0.00
Epoch 317 Iter 11 subLoss 2269.0 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 317 Acc: 98.40 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 226 train Loss: 2333.3 test Loss: 272.2
Epoch 318 Iter 0 subLoss 2307.3 multi -1.99 import weight 0.00
Epoch 318 Iter 1 subLoss 2383.3 multi -13.93 import weight 0.00
Epoch 318 Iter 2 subLoss 2514.8 multi 15.93 import weight 0.00
Epoch 318 Iter 3 subLoss 2185.0 multi 1.00 import weight 0.00
Epoch 318 Iter 4 subLoss 3126.2 multi 1.00 import weight 0.00
Epoch 318 Iter 5 subLoss 2552.2 multi -34.82 import weight 0.00
Epoch 318 Iter 6 subLoss 4418.7 multi -4.97 import weight 0.00
Epoch 318 Iter 7 subLoss 8033.8 multi -4.97 import weight 0.00
Epoch 318 Iter 8 subLoss 61110.5 multi 1.00 import weight 0.00
Epoch 318 Iter 9 subLoss 6026.2 multi 9.96 import weight 0.00
Epoch 318 Iter 10 subLoss 3165.7 multi -10.94 import weight 0.00
Epoch 318 Iter 11 subLoss 3319.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 318 Acc: 96.50 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 331 train Loss: 3894.1 test Loss: 566.9
Epoch 319 Iter 0 subLoss 3792.5 multi -13.93 import weight 0.00
Epoch 319 Iter 1 subLoss 9156.7 multi 9.96 import weight 0.00
Epoch 319 Iter 2 subLoss 4044.4 multi -7.96 import weight 0.00
Epoch 319 Iter 3 subLoss 5717.6 multi -4.97 import weight 0.00
Epoch 319 Iter 4 subLoss 13219.7 multi 1.00 import weight 0.00
Epoch 319 Iter 5 subLoss 8816.2 multi -4.97 import weight 0.00
Epoch 319 Iter 6 subLoss 29556.1 multi -1.99 import weight 0.00
Epoch 319 Iter 7 subLoss 65321.7 multi 1.00 import weight 0.00
Epoch 319 Iter 8 subLoss 30492.0 multi 3.99 import weight 0.00
Epoch 319 Iter 9 subLoss 13019.5 multi 3.99 import weight 0.00
Epoch 319 Iter 10 subLoss 7473.1 multi -1.99 import weight 0.00
Epoch 319 Iter 11 subLoss 9408.1 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 319 Acc: 97.61 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 940 train Loss: 3662.5 test Loss: 464.1
Epoch 320 Iter 0 subLoss 3632.7 multi -4.97 import weight 0.00
Epoch 320 Iter 1 subLoss 4193.0 multi 21.90 import weight 0.00
Epoch 320 Iter 2 subLoss 2604.8 multi -7.96 import weight 0.00
Epoch 320 Iter 3 subLoss 3326.6 multi -22.88 import weight 0.00
Epoch 320 Iter 4 subLoss 5418.7 multi 1.00 import weight 0.00
Epoch 320 Iter 5 subLoss 5361.4 multi -1.99 import weight 0.00
Epoch 320 Iter 6 subLoss 6663.9 multi 1.00 import weight 0.00
Epoch 320 Iter 7 subLoss 5862.0 multi -7.96 import weight 0.00
Epoch 320 Iter 8 subLoss 16538.7 multi 1.00 import weight 0.00
Epoch 320 Iter 9 subLoss 10666.7 multi -4.97 import weight 0.00
Epoch 320 Iter 10 subLoss 49531.5 multi 1.00 import weight 0.00
Epoch 320 Iter 11 subLoss 9433.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 320 Acc: 93.09 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 943 train Loss: 8144.2 test Loss: 1269.8
Epoch 321 Iter 0 subLoss 8175.1 multi 6.97 import weight 0.00
Epoch 321 Iter 1 subLoss 3561.1 multi 1.00 import weight 0.00
Epoch 321 Iter 2 subLoss 3455.0 multi -52.73 import weight 0.00
Epoch 321 Iter 3 subLoss 13068.9 multi -4.97 import weight 0.00
Epoch 321 Iter 4 subLoss 24916.2 multi -1.99 import weight 0.00
Epoch 321 Iter 5 subLoss 61709.3 multi 1.00 import weight 0.00
Epoch 321 Iter 6 subLoss 20168.0 multi 3.99 import weight 0.00
Epoch 321 Iter 7 subLoss 12477.8 multi 1.00 import weight 0.00
Epoch 321 Iter 8 subLoss 11581.8 multi 1.00 import weight 0.00
Epoch 321 Iter 9 subLoss 11709.7 multi 1.00 import weight 0.00
Epoch 321 Iter 10 subLoss 11236.8 multi 1.00 import weight 0.00
Epoch 321 Iter 11 subLoss 10597.9 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 321 Acc: 84.41 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1059 train Loss: 10033.4 test Loss: 2008.5
Epoch 322 Iter 0 subLoss 9636.0 multi 1.00 import weight 0.00
Epoch 322 Iter 1 subLoss 8531.1 multi 6.97 import weight 0.00
Epoch 322 Iter 2 subLoss 7422.5 multi 3.98 import weight 0.00
Epoch 322 Iter 3 subLoss 5896.9 multi 3.99 import weight 0.00
Epoch 322 Iter 4 subLoss 5487.2 multi -1.98 import weight 0.00
Epoch 322 Iter 5 subLoss 5435.2 multi 9.96 import weight 0.00
Epoch 322 Iter 6 subLoss 4217.2 multi 12.94 import weight 0.00
Epoch 322 Iter 7 subLoss 3176.5 multi 6.97 import weight 0.00
Epoch 322 Iter 8 subLoss 3223.7 multi -1.98 import weight 0.00
Epoch 322 Iter 9 subLoss 2986.8 multi 3.99 import weight 0.00
Epoch 322 Iter 10 subLoss 3068.8 multi 12.94 import weight 0.00
Epoch 322 Iter 11 subLoss 3562.2 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 322 Acc: 97.76 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 356 train Loss: 2991.3 test Loss: 378.1
Epoch 323 Iter 0 subLoss 2441.6 multi 3.99 import weight 0.00
Epoch 323 Iter 1 subLoss 2826.9 multi -10.94 import weight 0.00
Epoch 323 Iter 2 subLoss 2797.5 multi -1.99 import weight 0.00
Epoch 323 Iter 3 subLoss 2795.2 multi 1.00 import weight 0.00
Epoch 323 Iter 4 subLoss 2970.3 multi 9.96 import weight 0.00
Epoch 323 Iter 5 subLoss 3396.3 multi 1.00 import weight 0.00
Epoch 323 Iter 6 subLoss 2806.5 multi -7.96 import weight 0.00
Epoch 323 Iter 7 subLoss 3366.1 multi 3.98 import weight 0.00
Epoch 323 Iter 8 subLoss 2419.9 multi 9.96 import weight 0.00
Epoch 323 Iter 9 subLoss 2646.4 multi -10.94 import weight 0.00
Epoch 323 Iter 10 subLoss 2700.6 multi 9.96 import weight 0.00
Epoch 323 Iter 11 subLoss 3156.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 323 Acc: 97.70 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 315 train Loss: 2867.2 test Loss: 386.0
Epoch 324 Iter 0 subLoss 2863.1 multi -7.96 import weight 0.00
Epoch 324 Iter 1 subLoss 3032.4 multi 6.97 import weight 0.00
Epoch 324 Iter 2 subLoss 2830.7 multi 3.98 import weight 0.00
Epoch 324 Iter 3 subLoss 2614.8 multi -4.97 import weight 0.00
Epoch 324 Iter 4 subLoss 2872.7 multi -7.96 import weight 0.00
Epoch 324 Iter 5 subLoss 3256.6 multi -1.99 import weight 0.00
Epoch 324 Iter 6 subLoss 2854.5 multi 18.91 import weight 0.00
Epoch 324 Iter 7 subLoss 2319.0 multi 3.99 import weight 0.00
Epoch 324 Iter 8 subLoss 2986.1 multi 3.98 import weight 0.00
Epoch 324 Iter 9 subLoss 2882.8 multi -1.99 import weight 0.00
Epoch 324 Iter 10 subLoss 2444.6 multi 6.97 import weight 0.00
Epoch 324 Iter 11 subLoss 3016.6 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 324 Acc: 98.07 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 301 train Loss: 2862.3 test Loss: 323.5
Epoch 325 Iter 0 subLoss 2836.8 multi 6.97 import weight 0.00
Epoch 325 Iter 1 subLoss 3252.6 multi 1.00 import weight 0.00
Epoch 325 Iter 2 subLoss 2541.7 multi 30.85 import weight 0.00
Epoch 325 Iter 3 subLoss 2638.9 multi 18.91 import weight 0.00
Epoch 325 Iter 4 subLoss 2701.4 multi 12.94 import weight 0.00
Epoch 325 Iter 5 subLoss 2525.4 multi -4.97 import weight 0.00
Epoch 325 Iter 6 subLoss 2610.6 multi -1.99 import weight 0.00
Epoch 325 Iter 7 subLoss 2000.4 multi 6.97 import weight 0.00
Epoch 325 Iter 8 subLoss 2655.6 multi 15.93 import weight 0.00
Epoch 325 Iter 9 subLoss 2614.9 multi 1.00 import weight 0.00
Epoch 325 Iter 10 subLoss 2234.3 multi 3.98 import weight 0.00
Epoch 325 Iter 11 subLoss 2432.5 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 325 Acc: 98.46 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 243 train Loss: 2410.7 test Loss: 271.6
Epoch 326 Iter 0 subLoss 2053.2 multi -4.97 import weight 0.00
Epoch 326 Iter 1 subLoss 2383.7 multi -10.94 import weight 0.00
Epoch 326 Iter 2 subLoss 2881.7 multi 1.00 import weight 0.00
Epoch 326 Iter 3 subLoss 2219.5 multi -1.99 import weight 0.00
Epoch 326 Iter 4 subLoss 2251.0 multi 1.00 import weight 0.00
Epoch 326 Iter 5 subLoss 2183.0 multi 3.99 import weight 0.00
Epoch 326 Iter 6 subLoss 2221.2 multi 3.99 import weight 0.00
Epoch 326 Iter 7 subLoss 2313.7 multi 6.97 import weight 0.00
Epoch 326 Iter 8 subLoss 2465.5 multi -4.97 import weight 0.00
Epoch 326 Iter 9 subLoss 2802.8 multi -4.97 import weight 0.00
Epoch 326 Iter 10 subLoss 2422.6 multi -13.93 import weight 0.00
Epoch 326 Iter 11 subLoss 2925.0 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 326 Acc: 98.44 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 292 train Loss: 2550.3 test Loss: 282.6
Epoch 327 Iter 0 subLoss 2065.7 multi 6.97 import weight 0.00
Epoch 327 Iter 1 subLoss 2537.4 multi -13.93 import weight 0.00
Epoch 327 Iter 2 subLoss 2198.6 multi 3.99 import weight 0.00
Epoch 327 Iter 3 subLoss 2762.0 multi 15.93 import weight 0.00
Epoch 327 Iter 4 subLoss 2924.6 multi 15.93 import weight 0.00
Epoch 327 Iter 5 subLoss 2508.2 multi -22.88 import weight 0.00
Epoch 327 Iter 6 subLoss 2521.3 multi -1.99 import weight 0.00
Epoch 327 Iter 7 subLoss 2269.8 multi -1.98 import weight 0.00
Epoch 327 Iter 8 subLoss 2782.9 multi 9.96 import weight 0.00
Epoch 327 Iter 9 subLoss 2718.0 multi -1.98 import weight 0.00
Epoch 327 Iter 10 subLoss 2530.6 multi -13.93 import weight 0.00
Epoch 327 Iter 11 subLoss 2406.2 multi -37.81 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 327 Acc: 96.65 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -37.81 Pidx 240 train Loss: 3645.8 test Loss: 608.8
Epoch 328 Iter 0 subLoss 3572.6 multi -13.93 import weight 0.00
Epoch 328 Iter 1 subLoss 6364.4 multi -10.94 import weight 0.00
Epoch 328 Iter 2 subLoss 59317.9 multi 1.00 import weight 0.00
Epoch 328 Iter 3 subLoss 12103.8 multi -4.97 import weight 0.00
Epoch 328 Iter 4 subLoss 60313.1 multi 1.00 import weight 0.00
Epoch 328 Iter 5 subLoss 17854.4 multi 1.00 import weight 0.00
Epoch 328 Iter 6 subLoss 11168.4 multi 3.99 import weight 0.00
Epoch 328 Iter 7 subLoss 6030.6 multi -10.94 import weight 0.00
Epoch 328 Iter 8 subLoss 12315.0 multi -1.99 import weight 0.00
Epoch 328 Iter 9 subLoss 15539.1 multi 1.00 import weight 0.00
Epoch 328 Iter 10 subLoss 14001.4 multi 1.00 import weight 0.00
Epoch 328 Iter 11 subLoss 11940.2 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 328 Acc: 96.83 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 1194 train Loss: 4628.0 test Loss: 730.6
Epoch 329 Iter 0 subLoss 4580.7 multi 3.99 import weight 0.00
Epoch 329 Iter 1 subLoss 4185.2 multi -10.94 import weight 0.00
Epoch 329 Iter 2 subLoss 4898.7 multi 3.99 import weight 0.00
Epoch 329 Iter 3 subLoss 4257.8 multi -10.94 import weight 0.00
Epoch 329 Iter 4 subLoss 5907.8 multi -4.97 import weight 0.00
Epoch 329 Iter 5 subLoss 7878.4 multi 1.00 import weight 0.00
Epoch 329 Iter 6 subLoss 7693.3 multi -1.99 import weight 0.00
Epoch 329 Iter 7 subLoss 9104.4 multi -1.99 import weight 0.00
Epoch 329 Iter 8 subLoss 12114.0 multi -1.99 import weight 0.00
Epoch 329 Iter 9 subLoss 19745.2 multi 3.99 import weight 0.00
Epoch 329 Iter 10 subLoss 6866.5 multi -4.97 import weight 0.00
Epoch 329 Iter 11 subLoss 9290.1 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 329 Acc: 92.63 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 929 train Loss: 8590.6 test Loss: 1445.5
Epoch 330 Iter 0 subLoss 8690.0 multi 12.94 import weight 0.00
Epoch 330 Iter 1 subLoss 4578.2 multi 3.98 import weight 0.00
Epoch 330 Iter 2 subLoss 4125.2 multi 18.91 import weight 0.00
Epoch 330 Iter 3 subLoss 3393.2 multi 3.98 import weight 0.00
Epoch 330 Iter 4 subLoss 3148.2 multi 3.99 import weight 0.00
Epoch 330 Iter 5 subLoss 3119.3 multi 1.00 import weight 0.00
Epoch 330 Iter 6 subLoss 3065.0 multi 15.93 import weight 0.00
Epoch 330 Iter 7 subLoss 2739.2 multi 3.98 import weight 0.00
Epoch 330 Iter 8 subLoss 2612.8 multi 3.99 import weight 0.00
Epoch 330 Iter 9 subLoss 2688.9 multi -1.99 import weight 0.00
Epoch 330 Iter 10 subLoss 2747.1 multi 6.97 import weight 0.00
Epoch 330 Iter 11 subLoss 2655.1 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 330 Acc: 98.19 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 265 train Loss: 2564.2 test Loss: 316.1
Epoch 331 Iter 0 subLoss 2506.1 multi -19.90 import weight 0.00
Epoch 331 Iter 1 subLoss 3190.0 multi 3.98 import weight 0.00
Epoch 331 Iter 2 subLoss 2778.2 multi -4.97 import weight 0.00
Epoch 331 Iter 3 subLoss 3084.4 multi 12.94 import weight 0.00
Epoch 331 Iter 4 subLoss 2411.2 multi 9.96 import weight 0.00
Epoch 331 Iter 5 subLoss 2380.4 multi -7.96 import weight 0.00
Epoch 331 Iter 6 subLoss 2771.4 multi -1.99 import weight 0.00
Epoch 331 Iter 7 subLoss 2994.1 multi -7.96 import weight 0.00
Epoch 331 Iter 8 subLoss 2395.7 multi 30.85 import weight 0.00
Epoch 331 Iter 9 subLoss 2597.0 multi -1.99 import weight 0.00
Epoch 331 Iter 10 subLoss 2589.3 multi 9.96 import weight 0.00
Epoch 331 Iter 11 subLoss 2503.7 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 331 Acc: 97.35 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 250 train Loss: 2726.3 test Loss: 393.5
Epoch 332 Iter 0 subLoss 2507.7 multi -13.93 import weight 0.00
Epoch 332 Iter 1 subLoss 2276.3 multi -1.98 import weight 0.00
Epoch 332 Iter 2 subLoss 3093.2 multi -13.93 import weight 0.00
Epoch 332 Iter 3 subLoss 4076.6 multi 15.93 import weight 0.00
Epoch 332 Iter 4 subLoss 2394.8 multi 33.84 import weight 0.00
Epoch 332 Iter 5 subLoss 2592.9 multi -1.99 import weight 0.00
Epoch 332 Iter 6 subLoss 2669.6 multi 1.00 import weight 0.00
Epoch 332 Iter 7 subLoss 2257.8 multi 3.98 import weight 0.00
Epoch 332 Iter 8 subLoss 2665.2 multi 3.98 import weight 0.00
Epoch 332 Iter 9 subLoss 2516.9 multi 6.97 import weight 0.00
Epoch 332 Iter 10 subLoss 2740.3 multi 9.96 import weight 0.00
Epoch 332 Iter 11 subLoss 2656.2 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 332 Acc: 97.96 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 265 train Loss: 2502.5 test Loss: 316.6
Epoch 333 Iter 0 subLoss 2754.2 multi -28.85 import weight 0.00
Epoch 333 Iter 1 subLoss 2885.6 multi 3.98 import weight 0.00
Epoch 333 Iter 2 subLoss 2997.0 multi -4.97 import weight 0.00
Epoch 333 Iter 3 subLoss 3429.2 multi 6.97 import weight 0.00
Epoch 333 Iter 4 subLoss 2831.3 multi 9.96 import weight 0.00
Epoch 333 Iter 5 subLoss 2403.7 multi -40.79 import weight 0.00
Epoch 333 Iter 6 subLoss 2661.9 multi 3.99 import weight 0.00
Epoch 333 Iter 7 subLoss 2925.0 multi 18.91 import weight 0.00
Epoch 333 Iter 8 subLoss 2755.0 multi -25.87 import weight 0.00
Epoch 333 Iter 9 subLoss 2523.0 multi -1.98 import weight 0.00
Epoch 333 Iter 10 subLoss 2754.8 multi -22.88 import weight 0.00
Epoch 333 Iter 11 subLoss 3909.0 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 333 Acc: 97.76 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 390 train Loss: 2610.1 test Loss: 349.3
Epoch 334 Iter 0 subLoss 2751.1 multi -19.90 import weight 0.00
Epoch 334 Iter 1 subLoss 2864.1 multi -7.96 import weight 0.00
Epoch 334 Iter 2 subLoss 3727.3 multi -13.93 import weight 0.00
Epoch 334 Iter 3 subLoss 4581.9 multi 3.99 import weight 0.00
Epoch 334 Iter 4 subLoss 3254.8 multi 3.99 import weight 0.00
Epoch 334 Iter 5 subLoss 3672.0 multi 9.96 import weight 0.00
Epoch 334 Iter 6 subLoss 2747.7 multi 12.94 import weight 0.00
Epoch 334 Iter 7 subLoss 2724.3 multi -10.94 import weight 0.00
Epoch 334 Iter 8 subLoss 3152.0 multi -1.99 import weight 0.00
Epoch 334 Iter 9 subLoss 3146.4 multi 6.97 import weight 0.00
Epoch 334 Iter 10 subLoss 2884.6 multi 6.97 import weight 0.00
Epoch 334 Iter 11 subLoss 3189.4 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 334 Acc: 97.98 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 318 train Loss: 2632.4 test Loss: 331.4
Epoch 335 Iter 0 subLoss 2734.7 multi 3.99 import weight 0.00
Epoch 335 Iter 1 subLoss 2369.2 multi 12.94 import weight 0.00
Epoch 335 Iter 2 subLoss 2914.2 multi 9.96 import weight 0.00
Epoch 335 Iter 3 subLoss 2538.9 multi -13.93 import weight 0.00
Epoch 335 Iter 4 subLoss 2427.2 multi -13.93 import weight 0.00
Epoch 335 Iter 5 subLoss 2906.8 multi -22.88 import weight 0.00
Epoch 335 Iter 6 subLoss 4600.1 multi -7.96 import weight 0.00
Epoch 335 Iter 7 subLoss 8822.9 multi -4.97 import weight 0.00
Epoch 335 Iter 8 subLoss 31082.5 multi 1.00 import weight 0.00
Epoch 335 Iter 9 subLoss 19935.5 multi 1.00 import weight 0.00
Epoch 335 Iter 10 subLoss 15072.4 multi 1.00 import weight 0.00
Epoch 335 Iter 11 subLoss 8778.3 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 335 Acc: 76.38 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 877 train Loss: 24858.2 test Loss: 5977.1
Epoch 336 Iter 0 subLoss 24551.2 multi 1.00 import weight 0.00
Epoch 336 Iter 1 subLoss 18294.5 multi 1.00 import weight 0.00
Epoch 336 Iter 2 subLoss 12714.8 multi 1.00 import weight 0.00
Epoch 336 Iter 3 subLoss 10356.0 multi -1.99 import weight 0.00
Epoch 336 Iter 4 subLoss 17738.0 multi 1.00 import weight 0.00
Epoch 336 Iter 5 subLoss 11773.8 multi -1.99 import weight 0.00
Epoch 336 Iter 6 subLoss 19183.7 multi -1.99 import weight 0.00
Epoch 336 Iter 7 subLoss 27784.8 multi 1.00 import weight 0.00
Epoch 336 Iter 8 subLoss 22863.6 multi 3.99 import weight 0.00
Epoch 336 Iter 9 subLoss 9690.4 multi -1.99 import weight 0.00
Epoch 336 Iter 10 subLoss 13021.1 multi -1.98 import weight 0.00
Epoch 336 Iter 11 subLoss 19785.3 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 336 Acc: 74.22 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1978 train Loss: 26478.8 test Loss: 4822.2
Epoch 337 Iter 0 subLoss 25992.0 multi 1.00 import weight 0.00
Epoch 337 Iter 1 subLoss 21867.9 multi 1.00 import weight 0.00
Epoch 337 Iter 2 subLoss 18350.2 multi 3.99 import weight 0.00
Epoch 337 Iter 3 subLoss 10050.4 multi -4.97 import weight 0.00
Epoch 337 Iter 4 subLoss 18151.7 multi 6.97 import weight 0.00
Epoch 337 Iter 5 subLoss 7803.5 multi 1.00 import weight 0.00
Epoch 337 Iter 6 subLoss 6158.6 multi 6.97 import weight 0.00
Epoch 337 Iter 7 subLoss 3495.6 multi 3.98 import weight 0.00
Epoch 337 Iter 8 subLoss 3260.8 multi -10.94 import weight 0.00
Epoch 337 Iter 9 subLoss 4389.3 multi 9.96 import weight 0.00
Epoch 337 Iter 10 subLoss 3005.8 multi 9.96 import weight 0.00
Epoch 337 Iter 11 subLoss 3301.3 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 337 Acc: 97.41 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 330 train Loss: 2807.1 test Loss: 387.0
Epoch 338 Iter 0 subLoss 2734.6 multi 6.97 import weight 0.00
Epoch 338 Iter 1 subLoss 2722.5 multi -7.96 import weight 0.00
Epoch 338 Iter 2 subLoss 2646.5 multi -10.94 import weight 0.00
Epoch 338 Iter 3 subLoss 3198.1 multi 3.99 import weight 0.00
Epoch 338 Iter 4 subLoss 2851.6 multi 21.90 import weight 0.00
Epoch 338 Iter 5 subLoss 2418.6 multi 9.96 import weight 0.00
Epoch 338 Iter 6 subLoss 2458.4 multi -7.96 import weight 0.00
Epoch 338 Iter 7 subLoss 2270.0 multi -1.99 import weight 0.00
Epoch 338 Iter 8 subLoss 3146.4 multi 9.96 import weight 0.00
Epoch 338 Iter 9 subLoss 2602.3 multi -10.94 import weight 0.00
Epoch 338 Iter 10 subLoss 2330.4 multi 6.97 import weight 0.00
Epoch 338 Iter 11 subLoss 3018.4 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 338 Acc: 97.45 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 301 train Loss: 2987.9 test Loss: 407.2
Epoch 339 Iter 0 subLoss 2793.1 multi 1.00 import weight 0.00
Epoch 339 Iter 1 subLoss 2735.9 multi 6.97 import weight 0.00
Epoch 339 Iter 2 subLoss 2517.5 multi 9.96 import weight 0.00
Epoch 339 Iter 3 subLoss 2518.4 multi 12.94 import weight 0.00
Epoch 339 Iter 4 subLoss 2900.3 multi -19.90 import weight 0.00
Epoch 339 Iter 5 subLoss 2756.0 multi -19.90 import weight 0.00
Epoch 339 Iter 6 subLoss 3741.6 multi -4.97 import weight 0.00
Epoch 339 Iter 7 subLoss 5259.7 multi 9.96 import weight 0.00
Epoch 339 Iter 8 subLoss 3186.9 multi 9.96 import weight 0.00
Epoch 339 Iter 9 subLoss 2716.2 multi 1.00 import weight 0.00
Epoch 339 Iter 10 subLoss 2345.9 multi 3.99 import weight 0.00
Epoch 339 Iter 11 subLoss 2780.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 339 Acc: 98.05 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 278 train Loss: 2620.9 test Loss: 328.4
Epoch 340 Iter 0 subLoss 2682.4 multi 1.00 import weight 0.00
Epoch 340 Iter 1 subLoss 2156.7 multi 3.98 import weight 0.00
Epoch 340 Iter 2 subLoss 2743.7 multi 6.97 import weight 0.00
Epoch 340 Iter 3 subLoss 2581.7 multi 12.94 import weight 0.00
Epoch 340 Iter 4 subLoss 2637.6 multi 21.90 import weight 0.00
Epoch 340 Iter 5 subLoss 2219.9 multi 1.00 import weight 0.00
Epoch 340 Iter 6 subLoss 2334.8 multi 9.96 import weight 0.00
Epoch 340 Iter 7 subLoss 2147.9 multi 1.00 import weight 0.00
Epoch 340 Iter 8 subLoss 2160.9 multi -7.96 import weight 0.00
Epoch 340 Iter 9 subLoss 2750.1 multi -19.90 import weight 0.00
Epoch 340 Iter 10 subLoss 2684.1 multi 3.99 import weight 0.00
Epoch 340 Iter 11 subLoss 2151.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 340 Acc: 98.31 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 215 train Loss: 2683.1 test Loss: 295.8
Epoch 341 Iter 0 subLoss 2394.3 multi 36.82 import weight 0.00
Epoch 341 Iter 1 subLoss 2717.2 multi 3.99 import weight 0.00
Epoch 341 Iter 2 subLoss 2493.6 multi 18.91 import weight 0.00
Epoch 341 Iter 3 subLoss 2549.7 multi 24.88 import weight 0.00
Epoch 341 Iter 4 subLoss 2852.6 multi 24.88 import weight 0.00
Epoch 341 Iter 5 subLoss 2657.0 multi 21.90 import weight 0.00
Epoch 341 Iter 6 subLoss 2382.4 multi -4.97 import weight 0.00
Epoch 341 Iter 7 subLoss 2730.4 multi 9.96 import weight 0.00
Epoch 341 Iter 8 subLoss 2269.6 multi 1.00 import weight 0.00
Epoch 341 Iter 9 subLoss 2118.6 multi -1.99 import weight 0.00
Epoch 341 Iter 10 subLoss 2650.7 multi 24.88 import weight 0.00
Epoch 341 Iter 11 subLoss 2333.5 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 341 Acc: 98.54 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 233 train Loss: 2191.7 test Loss: 238.1
Epoch 342 Iter 0 subLoss 2169.2 multi -7.96 import weight 0.00
Epoch 342 Iter 1 subLoss 2111.0 multi 1.00 import weight 0.00
Epoch 342 Iter 2 subLoss 1991.0 multi 3.98 import weight 0.00
Epoch 342 Iter 3 subLoss 2135.8 multi -1.98 import weight 0.00
Epoch 342 Iter 4 subLoss 1973.7 multi 12.94 import weight 0.00
Epoch 342 Iter 5 subLoss 2314.6 multi 9.96 import weight 0.00
Epoch 342 Iter 6 subLoss 2163.9 multi -4.97 import weight 0.00
Epoch 342 Iter 7 subLoss 1976.0 multi 15.93 import weight 0.00
Epoch 342 Iter 8 subLoss 1890.0 multi 3.99 import weight 0.00
Epoch 342 Iter 9 subLoss 2452.4 multi -4.97 import weight 0.00
Epoch 342 Iter 10 subLoss 2199.8 multi 6.97 import weight 0.00
Epoch 342 Iter 11 subLoss 2001.6 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 342 Acc: 98.58 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 200 train Loss: 2101.7 test Loss: 226.1
Epoch 343 Iter 0 subLoss 1951.1 multi -7.96 import weight 0.00
Epoch 343 Iter 1 subLoss 1734.9 multi -1.99 import weight 0.00
Epoch 343 Iter 2 subLoss 2345.7 multi 1.00 import weight 0.00
Epoch 343 Iter 3 subLoss 2151.8 multi 6.97 import weight 0.00
Epoch 343 Iter 4 subLoss 1923.2 multi -1.98 import weight 0.00
Epoch 343 Iter 5 subLoss 2383.6 multi -1.99 import weight 0.00
Epoch 343 Iter 6 subLoss 2307.4 multi 1.00 import weight 0.00
Epoch 343 Iter 7 subLoss 1997.1 multi 6.97 import weight 0.00
Epoch 343 Iter 8 subLoss 1978.5 multi 18.91 import weight 0.00
Epoch 343 Iter 9 subLoss 2096.6 multi 6.97 import weight 0.00
Epoch 343 Iter 10 subLoss 1922.1 multi 1.00 import weight 0.00
Epoch 343 Iter 11 subLoss 2228.9 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 343 Acc: 98.64 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 222 train Loss: 2047.6 test Loss: 221.3
Epoch 344 Iter 0 subLoss 2008.5 multi 6.97 import weight 0.00
Epoch 344 Iter 1 subLoss 2259.5 multi 6.97 import weight 0.00
Epoch 344 Iter 2 subLoss 1833.8 multi 3.99 import weight 0.00
Epoch 344 Iter 3 subLoss 2050.7 multi -1.98 import weight 0.00
Epoch 344 Iter 4 subLoss 2240.0 multi 1.00 import weight 0.00
Epoch 344 Iter 5 subLoss 1898.9 multi 6.97 import weight 0.00
Epoch 344 Iter 6 subLoss 1928.2 multi 3.99 import weight 0.00
Epoch 344 Iter 7 subLoss 1928.2 multi 6.97 import weight 0.00
Epoch 344 Iter 8 subLoss 2091.4 multi 9.96 import weight 0.00
Epoch 344 Iter 9 subLoss 1682.4 multi -1.99 import weight 0.00
Epoch 344 Iter 10 subLoss 1780.7 multi 3.99 import weight 0.00
Epoch 344 Iter 11 subLoss 1946.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 344 Acc: 98.66 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 194 train Loss: 1967.6 test Loss: 223.3
Epoch 345 Iter 0 subLoss 2057.9 multi 1.00 import weight 0.00
Epoch 345 Iter 1 subLoss 2133.1 multi 1.00 import weight 0.00
Epoch 345 Iter 2 subLoss 2060.9 multi 3.99 import weight 0.00
Epoch 345 Iter 3 subLoss 1819.0 multi 3.98 import weight 0.00
Epoch 345 Iter 4 subLoss 2254.7 multi 9.96 import weight 0.00
Epoch 345 Iter 5 subLoss 1897.8 multi 9.96 import weight 0.00
Epoch 345 Iter 6 subLoss 1651.0 multi 1.00 import weight 0.00
Epoch 345 Iter 7 subLoss 1633.8 multi 1.00 import weight 0.00
Epoch 345 Iter 8 subLoss 2061.0 multi 6.97 import weight 0.00
Epoch 345 Iter 9 subLoss 1756.1 multi 1.00 import weight 0.00
Epoch 345 Iter 10 subLoss 1619.7 multi 1.00 import weight 0.00
Epoch 345 Iter 11 subLoss 1497.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 345 Acc: 98.77 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 149 train Loss: 1853.9 test Loss: 211.9
Epoch 346 Iter 0 subLoss 1918.5 multi -1.99 import weight 0.00
Epoch 346 Iter 1 subLoss 1868.2 multi 1.00 import weight 0.00
Epoch 346 Iter 2 subLoss 1537.1 multi 1.00 import weight 0.00
Epoch 346 Iter 3 subLoss 2178.6 multi -7.96 import weight 0.00
Epoch 346 Iter 4 subLoss 2037.8 multi 1.00 import weight 0.00
Epoch 346 Iter 5 subLoss 1765.4 multi -1.99 import weight 0.00
Epoch 346 Iter 6 subLoss 2617.0 multi 3.98 import weight 0.00
Epoch 346 Iter 7 subLoss 1875.4 multi -1.99 import weight 0.00
Epoch 346 Iter 8 subLoss 1538.8 multi 3.99 import weight 0.00
Epoch 346 Iter 9 subLoss 2008.9 multi 9.96 import weight 0.00
Epoch 346 Iter 10 subLoss 1765.3 multi 1.00 import weight 0.00
Epoch 346 Iter 11 subLoss 1812.5 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 346 Acc: 98.72 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 181 train Loss: 1921.4 test Loss: 209.1
Epoch 347 Iter 0 subLoss 1735.5 multi 1.00 import weight 0.00
Epoch 347 Iter 1 subLoss 1654.4 multi 3.99 import weight 0.00
Epoch 347 Iter 2 subLoss 1888.1 multi -1.99 import weight 0.00
Epoch 347 Iter 3 subLoss 1744.5 multi -4.97 import weight 0.00
Epoch 347 Iter 4 subLoss 1927.2 multi 6.97 import weight 0.00
Epoch 347 Iter 5 subLoss 2142.4 multi -1.99 import weight 0.00
Epoch 347 Iter 6 subLoss 1785.7 multi 6.97 import weight 0.00
Epoch 347 Iter 7 subLoss 2014.3 multi -19.90 import weight 0.00
Epoch 347 Iter 8 subLoss 1769.1 multi 3.98 import weight 0.00
Epoch 347 Iter 9 subLoss 1773.9 multi -7.96 import weight 0.00
Epoch 347 Iter 10 subLoss 2239.2 multi 3.99 import weight 0.00
Epoch 347 Iter 11 subLoss 1766.3 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 347 Acc: 98.66 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 176 train Loss: 1952.1 test Loss: 227.4
Epoch 348 Iter 0 subLoss 2306.5 multi 3.99 import weight 0.00
Epoch 348 Iter 1 subLoss 1824.1 multi -10.94 import weight 0.00
Epoch 348 Iter 2 subLoss 2004.4 multi 12.94 import weight 0.00
Epoch 348 Iter 3 subLoss 2174.8 multi -4.97 import weight 0.00
Epoch 348 Iter 4 subLoss 1879.2 multi 1.00 import weight 0.00
Epoch 348 Iter 5 subLoss 1859.2 multi -1.99 import weight 0.00
Epoch 348 Iter 6 subLoss 1908.9 multi -1.99 import weight 0.00
Epoch 348 Iter 7 subLoss 1925.7 multi 9.96 import weight 0.00
Epoch 348 Iter 8 subLoss 2152.3 multi 6.97 import weight 0.00
Epoch 348 Iter 9 subLoss 1464.4 multi 1.00 import weight 0.00
Epoch 348 Iter 10 subLoss 2236.4 multi 6.97 import weight 0.00
Epoch 348 Iter 11 subLoss 1684.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 348 Acc: 98.66 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 168 train Loss: 1871.5 test Loss: 223.2
Epoch 349 Iter 0 subLoss 2049.5 multi -1.99 import weight 0.00
Epoch 349 Iter 1 subLoss 1730.8 multi 3.98 import weight 0.00
Epoch 349 Iter 2 subLoss 1703.2 multi 1.00 import weight 0.00
Epoch 349 Iter 3 subLoss 1453.7 multi 1.00 import weight 0.00
Epoch 349 Iter 4 subLoss 1668.8 multi -4.97 import weight 0.00
Epoch 349 Iter 5 subLoss 1719.3 multi -1.99 import weight 0.00
Epoch 349 Iter 6 subLoss 2127.6 multi 1.00 import weight 0.00
Epoch 349 Iter 7 subLoss 1653.1 multi 6.97 import weight 0.00
Epoch 349 Iter 8 subLoss 1910.9 multi -1.99 import weight 0.00
Epoch 349 Iter 9 subLoss 2043.9 multi 1.00 import weight 0.00
Epoch 349 Iter 10 subLoss 1602.6 multi 1.00 import weight 0.00
Epoch 349 Iter 11 subLoss 2016.0 multi -19.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 349 Acc: 97.96 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -19.90 Pidx 201 train Loss: 2124.5 test Loss: 302.2
Epoch 350 Iter 0 subLoss 2085.3 multi -7.96 import weight 0.00
Epoch 350 Iter 1 subLoss 2992.9 multi -1.99 import weight 0.00
Epoch 350 Iter 2 subLoss 3415.8 multi -7.96 import weight 0.00
Epoch 350 Iter 3 subLoss 5558.9 multi -10.94 import weight 0.00
Epoch 350 Iter 4 subLoss 71281.2 multi 1.00 import weight 0.00
Epoch 350 Iter 5 subLoss 14262.8 multi -1.99 import weight 0.00
Epoch 350 Iter 6 subLoss 33664.9 multi 1.00 import weight 0.00
Epoch 350 Iter 7 subLoss 11922.9 multi 1.00 import weight 0.00
Epoch 350 Iter 8 subLoss 10136.7 multi -1.99 import weight 0.00
Epoch 350 Iter 9 subLoss 13935.9 multi 1.00 import weight 0.00
Epoch 350 Iter 10 subLoss 10425.5 multi -1.99 import weight 0.00
Epoch 350 Iter 11 subLoss 15493.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 350 Acc: 81.55 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1549 train Loss: 12446.2 test Loss: 2900.3
Epoch 351 Iter 0 subLoss 12875.6 multi 1.00 import weight 0.00
Epoch 351 Iter 1 subLoss 10401.0 multi 1.00 import weight 0.00
Epoch 351 Iter 2 subLoss 8287.4 multi -1.99 import weight 0.00
Epoch 351 Iter 3 subLoss 11789.1 multi 1.00 import weight 0.00
Epoch 351 Iter 4 subLoss 9492.5 multi -4.97 import weight 0.00
Epoch 351 Iter 5 subLoss 17420.8 multi 3.99 import weight 0.00
Epoch 351 Iter 6 subLoss 8174.6 multi 9.96 import weight 0.00
Epoch 351 Iter 7 subLoss 3713.6 multi 12.94 import weight 0.00
Epoch 351 Iter 8 subLoss 2309.7 multi 6.97 import weight 0.00
Epoch 351 Iter 9 subLoss 2220.0 multi 3.99 import weight 0.00
Epoch 351 Iter 10 subLoss 2425.4 multi -13.93 import weight 0.00
Epoch 351 Iter 11 subLoss 2323.6 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 351 Acc: 97.24 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 232 train Loss: 2784.7 test Loss: 425.8
Epoch 352 Iter 0 subLoss 2278.4 multi -4.97 import weight 0.00
Epoch 352 Iter 1 subLoss 2683.8 multi 6.97 import weight 0.00
Epoch 352 Iter 2 subLoss 2782.2 multi 9.96 import weight 0.00
Epoch 352 Iter 3 subLoss 3009.0 multi 9.96 import weight 0.00
Epoch 352 Iter 4 subLoss 2180.5 multi 1.00 import weight 0.00
Epoch 352 Iter 5 subLoss 2090.0 multi -4.97 import weight 0.00
Epoch 352 Iter 6 subLoss 2356.1 multi -16.91 import weight 0.00
Epoch 352 Iter 7 subLoss 2810.9 multi -7.96 import weight 0.00
Epoch 352 Iter 8 subLoss 3519.2 multi -13.93 import weight 0.00
Epoch 352 Iter 9 subLoss 9400.7 multi 9.96 import weight 0.00
Epoch 352 Iter 10 subLoss 7052.2 multi 1.00 import weight 0.00
Epoch 352 Iter 11 subLoss 4876.1 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 352 Acc: 97.47 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 487 train Loss: 3037.3 test Loss: 403.4
Epoch 353 Iter 0 subLoss 3060.5 multi 18.91 import weight 0.00
Epoch 353 Iter 1 subLoss 2339.5 multi 12.94 import weight 0.00
Epoch 353 Iter 2 subLoss 2164.3 multi -7.96 import weight 0.00
Epoch 353 Iter 3 subLoss 1638.7 multi 3.99 import weight 0.00
Epoch 353 Iter 4 subLoss 2033.9 multi 3.99 import weight 0.00
Epoch 353 Iter 5 subLoss 2620.5 multi -25.87 import weight 0.00
Epoch 353 Iter 6 subLoss 3323.1 multi -19.90 import weight 0.00
Epoch 353 Iter 7 subLoss 5047.0 multi 9.96 import weight 0.00
Epoch 353 Iter 8 subLoss 2704.3 multi 15.93 import weight 0.00
Epoch 353 Iter 9 subLoss 2624.1 multi -22.88 import weight 0.00
Epoch 353 Iter 10 subLoss 3200.5 multi -19.90 import weight 0.00
Epoch 353 Iter 11 subLoss 17710.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 353 Acc: 93.56 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1771 train Loss: 7815.7 test Loss: 1118.5
Epoch 354 Iter 0 subLoss 7087.0 multi -1.99 import weight 0.00
Epoch 354 Iter 1 subLoss 12551.3 multi 3.99 import weight 0.00
Epoch 354 Iter 2 subLoss 2796.9 multi -1.99 import weight 0.00
Epoch 354 Iter 3 subLoss 2996.5 multi 1.00 import weight 0.00
Epoch 354 Iter 4 subLoss 3176.3 multi 9.96 import weight 0.00
Epoch 354 Iter 5 subLoss 2459.1 multi -1.98 import weight 0.00
Epoch 354 Iter 6 subLoss 2865.8 multi -10.94 import weight 0.00
Epoch 354 Iter 7 subLoss 3398.5 multi 6.97 import weight 0.00
Epoch 354 Iter 8 subLoss 3246.2 multi -4.97 import weight 0.00
Epoch 354 Iter 9 subLoss 3004.8 multi 9.96 import weight 0.00
Epoch 354 Iter 10 subLoss 2794.2 multi 1.00 import weight 0.00
Epoch 354 Iter 11 subLoss 2588.9 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 354 Acc: 98.38 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 258 train Loss: 2461.3 test Loss: 281.8
Epoch 355 Iter 0 subLoss 2454.4 multi 1.00 import weight 0.00
Epoch 355 Iter 1 subLoss 2660.8 multi 1.00 import weight 0.00
Epoch 355 Iter 2 subLoss 2510.4 multi 15.93 import weight 0.00
Epoch 355 Iter 3 subLoss 2161.0 multi -4.97 import weight 0.00
Epoch 355 Iter 4 subLoss 2234.9 multi 9.96 import weight 0.00
Epoch 355 Iter 5 subLoss 2400.8 multi -40.79 import weight 0.00
Epoch 355 Iter 6 subLoss 3796.0 multi -10.94 import weight 0.00
Epoch 355 Iter 7 subLoss 6386.5 multi 1.00 import weight 0.00
Epoch 355 Iter 8 subLoss 4760.1 multi 12.94 import weight 0.00
Epoch 355 Iter 9 subLoss 2384.6 multi 1.00 import weight 0.00
Epoch 355 Iter 10 subLoss 1948.3 multi 9.96 import weight 0.00
Epoch 355 Iter 11 subLoss 2318.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 355 Acc: 98.44 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 231 train Loss: 2359.8 test Loss: 264.8
Epoch 356 Iter 0 subLoss 2026.1 multi -1.99 import weight 0.00
Epoch 356 Iter 1 subLoss 2517.3 multi 18.91 import weight 0.00
Epoch 356 Iter 2 subLoss 2667.3 multi 3.98 import weight 0.00
Epoch 356 Iter 3 subLoss 2371.0 multi 1.00 import weight 0.00
Epoch 356 Iter 4 subLoss 2388.2 multi 1.00 import weight 0.00
Epoch 356 Iter 5 subLoss 2364.9 multi 12.94 import weight 0.00
Epoch 356 Iter 6 subLoss 2094.9 multi 6.97 import weight 0.00
Epoch 356 Iter 7 subLoss 2149.2 multi 1.00 import weight 0.00
Epoch 356 Iter 8 subLoss 2015.6 multi -16.91 import weight 0.00
Epoch 356 Iter 9 subLoss 2353.4 multi -13.93 import weight 0.00
Epoch 356 Iter 10 subLoss 2309.5 multi 9.96 import weight 0.00
Epoch 356 Iter 11 subLoss 2095.7 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 356 Acc: 98.56 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 209 train Loss: 2286.4 test Loss: 256.1
Epoch 357 Iter 0 subLoss 1897.3 multi 9.96 import weight 0.00
Epoch 357 Iter 1 subLoss 2155.0 multi 6.97 import weight 0.00
Epoch 357 Iter 2 subLoss 2177.3 multi -7.96 import weight 0.00
Epoch 357 Iter 3 subLoss 2536.0 multi -10.94 import weight 0.00
Epoch 357 Iter 4 subLoss 2472.7 multi 33.84 import weight 0.00
Epoch 357 Iter 5 subLoss 3382.1 multi -16.91 import weight 0.00
Epoch 357 Iter 6 subLoss 13684.2 multi 1.00 import weight 0.00
Epoch 357 Iter 7 subLoss 7447.9 multi -1.99 import weight 0.00
Epoch 357 Iter 8 subLoss 11545.5 multi 1.00 import weight 0.00
Epoch 357 Iter 9 subLoss 8384.1 multi 3.99 import weight 0.00
Epoch 357 Iter 10 subLoss 2970.0 multi 12.94 import weight 0.00
Epoch 357 Iter 11 subLoss 2238.4 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 357 Acc: 98.64 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 223 train Loss: 2198.1 test Loss: 245.3
Epoch 358 Iter 0 subLoss 1858.6 multi 1.00 import weight 0.00
Epoch 358 Iter 1 subLoss 2010.7 multi -13.93 import weight 0.00
Epoch 358 Iter 2 subLoss 2230.7 multi 15.93 import weight 0.00
Epoch 358 Iter 3 subLoss 2526.2 multi -10.94 import weight 0.00
Epoch 358 Iter 4 subLoss 2226.3 multi 3.99 import weight 0.00
Epoch 358 Iter 5 subLoss 1966.7 multi -1.99 import weight 0.00
Epoch 358 Iter 6 subLoss 2145.9 multi 3.99 import weight 0.00
Epoch 358 Iter 7 subLoss 2746.1 multi 6.97 import weight 0.00
Epoch 358 Iter 8 subLoss 2155.6 multi 6.97 import weight 0.00
Epoch 358 Iter 9 subLoss 2325.9 multi -13.93 import weight 0.00
Epoch 358 Iter 10 subLoss 1919.0 multi 1.00 import weight 0.00
Epoch 358 Iter 11 subLoss 2044.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 358 Acc: 98.48 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 204 train Loss: 2250.2 test Loss: 258.1
Epoch 359 Iter 0 subLoss 1990.4 multi 9.96 import weight 0.00
Epoch 359 Iter 1 subLoss 2022.6 multi -4.97 import weight 0.00
Epoch 359 Iter 2 subLoss 2097.0 multi 12.94 import weight 0.00
Epoch 359 Iter 3 subLoss 2097.7 multi 15.93 import weight 0.00
Epoch 359 Iter 4 subLoss 2139.5 multi 1.00 import weight 0.00
Epoch 359 Iter 5 subLoss 1685.4 multi 3.98 import weight 0.00
Epoch 359 Iter 6 subLoss 1672.9 multi 1.00 import weight 0.00
Epoch 359 Iter 7 subLoss 1728.5 multi 1.00 import weight 0.00
Epoch 359 Iter 8 subLoss 1688.5 multi 3.99 import weight 0.00
Epoch 359 Iter 9 subLoss 2230.3 multi 15.93 import weight 0.00
Epoch 359 Iter 10 subLoss 1755.9 multi 1.00 import weight 0.00
Epoch 359 Iter 11 subLoss 2429.2 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 359 Acc: 98.70 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 242 train Loss: 2021.8 test Loss: 219.3
Epoch 360 Iter 0 subLoss 1986.7 multi -16.91 import weight 0.00
Epoch 360 Iter 1 subLoss 2292.6 multi 9.96 import weight 0.00
Epoch 360 Iter 2 subLoss 1796.9 multi -7.96 import weight 0.00
Epoch 360 Iter 3 subLoss 1592.0 multi 1.00 import weight 0.00
Epoch 360 Iter 4 subLoss 2131.4 multi 3.99 import weight 0.00
Epoch 360 Iter 5 subLoss 2368.2 multi 12.94 import weight 0.00
Epoch 360 Iter 6 subLoss 2083.2 multi -1.99 import weight 0.00
Epoch 360 Iter 7 subLoss 2146.4 multi 1.00 import weight 0.00
Epoch 360 Iter 8 subLoss 2183.1 multi 1.00 import weight 0.00
Epoch 360 Iter 9 subLoss 2022.2 multi -1.98 import weight 0.00
Epoch 360 Iter 10 subLoss 2126.7 multi 3.99 import weight 0.00
Epoch 360 Iter 11 subLoss 2076.7 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 360 Acc: 98.68 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 207 train Loss: 2097.4 test Loss: 246.0
Epoch 361 Iter 0 subLoss 2046.8 multi 3.98 import weight 0.00
Epoch 361 Iter 1 subLoss 2163.1 multi -7.96 import weight 0.00
Epoch 361 Iter 2 subLoss 2318.5 multi 3.98 import weight 0.00
Epoch 361 Iter 3 subLoss 2297.0 multi 12.94 import weight 0.00
Epoch 361 Iter 4 subLoss 2016.0 multi -10.94 import weight 0.00
Epoch 361 Iter 5 subLoss 1702.5 multi 3.99 import weight 0.00
Epoch 361 Iter 6 subLoss 1910.2 multi 3.98 import weight 0.00
Epoch 361 Iter 7 subLoss 2118.7 multi 3.98 import weight 0.00
Epoch 361 Iter 8 subLoss 2165.9 multi -4.97 import weight 0.00
Epoch 361 Iter 9 subLoss 2063.2 multi 9.96 import weight 0.00
Epoch 361 Iter 10 subLoss 2115.0 multi 6.97 import weight 0.00
Epoch 361 Iter 11 subLoss 1807.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 361 Acc: 98.83 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 180 train Loss: 1943.4 test Loss: 209.6
Epoch 362 Iter 0 subLoss 2141.4 multi 3.99 import weight 0.00
Epoch 362 Iter 1 subLoss 2167.4 multi -1.98 import weight 0.00
Epoch 362 Iter 2 subLoss 1621.9 multi -1.99 import weight 0.00
Epoch 362 Iter 3 subLoss 1785.7 multi 6.97 import weight 0.00
Epoch 362 Iter 4 subLoss 2186.4 multi 3.99 import weight 0.00
Epoch 362 Iter 5 subLoss 1679.4 multi 3.98 import weight 0.00
Epoch 362 Iter 6 subLoss 2082.1 multi -1.98 import weight 0.00
Epoch 362 Iter 7 subLoss 1961.2 multi 1.00 import weight 0.00
Epoch 362 Iter 8 subLoss 2108.9 multi -22.88 import weight 0.00
Epoch 362 Iter 9 subLoss 2128.6 multi 1.00 import weight 0.00
Epoch 362 Iter 10 subLoss 2221.0 multi 6.97 import weight 0.00
Epoch 362 Iter 11 subLoss 2166.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 362 Acc: 98.89 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 216 train Loss: 2050.1 test Loss: 208.8
Epoch 363 Iter 0 subLoss 1968.9 multi 3.98 import weight 0.00
Epoch 363 Iter 1 subLoss 2289.8 multi -7.96 import weight 0.00
Epoch 363 Iter 2 subLoss 1698.7 multi -10.94 import weight 0.00
Epoch 363 Iter 3 subLoss 2400.8 multi -37.81 import weight 0.00
Epoch 363 Iter 4 subLoss 12126.4 multi 1.00 import weight 0.00
Epoch 363 Iter 5 subLoss 7209.5 multi 3.98 import weight 0.00
Epoch 363 Iter 6 subLoss 3024.2 multi -7.96 import weight 0.00
Epoch 363 Iter 7 subLoss 3468.9 multi -4.97 import weight 0.00
Epoch 363 Iter 8 subLoss 6720.2 multi -1.99 import weight 0.00
Epoch 363 Iter 9 subLoss 8823.4 multi -1.98 import weight 0.00
Epoch 363 Iter 10 subLoss 19941.7 multi 1.00 import weight 0.00
Epoch 363 Iter 11 subLoss 8627.1 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 363 Acc: 98.33 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 862 train Loss: 2455.1 test Loss: 267.6
Epoch 364 Iter 0 subLoss 2556.8 multi -37.81 import weight 0.00
Epoch 364 Iter 1 subLoss 9787.2 multi -1.99 import weight 0.00
Epoch 364 Iter 2 subLoss 41103.4 multi 1.00 import weight 0.00
Epoch 364 Iter 3 subLoss 3235.8 multi 9.96 import weight 0.00
Epoch 364 Iter 4 subLoss 2481.5 multi -28.85 import weight 0.00
Epoch 364 Iter 5 subLoss 4371.6 multi -1.99 import weight 0.00
Epoch 364 Iter 6 subLoss 4203.8 multi -13.93 import weight 0.00
Epoch 364 Iter 7 subLoss 9359.8 multi 9.96 import weight 0.00
Epoch 364 Iter 8 subLoss 3927.0 multi 9.96 import weight 0.00
Epoch 364 Iter 9 subLoss 2772.7 multi 1.00 import weight 0.00
Epoch 364 Iter 10 subLoss 2779.7 multi 3.99 import weight 0.00
Epoch 364 Iter 11 subLoss 2977.5 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 364 Acc: 98.46 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 297 train Loss: 2680.0 test Loss: 268.6
Epoch 365 Iter 0 subLoss 2344.7 multi 1.00 import weight 0.00
Epoch 365 Iter 1 subLoss 2814.7 multi -4.97 import weight 0.00
Epoch 365 Iter 2 subLoss 2743.7 multi 9.96 import weight 0.00
Epoch 365 Iter 3 subLoss 2457.2 multi 3.99 import weight 0.00
Epoch 365 Iter 4 subLoss 2330.0 multi -13.93 import weight 0.00
Epoch 365 Iter 5 subLoss 2530.8 multi -10.94 import weight 0.00
Epoch 365 Iter 6 subLoss 2426.7 multi -7.96 import weight 0.00
Epoch 365 Iter 7 subLoss 3171.2 multi 12.94 import weight 0.00
Epoch 365 Iter 8 subLoss 2681.1 multi 9.96 import weight 0.00
Epoch 365 Iter 9 subLoss 2340.4 multi 3.99 import weight 0.00
Epoch 365 Iter 10 subLoss 2657.2 multi 27.87 import weight 0.00
Epoch 365 Iter 11 subLoss 2482.7 multi -25.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 365 Acc: 97.33 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -25.87 Pidx 248 train Loss: 3852.7 test Loss: 458.2
Epoch 366 Iter 0 subLoss 3306.1 multi 21.90 import weight 0.00
Epoch 366 Iter 1 subLoss 2769.9 multi 1.00 import weight 0.00
Epoch 366 Iter 2 subLoss 2975.5 multi 18.91 import weight 0.00
Epoch 366 Iter 3 subLoss 2363.7 multi 15.93 import weight 0.00
Epoch 366 Iter 4 subLoss 2183.7 multi 6.97 import weight 0.00
Epoch 366 Iter 5 subLoss 2321.5 multi -10.94 import weight 0.00
Epoch 366 Iter 6 subLoss 1920.3 multi 3.99 import weight 0.00
Epoch 366 Iter 7 subLoss 1913.1 multi 6.97 import weight 0.00
Epoch 366 Iter 8 subLoss 2208.0 multi 3.99 import weight 0.00
Epoch 366 Iter 9 subLoss 2319.0 multi 6.97 import weight 0.00
Epoch 366 Iter 10 subLoss 2328.9 multi -10.94 import weight 0.00
Epoch 366 Iter 11 subLoss 2301.2 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 366 Acc: 98.66 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 230 train Loss: 2229.6 test Loss: 212.7
Epoch 367 Iter 0 subLoss 2342.0 multi 6.97 import weight 0.00
Epoch 367 Iter 1 subLoss 1656.4 multi 9.96 import weight 0.00
Epoch 367 Iter 2 subLoss 2505.9 multi -13.93 import weight 0.00
Epoch 367 Iter 3 subLoss 2533.6 multi -7.96 import weight 0.00
Epoch 367 Iter 4 subLoss 2154.4 multi 3.98 import weight 0.00
Epoch 367 Iter 5 subLoss 2106.6 multi -19.90 import weight 0.00
Epoch 367 Iter 6 subLoss 2482.6 multi -22.88 import weight 0.00
Epoch 367 Iter 7 subLoss 7171.2 multi 1.00 import weight 0.00
Epoch 367 Iter 8 subLoss 3767.7 multi 1.00 import weight 0.00
Epoch 367 Iter 9 subLoss 4632.0 multi 6.97 import weight 0.00
Epoch 367 Iter 10 subLoss 2573.9 multi -10.94 import weight 0.00
Epoch 367 Iter 11 subLoss 3322.0 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 367 Acc: 94.55 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 332 train Loss: 7707.8 test Loss: 945.8
Epoch 368 Iter 0 subLoss 6962.8 multi 1.00 import weight 0.00
Epoch 368 Iter 1 subLoss 5211.4 multi -7.96 import weight 0.00
Epoch 368 Iter 2 subLoss 31183.6 multi 3.99 import weight 0.00
Epoch 368 Iter 3 subLoss 5674.7 multi -1.98 import weight 0.00
Epoch 368 Iter 4 subLoss 7019.8 multi -7.96 import weight 0.00
Epoch 368 Iter 5 subLoss 39776.0 multi 1.00 import weight 0.00
Epoch 368 Iter 6 subLoss 10986.9 multi -1.98 import weight 0.00
Epoch 368 Iter 7 subLoss 17417.8 multi -1.99 import weight 0.00
Epoch 368 Iter 8 subLoss 54752.5 multi 1.00 import weight 0.00
Epoch 368 Iter 9 subLoss 13694.0 multi -1.99 import weight 0.00
Epoch 368 Iter 10 subLoss 26768.1 multi 3.99 import weight 0.00
Epoch 368 Iter 11 subLoss 4516.9 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 368 Acc: 97.41 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 451 train Loss: 3669.6 test Loss: 493.1
Epoch 369 Iter 0 subLoss 3759.1 multi -13.93 import weight 0.00
Epoch 369 Iter 1 subLoss 5882.4 multi -1.99 import weight 0.00
Epoch 369 Iter 2 subLoss 7493.6 multi 1.00 import weight 0.00
Epoch 369 Iter 3 subLoss 6127.0 multi 6.97 import weight 0.00
Epoch 369 Iter 4 subLoss 3877.8 multi 12.94 import weight 0.00
Epoch 369 Iter 5 subLoss 2816.7 multi -1.99 import weight 0.00
Epoch 369 Iter 6 subLoss 3413.7 multi -4.97 import weight 0.00
Epoch 369 Iter 7 subLoss 3162.2 multi -13.93 import weight 0.00
Epoch 369 Iter 8 subLoss 4212.0 multi 12.94 import weight 0.00
Epoch 369 Iter 9 subLoss 3244.9 multi -4.97 import weight 0.00
Epoch 369 Iter 10 subLoss 3004.6 multi 12.94 import weight 0.00
Epoch 369 Iter 11 subLoss 2785.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 369 Acc: 98.33 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 278 train Loss: 2657.3 test Loss: 297.3
Epoch 370 Iter 0 subLoss 2403.6 multi -34.82 import weight 0.00
Epoch 370 Iter 1 subLoss 3191.4 multi 3.99 import weight 0.00
Epoch 370 Iter 2 subLoss 3431.9 multi -1.99 import weight 0.00
Epoch 370 Iter 3 subLoss 3367.7 multi 6.97 import weight 0.00
Epoch 370 Iter 4 subLoss 3018.5 multi -22.88 import weight 0.00
Epoch 370 Iter 5 subLoss 3753.4 multi -10.94 import weight 0.00
Epoch 370 Iter 6 subLoss 5869.8 multi -4.97 import weight 0.00
Epoch 370 Iter 7 subLoss 8103.9 multi -1.98 import weight 0.00
Epoch 370 Iter 8 subLoss 13084.8 multi 1.00 import weight 0.00
Epoch 370 Iter 9 subLoss 8370.6 multi 1.00 import weight 0.00
Epoch 370 Iter 10 subLoss 7362.6 multi 1.00 import weight 0.00
Epoch 370 Iter 11 subLoss 5589.9 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 370 Acc: 71.86 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 558 train Loss: 22698.0 test Loss: 4137.2
Epoch 371 Iter 0 subLoss 22680.2 multi -4.97 import weight 0.00
Epoch 371 Iter 1 subLoss 92073.3 multi 1.00 import weight 0.00
Epoch 371 Iter 2 subLoss 55309.7 multi 1.00 import weight 0.00
Epoch 371 Iter 3 subLoss 53528.3 multi 1.00 import weight 0.00
Epoch 371 Iter 4 subLoss 51119.8 multi 1.00 import weight 0.00
Epoch 371 Iter 5 subLoss 48647.7 multi 1.00 import weight 0.00
Epoch 371 Iter 6 subLoss 47614.0 multi 1.00 import weight 0.00
Epoch 371 Iter 7 subLoss 42038.5 multi 1.00 import weight 0.00
Epoch 371 Iter 8 subLoss 39131.0 multi 1.00 import weight 0.00
Epoch 371 Iter 9 subLoss 36610.7 multi -1.99 import weight 0.00
Epoch 371 Iter 10 subLoss 42964.0 multi 1.00 import weight 0.00
Epoch 371 Iter 11 subLoss 40025.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 371 Acc: 56.96 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 4002 train Loss: 37729.7 test Loss: 6274.8
Epoch 372 Iter 0 subLoss 37173.4 multi 1.00 import weight 0.00
Epoch 372 Iter 1 subLoss 32745.2 multi 1.00 import weight 0.00
Epoch 372 Iter 2 subLoss 29481.6 multi 1.00 import weight 0.00
Epoch 372 Iter 3 subLoss 25647.7 multi 1.00 import weight 0.00
Epoch 372 Iter 4 subLoss 21568.1 multi 1.00 import weight 0.00
Epoch 372 Iter 5 subLoss 18936.5 multi 1.00 import weight 0.00
Epoch 372 Iter 6 subLoss 15147.2 multi -1.99 import weight 0.00
Epoch 372 Iter 7 subLoss 21625.1 multi 3.99 import weight 0.00
Epoch 372 Iter 8 subLoss 10633.1 multi 1.00 import weight 0.00
Epoch 372 Iter 9 subLoss 9894.3 multi 3.99 import weight 0.00
Epoch 372 Iter 10 subLoss 6510.5 multi -4.97 import weight 0.00
Epoch 372 Iter 11 subLoss 8758.6 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 372 Acc: 76.86 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 875 train Loss: 20429.8 test Loss: 3399.3
Epoch 373 Iter 0 subLoss 20315.8 multi 1.00 import weight 0.00
Epoch 373 Iter 1 subLoss 17511.5 multi 1.00 import weight 0.00
Epoch 373 Iter 2 subLoss 15649.5 multi 6.97 import weight 0.00
Epoch 373 Iter 3 subLoss 8205.8 multi -7.96 import weight 0.00
Epoch 373 Iter 4 subLoss 11029.7 multi 3.99 import weight 0.00
Epoch 373 Iter 5 subLoss 8933.2 multi -7.96 import weight 0.00
Epoch 373 Iter 6 subLoss 12588.1 multi 1.00 import weight 0.00
Epoch 373 Iter 7 subLoss 11818.0 multi -1.99 import weight 0.00
Epoch 373 Iter 8 subLoss 14028.1 multi 1.00 import weight 0.00
Epoch 373 Iter 9 subLoss 12181.3 multi -1.99 import weight 0.00
Epoch 373 Iter 10 subLoss 14626.9 multi 3.99 import weight 0.00
Epoch 373 Iter 11 subLoss 11007.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 373 Acc: 93.66 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1100 train Loss: 10901.0 test Loss: 1314.4
Epoch 374 Iter 0 subLoss 10351.7 multi 1.00 import weight 0.00
Epoch 374 Iter 1 subLoss 9839.3 multi 9.96 import weight 0.00
Epoch 374 Iter 2 subLoss 6619.9 multi 1.00 import weight 0.00
Epoch 374 Iter 3 subLoss 6576.7 multi 15.93 import weight 0.00
Epoch 374 Iter 4 subLoss 4720.8 multi 9.96 import weight 0.00
Epoch 374 Iter 5 subLoss 3567.3 multi 6.97 import weight 0.00
Epoch 374 Iter 6 subLoss 3154.9 multi -4.97 import weight 0.00
Epoch 374 Iter 7 subLoss 3402.2 multi 1.00 import weight 0.00
Epoch 374 Iter 8 subLoss 3605.2 multi 1.00 import weight 0.00
Epoch 374 Iter 9 subLoss 3457.0 multi -49.75 import weight 0.00
Epoch 374 Iter 10 subLoss 7561.4 multi 3.99 import weight 0.00
Epoch 374 Iter 11 subLoss 5403.3 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 374 Acc: 96.13 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 540 train Loss: 6697.4 test Loss: 684.4
Epoch 375 Iter 0 subLoss 6498.3 multi 12.94 import weight 0.00
Epoch 375 Iter 1 subLoss 4548.5 multi 18.91 import weight 0.00
Epoch 375 Iter 2 subLoss 3645.5 multi -4.97 import weight 0.00
Epoch 375 Iter 3 subLoss 4882.7 multi -7.96 import weight 0.00
Epoch 375 Iter 4 subLoss 3744.6 multi -1.99 import weight 0.00
Epoch 375 Iter 5 subLoss 4389.9 multi 9.96 import weight 0.00
Epoch 375 Iter 6 subLoss 4184.7 multi -7.96 import weight 0.00
Epoch 375 Iter 7 subLoss 5027.6 multi 1.00 import weight 0.00
Epoch 375 Iter 8 subLoss 3885.9 multi 1.00 import weight 0.00
Epoch 375 Iter 9 subLoss 4259.3 multi -7.96 import weight 0.00
Epoch 375 Iter 10 subLoss 4319.2 multi -1.99 import weight 0.00
Epoch 375 Iter 11 subLoss 4832.8 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 375 Acc: 96.65 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 483 train Loss: 5041.9 test Loss: 519.7
Epoch 376 Iter 0 subLoss 4728.8 multi 12.94 import weight 0.00
Epoch 376 Iter 1 subLoss 4593.2 multi -4.97 import weight 0.00
Epoch 376 Iter 2 subLoss 4324.4 multi 12.94 import weight 0.00
Epoch 376 Iter 3 subLoss 4276.8 multi -1.98 import weight 0.00
Epoch 376 Iter 4 subLoss 3692.5 multi -1.99 import weight 0.00
Epoch 376 Iter 5 subLoss 4121.0 multi 21.90 import weight 0.00
Epoch 376 Iter 6 subLoss 3427.5 multi 3.99 import weight 0.00
Epoch 376 Iter 7 subLoss 3463.5 multi -4.97 import weight 0.00
Epoch 376 Iter 8 subLoss 3148.7 multi 12.94 import weight 0.00
Epoch 376 Iter 9 subLoss 3317.3 multi -1.99 import weight 0.00
Epoch 376 Iter 10 subLoss 3177.3 multi 12.94 import weight 0.00
Epoch 376 Iter 11 subLoss 2867.9 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0883 / 0.17243 / 11.64
Entropy seen (from low to high)
[2183, 313, 239, 188, 135, 160, 132, 125, 109, 84, 96, 86, 84, 79, 81, 94, 78, 88, 81, 88, 95, 83, 55, 54, 61, 42, 31, 36, 21, 26, 23, 18, 22, 12, 9, 8, 4, 6, 6, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 1, 1, 6, 3, 4, 7, 10, 18, 18, 17, 20, 27, 16, 24, 35, 39, 49, 57, 64, 63, 62, 72, 60, 50, 55, 72, 52, 67, 58, 68, 58, 66, 60, 70, 78, 87, 93, 88, 95, 108, 142, 170, 261, 520, 1870]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0, 36.8, 40.6, 43.7, 47.4, 50.7, 54.1, 57.6, 61.0, 64.6, 68.3]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 71.4, 16.6, 47.3, 39.3, 53.6, 47.9, 50.4, 44.6, 48.3, 51.0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 12, 19, 33, 69, 75, 103, 94, 89, 96]
Epoch 376 Acc: 98.37 BMA: 72.45 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 286 train Loss: 3169.6 test Loss: 278.5
Epoch 377 Iter 0 subLoss 3601.7 multi 3.99 import weight 0.00
Epoch 377 Iter 1 subLoss 2743.0 multi 12.94 import weight 0.00
Epoch 377 Iter 2 subLoss 3171.4 multi 15.93 import weight 0.00
Epoch 377 Iter 3 subLoss 3500.0 multi 6.97 import weight 0.00
Epoch 377 Iter 4 subLoss 2648.8 multi -10.94 import weight 0.00
Epoch 377 Iter 5 subLoss 2458.9 multi 6.97 import weight 0.00
Epoch 377 Iter 6 subLoss 2665.5 multi 3.99 import weight 0.00
Epoch 377 Iter 7 subLoss 2625.1 multi -19.90 import weight 0.00
Epoch 377 Iter 8 subLoss 2716.4 multi 3.99 import weight 0.00
Epoch 377 Iter 9 subLoss 3271.6 multi -13.93 import weight 0.00
Epoch 377 Iter 10 subLoss 3341.5 multi 3.99 import weight 0.00
Epoch 377 Iter 11 subLoss 3443.0 multi 48.76 import weight 1.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 377 Acc: 94.73 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 48.76 Pidx 344 train Loss: 5714.4 test Loss: 847.5
Epoch 378 Iter 0 subLoss 6283.0 multi 3.99 import weight 0.00
Epoch 378 Iter 1 subLoss 3089.6 multi 15.93 import weight 0.00
Epoch 378 Iter 2 subLoss 2427.5 multi -4.97 import weight 0.00
Epoch 378 Iter 3 subLoss 2920.0 multi 6.97 import weight 0.00
Epoch 378 Iter 4 subLoss 2425.3 multi -1.98 import weight 0.00
Epoch 378 Iter 5 subLoss 2166.0 multi 1.00 import weight 0.00
Epoch 378 Iter 6 subLoss 2887.1 multi 9.96 import weight 0.00
Epoch 378 Iter 7 subLoss 2106.6 multi -16.91 import weight 0.00
Epoch 378 Iter 8 subLoss 2667.4 multi 6.97 import weight 0.00
Epoch 378 Iter 9 subLoss 2316.3 multi 6.97 import weight 0.00
Epoch 378 Iter 10 subLoss 2207.5 multi 6.97 import weight 0.00
Epoch 378 Iter 11 subLoss 2621.0 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 378 Acc: 98.60 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 262 train Loss: 2526.7 test Loss: 247.4
Epoch 379 Iter 0 subLoss 2161.7 multi 3.99 import weight 0.00
Epoch 379 Iter 1 subLoss 2389.8 multi 3.99 import weight 0.00
Epoch 379 Iter 2 subLoss 2255.0 multi 12.94 import weight 0.00
Epoch 379 Iter 3 subLoss 2479.3 multi 36.82 import weight 0.00
Epoch 379 Iter 4 subLoss 2439.0 multi -4.97 import weight 0.00
Epoch 379 Iter 5 subLoss 2428.0 multi 1.00 import weight 0.00
Epoch 379 Iter 6 subLoss 2190.0 multi 9.96 import weight 0.00
Epoch 379 Iter 7 subLoss 2423.7 multi 3.99 import weight 0.00
Epoch 379 Iter 8 subLoss 1873.6 multi 3.98 import weight 0.00
Epoch 379 Iter 9 subLoss 2279.6 multi -1.99 import weight 0.00
Epoch 379 Iter 10 subLoss 2259.2 multi 15.93 import weight 0.00
Epoch 379 Iter 11 subLoss 2166.4 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 379 Acc: 98.62 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 216 train Loss: 2197.0 test Loss: 221.7
Epoch 380 Iter 0 subLoss 2507.5 multi -10.94 import weight 0.00
Epoch 380 Iter 1 subLoss 2067.8 multi 12.94 import weight 0.00
Epoch 380 Iter 2 subLoss 2278.1 multi 1.00 import weight 0.00
Epoch 380 Iter 3 subLoss 1979.0 multi 12.94 import weight 0.00
Epoch 380 Iter 4 subLoss 2153.4 multi 6.97 import weight 0.00
Epoch 380 Iter 5 subLoss 2268.0 multi -7.96 import weight 0.00
Epoch 380 Iter 6 subLoss 2317.7 multi 9.96 import weight 0.00
Epoch 380 Iter 7 subLoss 2020.8 multi -1.99 import weight 0.00
Epoch 380 Iter 8 subLoss 1900.6 multi -1.98 import weight 0.00
Epoch 380 Iter 9 subLoss 1899.0 multi 12.94 import weight 0.00
Epoch 380 Iter 10 subLoss 2188.2 multi 12.94 import weight 0.00
Epoch 380 Iter 11 subLoss 2199.4 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 380 Acc: 98.66 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 219 train Loss: 2290.6 test Loss: 217.0
Epoch 381 Iter 0 subLoss 2142.0 multi 6.97 import weight 0.00
Epoch 381 Iter 1 subLoss 2119.3 multi 1.00 import weight 0.00
Epoch 381 Iter 2 subLoss 2543.8 multi 18.91 import weight 0.00
Epoch 381 Iter 3 subLoss 2176.6 multi -25.87 import weight 0.00
Epoch 381 Iter 4 subLoss 2665.4 multi 9.96 import weight 0.00
Epoch 381 Iter 5 subLoss 2047.1 multi 6.97 import weight 0.00
Epoch 381 Iter 6 subLoss 1828.7 multi -7.96 import weight 0.00
Epoch 381 Iter 7 subLoss 2403.8 multi -31.84 import weight 0.00
Epoch 381 Iter 8 subLoss 3213.2 multi 12.94 import weight 0.00
Epoch 381 Iter 9 subLoss 2837.5 multi 12.94 import weight 0.00
Epoch 381 Iter 10 subLoss 2198.5 multi -4.97 import weight 0.00
Epoch 381 Iter 11 subLoss 2168.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 381 Acc: 98.38 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 216 train Loss: 2157.6 test Loss: 246.2
Epoch 382 Iter 0 subLoss 2323.2 multi -13.93 import weight 0.00
Epoch 382 Iter 1 subLoss 2247.0 multi -34.82 import weight 0.00
Epoch 382 Iter 2 subLoss 4207.6 multi -10.94 import weight 0.00
Epoch 382 Iter 3 subLoss 22007.6 multi 1.00 import weight 0.00
Epoch 382 Iter 4 subLoss 9224.7 multi 1.00 import weight 0.00
Epoch 382 Iter 5 subLoss 7836.7 multi -1.98 import weight 0.00
Epoch 382 Iter 6 subLoss 10973.2 multi 6.97 import weight 0.00
Epoch 382 Iter 7 subLoss 3481.1 multi -1.98 import weight 0.00
Epoch 382 Iter 8 subLoss 3835.6 multi -7.96 import weight 0.00
Epoch 382 Iter 9 subLoss 4881.9 multi -4.97 import weight 0.00
Epoch 382 Iter 10 subLoss 5863.4 multi -1.98 import weight 0.00
Epoch 382 Iter 11 subLoss 11963.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 382 Acc: 94.05 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1196 train Loss: 7178.3 test Loss: 1050.5
Epoch 383 Iter 0 subLoss 8165.7 multi -1.99 import weight 0.00
Epoch 383 Iter 1 subLoss 11099.8 multi 1.00 import weight 0.00
Epoch 383 Iter 2 subLoss 6601.1 multi -7.96 import weight 0.00
Epoch 383 Iter 3 subLoss 35903.5 multi 1.00 import weight 0.00
Epoch 383 Iter 4 subLoss 7871.5 multi 3.98 import weight 0.00
Epoch 383 Iter 5 subLoss 5979.7 multi 1.00 import weight 0.00
Epoch 383 Iter 6 subLoss 5873.3 multi -4.97 import weight 0.00
Epoch 383 Iter 7 subLoss 7351.4 multi 1.00 import weight 0.00
Epoch 383 Iter 8 subLoss 6545.1 multi -4.97 import weight 0.00
Epoch 383 Iter 9 subLoss 9353.0 multi 12.94 import weight 0.00
Epoch 383 Iter 10 subLoss 4359.1 multi -1.99 import weight 0.00
Epoch 383 Iter 11 subLoss 4919.7 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 383 Acc: 97.31 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 491 train Loss: 4786.9 test Loss: 555.9
Epoch 384 Iter 0 subLoss 4937.8 multi 6.97 import weight 0.00
Epoch 384 Iter 1 subLoss 3863.6 multi -13.93 import weight 0.00
Epoch 384 Iter 2 subLoss 4167.3 multi 12.94 import weight 0.00
Epoch 384 Iter 3 subLoss 2729.0 multi -13.93 import weight 0.00
Epoch 384 Iter 4 subLoss 3597.8 multi 27.87 import weight 0.00
Epoch 384 Iter 5 subLoss 2510.0 multi -7.96 import weight 0.00
Epoch 384 Iter 6 subLoss 2519.2 multi 12.94 import weight 0.00
Epoch 384 Iter 7 subLoss 2719.0 multi 6.97 import weight 0.00
Epoch 384 Iter 8 subLoss 2716.4 multi 9.96 import weight 0.00
Epoch 384 Iter 9 subLoss 2568.7 multi 18.91 import weight 0.00
Epoch 384 Iter 10 subLoss 2200.5 multi 3.98 import weight 0.00
Epoch 384 Iter 11 subLoss 2350.3 multi -19.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 384 Acc: 98.19 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -19.90 Pidx 235 train Loss: 2515.2 test Loss: 299.6
Epoch 385 Iter 0 subLoss 2254.4 multi 15.93 import weight 0.00
Epoch 385 Iter 1 subLoss 2338.4 multi 1.00 import weight 0.00
Epoch 385 Iter 2 subLoss 2702.6 multi 18.91 import weight 0.00
Epoch 385 Iter 3 subLoss 2232.3 multi 15.93 import weight 0.00
Epoch 385 Iter 4 subLoss 1984.9 multi -16.91 import weight 0.00
Epoch 385 Iter 5 subLoss 2181.1 multi 12.94 import weight 0.00
Epoch 385 Iter 6 subLoss 2066.1 multi 15.93 import weight 0.00
Epoch 385 Iter 7 subLoss 2132.5 multi 1.00 import weight 0.00
Epoch 385 Iter 8 subLoss 2268.5 multi -7.96 import weight 0.00
Epoch 385 Iter 9 subLoss 2303.9 multi 9.96 import weight 0.00
Epoch 385 Iter 10 subLoss 1832.9 multi 1.00 import weight 0.00
Epoch 385 Iter 11 subLoss 1512.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 385 Acc: 98.60 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 151 train Loss: 2168.9 test Loss: 237.4
Epoch 386 Iter 0 subLoss 2407.3 multi -28.85 import weight 0.00
Epoch 386 Iter 1 subLoss 2246.4 multi -34.82 import weight 0.00
Epoch 386 Iter 2 subLoss 4292.8 multi -7.96 import weight 0.00
Epoch 386 Iter 3 subLoss 15709.7 multi -1.99 import weight 0.00
Epoch 386 Iter 4 subLoss 45911.5 multi 1.00 import weight 0.00
Epoch 386 Iter 5 subLoss 18578.0 multi 1.00 import weight 0.00
Epoch 386 Iter 6 subLoss 13233.5 multi 1.00 import weight 0.00
Epoch 386 Iter 7 subLoss 8569.5 multi 3.98 import weight 0.00
Epoch 386 Iter 8 subLoss 3122.1 multi 1.00 import weight 0.00
Epoch 386 Iter 9 subLoss 3269.9 multi -7.96 import weight 0.00
Epoch 386 Iter 10 subLoss 4357.1 multi 1.00 import weight 0.00
Epoch 386 Iter 11 subLoss 4019.9 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 386 Acc: 94.55 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 401 train Loss: 4884.5 test Loss: 847.5
Epoch 387 Iter 0 subLoss 5307.5 multi 3.98 import weight 0.00
Epoch 387 Iter 1 subLoss 3486.7 multi 1.00 import weight 0.00
Epoch 387 Iter 2 subLoss 3198.5 multi 6.97 import weight 0.00
Epoch 387 Iter 3 subLoss 2350.9 multi -16.91 import weight 0.00
Epoch 387 Iter 4 subLoss 3230.5 multi 12.94 import weight 0.00
Epoch 387 Iter 5 subLoss 2301.7 multi 12.94 import weight 0.00
Epoch 387 Iter 6 subLoss 2559.6 multi -37.81 import weight 0.00
Epoch 387 Iter 7 subLoss 3308.0 multi 24.88 import weight 0.00
Epoch 387 Iter 8 subLoss 2367.2 multi 12.94 import weight 0.00
Epoch 387 Iter 9 subLoss 2618.7 multi 6.97 import weight 0.00
Epoch 387 Iter 10 subLoss 2350.7 multi -13.93 import weight 0.00
Epoch 387 Iter 11 subLoss 2442.1 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 387 Acc: 98.23 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 244 train Loss: 2504.0 test Loss: 299.0
Epoch 388 Iter 0 subLoss 2091.5 multi 12.94 import weight 0.00
Epoch 388 Iter 1 subLoss 2208.7 multi 6.97 import weight 0.00
Epoch 388 Iter 2 subLoss 1834.1 multi 3.99 import weight 0.00
Epoch 388 Iter 3 subLoss 2135.7 multi 3.99 import weight 0.00
Epoch 388 Iter 4 subLoss 2577.7 multi -10.94 import weight 0.00
Epoch 388 Iter 5 subLoss 2664.3 multi 12.94 import weight 0.00
Epoch 388 Iter 6 subLoss 1984.4 multi -13.93 import weight 0.00
Epoch 388 Iter 7 subLoss 2523.3 multi -10.94 import weight 0.00
Epoch 388 Iter 8 subLoss 3647.9 multi -1.99 import weight 0.00
Epoch 388 Iter 9 subLoss 3246.8 multi -4.97 import weight 0.00
Epoch 388 Iter 10 subLoss 5456.8 multi 15.93 import weight 0.00
Epoch 388 Iter 11 subLoss 3021.1 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 388 Acc: 97.72 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 302 train Loss: 3614.2 test Loss: 400.2
Epoch 389 Iter 0 subLoss 3991.1 multi -10.94 import weight 0.00
Epoch 389 Iter 1 subLoss 9027.2 multi 3.98 import weight 0.00
Epoch 389 Iter 2 subLoss 2641.3 multi -7.96 import weight 0.00
Epoch 389 Iter 3 subLoss 4167.5 multi 15.93 import weight 0.00
Epoch 389 Iter 4 subLoss 2459.1 multi 6.97 import weight 0.00
Epoch 389 Iter 5 subLoss 2158.0 multi 6.97 import weight 0.00
Epoch 389 Iter 6 subLoss 2303.6 multi 15.93 import weight 0.00
Epoch 389 Iter 7 subLoss 1780.0 multi -7.96 import weight 0.00
Epoch 389 Iter 8 subLoss 2471.8 multi 39.81 import weight 0.00
Epoch 389 Iter 9 subLoss 2337.0 multi 3.99 import weight 0.00
Epoch 389 Iter 10 subLoss 2124.1 multi 1.00 import weight 0.00
Epoch 389 Iter 11 subLoss 2652.9 multi 24.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 389 Acc: 98.38 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 24.88 Pidx 265 train Loss: 2151.6 test Loss: 255.2
Epoch 390 Iter 0 subLoss 1880.1 multi -4.97 import weight 0.00
Epoch 390 Iter 1 subLoss 2002.7 multi 12.94 import weight 0.00
Epoch 390 Iter 2 subLoss 2314.3 multi 3.99 import weight 0.00
Epoch 390 Iter 3 subLoss 2195.6 multi -4.97 import weight 0.00
Epoch 390 Iter 4 subLoss 2341.5 multi 3.98 import weight 0.00
Epoch 390 Iter 5 subLoss 2220.8 multi 9.96 import weight 0.00
Epoch 390 Iter 6 subLoss 1848.2 multi -7.96 import weight 0.00
Epoch 390 Iter 7 subLoss 1857.7 multi 1.00 import weight 0.00
Epoch 390 Iter 8 subLoss 2411.1 multi -1.99 import weight 0.00
Epoch 390 Iter 9 subLoss 2047.6 multi 9.96 import weight 0.00
Epoch 390 Iter 10 subLoss 2160.9 multi 6.97 import weight 0.00
Epoch 390 Iter 11 subLoss 2012.3 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 390 Acc: 98.42 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 201 train Loss: 2158.5 test Loss: 257.7
Epoch 391 Iter 0 subLoss 1834.4 multi 6.97 import weight 0.00
Epoch 391 Iter 1 subLoss 1876.6 multi 6.97 import weight 0.00
Epoch 391 Iter 2 subLoss 1864.8 multi -4.97 import weight 0.00
Epoch 391 Iter 3 subLoss 1911.2 multi 6.97 import weight 0.00
Epoch 391 Iter 4 subLoss 2107.4 multi -16.91 import weight 0.00
Epoch 391 Iter 5 subLoss 2519.1 multi 15.93 import weight 0.00
Epoch 391 Iter 6 subLoss 2416.0 multi 1.00 import weight 0.00
Epoch 391 Iter 7 subLoss 2071.0 multi -10.94 import weight 0.00
Epoch 391 Iter 8 subLoss 2084.5 multi -1.99 import weight 0.00
Epoch 391 Iter 9 subLoss 2083.9 multi 1.00 import weight 0.00
Epoch 391 Iter 10 subLoss 2103.9 multi -13.93 import weight 0.00
Epoch 391 Iter 11 subLoss 2721.5 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 391 Acc: 97.70 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 272 train Loss: 3343.4 test Loss: 384.0
Epoch 392 Iter 0 subLoss 3124.5 multi 3.99 import weight 0.00
Epoch 392 Iter 1 subLoss 3259.7 multi -1.99 import weight 0.00
Epoch 392 Iter 2 subLoss 2720.0 multi -13.93 import weight 0.00
Epoch 392 Iter 3 subLoss 5137.0 multi -1.99 import weight 0.00
Epoch 392 Iter 4 subLoss 5238.3 multi 6.97 import weight 0.00
Epoch 392 Iter 5 subLoss 2649.4 multi -4.97 import weight 0.00
Epoch 392 Iter 6 subLoss 3486.9 multi 3.99 import weight 0.00
Epoch 392 Iter 7 subLoss 3384.5 multi -13.93 import weight 0.00
Epoch 392 Iter 8 subLoss 6392.0 multi -7.96 import weight 0.00
Epoch 392 Iter 9 subLoss 15574.7 multi 1.00 import weight 0.00
Epoch 392 Iter 10 subLoss 10214.5 multi -4.97 import weight 0.00
Epoch 392 Iter 11 subLoss 24020.1 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 392 Acc: 88.89 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 2402 train Loss: 9851.7 test Loss: 1655.1
Epoch 393 Iter 0 subLoss 10015.2 multi 3.98 import weight 0.00
Epoch 393 Iter 1 subLoss 3811.5 multi 6.97 import weight 0.00
Epoch 393 Iter 2 subLoss 3153.3 multi -4.97 import weight 0.00
Epoch 393 Iter 3 subLoss 3629.7 multi 6.97 import weight 0.00
Epoch 393 Iter 4 subLoss 3630.4 multi -4.97 import weight 0.00
Epoch 393 Iter 5 subLoss 3407.2 multi 3.98 import weight 0.00
Epoch 393 Iter 6 subLoss 3616.2 multi -13.93 import weight 0.00
Epoch 393 Iter 7 subLoss 4286.8 multi 1.00 import weight 0.00
Epoch 393 Iter 8 subLoss 4547.5 multi 21.90 import weight 0.00
Epoch 393 Iter 9 subLoss 2989.1 multi -1.99 import weight 0.00
Epoch 393 Iter 10 subLoss 2788.8 multi 9.96 import weight 0.00
Epoch 393 Iter 11 subLoss 2537.9 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 393 Acc: 98.35 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 253 train Loss: 2767.4 test Loss: 301.9
Epoch 394 Iter 0 subLoss 2781.9 multi 12.94 import weight 0.00
Epoch 394 Iter 1 subLoss 2586.8 multi 12.94 import weight 0.00
Epoch 394 Iter 2 subLoss 2378.3 multi -7.96 import weight 0.00
Epoch 394 Iter 3 subLoss 2640.4 multi -1.98 import weight 0.00
Epoch 394 Iter 4 subLoss 2394.7 multi 24.88 import weight 0.00
Epoch 394 Iter 5 subLoss 2289.4 multi -10.94 import weight 0.00
Epoch 394 Iter 6 subLoss 2391.1 multi 27.87 import weight 0.00
Epoch 394 Iter 7 subLoss 2118.4 multi -1.99 import weight 0.00
Epoch 394 Iter 8 subLoss 2088.5 multi 3.99 import weight 0.00
Epoch 394 Iter 9 subLoss 2107.9 multi -10.94 import weight 0.00
Epoch 394 Iter 10 subLoss 1982.5 multi -10.94 import weight 0.00
Epoch 394 Iter 11 subLoss 2138.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 394 Acc: 98.31 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 213 train Loss: 2436.7 test Loss: 282.0
Epoch 395 Iter 0 subLoss 2430.9 multi -7.96 import weight 0.00
Epoch 395 Iter 1 subLoss 3125.9 multi 6.97 import weight 0.00
Epoch 395 Iter 2 subLoss 2238.6 multi 15.93 import weight 0.00
Epoch 395 Iter 3 subLoss 2227.8 multi 12.94 import weight 0.00
Epoch 395 Iter 4 subLoss 2294.7 multi 9.96 import weight 0.00
Epoch 395 Iter 5 subLoss 2543.5 multi 18.91 import weight 0.00
Epoch 395 Iter 6 subLoss 2014.8 multi -7.96 import weight 0.00
Epoch 395 Iter 7 subLoss 2053.1 multi -13.93 import weight 0.00
Epoch 395 Iter 8 subLoss 2219.3 multi -4.97 import weight 0.00
Epoch 395 Iter 9 subLoss 2120.8 multi 1.00 import weight 0.00
Epoch 395 Iter 10 subLoss 2312.1 multi 6.97 import weight 0.00
Epoch 395 Iter 11 subLoss 2368.3 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 395 Acc: 98.54 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 236 train Loss: 2192.5 test Loss: 248.2
Epoch 396 Iter 0 subLoss 1810.1 multi 6.97 import weight 0.00
Epoch 396 Iter 1 subLoss 1901.6 multi -1.99 import weight 0.00
Epoch 396 Iter 2 subLoss 2144.9 multi 1.00 import weight 0.00
Epoch 396 Iter 3 subLoss 2243.3 multi -34.82 import weight 0.00
Epoch 396 Iter 4 subLoss 2282.1 multi -7.96 import weight 0.00
Epoch 396 Iter 5 subLoss 2408.8 multi -31.84 import weight 0.00
Epoch 396 Iter 6 subLoss 5190.7 multi -4.97 import weight 0.00
Epoch 396 Iter 7 subLoss 10311.8 multi 1.00 import weight 0.00
Epoch 396 Iter 8 subLoss 7311.3 multi 1.00 import weight 0.00
Epoch 396 Iter 9 subLoss 5196.2 multi -1.98 import weight 0.00
Epoch 396 Iter 10 subLoss 8803.1 multi 6.97 import weight 0.00
Epoch 396 Iter 11 subLoss 2707.1 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 396 Acc: 98.42 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 270 train Loss: 2516.5 test Loss: 243.6
Epoch 397 Iter 0 subLoss 2379.2 multi -7.96 import weight 0.00
Epoch 397 Iter 1 subLoss 2564.9 multi 18.91 import weight 0.00
Epoch 397 Iter 2 subLoss 2323.4 multi -16.91 import weight 0.00
Epoch 397 Iter 3 subLoss 1976.4 multi 15.93 import weight 0.00
Epoch 397 Iter 4 subLoss 2422.8 multi 1.00 import weight 0.00
Epoch 397 Iter 5 subLoss 2048.5 multi 12.94 import weight 0.00
Epoch 397 Iter 6 subLoss 1915.8 multi 6.97 import weight 0.00
Epoch 397 Iter 7 subLoss 2212.6 multi -1.99 import weight 0.00
Epoch 397 Iter 8 subLoss 2014.4 multi -4.97 import weight 0.00
Epoch 397 Iter 9 subLoss 2075.6 multi -7.96 import weight 0.00
Epoch 397 Iter 10 subLoss 2468.0 multi -22.88 import weight 0.00
Epoch 397 Iter 11 subLoss 2134.1 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 397 Acc: 98.46 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 213 train Loss: 2539.2 test Loss: 277.9
Epoch 398 Iter 0 subLoss 2454.7 multi 9.96 import weight 0.00
Epoch 398 Iter 1 subLoss 2323.5 multi -13.93 import weight 0.00
Epoch 398 Iter 2 subLoss 2313.5 multi 9.96 import weight 0.00
Epoch 398 Iter 3 subLoss 2636.9 multi 12.94 import weight 0.00
Epoch 398 Iter 4 subLoss 1710.0 multi 3.98 import weight 0.00
Epoch 398 Iter 5 subLoss 1759.5 multi 3.98 import weight 0.00
Epoch 398 Iter 6 subLoss 2134.6 multi 6.97 import weight 0.00
Epoch 398 Iter 7 subLoss 1700.7 multi 6.97 import weight 0.00
Epoch 398 Iter 8 subLoss 2189.1 multi 15.93 import weight 0.00
Epoch 398 Iter 9 subLoss 2090.7 multi 6.97 import weight 0.00
Epoch 398 Iter 10 subLoss 2286.9 multi -4.97 import weight 0.00
Epoch 398 Iter 11 subLoss 1914.8 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 398 Acc: 98.72 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 191 train Loss: 2196.0 test Loss: 231.6
Epoch 399 Iter 0 subLoss 2081.0 multi 3.98 import weight 0.00
Epoch 399 Iter 1 subLoss 1909.7 multi 1.00 import weight 0.00
Epoch 399 Iter 2 subLoss 1761.9 multi 3.99 import weight 0.00
Epoch 399 Iter 3 subLoss 1916.0 multi 9.96 import weight 0.00
Epoch 399 Iter 4 subLoss 1952.4 multi -10.94 import weight 0.00
Epoch 399 Iter 5 subLoss 2247.3 multi -31.84 import weight 0.00
Epoch 399 Iter 6 subLoss 2692.6 multi -19.90 import weight 0.00
Epoch 399 Iter 7 subLoss 5623.2 multi 6.97 import weight 0.00
Epoch 399 Iter 8 subLoss 2866.1 multi -4.97 import weight 0.00
Epoch 399 Iter 9 subLoss 2892.1 multi 12.94 import weight 0.00
Epoch 399 Iter 10 subLoss 2297.8 multi 6.97 import weight 0.00
Epoch 399 Iter 11 subLoss 2601.4 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 399 Acc: 98.13 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 260 train Loss: 2598.4 test Loss: 323.5
Epoch 400 Iter 0 subLoss 2559.4 multi -37.81 import weight 0.00
Epoch 400 Iter 1 subLoss 4866.4 multi -13.93 import weight 0.00
Epoch 400 Iter 2 subLoss 11597.1 multi -1.98 import weight 0.00
Epoch 400 Iter 3 subLoss 21590.8 multi 1.00 import weight 0.00
Epoch 400 Iter 4 subLoss 12101.0 multi -1.98 import weight 0.00
Epoch 400 Iter 5 subLoss 17226.1 multi 3.99 import weight 0.00
Epoch 400 Iter 6 subLoss 6130.5 multi 3.99 import weight 0.00
Epoch 400 Iter 7 subLoss 5606.4 multi 6.97 import weight 0.00
Epoch 400 Iter 8 subLoss 3762.6 multi -1.99 import weight 0.00
Epoch 400 Iter 9 subLoss 3252.9 multi 1.00 import weight 0.00
Epoch 400 Iter 10 subLoss 3879.3 multi 12.94 import weight 0.00
Epoch 400 Iter 11 subLoss 2770.8 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 400 Acc: 97.98 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 277 train Loss: 2792.1 test Loss: 363.6
Epoch 401 Iter 0 subLoss 2663.5 multi 12.94 import weight 0.00
Epoch 401 Iter 1 subLoss 2368.2 multi 15.93 import weight 0.00
Epoch 401 Iter 2 subLoss 2333.3 multi 1.00 import weight 0.00
Epoch 401 Iter 3 subLoss 2524.1 multi -10.94 import weight 0.00
Epoch 401 Iter 4 subLoss 2603.6 multi -4.97 import weight 0.00
Epoch 401 Iter 5 subLoss 2814.4 multi 1.00 import weight 0.00
Epoch 401 Iter 6 subLoss 2354.1 multi -13.93 import weight 0.00
Epoch 401 Iter 7 subLoss 2975.9 multi 21.90 import weight 0.00
Epoch 401 Iter 8 subLoss 2286.0 multi -1.98 import weight 0.00
Epoch 401 Iter 9 subLoss 2206.8 multi 6.97 import weight 0.00
Epoch 401 Iter 10 subLoss 2646.1 multi -1.99 import weight 0.00
Epoch 401 Iter 11 subLoss 2028.4 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 401 Acc: 98.38 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 202 train Loss: 2611.3 test Loss: 286.9
Epoch 402 Iter 0 subLoss 2487.4 multi -25.87 import weight 0.00
Epoch 402 Iter 1 subLoss 4533.5 multi -16.91 import weight 0.00
Epoch 402 Iter 2 subLoss 64845.1 multi 1.00 import weight 0.00
Epoch 402 Iter 3 subLoss 24459.8 multi 1.00 import weight 0.00
Epoch 402 Iter 4 subLoss 15617.1 multi 1.00 import weight 0.00
Epoch 402 Iter 5 subLoss 11882.7 multi -1.99 import weight 0.00
Epoch 402 Iter 6 subLoss 17199.0 multi 1.00 import weight 0.00
Epoch 402 Iter 7 subLoss 15494.7 multi 3.98 import weight 0.00
Epoch 402 Iter 8 subLoss 4665.9 multi 3.99 import weight 0.00
Epoch 402 Iter 9 subLoss 3653.3 multi -16.91 import weight 0.00
Epoch 402 Iter 10 subLoss 6663.3 multi 3.99 import weight 0.00
Epoch 402 Iter 11 subLoss 4604.6 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 402 Acc: 93.07 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 460 train Loss: 8117.8 test Loss: 1135.7
Epoch 403 Iter 0 subLoss 7853.1 multi 6.97 import weight 0.00
Epoch 403 Iter 1 subLoss 4217.2 multi 12.94 import weight 0.00
Epoch 403 Iter 2 subLoss 2951.3 multi 3.98 import weight 0.00
Epoch 403 Iter 3 subLoss 2684.0 multi 12.94 import weight 0.00
Epoch 403 Iter 4 subLoss 2617.9 multi 3.99 import weight 0.00
Epoch 403 Iter 5 subLoss 2382.1 multi 1.00 import weight 0.00
Epoch 403 Iter 6 subLoss 3017.4 multi -19.90 import weight 0.00
Epoch 403 Iter 7 subLoss 2638.3 multi 15.93 import weight 0.00
Epoch 403 Iter 8 subLoss 2371.0 multi -7.96 import weight 0.00
Epoch 403 Iter 9 subLoss 2775.4 multi 6.97 import weight 0.00
Epoch 403 Iter 10 subLoss 3125.1 multi 9.96 import weight 0.00
Epoch 403 Iter 11 subLoss 2484.8 multi -22.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 403 Acc: 97.88 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -22.88 Pidx 248 train Loss: 2995.0 test Loss: 380.8
Epoch 404 Iter 0 subLoss 3180.0 multi 18.91 import weight 0.00
Epoch 404 Iter 1 subLoss 2429.3 multi 3.98 import weight 0.00
Epoch 404 Iter 2 subLoss 2297.9 multi 6.97 import weight 0.00
Epoch 404 Iter 3 subLoss 2218.1 multi -1.98 import weight 0.00
Epoch 404 Iter 4 subLoss 2678.7 multi -31.84 import weight 0.00
Epoch 404 Iter 5 subLoss 2917.6 multi 9.96 import weight 0.00
Epoch 404 Iter 6 subLoss 2541.5 multi 21.90 import weight 0.00
Epoch 404 Iter 7 subLoss 2497.8 multi 6.97 import weight 0.00
Epoch 404 Iter 8 subLoss 2120.3 multi 3.99 import weight 0.00
Epoch 404 Iter 9 subLoss 2101.5 multi -10.94 import weight 0.00
Epoch 404 Iter 10 subLoss 2673.8 multi -28.85 import weight 0.00
Epoch 404 Iter 11 subLoss 2749.5 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 404 Acc: 98.03 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 274 train Loss: 2670.2 test Loss: 332.3
Epoch 405 Iter 0 subLoss 2261.1 multi -4.97 import weight 0.00
Epoch 405 Iter 1 subLoss 2718.1 multi 6.97 import weight 0.00
Epoch 405 Iter 2 subLoss 2859.7 multi 27.87 import weight 0.00
Epoch 405 Iter 3 subLoss 2771.3 multi 9.96 import weight 0.00
Epoch 405 Iter 4 subLoss 2692.4 multi -19.90 import weight 0.00
Epoch 405 Iter 5 subLoss 2633.1 multi 18.91 import weight 0.00
Epoch 405 Iter 6 subLoss 1974.3 multi 18.91 import weight 0.00
Epoch 405 Iter 7 subLoss 2226.2 multi 6.97 import weight 0.00
Epoch 405 Iter 8 subLoss 2126.5 multi 6.97 import weight 0.00
Epoch 405 Iter 9 subLoss 2063.5 multi 15.93 import weight 0.00
Epoch 405 Iter 10 subLoss 2161.5 multi 9.96 import weight 0.00
Epoch 405 Iter 11 subLoss 2333.7 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 405 Acc: 98.58 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 233 train Loss: 2174.5 test Loss: 238.5
Epoch 406 Iter 0 subLoss 2186.3 multi 18.91 import weight 0.00
Epoch 406 Iter 1 subLoss 2173.3 multi -31.84 import weight 0.00
Epoch 406 Iter 2 subLoss 2459.6 multi 12.94 import weight 0.00
Epoch 406 Iter 3 subLoss 2324.7 multi -13.93 import weight 0.00
Epoch 406 Iter 4 subLoss 2110.1 multi -4.97 import weight 0.00
Epoch 406 Iter 5 subLoss 2591.2 multi -7.96 import weight 0.00
Epoch 406 Iter 6 subLoss 2959.6 multi 6.97 import weight 0.00
Epoch 406 Iter 7 subLoss 2413.5 multi 1.00 import weight 0.00
Epoch 406 Iter 8 subLoss 2850.0 multi -19.90 import weight 0.00
Epoch 406 Iter 9 subLoss 3043.9 multi -1.99 import weight 0.00
Epoch 406 Iter 10 subLoss 3542.7 multi 18.91 import weight 0.00
Epoch 406 Iter 11 subLoss 2326.2 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 406 Acc: 97.80 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 232 train Loss: 2706.3 test Loss: 364.9
Epoch 407 Iter 0 subLoss 2773.4 multi 12.94 import weight 0.00
Epoch 407 Iter 1 subLoss 2579.1 multi -10.94 import weight 0.00
Epoch 407 Iter 2 subLoss 2258.4 multi 9.96 import weight 0.00
Epoch 407 Iter 3 subLoss 2279.6 multi -4.97 import weight 0.00
Epoch 407 Iter 4 subLoss 2173.7 multi -28.85 import weight 0.00
Epoch 407 Iter 5 subLoss 3727.1 multi -13.93 import weight 0.00
Epoch 407 Iter 6 subLoss 8059.2 multi 6.97 import weight 0.00
Epoch 407 Iter 7 subLoss 2515.4 multi 18.91 import weight 0.00
Epoch 407 Iter 8 subLoss 2999.7 multi 1.00 import weight 0.00
Epoch 407 Iter 9 subLoss 2399.8 multi 27.87 import weight 1.00
Epoch 407 Iter 10 subLoss 2707.7 multi 18.91 import weight 0.00
Epoch 407 Iter 11 subLoss 2632.8 multi 21.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 407 Acc: 98.48 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 21.90 Pidx 263 train Loss: 2471.2 test Loss: 268.1
Epoch 408 Iter 0 subLoss 2445.3 multi 3.99 import weight 0.00
Epoch 408 Iter 1 subLoss 2295.8 multi 9.96 import weight 0.00
Epoch 408 Iter 2 subLoss 2424.2 multi 3.99 import weight 0.00
Epoch 408 Iter 3 subLoss 1862.9 multi -1.99 import weight 0.00
Epoch 408 Iter 4 subLoss 1642.7 multi -4.97 import weight 0.00
Epoch 408 Iter 5 subLoss 2161.6 multi 12.94 import weight 0.00
Epoch 408 Iter 6 subLoss 2281.9 multi -1.99 import weight 0.00
Epoch 408 Iter 7 subLoss 2122.0 multi 6.97 import weight 0.00
Epoch 408 Iter 8 subLoss 1889.8 multi -4.97 import weight 0.00
Epoch 408 Iter 9 subLoss 2322.7 multi -7.96 import weight 0.00
Epoch 408 Iter 10 subLoss 2775.2 multi 15.93 import weight 0.00
Epoch 408 Iter 11 subLoss 2480.1 multi -19.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0294 / 0.11353 / 31.56
Entropy seen (from low to high)
[1309, 452, 280, 228, 188, 134, 120, 93, 80, 82, 59, 61, 41, 44, 43, 45, 50, 76, 82, 310, 459, 179, 118, 91, 71, 75, 55, 60, 45, 41, 43, 44, 30, 15, 8, 11, 6, 4, 1, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 2, 1, 4, 12, 23, 25, 45, 62, 99, 125, 151, 184, 254, 209, 215, 219, 246, 257, 270, 287, 271, 295, 222, 73, 60, 55, 51, 57, 57, 61, 56, 65, 49, 68, 74, 82, 86, 70, 88, 110, 114, 78, 28]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 34.2, 37.0, 40.3, 43.7, 47.7, 50.6, 54.0, 57.7, 61.1, 64.8, 68.1]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.9, 33.3, 31.5, 23.5, 35.1, 82.3, 95.7, 94.3, 95.9, 93.3, 92.8]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 9, 19, 34, 74, 664, 236, 160, 124, 106, 112]
Epoch 408 Acc: 98.15 BMA: 96.89 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -19.90 Pidx 248 train Loss: 3021.5 test Loss: 336.6
Epoch 409 Iter 0 subLoss 2629.5 multi -19.90 import weight 0.00
Epoch 409 Iter 1 subLoss 7351.5 multi 3.98 import weight 0.00
Epoch 409 Iter 2 subLoss 3016.5 multi -16.91 import weight 0.00
Epoch 409 Iter 3 subLoss 6879.9 multi 6.97 import weight 0.00
Epoch 409 Iter 4 subLoss 2558.1 multi -37.81 import weight 0.00
Epoch 409 Iter 5 subLoss 5043.3 multi 12.94 import weight 0.00
Epoch 409 Iter 6 subLoss 2892.8 multi 15.93 import weight 0.00
Epoch 409 Iter 7 subLoss 2547.9 multi 24.88 import weight 0.00
Epoch 409 Iter 8 subLoss 2329.6 multi -4.97 import weight 0.00
Epoch 409 Iter 9 subLoss 2414.8 multi 3.98 import weight 0.00
Epoch 409 Iter 10 subLoss 2004.6 multi 15.93 import weight 0.00
Epoch 409 Iter 11 subLoss 2395.0 multi 30.85 import weight 1.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 409 Acc: 98.60 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 1.000 Grad mul 30.85 Pidx 239 train Loss: 2372.0 test Loss: 254.4
Epoch 410 Iter 0 subLoss 2055.9 multi -13.93 import weight 0.00
Epoch 410 Iter 1 subLoss 2911.5 multi 12.94 import weight 0.00
Epoch 410 Iter 2 subLoss 2456.8 multi 12.94 import weight 0.00
Epoch 410 Iter 3 subLoss 2286.1 multi 1.00 import weight 0.00
Epoch 410 Iter 4 subLoss 2632.3 multi 21.90 import weight 0.00
Epoch 410 Iter 5 subLoss 2232.5 multi 12.94 import weight 0.00
Epoch 410 Iter 6 subLoss 2011.0 multi -4.97 import weight 0.00
Epoch 410 Iter 7 subLoss 2146.8 multi -1.99 import weight 0.00
Epoch 410 Iter 8 subLoss 1724.6 multi 3.98 import weight 0.00
Epoch 410 Iter 9 subLoss 2037.5 multi -7.96 import weight 0.00
Epoch 410 Iter 10 subLoss 2169.8 multi 15.93 import weight 0.00
Epoch 410 Iter 11 subLoss 2045.6 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 410 Acc: 98.74 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 204 train Loss: 2074.8 test Loss: 218.0
Epoch 411 Iter 0 subLoss 2226.2 multi 9.96 import weight 0.00
Epoch 411 Iter 1 subLoss 2139.8 multi 1.00 import weight 0.00
Epoch 411 Iter 2 subLoss 2232.6 multi 12.94 import weight 0.00
Epoch 411 Iter 3 subLoss 1975.0 multi 21.90 import weight 0.00
Epoch 411 Iter 4 subLoss 1878.3 multi 3.99 import weight 0.00
Epoch 411 Iter 5 subLoss 1761.3 multi 6.97 import weight 0.00
Epoch 411 Iter 6 subLoss 2174.2 multi -31.84 import weight 0.00
Epoch 411 Iter 7 subLoss 2486.0 multi -16.91 import weight 0.00
Epoch 411 Iter 8 subLoss 4864.1 multi -10.94 import weight 0.00
Epoch 411 Iter 9 subLoss 116982.7 multi 1.00 import weight 0.00
Epoch 411 Iter 10 subLoss 4086.8 multi -1.99 import weight 0.00
Epoch 411 Iter 11 subLoss 4693.3 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 411 Acc: 98.52 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 469 train Loss: 2365.0 test Loss: 251.0
Epoch 412 Iter 0 subLoss 2170.6 multi -28.85 import weight 0.00
Epoch 412 Iter 1 subLoss 2872.6 multi -16.91 import weight 0.00
Epoch 412 Iter 2 subLoss 5438.0 multi 12.94 import weight 0.00
Epoch 412 Iter 3 subLoss 2251.0 multi 12.94 import weight 0.00
Epoch 412 Iter 4 subLoss 1981.6 multi -16.91 import weight 0.00
Epoch 412 Iter 5 subLoss 2197.6 multi -7.96 import weight 0.00
Epoch 412 Iter 6 subLoss 2949.1 multi -4.97 import weight 0.00
Epoch 412 Iter 7 subLoss 3236.1 multi 15.93 import weight 0.00
Epoch 412 Iter 8 subLoss 2293.4 multi 6.97 import weight 0.00
Epoch 412 Iter 9 subLoss 2353.8 multi -10.94 import weight 0.00
Epoch 412 Iter 10 subLoss 2361.1 multi 12.94 import weight 0.00
Epoch 412 Iter 11 subLoss 2341.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 412 Acc: 98.25 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 234 train Loss: 2317.6 test Loss: 271.0
Epoch 413 Iter 0 subLoss 2195.3 multi -4.97 import weight 0.00
Epoch 413 Iter 1 subLoss 2187.7 multi 9.96 import weight 0.00
Epoch 413 Iter 2 subLoss 2359.0 multi -10.94 import weight 0.00
Epoch 413 Iter 3 subLoss 2372.1 multi -7.96 import weight 0.00
Epoch 413 Iter 4 subLoss 2458.1 multi 15.93 import weight 0.00
Epoch 413 Iter 5 subLoss 2131.2 multi 3.98 import weight 0.00
Epoch 413 Iter 6 subLoss 2368.7 multi 12.94 import weight 0.00
Epoch 413 Iter 7 subLoss 1784.7 multi 6.97 import weight 0.00
Epoch 413 Iter 8 subLoss 1992.7 multi -1.99 import weight 0.00
Epoch 413 Iter 9 subLoss 2126.4 multi 9.96 import weight 0.00
Epoch 413 Iter 10 subLoss 1939.3 multi -19.90 import weight 0.00
Epoch 413 Iter 11 subLoss 2120.8 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 413 Acc: 98.60 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 212 train Loss: 2057.4 test Loss: 245.8
Epoch 414 Iter 0 subLoss 2054.5 multi -13.93 import weight 0.00
Epoch 414 Iter 1 subLoss 1819.3 multi 9.96 import weight 0.00
Epoch 414 Iter 2 subLoss 2358.9 multi -7.96 import weight 0.00
Epoch 414 Iter 3 subLoss 2225.0 multi 12.94 import weight 0.00
Epoch 414 Iter 4 subLoss 1848.9 multi -7.96 import weight 0.00
Epoch 414 Iter 5 subLoss 1985.8 multi -13.93 import weight 0.00
Epoch 414 Iter 6 subLoss 2527.3 multi -10.94 import weight 0.00
Epoch 414 Iter 7 subLoss 2414.9 multi 6.97 import weight 0.00
Epoch 414 Iter 8 subLoss 2465.1 multi -31.84 import weight 0.00
Epoch 414 Iter 9 subLoss 3356.0 multi 1.00 import weight 0.00
Epoch 414 Iter 10 subLoss 3368.4 multi 6.97 import weight 0.00
Epoch 414 Iter 11 subLoss 2643.6 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 414 Acc: 97.28 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 264 train Loss: 3118.0 test Loss: 435.3
Epoch 415 Iter 0 subLoss 3027.5 multi -10.94 import weight 0.00
Epoch 415 Iter 1 subLoss 3543.3 multi 21.90 import weight 0.00
Epoch 415 Iter 2 subLoss 3303.6 multi 27.87 import weight 0.00
Epoch 415 Iter 3 subLoss 2361.6 multi 12.94 import weight 0.00
Epoch 415 Iter 4 subLoss 2480.2 multi -13.93 import weight 0.00
Epoch 415 Iter 5 subLoss 2701.2 multi 21.90 import weight 0.00
Epoch 415 Iter 6 subLoss 2253.2 multi 15.93 import weight 0.00
Epoch 415 Iter 7 subLoss 1976.7 multi 24.88 import weight 0.00
Epoch 415 Iter 8 subLoss 2340.4 multi 3.99 import weight 0.00
Epoch 415 Iter 9 subLoss 1802.1 multi 3.98 import weight 0.00
Epoch 415 Iter 10 subLoss 2153.7 multi 3.98 import weight 0.00
Epoch 415 Iter 11 subLoss 1672.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 415 Acc: 98.79 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 167 train Loss: 2066.7 test Loss: 213.0
Epoch 416 Iter 0 subLoss 2347.4 multi 6.97 import weight 0.00
Epoch 416 Iter 1 subLoss 1799.0 multi -10.94 import weight 0.00
Epoch 416 Iter 2 subLoss 2180.7 multi 12.94 import weight 0.00
Epoch 416 Iter 3 subLoss 1845.0 multi -4.97 import weight 0.00
Epoch 416 Iter 4 subLoss 1858.5 multi -1.99 import weight 0.00
Epoch 416 Iter 5 subLoss 2136.3 multi 1.00 import weight 0.00
Epoch 416 Iter 6 subLoss 1894.3 multi 9.96 import weight 0.00
Epoch 416 Iter 7 subLoss 1806.0 multi 3.99 import weight 0.00
Epoch 416 Iter 8 subLoss 2157.8 multi 6.97 import weight 0.00
Epoch 416 Iter 9 subLoss 2060.4 multi 12.94 import weight 0.00
Epoch 416 Iter 10 subLoss 1774.3 multi -10.94 import weight 0.00
Epoch 416 Iter 11 subLoss 2133.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 416 Acc: 98.79 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 213 train Loss: 1955.8 test Loss: 210.7
Epoch 417 Iter 0 subLoss 1895.8 multi 12.94 import weight 0.00
Epoch 417 Iter 1 subLoss 2082.2 multi 6.97 import weight 0.00
Epoch 417 Iter 2 subLoss 1734.1 multi 1.00 import weight 0.00
Epoch 417 Iter 3 subLoss 1763.6 multi 9.96 import weight 0.00
Epoch 417 Iter 4 subLoss 1727.1 multi 6.97 import weight 0.00
Epoch 417 Iter 5 subLoss 1690.0 multi -7.96 import weight 0.00
Epoch 417 Iter 6 subLoss 2197.2 multi -7.96 import weight 0.00
Epoch 417 Iter 7 subLoss 1802.3 multi 6.97 import weight 0.00
Epoch 417 Iter 8 subLoss 1662.5 multi -7.96 import weight 0.00
Epoch 417 Iter 9 subLoss 2019.1 multi -1.98 import weight 0.00
Epoch 417 Iter 10 subLoss 1930.1 multi -16.91 import weight 0.00
Epoch 417 Iter 11 subLoss 2251.2 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 417 Acc: 98.77 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 225 train Loss: 1973.1 test Loss: 213.9
Epoch 418 Iter 0 subLoss 1654.5 multi 9.96 import weight 0.00
Epoch 418 Iter 1 subLoss 1840.2 multi -1.98 import weight 0.00
Epoch 418 Iter 2 subLoss 1962.6 multi 3.99 import weight 0.00
Epoch 418 Iter 3 subLoss 1878.3 multi 6.97 import weight 0.00
Epoch 418 Iter 4 subLoss 1984.4 multi -13.93 import weight 0.00
Epoch 418 Iter 5 subLoss 2148.2 multi -10.94 import weight 0.00
Epoch 418 Iter 6 subLoss 2944.5 multi -1.99 import weight 0.00
Epoch 418 Iter 7 subLoss 3150.5 multi -1.99 import weight 0.00
Epoch 418 Iter 8 subLoss 3926.5 multi 12.94 import weight 0.00
Epoch 418 Iter 9 subLoss 2273.2 multi -1.99 import weight 0.00
Epoch 418 Iter 10 subLoss 2502.8 multi -7.96 import weight 0.00
Epoch 418 Iter 11 subLoss 2543.3 multi 27.87 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 418 Acc: 98.37 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 27.87 Pidx 254 train Loss: 2300.8 test Loss: 271.5
Epoch 419 Iter 0 subLoss 2334.4 multi -4.97 import weight 0.00
Epoch 419 Iter 1 subLoss 3048.8 multi 1.00 import weight 0.00
Epoch 419 Iter 2 subLoss 2451.3 multi 18.91 import weight 0.00
Epoch 419 Iter 3 subLoss 1963.2 multi 6.97 import weight 0.00
Epoch 419 Iter 4 subLoss 2394.2 multi 33.84 import weight 1.00
Epoch 419 Iter 5 subLoss 1765.4 multi 12.94 import weight 0.00
Epoch 419 Iter 6 subLoss 2232.2 multi 12.94 import weight 0.00
Epoch 419 Iter 7 subLoss 1917.9 multi 12.94 import weight 0.00
Epoch 419 Iter 8 subLoss 1857.1 multi -1.98 import weight 0.00
Epoch 419 Iter 9 subLoss 1895.7 multi 15.93 import weight 0.00
Epoch 419 Iter 10 subLoss 1650.2 multi 12.94 import weight 0.00
Epoch 419 Iter 11 subLoss 1527.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 419 Acc: 98.81 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 152 train Loss: 1741.4 test Loss: 204.1
Epoch 420 Iter 0 subLoss 2006.3 multi 15.93 import weight 0.00
Epoch 420 Iter 1 subLoss 1981.6 multi -10.94 import weight 0.00
Epoch 420 Iter 2 subLoss 2101.0 multi -7.96 import weight 0.00
Epoch 420 Iter 3 subLoss 1684.6 multi 1.00 import weight 0.00
Epoch 420 Iter 4 subLoss 1648.6 multi -1.98 import weight 0.00
Epoch 420 Iter 5 subLoss 1549.7 multi -4.97 import weight 0.00
Epoch 420 Iter 6 subLoss 1923.5 multi -10.94 import weight 0.00
Epoch 420 Iter 7 subLoss 2664.4 multi 15.93 import weight 0.00
Epoch 420 Iter 8 subLoss 1952.0 multi -7.96 import weight 0.00
Epoch 420 Iter 9 subLoss 2091.8 multi 3.98 import weight 0.00
Epoch 420 Iter 10 subLoss 2249.3 multi -37.81 import weight 0.00
Epoch 420 Iter 11 subLoss 3563.2 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 420 Acc: 98.42 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 356 train Loss: 2139.3 test Loss: 231.0
Epoch 421 Iter 0 subLoss 2025.7 multi -10.94 import weight 0.00
Epoch 421 Iter 1 subLoss 2294.3 multi 9.96 import weight 0.00
Epoch 421 Iter 2 subLoss 2033.0 multi -7.96 import weight 0.00
Epoch 421 Iter 3 subLoss 2155.7 multi 6.97 import weight 0.00
Epoch 421 Iter 4 subLoss 1949.4 multi 6.97 import weight 0.00
Epoch 421 Iter 5 subLoss 1538.1 multi 3.98 import weight 0.00
Epoch 421 Iter 6 subLoss 2286.8 multi 1.00 import weight 0.00
Epoch 421 Iter 7 subLoss 2427.3 multi 1.00 import weight 0.00
Epoch 421 Iter 8 subLoss 1765.1 multi 15.93 import weight 0.00
Epoch 421 Iter 9 subLoss 1922.5 multi -7.96 import weight 0.00
Epoch 421 Iter 10 subLoss 1821.5 multi -10.94 import weight 0.00
Epoch 421 Iter 11 subLoss 2012.1 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 421 Acc: 98.83 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 201 train Loss: 2030.2 test Loss: 198.1
Epoch 422 Iter 0 subLoss 2380.5 multi -1.99 import weight 0.00
Epoch 422 Iter 1 subLoss 2271.7 multi 1.00 import weight 0.00
Epoch 422 Iter 2 subLoss 1678.0 multi 6.97 import weight 0.00
Epoch 422 Iter 3 subLoss 1921.1 multi -4.97 import weight 0.00
Epoch 422 Iter 4 subLoss 1951.7 multi -7.96 import weight 0.00
Epoch 422 Iter 5 subLoss 2735.1 multi 3.99 import weight 0.00
Epoch 422 Iter 6 subLoss 1972.0 multi 21.90 import weight 0.00
Epoch 422 Iter 7 subLoss 1910.0 multi 15.93 import weight 0.00
Epoch 422 Iter 8 subLoss 1822.2 multi -7.96 import weight 0.00
Epoch 422 Iter 9 subLoss 1765.4 multi 18.91 import weight 0.00
Epoch 422 Iter 10 subLoss 1610.0 multi 1.00 import weight 0.00
Epoch 422 Iter 11 subLoss 1886.8 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 422 Acc: 98.83 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 188 train Loss: 1842.2 test Loss: 195.7
Epoch 423 Iter 0 subLoss 1733.2 multi 1.00 import weight 0.00
Epoch 423 Iter 1 subLoss 1839.6 multi 3.98 import weight 0.00
Epoch 423 Iter 2 subLoss 1696.7 multi -7.96 import weight 0.00
Epoch 423 Iter 3 subLoss 2043.6 multi 12.94 import weight 0.00
Epoch 423 Iter 4 subLoss 1420.9 multi 1.00 import weight 0.00
Epoch 423 Iter 5 subLoss 2078.7 multi -10.94 import weight 0.00
Epoch 423 Iter 6 subLoss 1990.9 multi -7.96 import weight 0.00
Epoch 423 Iter 7 subLoss 1654.9 multi 12.94 import weight 0.00
Epoch 423 Iter 8 subLoss 2064.0 multi 15.93 import weight 0.00
Epoch 423 Iter 9 subLoss 1720.9 multi 9.96 import weight 0.00
Epoch 423 Iter 10 subLoss 1892.4 multi 15.93 import weight 0.00
Epoch 423 Iter 11 subLoss 2000.4 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 423 Acc: 98.77 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 200 train Loss: 1824.9 test Loss: 201.6
Epoch 424 Iter 0 subLoss 2015.4 multi -1.99 import weight 0.00
Epoch 424 Iter 1 subLoss 1677.9 multi 9.96 import weight 0.00
Epoch 424 Iter 2 subLoss 1734.2 multi 1.00 import weight 0.00
Epoch 424 Iter 3 subLoss 1861.9 multi -4.97 import weight 0.00
Epoch 424 Iter 4 subLoss 1688.5 multi -1.99 import weight 0.00
Epoch 424 Iter 5 subLoss 1731.3 multi 3.99 import weight 0.00
Epoch 424 Iter 6 subLoss 1773.1 multi -19.90 import weight 0.00
Epoch 424 Iter 7 subLoss 2081.9 multi 6.97 import weight 0.00
Epoch 424 Iter 8 subLoss 1928.5 multi -4.97 import weight 0.00
Epoch 424 Iter 9 subLoss 1459.7 multi 3.99 import weight 0.00
Epoch 424 Iter 10 subLoss 1917.5 multi 18.91 import weight 0.00
Epoch 424 Iter 11 subLoss 2040.9 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 424 Acc: 98.83 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 204 train Loss: 1842.0 test Loss: 200.9
Epoch 425 Iter 0 subLoss 1542.6 multi -4.97 import weight 0.00
Epoch 425 Iter 1 subLoss 1862.5 multi -1.98 import weight 0.00
Epoch 425 Iter 2 subLoss 1929.3 multi -4.97 import weight 0.00
Epoch 425 Iter 3 subLoss 2015.3 multi 1.00 import weight 0.00
Epoch 425 Iter 4 subLoss 1950.5 multi -4.97 import weight 0.00
Epoch 425 Iter 5 subLoss 1800.7 multi 9.96 import weight 0.00
Epoch 425 Iter 6 subLoss 1824.2 multi -4.97 import weight 0.00
Epoch 425 Iter 7 subLoss 2021.7 multi -16.91 import weight 0.00
Epoch 425 Iter 8 subLoss 1898.7 multi 18.91 import weight 0.00
Epoch 425 Iter 9 subLoss 1637.0 multi 3.98 import weight 0.00
Epoch 425 Iter 10 subLoss 1322.0 multi 1.00 import weight 0.00
Epoch 425 Iter 11 subLoss 1661.3 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 425 Acc: 98.72 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 166 train Loss: 1809.6 test Loss: 207.9
Epoch 426 Iter 0 subLoss 2085.2 multi 9.96 import weight 0.00
Epoch 426 Iter 1 subLoss 1694.0 multi -7.96 import weight 0.00
Epoch 426 Iter 2 subLoss 1498.9 multi 3.99 import weight 0.00
Epoch 426 Iter 3 subLoss 1634.6 multi 6.97 import weight 0.00
Epoch 426 Iter 4 subLoss 1880.1 multi -4.97 import weight 0.00
Epoch 426 Iter 5 subLoss 1706.3 multi 1.00 import weight 0.00
Epoch 426 Iter 6 subLoss 1877.0 multi 3.99 import weight 0.00
Epoch 426 Iter 7 subLoss 1751.7 multi 6.97 import weight 0.00
Epoch 426 Iter 8 subLoss 1706.7 multi 3.98 import weight 0.00
Epoch 426 Iter 9 subLoss 1686.9 multi 1.00 import weight 0.00
Epoch 426 Iter 10 subLoss 2154.1 multi 9.96 import weight 0.00
Epoch 426 Iter 11 subLoss 1631.2 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 426 Acc: 98.91 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 163 train Loss: 1701.7 test Loss: 193.9
Epoch 427 Iter 0 subLoss 1729.7 multi 12.94 import weight 0.00
Epoch 427 Iter 1 subLoss 1801.4 multi 12.94 import weight 0.00
Epoch 427 Iter 2 subLoss 1607.9 multi 1.00 import weight 0.00
Epoch 427 Iter 3 subLoss 1674.1 multi 9.96 import weight 0.00
Epoch 427 Iter 4 subLoss 1976.2 multi 24.88 import weight 0.00
Epoch 427 Iter 5 subLoss 1761.3 multi 18.91 import weight 0.00
Epoch 427 Iter 6 subLoss 1576.8 multi 1.00 import weight 0.00
Epoch 427 Iter 7 subLoss 1520.9 multi 1.00 import weight 0.00
Epoch 427 Iter 8 subLoss 1419.8 multi 1.00 import weight 0.00
Epoch 427 Iter 9 subLoss 1441.7 multi 1.00 import weight 0.00
Epoch 427 Iter 10 subLoss 1439.8 multi -1.99 import weight 0.00
Epoch 427 Iter 11 subLoss 1665.8 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 427 Acc: 98.95 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 166 train Loss: 1733.1 test Loss: 178.0
Epoch 428 Iter 0 subLoss 1831.9 multi 3.99 import weight 0.00
Epoch 428 Iter 1 subLoss 2014.9 multi 3.98 import weight 0.00
Epoch 428 Iter 2 subLoss 1803.9 multi 15.93 import weight 0.00
Epoch 428 Iter 3 subLoss 1478.0 multi -1.99 import weight 0.00
Epoch 428 Iter 4 subLoss 1473.1 multi 1.00 import weight 0.00
Epoch 428 Iter 5 subLoss 1898.8 multi 18.91 import weight 0.00
Epoch 428 Iter 6 subLoss 1775.1 multi -19.90 import weight 0.00
Epoch 428 Iter 7 subLoss 2017.1 multi 6.97 import weight 0.00
Epoch 428 Iter 8 subLoss 2014.1 multi 9.96 import weight 0.00
Epoch 428 Iter 9 subLoss 1834.3 multi 6.97 import weight 0.00
Epoch 428 Iter 10 subLoss 1688.4 multi 1.00 import weight 0.00
Epoch 428 Iter 11 subLoss 1534.8 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 428 Acc: 98.93 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 153 train Loss: 1719.5 test Loss: 184.6
Epoch 429 Iter 0 subLoss 1966.3 multi 1.00 import weight 0.00
Epoch 429 Iter 1 subLoss 1605.4 multi 3.98 import weight 0.00
Epoch 429 Iter 2 subLoss 1733.4 multi 3.99 import weight 0.00
Epoch 429 Iter 3 subLoss 1494.0 multi 6.97 import weight 0.00
Epoch 429 Iter 4 subLoss 1387.0 multi 1.00 import weight 0.00
Epoch 429 Iter 5 subLoss 1468.5 multi -1.98 import weight 0.00
Epoch 429 Iter 6 subLoss 1623.7 multi -1.98 import weight 0.00
Epoch 429 Iter 7 subLoss 1595.4 multi 3.99 import weight 0.00
Epoch 429 Iter 8 subLoss 1376.7 multi 1.00 import weight 0.00
Epoch 429 Iter 9 subLoss 1544.1 multi -4.97 import weight 0.00
Epoch 429 Iter 10 subLoss 1973.8 multi 24.88 import weight 0.00
Epoch 429 Iter 11 subLoss 1616.9 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 429 Acc: 98.60 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 161 train Loss: 1786.6 test Loss: 218.9
Epoch 430 Iter 0 subLoss 2153.5 multi 12.94 import weight 0.00
Epoch 430 Iter 1 subLoss 1914.1 multi 21.90 import weight 0.00
Epoch 430 Iter 2 subLoss 1748.3 multi -19.90 import weight 0.00
Epoch 430 Iter 3 subLoss 1701.6 multi 6.97 import weight 0.00
Epoch 430 Iter 4 subLoss 1629.4 multi -1.99 import weight 0.00
Epoch 430 Iter 5 subLoss 2102.9 multi -7.96 import weight 0.00
Epoch 430 Iter 6 subLoss 1875.3 multi 6.97 import weight 0.00
Epoch 430 Iter 7 subLoss 1338.8 multi -1.99 import weight 0.00
Epoch 430 Iter 8 subLoss 1649.6 multi -7.96 import weight 0.00
Epoch 430 Iter 9 subLoss 1989.9 multi -16.91 import weight 0.00
Epoch 430 Iter 10 subLoss 4814.5 multi -7.96 import weight 0.00
Epoch 430 Iter 11 subLoss 67000.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 430 Acc: 96.89 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 6700 train Loss: 3592.8 test Loss: 495.5
Epoch 431 Iter 0 subLoss 3711.2 multi 15.93 import weight 0.00
Epoch 431 Iter 1 subLoss 2211.1 multi 1.00 import weight 0.00
Epoch 431 Iter 2 subLoss 1998.4 multi -7.96 import weight 0.00
Epoch 431 Iter 3 subLoss 2497.8 multi 1.00 import weight 0.00
Epoch 431 Iter 4 subLoss 2456.2 multi 21.90 import weight 0.00
Epoch 431 Iter 5 subLoss 1921.3 multi -4.97 import weight 0.00
Epoch 431 Iter 6 subLoss 2154.9 multi 15.93 import weight 0.00
Epoch 431 Iter 7 subLoss 1436.5 multi 1.00 import weight 0.00
Epoch 431 Iter 8 subLoss 1723.5 multi 15.93 import weight 0.00
Epoch 431 Iter 9 subLoss 1593.6 multi 6.97 import weight 0.00
Epoch 431 Iter 10 subLoss 1729.2 multi 18.91 import weight 0.00
Epoch 431 Iter 11 subLoss 1638.8 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 431 Acc: 98.79 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 163 train Loss: 1667.8 test Loss: 193.6
Epoch 432 Iter 0 subLoss 1603.8 multi 1.00 import weight 0.00
Epoch 432 Iter 1 subLoss 1747.4 multi -16.91 import weight 0.00
Epoch 432 Iter 2 subLoss 2224.4 multi 12.94 import weight 0.00
Epoch 432 Iter 3 subLoss 1245.6 multi 1.00 import weight 0.00
Epoch 432 Iter 4 subLoss 1258.4 multi -1.99 import weight 0.00
Epoch 432 Iter 5 subLoss 1885.2 multi -7.96 import weight 0.00
Epoch 432 Iter 6 subLoss 1581.1 multi -1.99 import weight 0.00
Epoch 432 Iter 7 subLoss 1336.6 multi 1.00 import weight 0.00
Epoch 432 Iter 8 subLoss 1611.0 multi -1.99 import weight 0.00
Epoch 432 Iter 9 subLoss 1630.6 multi 9.96 import weight 0.00
Epoch 432 Iter 10 subLoss 1771.9 multi -16.91 import weight 0.00
Epoch 432 Iter 11 subLoss 1549.6 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 432 Acc: 98.77 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 154 train Loss: 1771.0 test Loss: 204.5
Epoch 433 Iter 0 subLoss 1922.1 multi -1.99 import weight 0.00
Epoch 433 Iter 1 subLoss 1817.0 multi -4.97 import weight 0.00
Epoch 433 Iter 2 subLoss 1562.7 multi 1.00 import weight 0.00
Epoch 433 Iter 3 subLoss 1781.9 multi -1.99 import weight 0.00
Epoch 433 Iter 4 subLoss 1643.2 multi -10.94 import weight 0.00
Epoch 433 Iter 5 subLoss 1488.5 multi -4.97 import weight 0.00
Epoch 433 Iter 6 subLoss 2237.6 multi 12.94 import weight 0.00
Epoch 433 Iter 7 subLoss 1665.2 multi -7.96 import weight 0.00
Epoch 433 Iter 8 subLoss 1340.6 multi -4.97 import weight 0.00
Epoch 433 Iter 9 subLoss 1706.8 multi 9.96 import weight 0.00
Epoch 433 Iter 10 subLoss 1664.2 multi -4.97 import weight 0.00
Epoch 433 Iter 11 subLoss 1820.2 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 433 Acc: 98.60 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 182 train Loss: 1753.6 test Loss: 233.3
Epoch 434 Iter 0 subLoss 1822.5 multi -1.99 import weight 0.00
Epoch 434 Iter 1 subLoss 1661.0 multi -1.99 import weight 0.00
Epoch 434 Iter 2 subLoss 1613.4 multi 1.00 import weight 0.00
Epoch 434 Iter 3 subLoss 1735.0 multi 1.00 import weight 0.00
Epoch 434 Iter 4 subLoss 1396.8 multi -1.99 import weight 0.00
Epoch 434 Iter 5 subLoss 1958.7 multi -1.99 import weight 0.00
Epoch 434 Iter 6 subLoss 1670.7 multi 1.00 import weight 0.00
Epoch 434 Iter 7 subLoss 2014.9 multi 12.94 import weight 0.00
Epoch 434 Iter 8 subLoss 1591.7 multi 6.97 import weight 0.00
Epoch 434 Iter 9 subLoss 1867.1 multi 1.00 import weight 0.00
Epoch 434 Iter 10 subLoss 1864.4 multi 3.99 import weight 0.00
Epoch 434 Iter 11 subLoss 1716.5 multi -19.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 434 Acc: 98.60 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -19.90 Pidx 171 train Loss: 1933.8 test Loss: 253.3
Epoch 435 Iter 0 subLoss 1888.3 multi -4.97 import weight 0.00
Epoch 435 Iter 1 subLoss 1656.5 multi 9.96 import weight 0.00
Epoch 435 Iter 2 subLoss 1726.6 multi 18.91 import weight 0.00
Epoch 435 Iter 3 subLoss 1899.3 multi 15.93 import weight 0.00
Epoch 435 Iter 4 subLoss 1555.6 multi -10.94 import weight 0.00
Epoch 435 Iter 5 subLoss 1568.1 multi 1.00 import weight 0.00
Epoch 435 Iter 6 subLoss 1916.0 multi 24.88 import weight 0.00
Epoch 435 Iter 7 subLoss 1618.0 multi 3.98 import weight 0.00
Epoch 435 Iter 8 subLoss 1653.0 multi 12.94 import weight 0.00
Epoch 435 Iter 9 subLoss 1625.8 multi -7.96 import weight 0.00
Epoch 435 Iter 10 subLoss 1802.1 multi 18.91 import weight 0.00
Epoch 435 Iter 11 subLoss 1465.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 435 Acc: 98.83 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 146 train Loss: 1671.7 test Loss: 189.1
Epoch 436 Iter 0 subLoss 1282.7 multi 1.00 import weight 0.00
Epoch 436 Iter 1 subLoss 1648.2 multi -7.96 import weight 0.00
Epoch 436 Iter 2 subLoss 1928.4 multi -1.99 import weight 0.00
Epoch 436 Iter 3 subLoss 1623.6 multi -4.97 import weight 0.00
Epoch 436 Iter 4 subLoss 1349.2 multi -1.98 import weight 0.00
Epoch 436 Iter 5 subLoss 1853.3 multi 1.00 import weight 0.00
Epoch 436 Iter 6 subLoss 1617.4 multi 6.97 import weight 0.00
Epoch 436 Iter 7 subLoss 1451.3 multi 3.98 import weight 0.00
Epoch 436 Iter 8 subLoss 1524.8 multi 3.98 import weight 0.00
Epoch 436 Iter 9 subLoss 1698.6 multi -10.94 import weight 0.00
Epoch 436 Iter 10 subLoss 2089.7 multi 12.94 import weight 0.00
Epoch 436 Iter 11 subLoss 1874.1 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 436 Acc: 98.83 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 187 train Loss: 1681.6 test Loss: 185.5
Epoch 437 Iter 0 subLoss 1399.6 multi 1.00 import weight 0.00
Epoch 437 Iter 1 subLoss 1721.0 multi 21.90 import weight 0.00
Epoch 437 Iter 2 subLoss 1558.4 multi -7.96 import weight 0.00
Epoch 437 Iter 3 subLoss 1655.4 multi 12.94 import weight 0.00
Epoch 437 Iter 4 subLoss 1674.0 multi 3.98 import weight 0.00
Epoch 437 Iter 5 subLoss 2040.4 multi 18.91 import weight 0.00
Epoch 437 Iter 6 subLoss 1485.0 multi -1.98 import weight 0.00
Epoch 437 Iter 7 subLoss 1308.2 multi 1.00 import weight 0.00
Epoch 437 Iter 8 subLoss 1649.9 multi -4.97 import weight 0.00
Epoch 437 Iter 9 subLoss 1680.8 multi -1.99 import weight 0.00
Epoch 437 Iter 10 subLoss 1608.5 multi 1.00 import weight 0.00
Epoch 437 Iter 11 subLoss 1646.0 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 437 Acc: 98.85 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 164 train Loss: 1610.8 test Loss: 176.8
Epoch 438 Iter 0 subLoss 1923.4 multi 1.00 import weight 0.00
Epoch 438 Iter 1 subLoss 1693.8 multi -10.94 import weight 0.00
Epoch 438 Iter 2 subLoss 1642.8 multi 1.00 import weight 0.00
Epoch 438 Iter 3 subLoss 1536.2 multi 3.99 import weight 0.00
Epoch 438 Iter 4 subLoss 1600.5 multi 3.98 import weight 0.00
Epoch 438 Iter 5 subLoss 1396.0 multi 3.98 import weight 0.00
Epoch 438 Iter 6 subLoss 1581.8 multi 1.00 import weight 0.00
Epoch 438 Iter 7 subLoss 1445.9 multi -1.98 import weight 0.00
Epoch 438 Iter 8 subLoss 1686.1 multi 1.00 import weight 0.00
Epoch 438 Iter 9 subLoss 1609.4 multi 6.97 import weight 0.00
Epoch 438 Iter 10 subLoss 1614.8 multi 1.00 import weight 0.00
Epoch 438 Iter 11 subLoss 1237.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 438 Acc: 98.89 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 123 train Loss: 1586.7 test Loss: 169.6
Epoch 439 Iter 0 subLoss 1440.0 multi 1.00 import weight 0.00
Epoch 439 Iter 1 subLoss 1586.5 multi 3.98 import weight 0.00
Epoch 439 Iter 2 subLoss 1454.2 multi 1.00 import weight 0.00
Epoch 439 Iter 3 subLoss 1540.0 multi 6.97 import weight 0.00
Epoch 439 Iter 4 subLoss 1687.2 multi 3.99 import weight 0.00
Epoch 439 Iter 5 subLoss 1776.5 multi -13.93 import weight 0.00
Epoch 439 Iter 6 subLoss 1495.8 multi 3.99 import weight 0.00
Epoch 439 Iter 7 subLoss 1841.4 multi -7.96 import weight 0.00
Epoch 439 Iter 8 subLoss 1574.8 multi -1.98 import weight 0.00
Epoch 439 Iter 9 subLoss 1581.0 multi 3.99 import weight 0.00
Epoch 439 Iter 10 subLoss 1422.0 multi 1.00 import weight 0.00
Epoch 439 Iter 11 subLoss 1797.8 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 439 Acc: 98.74 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 179 train Loss: 1909.8 test Loss: 214.5
Epoch 440 Iter 0 subLoss 2046.7 multi 21.90 import weight 0.00
Epoch 440 Iter 1 subLoss 1529.1 multi 6.97 import weight 0.00
Epoch 440 Iter 2 subLoss 1869.9 multi 3.99 import weight 0.00
Epoch 440 Iter 3 subLoss 1697.5 multi -13.93 import weight 0.00
Epoch 440 Iter 4 subLoss 1691.9 multi -10.94 import weight 0.00
Epoch 440 Iter 5 subLoss 1871.5 multi 3.99 import weight 0.00
Epoch 440 Iter 6 subLoss 1734.3 multi -1.99 import weight 0.00
Epoch 440 Iter 7 subLoss 2170.2 multi -25.87 import weight 0.00
Epoch 440 Iter 8 subLoss 4708.7 multi 3.99 import weight 0.00
Epoch 440 Iter 9 subLoss 2697.3 multi -16.91 import weight 0.00
Epoch 440 Iter 10 subLoss 9140.5 multi 3.99 import weight 0.00
Epoch 440 Iter 11 subLoss 3149.3 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 440 Acc: 98.87 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 314 train Loss: 1817.0 test Loss: 191.0
Epoch 441 Iter 0 subLoss 1844.7 multi -4.97 import weight 0.00
Epoch 441 Iter 1 subLoss 1625.8 multi -7.96 import weight 0.00
Epoch 441 Iter 2 subLoss 2008.5 multi 15.93 import weight 0.00
Epoch 441 Iter 3 subLoss 1921.2 multi 3.98 import weight 0.00
Epoch 441 Iter 4 subLoss 1387.5 multi 1.00 import weight 0.00
Epoch 441 Iter 5 subLoss 1680.6 multi 6.97 import weight 0.00
Epoch 441 Iter 6 subLoss 1830.7 multi 3.98 import weight 0.00
Epoch 441 Iter 7 subLoss 1390.1 multi 3.99 import weight 0.00
Epoch 441 Iter 8 subLoss 1745.8 multi -19.90 import weight 0.00
Epoch 441 Iter 9 subLoss 1821.1 multi 1.00 import weight 0.00
Epoch 441 Iter 10 subLoss 1562.7 multi 1.00 import weight 0.00
Epoch 441 Iter 11 subLoss 1830.2 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 441 Acc: 98.75 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 183 train Loss: 1733.4 test Loss: 185.8
Epoch 442 Iter 0 subLoss 1497.7 multi 6.97 import weight 0.00
Epoch 442 Iter 1 subLoss 2116.1 multi -7.96 import weight 0.00
Epoch 442 Iter 2 subLoss 1866.0 multi 6.97 import weight 0.00
Epoch 442 Iter 3 subLoss 1175.2 multi 1.00 import weight 0.00
Epoch 442 Iter 4 subLoss 1564.9 multi 3.99 import weight 0.00
Epoch 442 Iter 5 subLoss 1869.4 multi 9.96 import weight 0.00
Epoch 442 Iter 6 subLoss 1432.7 multi 1.00 import weight 0.00
Epoch 442 Iter 7 subLoss 1814.8 multi -4.97 import weight 0.00
Epoch 442 Iter 8 subLoss 1449.3 multi 1.00 import weight 0.00
Epoch 442 Iter 9 subLoss 1654.9 multi 6.97 import weight 0.00
Epoch 442 Iter 10 subLoss 1622.7 multi -4.97 import weight 0.00
Epoch 442 Iter 11 subLoss 1670.2 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 442 Acc: 98.87 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 167 train Loss: 1631.0 test Loss: 179.0
Epoch 443 Iter 0 subLoss 1405.3 multi -10.94 import weight 0.00
Epoch 443 Iter 1 subLoss 1983.0 multi -13.93 import weight 0.00
Epoch 443 Iter 2 subLoss 1619.6 multi 3.98 import weight 0.00
Epoch 443 Iter 3 subLoss 1945.5 multi 9.96 import weight 0.00
Epoch 443 Iter 4 subLoss 1750.3 multi 1.00 import weight 0.00
Epoch 443 Iter 5 subLoss 1540.0 multi -4.97 import weight 0.00
Epoch 443 Iter 6 subLoss 1638.5 multi 1.00 import weight 0.00
Epoch 443 Iter 7 subLoss 1560.6 multi 6.97 import weight 0.00
Epoch 443 Iter 8 subLoss 1698.0 multi -10.94 import weight 0.00
Epoch 443 Iter 9 subLoss 1491.2 multi 9.96 import weight 0.00
Epoch 443 Iter 10 subLoss 1597.6 multi 1.00 import weight 0.00
Epoch 443 Iter 11 subLoss 1658.1 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 443 Acc: 98.93 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 165 train Loss: 1608.7 test Loss: 173.0
Epoch 444 Iter 0 subLoss 1345.4 multi 1.00 import weight 0.00
Epoch 444 Iter 1 subLoss 1782.5 multi -1.99 import weight 0.00
Epoch 444 Iter 2 subLoss 1516.5 multi 3.99 import weight 0.00
Epoch 444 Iter 3 subLoss 1717.8 multi -16.91 import weight 0.00
Epoch 444 Iter 4 subLoss 1771.8 multi -10.94 import weight 0.00
Epoch 444 Iter 5 subLoss 1765.3 multi 18.91 import weight 0.00
Epoch 444 Iter 6 subLoss 1955.2 multi -1.99 import weight 0.00
Epoch 444 Iter 7 subLoss 1702.9 multi -1.99 import weight 0.00
Epoch 444 Iter 8 subLoss 1819.2 multi -1.99 import weight 0.00
Epoch 444 Iter 9 subLoss 1958.7 multi 1.00 import weight 0.00
Epoch 444 Iter 10 subLoss 1666.8 multi -13.93 import weight 0.00
Epoch 444 Iter 11 subLoss 1928.6 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 444 Acc: 98.97 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 192 train Loss: 1764.3 test Loss: 167.1
Epoch 445 Iter 0 subLoss 1798.6 multi -10.94 import weight 0.00
Epoch 445 Iter 1 subLoss 1911.2 multi 27.87 import weight 0.00
Epoch 445 Iter 2 subLoss 2391.1 multi 33.84 import weight 1.00
Epoch 445 Iter 3 subLoss 3774.5 multi -1.99 import weight 0.00
Epoch 445 Iter 4 subLoss 4962.6 multi 6.97 import weight 0.00
Epoch 445 Iter 5 subLoss 1767.6 multi 21.90 import weight 0.00
Epoch 445 Iter 6 subLoss 1212.5 multi 1.00 import weight 0.00
Epoch 445 Iter 7 subLoss 1582.1 multi 6.97 import weight 0.00
Epoch 445 Iter 8 subLoss 1335.6 multi 3.98 import weight 0.00
Epoch 445 Iter 9 subLoss 1667.2 multi -10.94 import weight 0.00
Epoch 445 Iter 10 subLoss 1766.2 multi 24.88 import weight 0.00
Epoch 445 Iter 11 subLoss 2145.0 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 445 Acc: 98.85 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 214 train Loss: 1926.5 test Loss: 191.5
Epoch 446 Iter 0 subLoss 1870.2 multi 1.00 import weight 0.00
Epoch 446 Iter 1 subLoss 1930.3 multi -46.76 import weight 0.00
Epoch 446 Iter 2 subLoss 11437.3 multi 1.00 import weight 0.00
Epoch 446 Iter 3 subLoss 7630.0 multi 1.00 import weight 0.00
Epoch 446 Iter 4 subLoss 4877.6 multi 1.00 import weight 0.00
Epoch 446 Iter 5 subLoss 4311.2 multi 1.00 import weight 0.00
Epoch 446 Iter 6 subLoss 3760.7 multi 1.00 import weight 0.00
Epoch 446 Iter 7 subLoss 3534.7 multi 1.00 import weight 0.00
Epoch 446 Iter 8 subLoss 3343.3 multi 6.97 import weight 0.00
Epoch 446 Iter 9 subLoss 2164.8 multi 1.00 import weight 0.00
Epoch 446 Iter 10 subLoss 2033.3 multi -7.96 import weight 0.00
Epoch 446 Iter 11 subLoss 2335.2 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 446 Acc: 98.64 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 233 train Loss: 2580.0 test Loss: 221.3
Epoch 447 Iter 0 subLoss 2640.5 multi -7.96 import weight 0.00
Epoch 447 Iter 1 subLoss 3302.2 multi 30.85 import weight 0.00
Epoch 447 Iter 2 subLoss 3207.2 multi -22.88 import weight 0.00
Epoch 447 Iter 3 subLoss 32462.2 multi 1.00 import weight 0.00
Epoch 447 Iter 4 subLoss 9074.4 multi -1.98 import weight 0.00
Epoch 447 Iter 5 subLoss 15530.5 multi 3.99 import weight 0.00
Epoch 447 Iter 6 subLoss 3077.5 multi -7.96 import weight 0.00
Epoch 447 Iter 7 subLoss 4896.5 multi 1.00 import weight 0.00
Epoch 447 Iter 8 subLoss 4290.3 multi -7.96 import weight 0.00
Epoch 447 Iter 9 subLoss 9267.6 multi 1.00 import weight 0.00
Epoch 447 Iter 10 subLoss 7372.4 multi 6.97 import weight 0.00
Epoch 447 Iter 11 subLoss 2348.2 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 447 Acc: 98.38 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 234 train Loss: 2453.9 test Loss: 285.6
Epoch 448 Iter 0 subLoss 2298.9 multi 9.96 import weight 0.00
Epoch 448 Iter 1 subLoss 2032.8 multi -4.97 import weight 0.00
Epoch 448 Iter 2 subLoss 2182.1 multi 12.94 import weight 0.00
Epoch 448 Iter 3 subLoss 1613.8 multi 6.97 import weight 0.00
Epoch 448 Iter 4 subLoss 2171.8 multi -25.87 import weight 0.00
Epoch 448 Iter 5 subLoss 2465.2 multi -34.82 import weight 0.00
Epoch 448 Iter 6 subLoss 8654.8 multi 3.98 import weight 0.00
Epoch 448 Iter 7 subLoss 3059.7 multi -13.93 import weight 0.00
Epoch 448 Iter 8 subLoss 4317.2 multi 3.98 import weight 0.00
Epoch 448 Iter 9 subLoss 3743.7 multi 1.00 import weight 0.00
Epoch 448 Iter 10 subLoss 3581.9 multi -10.94 import weight 0.00
Epoch 448 Iter 11 subLoss 5086.4 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 448 Acc: 98.03 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 508 train Loss: 2839.3 test Loss: 363.7
Epoch 449 Iter 0 subLoss 2455.2 multi 24.88 import weight 0.00
Epoch 449 Iter 1 subLoss 2176.2 multi -22.88 import weight 0.00
Epoch 449 Iter 2 subLoss 2879.0 multi -13.93 import weight 0.00
Epoch 449 Iter 3 subLoss 7603.8 multi -7.96 import weight 0.00
Epoch 449 Iter 4 subLoss 130052.9 multi 1.00 import weight 0.00
Epoch 449 Iter 5 subLoss 6382.2 multi 3.99 import weight 0.00
Epoch 449 Iter 6 subLoss 3668.7 multi 6.97 import weight 0.00
Epoch 449 Iter 7 subLoss 2615.1 multi 6.97 import weight 0.00
Epoch 449 Iter 8 subLoss 2523.8 multi -7.96 import weight 0.00
Epoch 449 Iter 9 subLoss 2747.6 multi 15.93 import weight 0.00
Epoch 449 Iter 10 subLoss 2295.5 multi 12.94 import weight 0.00
Epoch 449 Iter 11 subLoss 2416.5 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 449 Acc: 98.42 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 241 train Loss: 2107.3 test Loss: 227.6
Epoch 450 Iter 0 subLoss 1919.7 multi 30.85 import weight 0.00
Epoch 450 Iter 1 subLoss 1710.6 multi -16.91 import weight 0.00
Epoch 450 Iter 2 subLoss 2160.4 multi 3.99 import weight 0.00
Epoch 450 Iter 3 subLoss 2031.7 multi -1.99 import weight 0.00
Epoch 450 Iter 4 subLoss 2372.3 multi -10.94 import weight 0.00
Epoch 450 Iter 5 subLoss 1982.0 multi -10.94 import weight 0.00
Epoch 450 Iter 6 subLoss 2213.4 multi 3.99 import weight 0.00
Epoch 450 Iter 7 subLoss 1707.2 multi 1.00 import weight 0.00
Epoch 450 Iter 8 subLoss 2039.6 multi 1.00 import weight 0.00
Epoch 450 Iter 9 subLoss 1983.9 multi -7.96 import weight 0.00
Epoch 450 Iter 10 subLoss 1836.8 multi 6.97 import weight 0.00
Epoch 450 Iter 11 subLoss 2166.3 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 450 Acc: 98.38 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 216 train Loss: 1987.2 test Loss: 246.1
Epoch 451 Iter 0 subLoss 1962.6 multi -4.97 import weight 0.00
Epoch 451 Iter 1 subLoss 2122.2 multi 12.94 import weight 0.00
Epoch 451 Iter 2 subLoss 2051.4 multi -22.88 import weight 0.00
Epoch 451 Iter 3 subLoss 2170.5 multi -25.87 import weight 0.00
Epoch 451 Iter 4 subLoss 3621.2 multi 6.97 import weight 0.00
Epoch 451 Iter 5 subLoss 2234.1 multi 15.93 import weight 0.00
Epoch 451 Iter 6 subLoss 1773.9 multi -16.91 import weight 0.00
Epoch 451 Iter 7 subLoss 2984.7 multi -1.98 import weight 0.00
Epoch 451 Iter 8 subLoss 2673.6 multi -28.85 import weight 0.00
Epoch 451 Iter 9 subLoss 13514.8 multi 6.97 import weight 0.00
Epoch 451 Iter 10 subLoss 3669.3 multi 9.96 import weight 0.00
Epoch 451 Iter 11 subLoss 2470.6 multi 33.84 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 451 Acc: 98.42 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 33.84 Pidx 247 train Loss: 2219.8 test Loss: 249.3
Epoch 452 Iter 0 subLoss 2350.7 multi -13.93 import weight 0.00
Epoch 452 Iter 1 subLoss 2342.4 multi 6.97 import weight 0.00
Epoch 452 Iter 2 subLoss 2029.8 multi -25.87 import weight 0.00
Epoch 452 Iter 3 subLoss 3208.0 multi -19.90 import weight 0.00
Epoch 452 Iter 4 subLoss 13729.0 multi 3.98 import weight 0.00
Epoch 452 Iter 5 subLoss 3767.3 multi 3.99 import weight 0.00
Epoch 452 Iter 6 subLoss 3442.0 multi 51.75 import weight 0.00
Epoch 452 Iter 7 subLoss 13486.0 multi -1.99 import weight 0.00
Epoch 452 Iter 8 subLoss 47498.4 multi 1.00 import weight 0.00
Epoch 452 Iter 9 subLoss 6248.5 multi 1.00 import weight 0.00
Epoch 452 Iter 10 subLoss 5585.0 multi -7.96 import weight 0.00
Epoch 452 Iter 11 subLoss 13840.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 452 Acc: 89.57 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1384 train Loss: 9648.7 test Loss: 1536.5
Epoch 453 Iter 0 subLoss 9580.7 multi 1.00 import weight 0.00
Epoch 453 Iter 1 subLoss 7593.5 multi 6.97 import weight 0.00
Epoch 453 Iter 2 subLoss 3319.9 multi -7.96 import weight 0.00
Epoch 453 Iter 3 subLoss 4677.0 multi -10.94 import weight 0.00
Epoch 453 Iter 4 subLoss 7902.6 multi -10.94 import weight 0.00
Epoch 453 Iter 5 subLoss 49259.6 multi -1.99 import weight 0.00
Epoch 453 Iter 6 subLoss 529170.6 multi 1.00 import weight 0.00
Epoch 453 Iter 7 subLoss 37923.4 multi 1.00 import weight 0.00
Epoch 453 Iter 8 subLoss 27674.1 multi 1.00 import weight 0.00
Epoch 453 Iter 9 subLoss 20550.8 multi 1.00 import weight 0.00
Epoch 453 Iter 10 subLoss 16972.7 multi 1.00 import weight 0.00
Epoch 453 Iter 11 subLoss 13441.3 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 453 Acc: 82.29 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1344 train Loss: 19023.7 test Loss: 2543.5
Epoch 454 Iter 0 subLoss 18690.5 multi -1.99 import weight 0.00
Epoch 454 Iter 1 subLoss 25967.1 multi 1.00 import weight 0.00
Epoch 454 Iter 2 subLoss 20998.9 multi 1.00 import weight 0.00
Epoch 454 Iter 3 subLoss 17155.5 multi -1.99 import weight 0.00
Epoch 454 Iter 4 subLoss 24557.7 multi 3.99 import weight 0.00
Epoch 454 Iter 5 subLoss 12106.3 multi 1.00 import weight 0.00
Epoch 454 Iter 6 subLoss 10082.2 multi -1.98 import weight 0.00
Epoch 454 Iter 7 subLoss 12973.3 multi 1.00 import weight 0.00
Epoch 454 Iter 8 subLoss 11101.9 multi -1.98 import weight 0.00
Epoch 454 Iter 9 subLoss 15339.7 multi 3.99 import weight 0.00
Epoch 454 Iter 10 subLoss 8599.1 multi -1.99 import weight 0.00
Epoch 454 Iter 11 subLoss 10267.3 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 454 Acc: 88.27 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1026 train Loss: 12176.7 test Loss: 1767.8
Epoch 455 Iter 0 subLoss 12053.6 multi 6.97 import weight 0.00
Epoch 455 Iter 1 subLoss 5637.1 multi -1.99 import weight 0.00
Epoch 455 Iter 2 subLoss 6481.5 multi -1.99 import weight 0.00
Epoch 455 Iter 3 subLoss 6845.4 multi -1.99 import weight 0.00
Epoch 455 Iter 4 subLoss 7402.2 multi 1.00 import weight 0.00
Epoch 455 Iter 5 subLoss 7947.5 multi 3.99 import weight 0.00
Epoch 455 Iter 6 subLoss 5538.3 multi -10.94 import weight 0.00
Epoch 455 Iter 7 subLoss 9685.3 multi 6.97 import weight 0.00
Epoch 455 Iter 8 subLoss 6501.1 multi -7.96 import weight 0.00
Epoch 455 Iter 9 subLoss 9264.7 multi 3.98 import weight 0.00
Epoch 455 Iter 10 subLoss 7132.7 multi 3.99 import weight 0.00
Epoch 455 Iter 11 subLoss 5879.5 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 455 Acc: 93.36 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 587 train Loss: 6464.2 test Loss: 1003.9
Epoch 456 Iter 0 subLoss 6197.3 multi -1.98 import weight 0.00
Epoch 456 Iter 1 subLoss 7078.4 multi -4.97 import weight 0.00
Epoch 456 Iter 2 subLoss 8302.3 multi -7.96 import weight 0.00
Epoch 456 Iter 3 subLoss 12424.7 multi 3.99 import weight 0.00
Epoch 456 Iter 4 subLoss 9496.1 multi -1.99 import weight 0.00
Epoch 456 Iter 5 subLoss 11097.8 multi 3.98 import weight 0.00
Epoch 456 Iter 6 subLoss 9205.1 multi 3.99 import weight 0.00
Epoch 456 Iter 7 subLoss 7367.6 multi -1.99 import weight 0.00
Epoch 456 Iter 8 subLoss 8319.2 multi -1.99 import weight 0.00
Epoch 456 Iter 9 subLoss 8653.3 multi 6.97 import weight 0.00
Epoch 456 Iter 10 subLoss 6124.4 multi 9.96 import weight 0.00
Epoch 456 Iter 11 subLoss 4288.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 456 Acc: 97.12 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 428 train Loss: 3987.4 test Loss: 508.1
Epoch 457 Iter 0 subLoss 3973.0 multi 12.94 import weight 0.00
Epoch 457 Iter 1 subLoss 2856.0 multi 27.87 import weight 0.00
Epoch 457 Iter 2 subLoss 2607.7 multi -4.97 import weight 0.00
Epoch 457 Iter 3 subLoss 2119.9 multi -4.97 import weight 0.00
Epoch 457 Iter 4 subLoss 2434.9 multi -16.91 import weight 0.00
Epoch 457 Iter 5 subLoss 2774.8 multi 18.91 import weight 0.00
Epoch 457 Iter 6 subLoss 2652.9 multi 12.94 import weight 0.00
Epoch 457 Iter 7 subLoss 2797.6 multi -4.97 import weight 0.00
Epoch 457 Iter 8 subLoss 2402.9 multi -40.79 import weight 0.00
Epoch 457 Iter 9 subLoss 4289.1 multi 6.97 import weight 0.00
Epoch 457 Iter 10 subLoss 2598.4 multi -4.97 import weight 0.00
Epoch 457 Iter 11 subLoss 3110.9 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 457 Acc: 97.57 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 311 train Loss: 2972.2 test Loss: 404.4
Epoch 458 Iter 0 subLoss 3011.2 multi -13.93 import weight 0.00
Epoch 458 Iter 1 subLoss 3549.5 multi 21.90 import weight 0.00
Epoch 458 Iter 2 subLoss 2597.4 multi -1.99 import weight 0.00
Epoch 458 Iter 3 subLoss 2534.6 multi -13.93 import weight 0.00
Epoch 458 Iter 4 subLoss 3379.1 multi -1.98 import weight 0.00
Epoch 458 Iter 5 subLoss 3644.2 multi -1.98 import weight 0.00
Epoch 458 Iter 6 subLoss 3453.8 multi -52.73 import weight 0.00
Epoch 458 Iter 7 subLoss 105958.9 multi 1.00 import weight 0.00
Epoch 458 Iter 8 subLoss 20007.0 multi 3.99 import weight 0.00
Epoch 458 Iter 9 subLoss 7428.5 multi 6.97 import weight 0.00
Epoch 458 Iter 10 subLoss 4606.9 multi -4.97 import weight 0.00
Epoch 458 Iter 11 subLoss 4763.9 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 458 Acc: 96.65 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 476 train Loss: 3661.7 test Loss: 539.4
Epoch 459 Iter 0 subLoss 3998.8 multi -7.96 import weight 0.00
Epoch 459 Iter 1 subLoss 3578.3 multi -16.91 import weight 0.00
Epoch 459 Iter 2 subLoss 5176.9 multi -10.94 import weight 0.00
Epoch 459 Iter 3 subLoss 7865.5 multi -4.97 import weight 0.00
Epoch 459 Iter 4 subLoss 11766.3 multi -1.98 import weight 0.00
Epoch 459 Iter 5 subLoss 14960.1 multi 1.00 import weight 0.00
Epoch 459 Iter 6 subLoss 12927.7 multi 3.98 import weight 0.00
Epoch 459 Iter 7 subLoss 7741.0 multi 3.99 import weight 0.00
Epoch 459 Iter 8 subLoss 5885.3 multi -4.97 import weight 0.00
Epoch 459 Iter 9 subLoss 7462.1 multi 1.00 import weight 0.00
Epoch 459 Iter 10 subLoss 6880.8 multi -10.94 import weight 0.00
Epoch 459 Iter 11 subLoss 11985.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 459 Acc: 86.57 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1198 train Loss: 13187.0 test Loss: 2204.5
Epoch 460 Iter 0 subLoss 13317.0 multi 1.00 import weight 0.00
Epoch 460 Iter 1 subLoss 12321.2 multi 1.00 import weight 0.00
Epoch 460 Iter 2 subLoss 11450.3 multi 1.00 import weight 0.00
Epoch 460 Iter 3 subLoss 10940.4 multi 6.97 import weight 0.00
Epoch 460 Iter 4 subLoss 6944.4 multi 1.00 import weight 0.00
Epoch 460 Iter 5 subLoss 6468.1 multi 1.00 import weight 0.00
Epoch 460 Iter 6 subLoss 6232.8 multi 1.00 import weight 0.00
Epoch 460 Iter 7 subLoss 6006.0 multi -1.98 import weight 0.00
Epoch 460 Iter 8 subLoss 6579.3 multi 18.91 import weight 0.00
Epoch 460 Iter 9 subLoss 4233.1 multi -10.94 import weight 0.00
Epoch 460 Iter 10 subLoss 5278.0 multi -1.99 import weight 0.00
Epoch 460 Iter 11 subLoss 6122.1 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 460 Acc: 96.40 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 612 train Loss: 4223.1 test Loss: 584.7
Epoch 461 Iter 0 subLoss 4157.5 multi 1.00 import weight 0.00
Epoch 461 Iter 1 subLoss 3775.4 multi -4.97 import weight 0.00
Epoch 461 Iter 2 subLoss 4258.8 multi -4.97 import weight 0.00
Epoch 461 Iter 3 subLoss 5987.4 multi 3.99 import weight 0.00
Epoch 461 Iter 4 subLoss 4130.8 multi -22.88 import weight 0.00
Epoch 461 Iter 5 subLoss 7709.2 multi -4.97 import weight 0.00
Epoch 461 Iter 6 subLoss 15536.1 multi 6.97 import weight 0.00
Epoch 461 Iter 7 subLoss 6800.6 multi 1.00 import weight 0.00
Epoch 461 Iter 8 subLoss 5868.8 multi 1.00 import weight 0.00
Epoch 461 Iter 9 subLoss 5716.7 multi -1.99 import weight 0.00
Epoch 461 Iter 10 subLoss 6005.2 multi 1.00 import weight 0.00
Epoch 461 Iter 11 subLoss 6412.7 multi 9.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 461 Acc: 95.64 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 9.96 Pidx 641 train Loss: 4262.1 test Loss: 690.5
Epoch 462 Iter 0 subLoss 4409.8 multi 1.00 import weight 0.00
Epoch 462 Iter 1 subLoss 3830.6 multi -4.97 import weight 0.00
Epoch 462 Iter 2 subLoss 4574.7 multi 6.97 import weight 0.00
Epoch 462 Iter 3 subLoss 4097.2 multi -7.96 import weight 0.00
Epoch 462 Iter 4 subLoss 4707.3 multi 6.97 import weight 0.00
Epoch 462 Iter 5 subLoss 3934.9 multi 1.00 import weight 0.00
Epoch 462 Iter 6 subLoss 4331.3 multi 1.00 import weight 0.00
Epoch 462 Iter 7 subLoss 3741.0 multi 3.98 import weight 0.00
Epoch 462 Iter 8 subLoss 3388.8 multi -13.93 import weight 0.00
Epoch 462 Iter 9 subLoss 4788.6 multi 6.97 import weight 0.00
Epoch 462 Iter 10 subLoss 3940.6 multi -7.96 import weight 0.00
Epoch 462 Iter 11 subLoss 4419.6 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 462 Acc: 94.80 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 441 train Loss: 4842.1 test Loss: 841.0
Epoch 463 Iter 0 subLoss 4511.5 multi 6.97 import weight 0.00
Epoch 463 Iter 1 subLoss 4351.7 multi 3.99 import weight 0.00
Epoch 463 Iter 2 subLoss 4539.7 multi -13.93 import weight 0.00
Epoch 463 Iter 3 subLoss 4771.1 multi -22.88 import weight 0.00
Epoch 463 Iter 4 subLoss 7759.2 multi -7.96 import weight 0.00
Epoch 463 Iter 5 subLoss 11638.5 multi 1.00 import weight 0.00
Epoch 463 Iter 6 subLoss 9923.9 multi 1.00 import weight 0.00
Epoch 463 Iter 7 subLoss 9018.9 multi 1.00 import weight 0.00
Epoch 463 Iter 8 subLoss 8497.9 multi 3.98 import weight 0.00
Epoch 463 Iter 9 subLoss 6824.3 multi -1.99 import weight 0.00
Epoch 463 Iter 10 subLoss 8211.3 multi 1.00 import weight 0.00
Epoch 463 Iter 11 subLoss 6791.8 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 463 Acc: 91.98 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 679 train Loss: 7245.9 test Loss: 1246.8
Epoch 464 Iter 0 subLoss 7101.4 multi 3.98 import weight 0.00
Epoch 464 Iter 1 subLoss 6093.6 multi -13.93 import weight 0.00
Epoch 464 Iter 2 subLoss 9158.0 multi 9.96 import weight 0.00
Epoch 464 Iter 3 subLoss 6646.1 multi -13.93 import weight 0.00
Epoch 464 Iter 4 subLoss 8884.5 multi 1.00 import weight 0.00
Epoch 464 Iter 5 subLoss 8309.7 multi -4.97 import weight 0.00
Epoch 464 Iter 6 subLoss 10187.0 multi 3.99 import weight 0.00
Epoch 464 Iter 7 subLoss 8257.0 multi 3.98 import weight 0.00
Epoch 464 Iter 8 subLoss 7648.1 multi 1.00 import weight 0.00
Epoch 464 Iter 9 subLoss 7828.5 multi -1.99 import weight 0.00
Epoch 464 Iter 10 subLoss 7889.1 multi -7.96 import weight 0.00
Epoch 464 Iter 11 subLoss 9365.4 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 464 Acc: 85.97 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 936 train Loss: 12951.9 test Loss: 2056.1
Epoch 465 Iter 0 subLoss 12529.7 multi 1.00 import weight 0.00
Epoch 465 Iter 1 subLoss 12238.1 multi 1.00 import weight 0.00
Epoch 465 Iter 2 subLoss 11436.1 multi 3.99 import weight 0.00
Epoch 465 Iter 3 subLoss 10448.8 multi 1.00 import weight 0.00
Epoch 465 Iter 4 subLoss 9186.1 multi 1.00 import weight 0.00
Epoch 465 Iter 5 subLoss 9140.7 multi 6.97 import weight 0.00
Epoch 465 Iter 6 subLoss 8028.4 multi 6.97 import weight 0.00
Epoch 465 Iter 7 subLoss 6752.3 multi -1.98 import weight 0.00
Epoch 465 Iter 8 subLoss 7068.0 multi 3.99 import weight 0.00
Epoch 465 Iter 9 subLoss 6306.7 multi 1.00 import weight 0.00
Epoch 465 Iter 10 subLoss 6275.5 multi -1.99 import weight 0.00
Epoch 465 Iter 11 subLoss 6245.4 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 465 Acc: 92.43 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 624 train Loss: 6460.3 test Loss: 1104.4
Epoch 466 Iter 0 subLoss 6093.6 multi -10.94 import weight 0.00
Epoch 466 Iter 1 subLoss 7778.8 multi -7.96 import weight 0.00
Epoch 466 Iter 2 subLoss 9362.6 multi -4.97 import weight 0.00
Epoch 466 Iter 3 subLoss 10523.0 multi 6.97 import weight 0.00
Epoch 466 Iter 4 subLoss 8406.4 multi 3.99 import weight 0.00
Epoch 466 Iter 5 subLoss 8091.1 multi 1.00 import weight 0.00
Epoch 466 Iter 6 subLoss 6575.4 multi 21.90 import weight 0.00
Epoch 466 Iter 7 subLoss 5276.6 multi 1.00 import weight 0.00
Epoch 466 Iter 8 subLoss 5028.2 multi 3.98 import weight 0.00
Epoch 466 Iter 9 subLoss 4788.2 multi 6.97 import weight 0.00
Epoch 466 Iter 10 subLoss 4769.5 multi 18.91 import weight 0.00
Epoch 466 Iter 11 subLoss 4166.4 multi 15.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 466 Acc: 96.22 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 15.93 Pidx 416 train Loss: 3582.2 test Loss: 549.4
Epoch 467 Iter 0 subLoss 3324.0 multi -19.90 import weight 0.00
Epoch 467 Iter 1 subLoss 4727.1 multi 15.93 import weight 0.00
Epoch 467 Iter 2 subLoss 3744.0 multi 6.97 import weight 0.00
Epoch 467 Iter 3 subLoss 3467.0 multi -4.97 import weight 0.00
Epoch 467 Iter 4 subLoss 3630.4 multi -4.97 import weight 0.00
Epoch 467 Iter 5 subLoss 4077.5 multi 18.91 import weight 0.00
Epoch 467 Iter 6 subLoss 3145.4 multi 18.91 import weight 0.00
Epoch 467 Iter 7 subLoss 3307.5 multi 33.84 import weight 0.00
Epoch 467 Iter 8 subLoss 2435.5 multi -13.93 import weight 0.00
Epoch 467 Iter 9 subLoss 2566.5 multi 15.93 import weight 0.00
Epoch 467 Iter 10 subLoss 2199.5 multi -7.96 import weight 0.00
Epoch 467 Iter 11 subLoss 2748.0 multi 18.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 467 Acc: 98.21 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 18.91 Pidx 274 train Loss: 2499.3 test Loss: 287.2
Epoch 468 Iter 0 subLoss 2231.0 multi 18.91 import weight 0.00
Epoch 468 Iter 1 subLoss 2119.6 multi -1.99 import weight 0.00
Epoch 468 Iter 2 subLoss 2394.1 multi 36.82 import weight 1.00
Epoch 468 Iter 3 subLoss 2274.2 multi 3.98 import weight 0.00
Epoch 468 Iter 4 subLoss 2448.0 multi 1.00 import weight 0.00
Epoch 468 Iter 5 subLoss 2225.3 multi 12.94 import weight 0.00
Epoch 468 Iter 6 subLoss 2346.3 multi 9.96 import weight 0.00
Epoch 468 Iter 7 subLoss 2294.9 multi 15.93 import weight 0.00
Epoch 468 Iter 8 subLoss 1842.5 multi -10.94 import weight 0.00
Epoch 468 Iter 9 subLoss 2370.9 multi -7.96 import weight 0.00
Epoch 468 Iter 10 subLoss 2432.5 multi -10.94 import weight 0.00
Epoch 468 Iter 11 subLoss 2636.3 multi 24.88 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 468 Acc: 98.40 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 24.88 Pidx 263 train Loss: 2301.8 test Loss: 252.6
Epoch 469 Iter 0 subLoss 2310.7 multi 12.94 import weight 0.00
Epoch 469 Iter 1 subLoss 2083.7 multi 15.93 import weight 0.00
Epoch 469 Iter 2 subLoss 2206.0 multi -1.99 import weight 0.00
Epoch 469 Iter 3 subLoss 2060.7 multi 15.93 import weight 0.00
Epoch 469 Iter 4 subLoss 1932.9 multi -43.78 import weight 0.00
Epoch 469 Iter 5 subLoss 2322.6 multi -4.97 import weight 0.00
Epoch 469 Iter 6 subLoss 2839.6 multi 15.93 import weight 0.00
Epoch 469 Iter 7 subLoss 1802.7 multi 15.93 import weight 0.00
Epoch 469 Iter 8 subLoss 1994.8 multi -13.93 import weight 0.00
Epoch 469 Iter 9 subLoss 2419.1 multi 9.96 import weight 0.00
Epoch 469 Iter 10 subLoss 2299.0 multi 18.91 import weight 0.00
Epoch 469 Iter 11 subLoss 2078.6 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 469 Acc: 98.52 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 207 train Loss: 2171.5 test Loss: 232.5
Epoch 470 Iter 0 subLoss 1931.3 multi -40.79 import weight 0.00
Epoch 470 Iter 1 subLoss 2617.9 multi 6.97 import weight 0.00
Epoch 470 Iter 2 subLoss 2439.9 multi -7.96 import weight 0.00
Epoch 470 Iter 3 subLoss 2990.4 multi 1.00 import weight 0.00
Epoch 470 Iter 4 subLoss 2824.3 multi -19.90 import weight 0.00
Epoch 470 Iter 5 subLoss 4144.3 multi 12.94 import weight 0.00
Epoch 470 Iter 6 subLoss 2568.6 multi 18.91 import weight 0.00
Epoch 470 Iter 7 subLoss 1970.0 multi 24.88 import weight 0.00
Epoch 470 Iter 8 subLoss 2112.6 multi 1.00 import weight 0.00
Epoch 470 Iter 9 subLoss 1754.7 multi 3.98 import weight 0.00
Epoch 470 Iter 10 subLoss 2402.8 multi -40.79 import weight 0.00
Epoch 470 Iter 11 subLoss 2572.6 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 470 Acc: 97.04 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 257 train Loss: 4537.9 test Loss: 487.5
Epoch 471 Iter 0 subLoss 4262.6 multi 1.00 import weight 0.00
Epoch 471 Iter 1 subLoss 3861.3 multi -10.94 import weight 0.00
Epoch 471 Iter 2 subLoss 10374.4 multi 3.99 import weight 0.00
Epoch 471 Iter 3 subLoss 3554.0 multi -13.93 import weight 0.00
Epoch 471 Iter 4 subLoss 6186.3 multi -13.93 import weight 0.00
Epoch 471 Iter 5 subLoss 26968.3 multi 1.00 import weight 0.00
Epoch 471 Iter 6 subLoss 16013.3 multi 3.99 import weight 0.00
Epoch 471 Iter 7 subLoss 5215.2 multi -4.97 import weight 0.00
Epoch 471 Iter 8 subLoss 7483.9 multi 1.00 import weight 0.00
Epoch 471 Iter 9 subLoss 7005.6 multi 6.97 import weight 0.00
Epoch 471 Iter 10 subLoss 4456.7 multi -10.94 import weight 0.00
Epoch 471 Iter 11 subLoss 6714.6 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 471 Acc: 95.84 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 671 train Loss: 5013.0 test Loss: 637.9
Epoch 472 Iter 0 subLoss 4603.1 multi -1.99 import weight 0.00
Epoch 472 Iter 1 subLoss 5189.2 multi 6.97 import weight 0.00
Epoch 472 Iter 2 subLoss 3773.9 multi -1.98 import weight 0.00
Epoch 472 Iter 3 subLoss 4495.4 multi -7.96 import weight 0.00
Epoch 472 Iter 4 subLoss 5791.3 multi 3.98 import weight 0.00
Epoch 472 Iter 5 subLoss 4306.3 multi -7.96 import weight 0.00
Epoch 472 Iter 6 subLoss 6149.9 multi -16.91 import weight 0.00
Epoch 472 Iter 7 subLoss 17482.2 multi -1.99 import weight 0.00
Epoch 472 Iter 8 subLoss 29344.0 multi 1.00 import weight 0.00
Epoch 472 Iter 9 subLoss 18322.2 multi 3.99 import weight 0.00
Epoch 472 Iter 10 subLoss 10200.3 multi 3.98 import weight 0.00
Epoch 472 Iter 11 subLoss 8071.5 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 472 Acc: 87.33 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 807 train Loss: 8943.3 test Loss: 1435.9
Epoch 473 Iter 0 subLoss 8934.0 multi -4.97 import weight 0.00
Epoch 473 Iter 1 subLoss 11198.6 multi 1.00 import weight 0.00
Epoch 473 Iter 2 subLoss 9414.0 multi -1.99 import weight 0.00
Epoch 473 Iter 3 subLoss 11301.4 multi 1.00 import weight 0.00
Epoch 473 Iter 4 subLoss 10284.0 multi 1.00 import weight 0.00
Epoch 473 Iter 5 subLoss 10690.7 multi -4.97 import weight 0.00
Epoch 473 Iter 6 subLoss 12551.4 multi 6.97 import weight 0.00
Epoch 473 Iter 7 subLoss 9724.8 multi -1.99 import weight 0.00
Epoch 473 Iter 8 subLoss 9613.9 multi 1.00 import weight 0.00
Epoch 473 Iter 9 subLoss 9375.5 multi -4.97 import weight 0.00
Epoch 473 Iter 10 subLoss 11985.1 multi 1.00 import weight 0.00
Epoch 473 Iter 11 subLoss 9602.3 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 473 Acc: 81.59 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 960 train Loss: 13293.3 test Loss: 2123.6
Epoch 474 Iter 0 subLoss 12831.6 multi -1.99 import weight 0.00
Epoch 474 Iter 1 subLoss 13759.6 multi -1.99 import weight 0.00
Epoch 474 Iter 2 subLoss 15547.8 multi -7.96 import weight 0.00
Epoch 474 Iter 3 subLoss 38290.3 multi 1.00 import weight 0.00
Epoch 474 Iter 4 subLoss 22985.9 multi 3.99 import weight 0.00
Epoch 474 Iter 5 subLoss 15335.1 multi 6.97 import weight 0.00
Epoch 474 Iter 6 subLoss 11096.4 multi 6.97 import weight 0.00
Epoch 474 Iter 7 subLoss 9753.9 multi 1.00 import weight 0.00
Epoch 474 Iter 8 subLoss 9802.1 multi -1.99 import weight 0.00
Epoch 474 Iter 9 subLoss 9938.5 multi -4.97 import weight 0.00
Epoch 474 Iter 10 subLoss 10987.9 multi -1.99 import weight 0.00
Epoch 474 Iter 11 subLoss 12229.6 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 474 Acc: 80.85 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1222 train Loss: 12560.1 test Loss: 2135.0
Epoch 475 Iter 0 subLoss 11477.8 multi 1.00 import weight 0.00
Epoch 475 Iter 1 subLoss 11701.2 multi 3.98 import weight 0.00
Epoch 475 Iter 2 subLoss 11805.6 multi 1.00 import weight 0.00
Epoch 475 Iter 3 subLoss 10638.3 multi 3.99 import weight 0.00
Epoch 475 Iter 4 subLoss 9067.3 multi 3.98 import weight 0.00
Epoch 475 Iter 5 subLoss 8013.1 multi 3.99 import weight 0.00
Epoch 475 Iter 6 subLoss 7374.7 multi 6.97 import weight 0.00
Epoch 475 Iter 7 subLoss 6737.8 multi -4.97 import weight 0.00
Epoch 475 Iter 8 subLoss 7228.7 multi 6.97 import weight 0.00
Epoch 475 Iter 9 subLoss 5585.4 multi -4.97 import weight 0.00
Epoch 475 Iter 10 subLoss 6486.9 multi 1.00 import weight 0.00
Epoch 475 Iter 11 subLoss 6292.0 multi -13.93 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 475 Acc: 87.97 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -13.93 Pidx 629 train Loss: 9025.7 test Loss: 1454.6
Epoch 476 Iter 0 subLoss 9680.1 multi 9.96 import weight 0.00
Epoch 476 Iter 1 subLoss 6828.2 multi 1.00 import weight 0.00
Epoch 476 Iter 2 subLoss 6993.3 multi 1.00 import weight 0.00
Epoch 476 Iter 3 subLoss 6537.8 multi 3.98 import weight 0.00
Epoch 476 Iter 4 subLoss 5806.1 multi -1.99 import weight 0.00
Epoch 476 Iter 5 subLoss 6423.8 multi -4.97 import weight 0.00
Epoch 476 Iter 6 subLoss 7239.2 multi 1.00 import weight 0.00
Epoch 476 Iter 7 subLoss 6645.2 multi -10.94 import weight 0.00
Epoch 476 Iter 8 subLoss 8723.1 multi 1.00 import weight 0.00
Epoch 476 Iter 9 subLoss 8758.9 multi -7.96 import weight 0.00
Epoch 476 Iter 10 subLoss 10580.9 multi 3.99 import weight 0.00
Epoch 476 Iter 11 subLoss 9905.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 476 Acc: 82.82 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 990 train Loss: 10649.5 test Loss: 1930.6
Epoch 477 Iter 0 subLoss 9825.4 multi -7.96 import weight 0.00
Epoch 477 Iter 1 subLoss 12997.0 multi 1.00 import weight 0.00
Epoch 477 Iter 2 subLoss 12835.9 multi 1.00 import weight 0.00
Epoch 477 Iter 3 subLoss 11204.6 multi 1.00 import weight 0.00
Epoch 477 Iter 4 subLoss 11160.8 multi 6.97 import weight 0.00
Epoch 477 Iter 5 subLoss 9528.9 multi -4.97 import weight 0.00
Epoch 477 Iter 6 subLoss 9620.6 multi -1.98 import weight 0.00
Epoch 477 Iter 7 subLoss 10958.4 multi -7.96 import weight 0.00
Epoch 477 Iter 8 subLoss 12902.2 multi 3.99 import weight 0.00
Epoch 477 Iter 9 subLoss 11133.2 multi 1.00 import weight 0.00
Epoch 477 Iter 10 subLoss 11000.5 multi 3.99 import weight 0.00
Epoch 477 Iter 11 subLoss 9750.6 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 477 Acc: 84.34 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 975 train Loss: 9685.9 test Loss: 1739.6
Epoch 478 Iter 0 subLoss 8671.5 multi 1.00 import weight 0.00
Epoch 478 Iter 1 subLoss 9312.7 multi -1.98 import weight 0.00
Epoch 478 Iter 2 subLoss 9960.8 multi 3.99 import weight 0.00
Epoch 478 Iter 3 subLoss 9005.4 multi 1.00 import weight 0.00
Epoch 478 Iter 4 subLoss 9125.0 multi -1.99 import weight 0.00
Epoch 478 Iter 5 subLoss 10169.2 multi 3.99 import weight 0.00
Epoch 478 Iter 6 subLoss 9040.2 multi -1.99 import weight 0.00
Epoch 478 Iter 7 subLoss 8416.6 multi 1.00 import weight 0.00
Epoch 478 Iter 8 subLoss 8765.0 multi 3.99 import weight 0.00
Epoch 478 Iter 9 subLoss 8552.5 multi -4.97 import weight 0.00
Epoch 478 Iter 10 subLoss 8609.1 multi -1.99 import weight 0.00
Epoch 478 Iter 11 subLoss 9993.3 multi 3.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 478 Acc: 86.87 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.98 Pidx 999 train Loss: 8540.3 test Loss: 1492.4
Epoch 479 Iter 0 subLoss 8464.6 multi 3.99 import weight 0.00
Epoch 479 Iter 1 subLoss 7558.1 multi 3.98 import weight 0.00
Epoch 479 Iter 2 subLoss 7774.3 multi -4.97 import weight 0.00
Epoch 479 Iter 3 subLoss 8063.7 multi -1.99 import weight 0.00
Epoch 479 Iter 4 subLoss 7946.7 multi 6.97 import weight 0.00
Epoch 479 Iter 5 subLoss 7076.4 multi -4.97 import weight 0.00
Epoch 479 Iter 6 subLoss 7355.8 multi 6.97 import weight 0.00
Epoch 479 Iter 7 subLoss 7560.0 multi 6.97 import weight 0.00
Epoch 479 Iter 8 subLoss 6664.8 multi 6.97 import weight 0.00
Epoch 479 Iter 9 subLoss 5786.1 multi -1.98 import weight 0.00
Epoch 479 Iter 10 subLoss 6000.8 multi 3.99 import weight 0.00
Epoch 479 Iter 11 subLoss 5774.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 479 Acc: 94.49 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 577 train Loss: 5710.0 test Loss: 791.5
Epoch 480 Iter 0 subLoss 6115.7 multi 1.00 import weight 0.00
Epoch 480 Iter 1 subLoss 5246.9 multi -13.93 import weight 0.00
Epoch 480 Iter 2 subLoss 6885.0 multi -7.96 import weight 0.00
Epoch 480 Iter 3 subLoss 7982.7 multi -1.99 import weight 0.00
Epoch 480 Iter 4 subLoss 8144.1 multi -1.98 import weight 0.00
Epoch 480 Iter 5 subLoss 8721.7 multi 3.99 import weight 0.00
Epoch 480 Iter 6 subLoss 8057.1 multi 9.96 import weight 0.00
Epoch 480 Iter 7 subLoss 7058.7 multi 3.98 import weight 0.00
Epoch 480 Iter 8 subLoss 6425.1 multi -1.99 import weight 0.00
Epoch 480 Iter 9 subLoss 6434.8 multi 1.00 import weight 0.00
Epoch 480 Iter 10 subLoss 6945.6 multi 3.98 import weight 0.00
Epoch 480 Iter 11 subLoss 5244.6 multi -10.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 480 Acc: 91.32 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -10.94 Pidx 524 train Loss: 7139.9 test Loss: 1073.9
Epoch 481 Iter 0 subLoss 6975.4 multi -1.99 import weight 0.00
Epoch 481 Iter 1 subLoss 7283.7 multi 6.97 import weight 0.00
Epoch 481 Iter 2 subLoss 6536.4 multi 6.97 import weight 0.00
Epoch 481 Iter 3 subLoss 5626.0 multi 9.96 import weight 0.00
Epoch 481 Iter 4 subLoss 5489.7 multi 1.00 import weight 0.00
Epoch 481 Iter 5 subLoss 4958.3 multi -4.97 import weight 0.00
Epoch 481 Iter 6 subLoss 5637.8 multi -1.99 import weight 0.00
Epoch 481 Iter 7 subLoss 5747.3 multi -1.98 import weight 0.00
Epoch 481 Iter 8 subLoss 5875.6 multi -1.99 import weight 0.00
Epoch 481 Iter 9 subLoss 6298.5 multi -10.94 import weight 0.00
Epoch 481 Iter 10 subLoss 7047.7 multi -1.98 import weight 0.00
Epoch 481 Iter 11 subLoss 7490.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 481 Acc: 91.19 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 749 train Loss: 7302.4 test Loss: 1093.3
Epoch 482 Iter 0 subLoss 6867.7 multi -1.99 import weight 0.00
Epoch 482 Iter 1 subLoss 7434.1 multi -7.96 import weight 0.00
Epoch 482 Iter 2 subLoss 8273.3 multi 3.99 import weight 0.00
Epoch 482 Iter 3 subLoss 7878.0 multi 3.99 import weight 0.00
Epoch 482 Iter 4 subLoss 7213.6 multi -7.96 import weight 0.00
Epoch 482 Iter 5 subLoss 8064.6 multi -1.99 import weight 0.00
Epoch 482 Iter 6 subLoss 8485.8 multi -1.98 import weight 0.00
Epoch 482 Iter 7 subLoss 9270.7 multi -4.97 import weight 0.00
Epoch 482 Iter 8 subLoss 11349.8 multi -1.99 import weight 0.00
Epoch 482 Iter 9 subLoss 16630.5 multi 1.00 import weight 0.00
Epoch 482 Iter 10 subLoss 10891.4 multi 1.00 import weight 0.00
Epoch 482 Iter 11 subLoss 11351.0 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 482 Acc: 83.42 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 1135 train Loss: 12931.6 test Loss: 2180.1
Epoch 483 Iter 0 subLoss 13215.8 multi 3.98 import weight 0.00
Epoch 483 Iter 1 subLoss 8943.6 multi -7.96 import weight 0.00
Epoch 483 Iter 2 subLoss 9686.0 multi 12.94 import weight 0.00
Epoch 483 Iter 3 subLoss 7604.6 multi -7.96 import weight 0.00
Epoch 483 Iter 4 subLoss 9059.7 multi 1.00 import weight 0.00
Epoch 483 Iter 5 subLoss 7940.7 multi 9.96 import weight 0.00
Epoch 483 Iter 6 subLoss 7008.9 multi 6.97 import weight 0.00
Epoch 483 Iter 7 subLoss 6780.3 multi -4.97 import weight 0.00
Epoch 483 Iter 8 subLoss 8069.2 multi 1.00 import weight 0.00
Epoch 483 Iter 9 subLoss 8044.6 multi -4.97 import weight 0.00
Epoch 483 Iter 10 subLoss 7765.9 multi 9.96 import weight 0.00
Epoch 483 Iter 11 subLoss 6871.7 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 483 Acc: 92.47 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 687 train Loss: 6750.1 test Loss: 927.3
Epoch 484 Iter 0 subLoss 6231.1 multi 3.99 import weight 0.00
Epoch 484 Iter 1 subLoss 6231.1 multi 6.97 import weight 0.00
Epoch 484 Iter 2 subLoss 5800.0 multi 3.99 import weight 0.00
Epoch 484 Iter 3 subLoss 5726.9 multi -1.99 import weight 0.00
Epoch 484 Iter 4 subLoss 6294.1 multi -7.96 import weight 0.00
Epoch 484 Iter 5 subLoss 6668.3 multi 9.96 import weight 0.00
Epoch 484 Iter 6 subLoss 5571.0 multi 9.96 import weight 0.00
Epoch 484 Iter 7 subLoss 4976.9 multi 3.99 import weight 0.00
Epoch 484 Iter 8 subLoss 5088.2 multi 12.94 import weight 0.00
Epoch 484 Iter 9 subLoss 4377.1 multi 1.00 import weight 0.00
Epoch 484 Iter 10 subLoss 4024.2 multi 9.96 import weight 0.00
Epoch 484 Iter 11 subLoss 3636.7 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 484 Acc: 97.47 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 363 train Loss: 3551.6 test Loss: 412.3
Epoch 485 Iter 0 subLoss 3456.1 multi -49.75 import weight 0.00
Epoch 485 Iter 1 subLoss 6398.9 multi -7.96 import weight 0.00
Epoch 485 Iter 2 subLoss 8448.2 multi -1.98 import weight 0.00
Epoch 485 Iter 3 subLoss 10022.0 multi -7.96 import weight 0.00
Epoch 485 Iter 4 subLoss 37471.5 multi 1.00 import weight 0.00
Epoch 485 Iter 5 subLoss 20824.1 multi 1.00 import weight 0.00
Epoch 485 Iter 6 subLoss 16744.0 multi 1.00 import weight 0.00
Epoch 485 Iter 7 subLoss 13691.8 multi 1.00 import weight 0.00
Epoch 485 Iter 8 subLoss 11310.6 multi 1.00 import weight 0.00
Epoch 485 Iter 9 subLoss 9705.4 multi -4.97 import weight 0.00
Epoch 485 Iter 10 subLoss 16035.3 multi -1.99 import weight 0.00
Epoch 485 Iter 11 subLoss 23138.2 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 485 Acc: 73.40 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 2313 train Loss: 18602.5 test Loss: 4060.7
Epoch 486 Iter 0 subLoss 18280.4 multi 1.00 import weight 0.00
Epoch 486 Iter 1 subLoss 14930.9 multi 6.97 import weight 0.00
Epoch 486 Iter 2 subLoss 7005.6 multi 9.96 import weight 0.00
Epoch 486 Iter 3 subLoss 6000.6 multi 6.97 import weight 0.00
Epoch 486 Iter 4 subLoss 5885.7 multi -4.97 import weight 0.00
Epoch 486 Iter 5 subLoss 6363.1 multi -7.96 import weight 0.00
Epoch 486 Iter 6 subLoss 6935.1 multi -4.97 import weight 0.00
Epoch 486 Iter 7 subLoss 7181.4 multi -4.97 import weight 0.00
Epoch 486 Iter 8 subLoss 7990.4 multi -1.99 import weight 0.00
Epoch 486 Iter 9 subLoss 8059.9 multi 9.96 import weight 0.00
Epoch 486 Iter 10 subLoss 7025.1 multi 3.98 import weight 0.00
Epoch 486 Iter 11 subLoss 6764.0 multi 3.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 486 Acc: 93.58 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 3.99 Pidx 676 train Loss: 6592.3 test Loss: 844.7
Epoch 487 Iter 0 subLoss 6078.3 multi -1.98 import weight 0.00
Epoch 487 Iter 1 subLoss 6256.1 multi -1.99 import weight 0.00
Epoch 487 Iter 2 subLoss 6825.6 multi 3.99 import weight 0.00
Epoch 487 Iter 3 subLoss 6284.4 multi 3.98 import weight 0.00
Epoch 487 Iter 4 subLoss 7055.8 multi 3.99 import weight 0.00
Epoch 487 Iter 5 subLoss 6113.0 multi 3.99 import weight 0.00
Epoch 487 Iter 6 subLoss 5086.3 multi 15.93 import weight 0.00
Epoch 487 Iter 7 subLoss 4891.9 multi 3.98 import weight 0.00
Epoch 487 Iter 8 subLoss 4430.0 multi 1.00 import weight 0.00
Epoch 487 Iter 9 subLoss 5048.9 multi 15.93 import weight 0.00
Epoch 487 Iter 10 subLoss 3872.4 multi 12.94 import weight 0.00
Epoch 487 Iter 11 subLoss 3417.4 multi -7.96 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 487 Acc: 97.43 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -7.96 Pidx 341 train Loss: 3787.9 test Loss: 448.5
Epoch 488 Iter 0 subLoss 3819.1 multi 9.96 import weight 0.00
Epoch 488 Iter 1 subLoss 3072.9 multi -4.97 import weight 0.00
Epoch 488 Iter 2 subLoss 3175.1 multi 21.90 import weight 0.00
Epoch 488 Iter 3 subLoss 2861.3 multi -7.96 import weight 0.00
Epoch 488 Iter 4 subLoss 2798.1 multi -1.99 import weight 0.00
Epoch 488 Iter 5 subLoss 3170.2 multi 24.88 import weight 0.00
Epoch 488 Iter 6 subLoss 3177.1 multi 27.87 import weight 0.00
Epoch 488 Iter 7 subLoss 2627.8 multi -22.88 import weight 0.00
Epoch 488 Iter 8 subLoss 5596.0 multi -7.96 import weight 0.00
Epoch 488 Iter 9 subLoss 102565.2 multi 1.00 import weight 0.00
Epoch 488 Iter 10 subLoss 4665.7 multi 6.97 import weight 0.00
Epoch 488 Iter 11 subLoss 3079.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 488 Acc: 97.80 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 307 train Loss: 3294.3 test Loss: 400.7
Epoch 489 Iter 0 subLoss 3073.9 multi 1.00 import weight 0.00
Epoch 489 Iter 1 subLoss 3364.0 multi 9.96 import weight 0.00
Epoch 489 Iter 2 subLoss 2992.5 multi 3.99 import weight 0.00
Epoch 489 Iter 3 subLoss 2575.7 multi -10.94 import weight 0.00
Epoch 489 Iter 4 subLoss 2818.0 multi 3.98 import weight 0.00
Epoch 489 Iter 5 subLoss 2994.0 multi 6.97 import weight 0.00
Epoch 489 Iter 6 subLoss 2913.4 multi 15.93 import weight 0.00
Epoch 489 Iter 7 subLoss 2043.4 multi 12.94 import weight 0.00
Epoch 489 Iter 8 subLoss 2459.3 multi 24.88 import weight 0.00
Epoch 489 Iter 9 subLoss 1986.9 multi -7.96 import weight 0.00
Epoch 489 Iter 10 subLoss 2598.2 multi 1.00 import weight 0.00
Epoch 489 Iter 11 subLoss 2845.0 multi -19.90 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 489 Acc: 97.70 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -19.90 Pidx 284 train Loss: 3223.7 test Loss: 404.2
Epoch 490 Iter 0 subLoss 3331.3 multi -13.93 import weight 0.00
Epoch 490 Iter 1 subLoss 7030.4 multi -1.99 import weight 0.00
Epoch 490 Iter 2 subLoss 11017.9 multi -4.97 import weight 0.00
Epoch 490 Iter 3 subLoss 141333.3 multi 1.00 import weight 0.00
Epoch 490 Iter 4 subLoss 9659.8 multi 1.00 import weight 0.00
Epoch 490 Iter 5 subLoss 6697.7 multi 6.97 import weight 0.00
Epoch 490 Iter 6 subLoss 3441.1 multi 54.73 import weight 0.00
Epoch 490 Iter 7 subLoss 3729.6 multi -13.93 import weight 0.00
Epoch 490 Iter 8 subLoss 35670.4 multi -1.99 import weight 0.00
Epoch 490 Iter 9 subLoss 533536.9 multi 1.00 import weight 0.00
Epoch 490 Iter 10 subLoss 13447.3 multi 1.00 import weight 0.00
Epoch 490 Iter 11 subLoss 11692.5 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 490 Acc: 95.02 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 1169 train Loss: 10807.4 test Loss: 1194.0
Epoch 491 Iter 0 subLoss 9751.4 multi 6.97 import weight 0.00
Epoch 491 Iter 1 subLoss 6495.8 multi 9.96 import weight 0.00
Epoch 491 Iter 2 subLoss 4168.4 multi 18.91 import weight 0.00
Epoch 491 Iter 3 subLoss 3532.2 multi 3.99 import weight 0.00
Epoch 491 Iter 4 subLoss 3465.5 multi -4.97 import weight 0.00
Epoch 491 Iter 5 subLoss 3340.3 multi 6.97 import weight 0.00
Epoch 491 Iter 6 subLoss 3109.2 multi -1.98 import weight 0.00
Epoch 491 Iter 7 subLoss 3738.2 multi 9.96 import weight 0.00
Epoch 491 Iter 8 subLoss 3291.9 multi -19.90 import weight 0.00
Epoch 491 Iter 9 subLoss 3357.4 multi -1.99 import weight 0.00
Epoch 491 Iter 10 subLoss 3256.7 multi 3.99 import weight 0.00
Epoch 491 Iter 11 subLoss 3887.2 multi -1.99 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 491 Acc: 97.92 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.99 Pidx 388 train Loss: 3758.3 test Loss: 332.0
Epoch 492 Iter 0 subLoss 4153.5 multi 1.00 import weight 0.00
Epoch 492 Iter 1 subLoss 3034.5 multi 1.00 import weight 0.00
Epoch 492 Iter 2 subLoss 3363.2 multi 9.96 import weight 0.00
Epoch 492 Iter 3 subLoss 2932.3 multi -13.93 import weight 0.00
Epoch 492 Iter 4 subLoss 4157.6 multi 3.98 import weight 0.00
Epoch 492 Iter 5 subLoss 3130.8 multi -19.90 import weight 0.00
Epoch 492 Iter 6 subLoss 4141.3 multi 15.93 import weight 0.00
Epoch 492 Iter 7 subLoss 3566.2 multi 9.96 import weight 0.00
Epoch 492 Iter 8 subLoss 3175.6 multi 30.85 import weight 0.00
Epoch 492 Iter 9 subLoss 2892.7 multi 18.91 import weight 0.00
Epoch 492 Iter 10 subLoss 3252.1 multi 6.97 import weight 0.00
Epoch 492 Iter 11 subLoss 2783.7 multi -1.98 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 492 Acc: 98.02 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -1.98 Pidx 278 train Loss: 2725.2 test Loss: 320.2
Epoch 493 Iter 0 subLoss 2376.2 multi -4.97 import weight 0.00
Epoch 493 Iter 1 subLoss 2969.3 multi -19.90 import weight 0.00
Epoch 493 Iter 2 subLoss 3123.0 multi 9.96 import weight 0.00
Epoch 493 Iter 3 subLoss 2817.2 multi 6.97 import weight 0.00
Epoch 493 Iter 4 subLoss 2709.6 multi 21.90 import weight 0.00
Epoch 493 Iter 5 subLoss 2809.6 multi -16.91 import weight 0.00
Epoch 493 Iter 6 subLoss 2954.2 multi 3.99 import weight 0.00
Epoch 493 Iter 7 subLoss 2882.7 multi 6.97 import weight 0.00
Epoch 493 Iter 8 subLoss 2803.7 multi -13.93 import weight 0.00
Epoch 493 Iter 9 subLoss 2614.8 multi 9.96 import weight 0.00
Epoch 493 Iter 10 subLoss 2312.8 multi 15.93 import weight 0.00
Epoch 493 Iter 11 subLoss 2924.5 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 493 Acc: 98.42 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 292 train Loss: 2668.2 test Loss: 280.5
Epoch 494 Iter 0 subLoss 2391.7 multi 39.81 import weight 1.00
Epoch 494 Iter 1 subLoss 2684.7 multi 6.97 import weight 0.00
Epoch 494 Iter 2 subLoss 2658.3 multi 15.93 import weight 0.00
Epoch 494 Iter 3 subLoss 2526.0 multi -4.97 import weight 0.00
Epoch 494 Iter 4 subLoss 2438.8 multi -4.97 import weight 0.00
Epoch 494 Iter 5 subLoss 2466.0 multi -37.81 import weight 0.00
Epoch 494 Iter 6 subLoss 6093.3 multi -7.96 import weight 0.00
Epoch 494 Iter 7 subLoss 30685.8 multi 3.99 import weight 0.00
Epoch 494 Iter 8 subLoss 5968.5 multi -4.97 import weight 0.00
Epoch 494 Iter 9 subLoss 11420.3 multi -1.99 import weight 0.00
Epoch 494 Iter 10 subLoss 22568.5 multi 1.00 import weight 0.00
Epoch 494 Iter 11 subLoss 10162.0 multi 6.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 494 Acc: 96.96 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 6.97 Pidx 1016 train Loss: 4096.0 test Loss: 641.8
Epoch 495 Iter 0 subLoss 4319.4 multi 3.99 import weight 0.00
Epoch 495 Iter 1 subLoss 3218.7 multi 9.96 import weight 0.00
Epoch 495 Iter 2 subLoss 3512.7 multi -10.94 import weight 0.00
Epoch 495 Iter 3 subLoss 3621.9 multi 9.96 import weight 0.00
Epoch 495 Iter 4 subLoss 3360.1 multi 12.94 import weight 0.00
Epoch 495 Iter 5 subLoss 2408.9 multi -40.79 import weight 0.00
Epoch 495 Iter 6 subLoss 3303.8 multi 33.84 import weight 0.00
Epoch 495 Iter 7 subLoss 3185.6 multi -13.93 import weight 0.00
Epoch 495 Iter 8 subLoss 5361.6 multi 1.00 import weight 0.00
Epoch 495 Iter 9 subLoss 5793.4 multi 6.97 import weight 0.00
Epoch 495 Iter 10 subLoss 2396.4 multi 42.79 import weight 1.00
Epoch 495 Iter 11 subLoss 2660.2 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 495 Acc: 98.66 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 266 train Loss: 2464.6 test Loss: 241.7
Epoch 496 Iter 0 subLoss 2466.4 multi -34.82 import weight 0.00
Epoch 496 Iter 1 subLoss 3468.4 multi -1.99 import weight 0.00
Epoch 496 Iter 2 subLoss 3692.4 multi 1.00 import weight 0.00
Epoch 496 Iter 3 subLoss 3875.1 multi 15.93 import weight 0.00
Epoch 496 Iter 4 subLoss 2790.5 multi -1.98 import weight 0.00
Epoch 496 Iter 5 subLoss 2716.8 multi 1.00 import weight 0.00
Epoch 496 Iter 6 subLoss 2852.6 multi 27.87 import weight 0.00
Epoch 496 Iter 7 subLoss 2756.8 multi -34.82 import weight 0.00
Epoch 496 Iter 8 subLoss 3879.5 multi 18.91 import weight 0.00
Epoch 496 Iter 9 subLoss 2849.0 multi -16.91 import weight 0.00
Epoch 496 Iter 10 subLoss 11106.2 multi -4.97 import weight 0.00
Epoch 496 Iter 11 subLoss 128041.6 multi 1.00 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 496 Acc: 76.01 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 1.00 Pidx 12804 train Loss: 30566.4 test Loss: 5623.5
Epoch 497 Iter 0 subLoss 30475.8 multi 1.00 import weight 0.00
Epoch 497 Iter 1 subLoss 21207.8 multi 1.00 import weight 0.00
Epoch 497 Iter 2 subLoss 16107.7 multi 1.00 import weight 0.00
Epoch 497 Iter 3 subLoss 11822.0 multi 3.98 import weight 0.00
Epoch 497 Iter 4 subLoss 4545.9 multi 18.91 import weight 0.00
Epoch 497 Iter 5 subLoss 2373.1 multi -1.99 import weight 0.00
Epoch 497 Iter 6 subLoss 2907.1 multi -25.87 import weight 0.00
Epoch 497 Iter 7 subLoss 6749.5 multi -1.99 import weight 0.00
Epoch 497 Iter 8 subLoss 9787.0 multi 1.00 import weight 0.00
Epoch 497 Iter 9 subLoss 7586.5 multi 1.00 import weight 0.00
Epoch 497 Iter 10 subLoss 6452.9 multi 3.99 import weight 0.00
Epoch 497 Iter 11 subLoss 4110.5 multi -16.91 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 497 Acc: 91.52 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -16.91 Pidx 411 train Loss: 8731.8 test Loss: 1546.0
Epoch 498 Iter 0 subLoss 8115.8 multi -4.97 import weight 0.00
Epoch 498 Iter 1 subLoss 26138.3 multi 1.00 import weight 0.00
Epoch 498 Iter 2 subLoss 11649.9 multi 1.00 import weight 0.00
Epoch 498 Iter 3 subLoss 10233.8 multi 1.00 import weight 0.00
Epoch 498 Iter 4 subLoss 6290.6 multi -7.96 import weight 0.00
Epoch 498 Iter 5 subLoss 23860.8 multi 3.99 import weight 0.00
Epoch 498 Iter 6 subLoss 4139.9 multi -19.90 import weight 0.00
Epoch 498 Iter 7 subLoss 10273.0 multi 1.00 import weight 0.00
Epoch 498 Iter 8 subLoss 8318.2 multi -1.98 import weight 0.00
Epoch 498 Iter 9 subLoss 10898.2 multi 3.99 import weight 0.00
Epoch 498 Iter 10 subLoss 6256.8 multi 1.00 import weight 0.00
Epoch 498 Iter 11 subLoss 6391.1 multi -4.97 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 498 Acc: 95.66 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul -4.97 Pidx 639 train Loss: 6965.7 test Loss: 1031.3
Epoch 499 Iter 0 subLoss 7035.5 multi 1.00 import weight 0.00
Epoch 499 Iter 1 subLoss 6363.2 multi -4.97 import weight 0.00
Epoch 499 Iter 2 subLoss 8847.9 multi 6.97 import weight 0.00
Epoch 499 Iter 3 subLoss 6007.6 multi 9.96 import weight 0.00
Epoch 499 Iter 4 subLoss 4624.1 multi 3.99 import weight 0.00
Epoch 499 Iter 5 subLoss 4326.2 multi 6.97 import weight 0.00
Epoch 499 Iter 6 subLoss 4000.2 multi -4.97 import weight 0.00
Epoch 499 Iter 7 subLoss 4329.7 multi 9.96 import weight 0.00
Epoch 499 Iter 8 subLoss 4054.6 multi 9.96 import weight 0.00
Epoch 499 Iter 9 subLoss 2836.5 multi 15.93 import weight 0.00
Epoch 499 Iter 10 subLoss 2601.9 multi -10.94 import weight 0.00
Epoch 499 Iter 11 subLoss 3219.5 multi 12.94 import weight 0.00

======================================================================================================================================================
vanilla scaling 1 Seen / Unseen / ECE 0.0157 / 0.11015 / 29.21
Entropy seen (from low to high)
[1467, 464, 317, 242, 167, 136, 102, 101, 68, 52, 60, 46, 51, 83, 76, 81, 124, 169, 529, 166, 106, 85, 62, 59, 43, 43, 24, 26, 21, 30, 33, 25, 31, 17, 7, 6, 4, 6, 2, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0]
Entropy unseen (from high to low)
[0, 0, 0, 0, 0, 0, 1, 1, 4, 20, 51, 92, 115, 142, 147, 204, 250, 240, 192, 189, 220, 232, 205, 169, 167, 180, 157, 151, 134, 119, 98, 86, 79, 78, 55, 57, 68, 50, 55, 61, 47, 57, 93, 78, 86, 81, 107, 116, 93, 34]
Calibration acc (top: base v.s. mid: proposed v.s. bottom: counts)
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.4, 34.0, 36.8, 40.1, 43.8, 47.2, 50.8, 54.5, 57.6, 61.5, 65.2, 67.7]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.4, 61.1, 59.9, 69.5, 58.3, 75.6, 69.2, 75.5, 91.0, 97.4, 99.5]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 18, 20, 23, 24, 37, 39, 49, 78, 159, 611]
Epoch 499 Acc: 98.31 BMA: 98.17 lr: 2.00E-06 T: 1.00E-01  Weight 0.000 Grad mul 12.94 Pidx 321 train Loss: 2964.5 test Loss: 302.2
Sampling Time used: 9483.1
Grad mul
